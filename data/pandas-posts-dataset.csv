;Unnamed: 0;AcceptedAnswerId;AnswerCount;Body;ClosedDate;CommentCount;CommunityOwnedDate;CreationDate;FavoriteCount;Id;LastActivityDate;LastEditDate;LastEditorDisplayName;LastEditorUserId;OwnerDisplayName;OwnerUserId;ParentId;PostTypeId;Score;Tags;Title;ViewCount
27;27;7779260.0;2.0;"<p>When I try to merge two dataframes by rows doing:</p>

<pre><code>bigdata = data1.append(data2)
</code></pre>

<p>I get the following error:</p>

<blockquote>
<pre><code>Exception: Index cannot contain duplicate values!
</code></pre>
</blockquote>

<p>The index of the first data frame starts from 0 to 38 and the second one from 0 to 48. I didn't understand that I have to modify the index of one of the data frame before merging, but I don't know how to.</p>

<p>Thank you.</p>

<p>These are the two dataframes:</p>

<p><code>data1</code>:</p>

<pre><code>    meta  particle  ratio   area    type    
0   2     part10    1.348   0.8365  touching
1   2     part18    1.558   0.8244  single  
2   2     part2     1.893   0.894   single  
3   2     part37    0.6695  1.005   single  
....clip...
36  2     part23    1.051   0.8781  single  
37  2     part3     80.54   0.9714  nuclei  
38  2     part34    1.071   0.9337  single  
</code></pre>

<p><code>data2</code>:</p>

<pre><code>    meta  particle  ratio    area    type    
0   3     part10    0.4756   1.025   single  
1   3     part18    0.04387  1.232   dusts   
2   3     part2     1.132    0.8927  single  
...clip...
46  3     part46    13.71    1.001   nuclei  
47  3     part3     0.7439   0.9038  single  
48  3     part34    0.4349   0.9956  single 
</code></pre>

<p>the first column is the index</p>
";;7;;2011-10-15T08:21:17.460;4.0;7776679;2017-01-04T20:22:22.867;2017-01-04T20:22:22.867;;2336654.0;;601314.0;;1;25;<python><pandas>;append two data frame with pandas;25479.0
31;31;11617194.0;7.0;"<p>I want to perform my own complex operations on financial data in dataframes in a sequential manner.</p>

<p>For example I am using the following MSFT CSV file taken from <a href=""http://finance.yahoo.com/q/hp?s=MSFT"">Yahoo Finance</a>:</p>

<pre><code>Date,Open,High,Low,Close,Volume,Adj Close
2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13
2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31
2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98
2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27

....
</code></pre>

<p>I then do the following:</p>

<pre><code>#!/usr/bin/env python
from pandas import *

df = read_csv('table.csv')

for i, row in enumerate(df.values):
    date = df.index[i]
    open, high, low, close, adjclose = row
    #now perform analysis on open/close based on date, etc..
</code></pre>

<p>Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? <code>df.iteritems</code> unfortunately only iterates column by column.</p>
";;3;;2011-10-20T14:46:14.633;108.0;7837722;2017-01-04T20:24:46.293;2017-01-04T20:24:46.293;;2336654.0;;1005409.0;;1;187;<python><performance><for-loop><pandas>;What is the most efficient way to loop through dataframes with pandas?;195364.0
49;49;8916746.0;2.0;"<p>I have a dataframe <code>df</code> in pandas that was built using <code>pandas.read_table</code> from a csv file. The dataframe has several columns and it is indexed by one of the columns (which is unique, in that each row has a unique value for that column used for indexing.) </p>

<p>How can I select rows of my dataframe based on a ""complex"" filter applied to multiple columns? I can easily select out the slice of the dataframe where column <code>colA</code> is greater than 10 for example:</p>

<pre><code>df_greater_than10 = df[df[""colA""] &gt; 10]
</code></pre>

<p>But what if I wanted a filter like: select the slice of <code>df</code> where <em>any</em> of the columns are greater than 10? </p>

<p>Or where the value for <code>colA</code> is greater than 10 but the value for <code>colB</code> is less than 5?</p>

<p>How are these implemented in pandas?
Thanks.</p>
";;0;;2012-01-18T19:41:27.017;12.0;8916302;2017-01-04T20:37:23.907;2017-01-04T20:37:23.907;;2336654.0;;248237.0;;1;26;<python><csv><numpy><tab-delimited><pandas>;selecting across multiple columns with python pandas?;19805.0
54;54;8997908.0;3.0;"<p>I recently came across the <a href=""http://pandas.sourceforge.net/"" rel=""noreferrer"">pandas</a> library for python, which according to <a href=""http://wesmckinney.com/blog/some-pandas-database-join-merge-benchmarks-vs-r-basemerge/"" rel=""noreferrer"">this benchmark</a> performs very fast in-memory merges.  It's even faster than the <a href=""http://cran.r-project.org/web/packages/data.table/index.html"" rel=""noreferrer"">data.table</a> package in R (my language of choice for analysis).</p>

<p>Why is <code>pandas</code> so much faster than <code>data.table</code>?  Is it because of an inherent speed advantage python has over R, or is there some tradeoff I'm not aware of?  Is there a way to perform inner and outer joins in <code>data.table</code> without resorting to <code>merge(X, Y, all=FALSE)</code> and <code>merge(X, Y, all=TRUE)</code>?</p>

<p><img src=""https://i.stack.imgur.com/0pCvh.png"" alt=""Comparison""></p>

<p>Here's the <a href=""https://github.com/wesm/pandas/blob/master/bench/bench_merge.R"" rel=""noreferrer"">R code</a> and the <a href=""https://github.com/wesm/pandas/blob/master/bench/bench_merge.py"" rel=""noreferrer"">Python code</a> used to benchmark the various packages.</p>
";;16;;2012-01-24T17:59:53.850;58.0;8991709;2017-04-19T22:55:26.137;2016-08-23T08:11:19.453;;1129194.0;;345660.0;;1;134;<python><r><join><data.table><pandas>;Why are pandas merges in python faster than data.table merges in R?;16173.0
73;73;9557319.0;8.0;"<p>I used Enthought's python distribution as a graduate student for data analysis and really enjoyed it.  But I've recently taken a job which takes away my ability to use it.  </p>

<p>I prefer Python for initial scoping and cleaning the data, and R for the stats side.  Part of the impetus for wanting this though, is trying out pandas.  And other other part is I don't have proper license (or the means to pay), which is clearly a problem.</p>

<p>So is there some other well put together easy to install Python distribution that I can get <code>numpy</code>, <code>scipy</code>, <code>sci-kits</code>, and all the other goodness?</p>
";2013-12-04T20:55:59.357;5;;2012-03-04T14:25:36.287;6.0;9555635;2017-01-04T20:49:56.930;2017-01-04T20:49:56.930;;2336654.0;;402468.0;;1;19;<python><numpy><scipy><enthought><pandas>;Open source Enthought Python alternative;6585.0
80;80;9620832.0;2.0;"<p>I stumbled across <a href=""http://pandas.pydata.org/"" rel=""noreferrer"">pandas</a> and it looks ideal for simple calculations that I'd like to do. I have a SAS background and was thinking it'd replace proc freq -- it looks like it'll scale to what I may want to do in the future. However, I just can't seem to get my head around a simple task (I'm not sure if I'm supposed to look at <code>pivot/crosstab/indexing</code> - whether I should have a <code>Panel</code> or <code>DataFrames</code> etc...). Could someone give me some pointers on how to do the following:</p>

<p>I have two CSV files (one for year 2010, one for year 2011 - simple transactional data) - The columns are category and amount</p>

<p>2010:</p>

<pre><code>AB,100.00
AB,200.00
AC,150.00
AD,500.00
</code></pre>

<p>2011:</p>

<pre><code>AB,500.00
AC,250.00
AX,900.00
</code></pre>

<p>These are loaded into separate DataFrame objects.</p>

<p>What I'd like to do is get the category, the sum of the category, and the frequency of the category, eg:</p>

<p>2010:</p>

<pre><code>AB,300.00,2
AC,150.00,1
AD,500.00,1
</code></pre>

<p>2011:</p>

<pre><code>AB,500.00,1
AC,250.00,1
AX,900.00,1
</code></pre>

<p>I can't work out whether I should be using <code>pivot/crosstab/groupby/an index</code>
etc... I can get either the sum or the frequency - I can't seem to get both... It gets a bit more complex because I would like to do it on a month by month basis, but I think if someone would be so kind to point me to the right technique/direction I'll be able to go from there.</p>
";;2;;2012-03-06T17:01:47.107;5.0;9588331;2014-06-25T21:39:42.003;2014-01-27T12:03:36.990;;100297.0;;1252759.0;;1;21;<python><pandas>;Simple cross-tabulation in pandas;9493.0
91;91;9652858.0;3.0;"<p>I'm new to python and pandas.  I'm trying to get a <code>tsv</code> file loaded into a pandas <code>DataFrame</code>.  </p>

<p>This is what I'm trying and the error I'm getting:</p>

<pre><code>&gt;&gt;&gt; df1 = DataFrame(csv.reader(open('c:/~/trainSetRel3.txt'), delimiter='\t'))

Traceback (most recent call last):
  File ""&lt;pyshell#28&gt;"", line 1, in &lt;module&gt;
    df1 = DataFrame(csv.reader(open('c:/~/trainSetRel3.txt'), delimiter='\t'))
  File ""C:\Python27\lib\site-packages\pandas\core\frame.py"", line 318, in __init__
    raise PandasError('DataFrame constructor not properly called!')
PandasError: DataFrame constructor not properly called!
</code></pre>
";;0;;2012-03-11T06:00:56.347;3.0;9652832;2016-09-29T05:30:07.943;2014-07-31T15:54:55.830;;539153.0;;914308.0;;1;38;<python><pandas><tsv>;How to I load a tsv file into a Pandas DataFrame?;32040.0
103;103;9762084.0;6.0;"<p>I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a ""row"" of the dataframe.</p>

<p>My DataFrame looks something like:</p>

<pre><code>In [182]: data_set
Out[182]: 
  index data_date   data_1  data_2
0  14303 2012-02-17  24.75   25.03 
1  12009 2012-02-16  25.00   25.07 
2  11830 2012-02-15  24.99   25.15 
3  6274  2012-02-14  24.68   25.05 
4  2302  2012-02-13  24.62   24.77 
5  14085 2012-02-10  24.38   24.61 
</code></pre>

<p>I want to convert it to an array of tuples like:</p>

<pre><code>[(datetime.date(2012,2,17),24.75,25.03),
(datetime.date(2012,2,16),25.00,25.07),
...etc. ]
</code></pre>

<p>Any suggestion on how I can efficiently do this?</p>
";;0;;2012-03-18T12:53:06.683;13.0;9758450;2017-06-04T02:52:43.353;2017-01-04T21:49:07.303;;2336654.0;;939715.0;;1;34;<python><pandas>;Pandas convert dataframe to array of tuples;36433.0
107;107;9772031.0;2.0;"<p>I'm a beginning pandas user, and after studying the documentation I still can't find a straightforward way to do the following. </p>

<p>I have a DataFrame with a pandas.DateRange index, and I want to add a column with values for part of the same DateRange.  </p>

<p>Suppose I have </p>

<pre><code>df

                            A         B
2010-01-01 00:00:00  0.340717  0.702432
2010-01-01 01:00:00  0.649970  0.411799
2010-01-01 02:00:00  0.932367  0.108047
2010-01-01 03:00:00  0.051942  0.526318
2010-01-01 04:00:00  0.518301  0.057809
2010-01-01 05:00:00  0.779988  0.756221
2010-01-01 06:00:00  0.597444  0.312495
</code></pre>

<p>and </p>

<pre><code>df2

                     C
2010-01-01 03:00:00  5
2010-01-01 04:00:00  5
2010-01-01 05:00:00  5
</code></pre>

<p>How can I obtain something like this:</p>

<pre><code>                            A         B    C
2010-01-01 00:00:00  0.340717  0.702432    nan
2010-01-01 01:00:00  0.649970  0.411799    nan
2010-01-01 02:00:00  0.932367  0.108047    nan
2010-01-01 03:00:00  0.051942  0.526318    5
2010-01-01 04:00:00  0.518301  0.057809    5
2010-01-01 05:00:00  0.779988  0.756221    5
2010-01-01 06:00:00  0.597444  0.312495    nan
</code></pre>
";;0;;2012-03-18T22:34:26.183;2.0;9762935;2017-01-04T21:51:10.637;2017-01-04T21:51:10.637;;2336654.0;;566942.0;;1;11;<python><pandas>;Add indexed column to DataFrame with pandas;8189.0
109;109;9794891.0;2.0;"<p>I want to perform a join/merge/append operation on a dataframe with datetime index.</p>

<p>Let's say I have <code>df1</code> and I want to add <code>df2</code> to it.  <code>df2</code> can have fewer or more columns, and overlapping indexes.  For all rows where the indexes match, if <code>df2</code> has the same column as <code>df1</code>, I want the values of <code>df1</code> be overwritten with those from <code>df2</code>. </p>

<p>How can I obtain the desired result? </p>
";;0;;2012-03-20T13:36:09.263;6.0;9787853;2017-03-29T03:32:43.067;2012-03-20T15:48:45.950;;192839.0;;566942.0;;1;17;<python><pandas>;join or merge with overwrite in pandas;6054.0
117;117;;6.0;"<p>I have the following dataframe:</p>

<pre><code>   obj_id   data_date   value
0  4        2011-11-01  59500    
1  2        2011-10-01  35200 
2  4        2010-07-31  24860   
3  1        2009-07-28  15860
4  2        2008-10-15  200200
</code></pre>

<p>I want to get a subset of this data so that I only have the most recent (largest <code>'data_date'</code>) <code>'value'</code> for each <code>'obj_id'</code>. </p>

<p>I've hacked together a solution, but it feels dirty. I was wondering if anyone has a better way. I'm sure I must be missing some easy way to do it through pandas. </p>

<p>My method is essentially to group, sort, retrieve, and recombine as follows:</p>

<pre><code>row_arr = []
for grp, grp_df in df.groupby('obj_id'):
    row_arr.append(dfg.sort('data_date', ascending = False)[:1].values[0])

df_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))
</code></pre>
";;0;;2012-03-24T10:26:50.303;3.0;9850954;2017-08-13T23:58:06.307;2017-01-04T21:59:06.327;;2336654.0;;939715.0;;1;11;<python><pandas>;pandas - get most recent value of a particular column indexed by another column (get maximum value of a particular column indexed by another column);3543.0
147;147;11603242.0;4.0;"<p>I have a times series with temperature and radiation in a pandas <code>dataframe</code>. The time resolution is 1 minute in regular steps.</p>

<pre><code>import datetime
import pandas as pd
import numpy as np

date_times = pd.date_range(datetime.datetime(2012, 4, 5, 8, 0),
                           datetime.datetime(2012, 4, 5, 12, 0),
                           freq='1min')
tamb = np.random.sample(date_times.size) * 10.0
radiation = np.random.sample(date_times.size) * 10.0
frame = pd.DataFrame(data={'tamb': tamb, 'radiation': radiation},
                     index=date_times)
frame
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 241 entries, 2012-04-05 08:00:00 to 2012-04-05 12:00:00
Freq: T
Data columns:
radiation    241  non-null values
tamb         241  non-null values
dtypes: float64(2)
</code></pre>

<p>How can I down-sample this <code>dataframe</code> to a resolution of one hour, computing the hourly <strong>mean</strong> for the temperature and the hourly <strong>sum</strong> for radiation? </p>
";;2;;2012-04-04T23:17:23.453;15.0;10020591;2017-01-04T22:10:55.163;2017-01-04T22:10:55.163;;2336654.0;;1301710.0;;1;19;<python><numpy><time-series><pandas>;How to resample a dataframe with different functions applied to each column?;18532.0
160;160;;11.0;"<p>The documentation for Pandas has numerous examples of best practices for working with data stored in various formats.</p>

<p>However, I am unable to find any good examples for working with databases like MySQL for example.</p>

<p>Can anyone point me to links or give some code snippets of how to convert query results using <strong>mysql-python</strong> to data frames in Panda efficiently ?</p>
";;4;;2012-04-08T18:01:13.310;63.0;10065051;2017-01-31T04:20:27.857;2017-01-04T22:12:00.193;;2336654.0;;1320615.0;;1;77;<python><pandas>;python-pandas and databases like mysql;72432.0
161;161;10114652.0;1.0;"<p>I like to think I'm not an idiot, but maybe I'm wrong. Can anyone explain to me why this isn't working? I can achieve the desired results using 'merge'. But I eventually need to join multiple <code>pandas</code> <code>DataFrames</code> so I need to get this method working.</p>

<pre><code>In [2]: left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})

In [3]: right = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value2': [6.218, 0.001]})

In [4]: left.join(right, on='ST_NAME', lsuffix='_left', rsuffix='_right')
Out[4]: 
  ST_NAME_left  value ST_NAME_right  value2
0       Oregon  4.685           NaN     NaN
1     Nebraska  2.491           NaN     NaN
</code></pre>
";;0;;2012-04-11T21:40:38.143;;10114399;2017-01-04T22:12:28.257;2017-01-04T22:12:28.257;;2336654.0;;1327660.0;;1;12;<pandas>;Pandas: simple 'join' not working?;4120.0
175;175;10182172.0;2.0;"<p>I have the following Pandas Dataframe with a MultiIndex(Z,A):</p>

<pre><code>             H1       H2  
   Z    A 
0  100  200  0.3112   -0.4197   
1  100  201  0.2967   0.4893    
2  100  202  0.3084   -0.4873   
3  100  203  0.3069   NaN        
4  101  203  -0.4956  NaN       
</code></pre>

<p>Question: How can I select all items with A=203?
I tried <code>df[:,'A']</code>  but it doesn't work. Then I found <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#cross-section-with-hierarchical-index"" rel=""noreferrer"">this</a> in the online documentation so I tried:<br>
<code>df.xs(203,level='A')</code><br>
but I get:<br>
""<code>TypeError: xs() got an unexpected keyword argument 'level'</code>""<br>
Also I dont see this parameter in the installed doc(<code>df.xs?</code>):<br>
""Parameters ---------- key : object Some label contained in the index, or partially in a MultiIndex axis : int, default 0 Axis to retrieve cross-section on copy : boolean, default True Whether to make a copy of the data""<br>
Note:I have the development version.</p>

<p>Edit: I found <a href=""http://www.digipedia.pl/usenet/thread/16070/6393/"" rel=""noreferrer"">this thread</a>. They recommend something like:</p>

<pre><code>df.select(lambda x: x[1]==200, axis=0)  
</code></pre>

<p>I still would like to know what happened with df.xs with the level parameter or what is the recommended way in the current version.</p>
";;3;;2012-04-16T13:27:44.143;5.0;10175068;2014-12-13T00:02:21.893;2012-04-18T08:17:02.323;;1330293.0;;1330293.0;;1;15;<python><pandas>;Select data at a particular level from a MultiIndex;6774.0
183;183;10202789.0;4.0;"<p>How can I find the row for which the value of a specific column is <strong>maximal</strong>?</p>

<p><code>df.max()</code> will give me the maximal value for each column, I don't know how to get the corresponding row.</p>
";;2;;2012-04-18T03:59:55.663;24.0;10202570;2017-07-05T19:00:44.137;2017-01-04T22:22:36.050;;2336654.0;;7650.0;;1;84;<python><pandas>;Pandas DataFrame - Find row where values for column is maximal;75652.0
194;194;10374456.0;6.0;"<p>I'm starting with input data like this</p>

<pre><code>df1 = pandas.DataFrame( { 
    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , 
    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )
</code></pre>

<p>Which when printed appears as this:</p>

<pre><code>   City     Name
0   Seattle    Alice
1   Seattle      Bob
2  Portland  Mallory
3   Seattle  Mallory
4   Seattle      Bob
5  Portland  Mallory
</code></pre>

<p>Grouping is simple enough:</p>

<pre><code>g1 = df1.groupby( [ ""Name"", ""City""] ).count()
</code></pre>

<p>and printing yields a <code>GroupBy</code> object:</p>

<pre><code>                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
        Seattle      1     1
</code></pre>

<p>But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:</p>

<pre><code>                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
Mallory Seattle      1     1
</code></pre>

<p>I can't quite see how to accomplish this in the pandas documentation. Any hints would be welcome.</p>
";;0;;2012-04-29T16:10:35.413;78.0;10373660;2017-05-06T14:09:35.813;2017-05-06T14:09:35.813;;2901002.0;;468604.0;;1;173;<python><pandas><dataframe><group-by><multi-index>;Converting a Pandas GroupBy object to DataFrame;167415.0
196;196;10377863.0;2.0;"<p>This is a beginner <code>python</code> installation question.  This the first time I have tried to install and call a package.  I've got <code>pip</code> installed, and I tried to install two modules - <code>numpy</code> and <code>pandas</code>.  </p>

<p>In terminal, I ran the following commands:</p>

<pre><code>sudo pip install numpy

sudo pip install pandas
</code></pre>

<p>Both commands returned with a success message.  Here is the <code>pandas</code> success message (it's the second package I installed and was still in my terminal history):</p>

<pre><code>Successfully installed pandas
Cleaning up...
</code></pre>

<p><code>pip</code> returned a similar message after <code>numpy</code> was installed.  </p>

<p>Now, when I launch <code>python</code> and try to call it with:</p>

<pre><code>import pandas
</code></pre>

<p>I get this error message:</p>

<blockquote>
<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named pandas
</code></pre>
</blockquote>

<p>Same when I try <code>numpy</code>.  </p>

<p>Can anyone tell me what I'm doing incorrectly?  </p>
";;14;;2012-04-29T22:41:28.510;3.0;10376647;2017-01-04T22:26:29.753;2017-01-04T22:26:29.753;;2336654.0;;854739.0;;1;12;<python><numpy><pandas>;Installed Python Modules - Python can't find them;25583.0
206;206;10458386.0;2.0;"<p>I am trying to re-index a pandas <code>DataFrame</code> object, like so,</p>

<pre><code>From:
            a   b   c
        0   1   2   3
        1  10  11  12
        2  20  21  22

To :
           b   c
       1   2   3
      10  11  12
      20  21  22
</code></pre>

<p>I am going about this as shown below and am getting the wrong answer. Any clues on how to do this?</p>

<pre><code>&gt;&gt;&gt; col = ['a','b','c']
&gt;&gt;&gt; data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)
&gt;&gt;&gt; data
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22
&gt;&gt;&gt; idx2 = data.a.values
&gt;&gt;&gt; idx2
array([ 1, 10, 20], dtype=int64)
&gt;&gt;&gt; data2 = DataFrame(data,index=idx2,columns=col[1:])
&gt;&gt;&gt; data2
     b   c
1   11  12
10 NaN NaN
20 NaN NaN
</code></pre>

<p>Any idea why this is happening?</p>
";;1;;2012-05-05T00:00:12.300;17.0;10457584;2017-05-10T12:39:42.300;2017-01-04T22:53:12.850;;2336654.0;;1257953.0;;1;68;<python><pandas>;Redefining the Index in a Pandas DataFrame object;107691.0
212;212;10465162.0;2.0;"<p>I have a DataFrame, say a volatility surface with index as time and column as strike. How do I do two dimensional interpolation? I can <code>reindex</code> but how do i deal with <code>NaN</code>? I know we can <code>fillna(method='pad')</code> but it is not even linear interpolation. Is there a way we can plug in our own method to do interpolation?</p>
";;0;;2012-05-05T18:25:59.063;14.0;10464738;2017-01-04T22:54:22.343;2017-01-04T22:54:22.343;;2336654.0;;1377107.0;;1;23;<python><pandas>;Interpolation on DataFrame in pandas;15524.0
222;222;23901625.0;6.0;"<p>I am trying to plot some data using pandas in Ipython Notebook, and while it gives me the object, it doesn't actually plot the graph itself. So it looks like this:</p>

<pre><code>In [7]:

pledge.Amount.plot()

Out[7]:

&lt;matplotlib.axes.AxesSubplot at 0x9397c6c&gt;
</code></pre>

<p>The graph should follow after that, but it simply doesn't appear. I have imported matplotlib, so that's not the problem. Is there any other module I need to import ?</p>
";;3;;2012-05-09T06:45:29.727;20.0;10511024;2017-01-04T22:56:29.223;2017-01-04T22:56:29.223;;2336654.0;;442158.0;;1;64;<python><ipython><pandas>;in Ipython notebook, Pandas is not displying the graph I try to plot;44369.0
234;234;10565742.0;3.0;"<p>I worked now for quite some time using python and pandas for analysing a set of hourly data and find it quite nice (Coming from Matlab.)</p>

<p>Now I am kind of stuck. I created my <code>DataFrame</code> like that:</p>

<pre><code>SamplingRateMinutes=60
index = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))
ts=DataFrame(data, index=index)
</code></pre>

<p>What I want to do now is to select the Data for all days at the hours 10 to 13 and 20-23 to use the data for further calculations.
So far I sliced the data using</p>

<pre><code> selectedData=ts[begin:end]
</code></pre>

<p>And I am sure to get some kind of dirty looping to select the data needed. But there must be a more elegant way to index exacly what I want. I am sure this is a common problem and the solution in pseudocode should look somewhat like that:</p>

<pre><code>myIndex=ts.index[10&lt;=ts.index.hour&lt;=13 or 20&lt;=ts.index.hour&lt;=23]
selectedData=ts[myIndex]
</code></pre>

<p>To mention I am an engineer and no programer :) ... yet</p>
";;0;;2012-05-12T16:05:17.147;19.0;10565282;2017-01-04T22:59:54.407;2017-01-04T22:59:54.407;;2336654.0;;1391241.0;;1;16;<python><indexing><time-series><pandas>;pandas, python - how to select specific times in timeseries;16624.0
238;238;;2.0;"<p>I am just getting started with Pandas and I am reading in a csv file using the <code>read_csv()</code> method. The difficulty I am having is preventing pandas from converting my telephone numbers to large numbers, instead of keeping them as strings. I defined a converter which just left the numbers alone, but then they still converted to numbers. When I changed my converter to prepend a 'z' to the phone numbers, then they stayed strings. Is there some way to keep them strings without modifying the values of the fields?</p>
";;2;;2012-05-14T21:01:58.680;5.0;10591000;2017-01-04T23:00:37.627;2017-01-04T23:00:37.627;;2336654.0;;1394684.0;;1;25;<python><pandas>;Specifying data type in Pandas csv reader;17261.0
247;247;12036847.0;14.0;"<p>I'm using the Pandas package and it creates a DataFrame object, which is basically a labeled matrix. Often I have columns that have long string fields, or dataframes with many columns, so the simple print command doesn't work well. I've written some text output functions, but they aren't great.</p>

<p>What I'd really love is a simple GUI that lets me interact with a dataframe / matrix / table. Just like you would find in a SQL tool. Basically a window that has a read-only spreadsheet like view into the data. I can expand columns, page up and down through long tables, etc.</p>

<p>I would suspect something like this exists, but I must be Googling with the wrong terms. It would be great if it is pandas specific, but I would guess I could use any matrix-accepting tool. (BTW - I'm on Windows.)</p>

<p>Any pointers?</p>

<p>Or, conversely, if someone knows this space well and knows this probably doesn't exist, any suggestions on if there is a simple GUI framework / widget I could use to roll my own? (But since my needs are limited, I'm reluctant to have to learn a big GUI framework and do a bunch of coding for this one piece.)</p>
";;5;;2012-05-17T12:48:00.407;25.0;10636024;2017-07-23T07:49:49.370;2017-01-04T23:02:33.273;;2336654.0;;1400991.0;;1;33;<python><user-interface><pandas><dataframe>;Python / Pandas - GUI for viewing a DataFrame or Matrix;26216.0
251;251;44736467.0;7.0;"<p>I load a some machine learning data from a csv file. The first 2 columns are observations and the remaining columns are features.</p>

<p>Currently, I do the following :</p>

<pre><code>data = pandas.read_csv('mydata.csv')
</code></pre>

<p>which gives something like:</p>

<pre><code>data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))
</code></pre>

<p>I'd like to slice this dataframe in two dataframes: one containing the columns <code>a</code> and <code>b</code> and one containing the columns <code>c</code>, <code>d</code> and <code>e</code>.</p>

<p>It is not possible to write something like </p>

<pre><code>observations = data[:'c']
features = data['c':]
</code></pre>

<p>I'm not sure what the best method is. Do I need a <code>pd.Panel</code>?</p>

<p>By the way, I find dataframe indexing pretty inconsistent: <code>data['a']</code> is permitted, but <code>data[0]</code> is not. On the other side, <code>data['a':]</code> is not permitted but <code>data[0:]</code> is.
Is there a practical reason for this? This is really confusing if columns are indexed by Int, given that <code>data[0] != data[0:1]</code></p>
";;3;;2012-05-19T14:11:42.603;58.0;10665889;2017-08-15T11:34:35.200;2017-01-04T23:03:05.363;;2336654.0;;1350862.0;;1;126;<python><numpy><pandas><slice>;How to take column-slices of dataframe in pandas;154109.0
262;262;10726275.0;2.0;"<p>I have a <code>DataFrame</code> with a few columns. One columns contains a symbol for which currency is being used, for instance a euro or a dollar sign. Another column contains a budget value. So for instance in one row it could mean a budget of 5000 in euro and in the next row it could say a budget of 2000 in dollar.</p>

<p>In pandas I would like to add an extra column to my DataFrame, normalizing the budgets in euro. So basically, for each row the value in the new column should be the value from the budget column * 1 if the symbol in the currency column is a euro sign, and the value in the new column should be the value of the budget column * 0.78125 if the symbol in the currency column is a dollar sign.</p>

<p>I know how to add a column, fill it with values, copy values from another column etc. but not how to fill the new column conditionally based on the value of another column. </p>

<p>Any suggestions?</p>
";;0;;2012-05-23T07:41:57.010;11.0;10715519;2017-02-23T14:05:29.237;2017-01-04T23:05:48.043;;2336654.0;;493499.0;;1;19;<dataframe><pandas>;Conditionally fill column values based on another columns value in pandas;14199.0
263;263;24888331.0;13.0;"<p>I understand that pandas is designed to load fully populated <code>DataFrame</code> but I need to <strong>create an empty DataFrame then add rows, one by one</strong>.
What is the best way to do this ?</p>

<p>I successfully created an empty DataFrame with :</p>

<pre><code>res = DataFrame(columns=('lib', 'qty1', 'qty2'))
</code></pre>

<p>Then I can add a new row and fill a field with :</p>

<pre><code>res = res.set_value(len(res), 'qty1', 10.0)
</code></pre>

<p>It works but seems very odd :-/ (it fails for adding string value)</p>

<p>How can I add a new row to my DataFrame (with different columns type) ?</p>
";;3;;2012-05-23T08:12:31.843;82.0;10715965;2017-08-25T15:47:04.097;2017-01-04T23:06:07.183;;2336654.0;;31335.0;;1;312;<python><pandas>;add one row in a pandas.DataFrame;349935.0
272;272;10739432.0;1.0;"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">What is the most efficient way to loop through dataframes with pandas?</a>  </p>
</blockquote>



<p>I'm looking to iterate row by row through a pandas <code>DataFrame</code>.  The way I'm doing it so far is as follows:</p>

<pre><code>for i in df.index:
    do_something(df.ix[i])
</code></pre>

<p>Is there a more performant and/or more idiomatic way to do this?  I know about apply, but sometimes it's more convenient to use a for loop.  Thanks in advance.</p>
";2012-05-29T13:14:21.157;2;;2012-05-23T23:21:18.343;23.0;10729210;2017-01-04T23:07:54.263;2017-05-23T12:17:58.683;;-1.0;;1413778.0;;1;122;<python><pandas>;iterating row by row through a pandas dataframe;174898.0
279;279;;1.0;"<p>I'm using a Pandas <code>DataFrame</code> to do a row-wise t-test as per this example:</p>

<pre><code>import numpy
import pandas

df = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), 
                      columns=[""a"", ""b"", ""c"", ""d""])

df = df.dropna()
</code></pre>

<p>Now, supposing I have ""a"" and ""b"" as one group, and ""c"" and ""d"" at the other, I'm performing the t-test row-wise. This is fairly trivial with pandas, using <code>apply</code> with axis=1. However, I can either return a DataFrame of the same shape if my function doesn't aggregate, or a Series if it aggregates.</p>

<p>Normally I would just output the p-value (so, aggregation) but I would like to generate an additional value based on other calculations (in other words, return two values). I can of course do two runs, aggregating the p-values first, then doing the other work, but I was wondering if there is a more efficient way to do so as the data is reasonably large.</p>

<p>As an example of the calculation, a hypotethical function would be:</p>

<pre><code>from scipy.stats import ttest_ind

def t_test_and_mean(series, first, second):
    first_group = series[first]
    second_group = series[second]
    _, pvalue = ttest_ind(first_group, second_group)

    mean_ratio = second_group.mean() / first_group.mean()

    return (pvalue, mean_ratio)
</code></pre>

<p>Then invoked with </p>

<pre><code>df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)
</code></pre>

<p>Of course in this case it returns a single Series with the two tuples as value.</p>

<p>Instead, ny expected output would be a DataFrame with two columns, one for the first result, and one for the second. Is this possible or I have to do two runs for the two calculations, then merge them together?</p>
";;2;;2012-05-25T08:35:27.927;13.0;10751127;2017-01-04T23:08:08.837;2017-01-04T23:08:08.837;;2336654.0;;241515.0;;1;41;<python><pandas>;Returning multiple values from pandas apply on a DataFrame;19415.0
304;304;10859883.0;1.0;"<p>I have a <code>dataFrame</code> in pandas and several of the columns have all null values. Is there a built in function which will let me remove those columns?</p>

<p>Thank you!</p>
";;1;;2012-06-01T22:05:44.497;9.0;10857924;2017-01-04T23:13:23.903;2017-01-04T23:13:23.903;;2336654.0;;1165294.0;;1;39;<python><pandas>;Remove NULL columns in a dataframe Pandas?;33392.0
306;306;11005208.0;3.0;"<p>I'm using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string <code>""nan""</code> is a possible value, as is an empty string.  I managed to get pandas to read ""nan"" as a string, but I can't figure out how to get it not to read an empty value as NaN.  Here's sample data and output</p>

<pre><code>One,Two,Three
a,1,one
b,2,two
,3,three
d,4,nan
e,5,five
nan,6,
g,7,seven

&gt;&gt;&gt; pandas.read_csv('test.csv', na_values={'One': [], ""Three"": []})
    One  Two  Three
0    a    1    one
1    b    2    two
2  NaN    3  three
3    d    4    nan
4    e    5   five
5  nan    6    NaN
6    g    7  seven
</code></pre>

<p>It correctly reads ""nan"" as the string ""nan', but still reads the empty cells as NaN.  I tried passing in <code>str</code> in the <code>converters</code> argument to read_csv (with <code>converters={'One': str})</code>), but it still reads the empty cells as NaN.</p>

<p>I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?</p>
";;0;;2012-06-03T00:38:37.717;5.0;10867028;2017-05-07T14:55:36.283;2017-01-04T23:13:53.947;;2336654.0;;1427416.0;;1;23;<python><csv><pandas>;Get pandas.read_csv to read empty values as empty string instead of nan;21259.0
316;316;10943545.0;1.0;"<p>Is it possible to reindex a pandas <code>DataFrame</code> using a column made up of datetime objects?</p>

<p>I have a DataFrame <code>df</code> with the following columns:</p>

<pre><code>Int64Index: 19610 entries, 0 to 19609
Data columns:
cntr                  19610  non-null values  #int
datflt                19610  non-null values  #float
dtstamp               19610  non-null values  #datetime object
DOYtimestamp          19610  non-null values  #float
dtypes: int64(1), float64(2), object(1)
</code></pre>

<p>I can reindex the <code>df</code> easily along <code>DOYtimestamp</code> with: <code>df.reindex(index=df.dtstamp)</code>
and <code>DOYtimestamp</code> has the following values:</p>

<pre><code>&gt;&gt;&gt; df['DOYtimestamp'].values
    array([ 153.76252315,  153.76253472,  153.7625463 , ...,  153.98945602,
    153.98946759,  153.98947917])
</code></pre>

<p>but I'd like to reindex the DataFrame along <code>dtstamp</code> which is made up of datetime objects so that I generate different timestamps directly from the index. The <code>dtstamp</code> column has values which look like:</p>

<pre><code> &gt;&gt;&gt; df['dtstamp'].values
     array([2012-06-02 18:18:02, 2012-06-02 18:18:03, 2012-06-02 18:18:04, ...,
     2012-06-02 23:44:49, 2012-06-02 23:44:50, 2012-06-02 23:44:51], 
     dtype=object)
</code></pre>

<p>When I try and reindex <code>df</code> along <code>dtstamp</code> I get the following:</p>

<pre><code>&gt;&gt;&gt; df.reindex(index=df.dtstamp)
    TypeError: can't compare datetime.datetime to long
</code></pre>

<p>I'm just not sure what I need to do get the index to be of a datetime type. Any thoughts?</p>
";;0;;2012-06-08T05:24:57.607;4.0;10943478;2017-01-04T23:16:00.827;2017-01-04T23:16:00.827;;2336654.0;;983310.0;;1;12;<python><dataframe><pandas><reindex>;pandas reindex DataFrame with datetime objects;10727.0
320;320;;5.0;"<p>Is there a way to write an aggregation function as is used in <code>DataFrame.agg</code> method, that would have access to more than one column of the data that is being aggregated? Typical use cases would be weighted average, weighted standard deviation funcs.</p>

<p>I would like to be able to write something like</p>

<pre><code>def wAvg(c, w):
    return ((c * w).sum() / w.sum())

df = DataFrame(....) # df has columns c and w, i want weighted average
                     # of c using w as weight.
df.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...
</code></pre>
";;0;;2012-06-08T15:01:32.727;27.0;10951341;2017-06-21T18:21:35.253;2013-08-13T09:05:19.130;;170005.0;;1444817.0;;1;45;<python><pandas>;Pandas DataFrame aggregate function using multiple columns;17889.0
324;324;10972557.0;5.0;"<p>I have a pandas <code>DataFrame</code> that has multiple columns in it:</p>

<pre><code>Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51
Data columns:
foo                   11516  non-null values
bar                   228381  non-null values
Time_UTC              239897  non-null values
dtstamp               239897  non-null values
dtypes: float64(4), object(1)
</code></pre>

<p>where <code>foo</code> and <code>bar</code> are columns which contain the same data yet are named differently. Is there are a way to move the rows which make up <code>foo</code> into <code>bar</code>, ideally whilst maintaining the name of <code>bar</code>? </p>

<p>In the end the DataFrame should appear as:</p>

<pre><code>Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51
Data columns:
bar                   239897  non-null values
Time_UTC              239897  non-null values
dtstamp               239897  non-null values
dtypes: float64(4), object(1)
</code></pre>

<p>That is the NaN values that made up bar were replaced by the values from <code>foo</code>.</p>
";;0;;2012-06-10T21:12:43.400;5.0;10972410;2017-01-04T23:17:03.463;2017-01-04T23:17:03.463;;2336654.0;;983310.0;;1;18;<python><dataframe><pandas>;pandas: combine two columns in a DataFrame;41826.0
328;328;10982198.0;3.0;"<p>I would like to shift a column in a Pandas <code>DataFrame</code>, but I haven't been able to find a method to do it from the documentation without rewriting the whole DF. Does anyone know how to do it? 
DataFrame:</p>

<pre><code>##    x1   x2
##0  206  214
##1  226  234
##2  245  253
##3  265  272
##4  283  291
</code></pre>

<p>Desired output:</p>

<pre><code>##    x1   x2
##0  206  nan
##1  226  214
##2  245  234
##3  265  253
##4  283  272
##5  nan  291
</code></pre>
";;1;;2012-06-11T14:28:49.360;9.0;10982089;2017-06-26T19:17:44.513;2017-06-09T12:44:14.290;;1199589.0;;1199589.0;;1;38;<python><pandas><dataframe>;How to shift a column in Pandas DataFrame;31697.0
348;348;11856979.0;2.0;"<p>how do I add <code>'d'</code> to the index below without having to reset it first?</p>

<pre><code>from pandas import DataFrame
df = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )
df.set_index(['a','b'], inplace=True)
df['d'] = range(6)

# how do I set index to 'a b d' without having to reset it first?
df.reset_index(['a','b','d'], inplace=True)
df.set_index(['a','b','d'], inplace=True)

df
</code></pre>
";;0;;2012-06-14T20:09:38.423;1.0;11040626;2017-01-04T23:21:25.750;2017-01-04T23:21:25.750;;2336654.0;;1441053.0;;1;11;<python><dataframe><pandas>;Pandas DataFrame Add column to index without resetting;9132.0
354;354;11067072.0;11.0;"<p>I have a <code>dataframe</code> with over 200 columns (don't ask why). The issue is as they were generated the order is</p>

<pre><code>['Q1.3','Q6.1','Q1.2','Q1.1',......]
</code></pre>

<p>I need to re-order the columns as follows:</p>

<pre><code>['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]
</code></pre>

<p>Is there some way for me to do this within python?</p>
";;1;;2012-06-16T21:05:01.030;27.0;11067027;2017-07-24T10:04:15.597;2017-01-04T23:22:21.290;;2336654.0;;1419123.0;;1;102;<python><pandas><order>;Python Pandas - Re-ordering columns in a dataframe based on column name;74414.0
357;357;11073962.0;1.0;"<p>I have some data from log files and would like to group entries by a minute:</p>

<pre><code> def gen(date, count=10):
     while count &gt; 0:
         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))
         count -= 1
         date += DateOffset(seconds=randint(40))

 df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index='Time', columns=['Time', 'Event', 'Source'])
</code></pre>

<p>df:</p>

<pre><code> Event  Source
 2012-01-01 12:30:00     event3  source1
 2012-01-01 12:30:12     event2  source2
 2012-01-01 12:30:12     event2  source2
 2012-01-01 12:30:29     event6  source1
 2012-01-01 12:30:38     event1  source1
 2012-01-01 12:31:05     event4  source2
 2012-01-01 12:31:38     event4  source1
 2012-01-01 12:31:44     event5  source1
 2012-01-01 12:31:48     event5  source2
 2012-01-01 12:32:23     event6  source1
</code></pre>

<p>I tried these options:</p>

<ol>
<li><code>df.resample('Min')</code> is too high level and wants to aggregate.</li>
<li><code>df.groupby(date_range(datetime(2012,1,1,12, 30), freq='Min',
periods=4))</code> fails with exception.</li>
<li><p><code>df.groupby(TimeGrouper(freq='Min'))</code> works fine and returns a <code>DataFrameGroupBy</code> object for further processing, e.g.:</p>

<pre><code>grouped = df.groupby(TimeGrouper(freq='Min'))
grouped.Source.value_counts()
2012-01-01 12:30:00  source1    1
2012-01-01 12:31:00  source2    2
                     source1    2
2012-01-01 12:32:00  source2    2
                     source1    2
2012-01-01 12:33:00  source1    1
</code></pre></li>
</ol>

<p><em>However</em>, the <code>TimeGrouper</code> class is not documented.</p>

<p>What is the correct way to group by a period of time? How can I group the data by a minute AND by the Source column, e.g. <code>groupby([TimeGrouper(freq='Min'), df.Source])</code>?</p>
";;0;;2012-06-17T18:07:39.223;5.0;11073609;2017-06-16T18:48:11.237;2017-06-16T18:48:11.237;;1391441.0;;1461554.0;;1;28;<python><pandas>;How to group DataFrame by a period of time?;20945.0
359;359;11077215.0;3.0;"<p>They both seem <em>exceedingly</em> similar and I'm curious as to which package would be more beneficial for financial data analysis. </p>
";2015-07-21T00:21:27.507;3;;2012-06-18T04:45:48.650;25.0;11077023;2017-01-04T23:23:28.707;2017-01-04T23:23:28.707;;2336654.0;user1462733;;;1;106;<python><numpy><scipy><pandas>;What are the differences between Pandas and NumPy+SciPy in Python?;54531.0
364;364;11112419.0;4.0;"<p>I have two <code>dataframes</code>, both indexed by <code>timeseries</code>.  I need to add the elements together to form a new <code>dataframe</code>, but only if the index and column are the same.  If the item does not exist in one of the <code>dataframe</code>s then it should be treated as a zero.</p>

<p>I've tried using <code>.add</code> but this sums regardless of index and column.  Also tried a simple <code>combined_data = dataframe1 + dataframe2</code> but this give a <code>NaN</code> if both dataframes don't have the element.</p>

<p>Any suggestions?</p>
";;1;;2012-06-19T18:11:14.613;6.0;11106823;2017-02-16T12:19:46.397;2017-01-04T23:24:52.547;;2336654.0;;1223860.0;;1;26;<python><pandas>;Adding two pandas dataframes;19395.0
400;400;11246087.0;4.0;"<p>On Pandas documentation of the <code>pivot</code> method, we have:</p>

<pre><code>Examples
--------
&gt;&gt;&gt; df
    foo   bar  baz
0   one   A    1.
1   one   B    2.
2   one   C    3.
3   two   A    4.
4   two   B    5.
5   two   C    6.

&gt;&gt;&gt; df.pivot('foo', 'bar', 'baz')
     A   B   C
one  1   2   3
two  4   5   6
</code></pre>

<p>My <code>DataFrame</code> is structured like this:</p>

<pre><code>   name   id     x
----------------------
0  john   1      0
1  john   2      0
2  mike   1      1
3  mike   2      0
</code></pre>

<p>And I want something like this:</p>

<pre><code>      1    2   # (this is the id as columns)
----------------------
mike  0    0   # (and this is the 'x' as values)
john  1    0
</code></pre>

<p>But when I run the <code>pivot</code> method, it is saying:</p>

<pre><code>*** ReshapeError: Index contains duplicate entries, cannot reshape
</code></pre>

<p>Which doesn't makes sense, even in example there are repeated entries on the <code>foo</code> column. I'm using the <code>name</code> column as the index of the pivot, the first argument of the <code>pivot</code> method call.</p>
";;0;;2012-06-27T17:58:38.513;4.0;11232275;2017-01-04T23:31:38.847;2017-01-04T23:31:38.847;;2336654.0;;179372.0;;1;28;<python><pandas>;Pandas pivot warning about repeated entries on index;12221.0
413;413;11287278.0;7.0;"<p>I have data in different columns but I don't know how to extract it to save it in another variable.</p>

<pre><code>index  a   b   c
1      2   3   4
2      3   4   5
</code></pre>

<p>How do I select <code>'b'</code>, <code>'c'</code> and save it in to df1?</p>

<p>I tried </p>

<pre><code>df1 = df['a':'b']
df1 = df.ix[:, 'a':'b']
</code></pre>

<p>None seem to work. Any ideas would help thanks.</p>
";;4;;2012-07-01T21:03:16.987;88.0;11285613;2017-05-02T09:41:52.950;2017-01-04T23:34:12.147;;2336654.0;;1610626.0;;1;280;<python><pandas>;Selecting columns;413954.0
433;433;11346337.0;16.0;"<p>I have a data table using pandas and column labels that I need to edit to replace the original column labels. </p>

<p>I'd like to change the column names in a data table <code>A</code> where the original column names are:</p>

<pre><code>['$a', '$b', '$c', '$d', '$e'] 
</code></pre>

<p>to </p>

<pre><code>['a', 'b', 'c', 'd', 'e'].
</code></pre>

<p>I have the edited column names stored it in a list, but I don't know how to replace the column names.</p>
";;0;;2012-07-05T14:21:15.090;206.0;11346283;2017-07-05T13:19:35.273;2016-09-29T12:31:00.833;;3730397.0;;1504276.0;;1;750;<python><pandas><replace><dataframe><rename>;Renaming columns in pandas;767316.0
435;435;11384667.0;2.0;"<p>I have a pandas <code>DataFrame</code> and I want to plot a bar chart that includes a legend.</p>

<pre><code>import pylab as pl
from pandas import *

x = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})
</code></pre>

<p>If I call plot directly, then it puts the legend above the plot:</p>

<pre><code>x.plot(kind=""bar"")
</code></pre>

<p>If I turn of the legend in the plot and try to add it later, then it doesn't retain the colors associated with the two columns in the DataFrame (see below):</p>

<pre><code>x.plot(kind=""bar"", legend=False)
l = pl.legend(('Alpha','Beta'), loc='best')
</code></pre>

<p>What's the right way to include a legend in a matplotlib plot from a Pandas DataFrame?
<img src=""https://i.stack.imgur.com/HOxth.png"" alt=""enter image description here""></p>
";;3;;2012-07-05T15:59:54.960;6.0;11348183;2017-01-04T23:45:39.237;2017-01-04T23:45:39.237;;2336654.0;;163053.0;;1;15;<python><legend><pandas>;Pandas bar plot with specific colors and legend location?;23856.0
437;437;;6.0;"<p>I have a <code>DataFrame</code> with 4 columns of which 2 contain string values. I was wondering if there was a way to select rows based on a partial string match against a particular column?</p>

<p>In other words, a function or lambda function that would do something like </p>

<pre><code>re.search(pattern, cell_in_question) 
</code></pre>

<p>returning a boolean. I am familiar with the syntax of <code>df[df['A'] == ""hello world""]</code> but can't seem to find a way to do the same with a partial string match say <code>'hello'</code>.</p>

<p>Would someone be able to point me in the right direction?</p>
";;0;;2012-07-05T18:57:34.047;48.0;11350770;2017-01-04T23:46:31.180;2017-01-04T23:46:31.180;;2336654.0;;1170342.0;;1;132;<python><pandas>;pandas + dataframe - select by partial string;130134.0
441;441;11362056.0;6.0;"<p>I have a csv file with the name <code>params.csv</code>. I opened up <code>ipython qtconsole</code> and created a pandas <code>dataframe</code> using:</p>

<pre><code>import pandas
paramdata = pandas.read_csv('params.csv', names=paramnames)
</code></pre>

<p>where, <code>paramnames</code> is a python list of string objects. Example of <code>paramnames</code> (the length of actual list is 22):</p>

<pre><code>paramnames = [""id"",
""fc"",
""mc"",
""markup"",
""asplevel"",
""aspreview"",
""reviewpd""]
</code></pre>

<p>At the ipython prompt if I type <code>paramdata</code> and press enter then I do not get the dataframe with columns and values as shown in examples on <a href=""http://pandas.sourceforge.net/indexing.html#additional-column-access"" rel=""noreferrer"">Pandas website</a>. Instead, I get information about the dataframe. I get:</p>

<pre><code>In[35]: paramdata
Out[35]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 59 entries, 0 to 58
Data columns:
id                    59  non-null values
fc                    59  non-null values
mc                    59  non-null values
markup                59  non-null values
asplevel              59  non-null values
aspreview             59  non-null values
reviewpd              59  non-null values
</code></pre>

<p>If I type <code>paramdata['mc']</code> then I do get the values as expected for the <code>mc</code> column. I have two questions:</p>

<p>(1) In the examples on the pandas website (see, for example, the output of <code>df</code> here: <a href=""http://pandas.sourceforge.net/indexing.html#additional-column-access"" rel=""noreferrer"">http://pandas.sourceforge.net/indexing.html#additional-column-access</a>) typing the name of the dataframe gives the actual data. Why am I getting information about the dataframe as shown above instead of the actual data? Do I need to set some output options somewhere?</p>

<p>(2) How do I output all columns in the dataframe to the screen without having to type their names, i.e., without having to type something like <code>paramdata[['id','fc','mc']]</code>. </p>

<p>I am using pandas version 0.8. </p>

<p>Thank you.</p>
";;1;;2012-07-06T12:12:49.907;16.0;11361985;2017-01-04T23:47:00.740;2017-01-04T23:47:00.740;;2336654.0;;316357.0;;1;59;<python><numpy><pandas>;Output data from all columns in a dataframe in pandas;106412.0
456;456;11397052.0;3.0;"<p>A Pandas <code>DataFrame</code> contains column named <code>""date""</code> that contains non-unique <code>datetime</code> values. 
I can group the lines in this frame using:</p>

<pre><code>data.groupby(data['date'])
</code></pre>

<p>However, this splits the data by the <code>datetime</code> values. I would like to group these data by the year stored in the ""date"" column. <a href=""http://wesmckinney.com/blog/?p=125"" rel=""noreferrer"">This page</a> shows how to group by year in cases where the time stamp is used as an index, which is not true in my case.</p>

<p>How do I achieve this grouping?</p>
";;0;;2012-07-09T09:04:33.723;12.0;11391969;2017-01-04T23:47:58.137;2017-01-04T23:47:58.137;;2336654.0;;17523.0;;1;39;<python><pandas>;How to group pandas DataFrame entries by date in a non-unique column;24050.0
468;468;11415882.0;2.0;"<p>I've inherited a data file saved in the Stata .dta format. I can load it in with <code>scikits.statsmodels</code> <code>genfromdta()</code> function. This puts my data into a 1-dimensional NumPy array, where each entry is a row of data, stored in a 24-tuple.</p>

<pre><code>In [2]: st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)
666.523324013

In [3]: type(initialload)
Out[3]: numpy.ndarray

In [4]: initialload.shape
Out[4]: (4809584,)

In [5]: initialload[0]
Out[5]: (19901130.0, 289.0, 1990.0, 12.0, 19901231.0, 18.0, 40301000.0, 'GB', 18242.0, -2.368063, 1.0, 1.7783716290878204, 4379.355, 66.17669677734375, -999.0, -999.0, -0.60000002, -999.0, -999.0, -999.0, -999.0, -999.0, 0.2, 371.0)
</code></pre>

<p>I am curious if there's an efficient way to arrange this into a Pandas DataFrame. From what I've read, building up a DataFrame row-by-row seems quite inefficient... but what are my options?</p>

<p>I've written a pretty slow first-pass that just reads each tuple as a single-row DataFrame and appends it. Just wondering if anything else is known to be better.</p>
";;2;;2012-07-10T14:36:11.787;2.0;11415701;2017-01-04T23:54:59.330;2017-01-04T23:54:59.330;;2336654.0;;567620.0;;1;12;<python><tuples><pandas><dta>;Efficiently construct Pandas DataFrame from large list of tuples/rows;17794.0
472;472;11475486.0;3.0;"<p>I would like to filter rows by a function of each row, e.g.</p>

<pre><code>def f(row):
  return sin(row['velocity'])/np.prod(['masses']) &gt; 5

df = pandas.DataFrame(...)
filtered = df[apply_to_all_rows(df, f)]
</code></pre>

<p>Or for another more complex, contrived example,</p>

<pre><code>def g(row):
  if row['col1'].method1() == 1:
    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])
  else:
    val = row['col2'].method5(row['col6'])
  return np.sin(val)

df = pandas.DataFrame(...)
filtered = df[apply_to_all_rows(df, g)]
</code></pre>

<p><em>How can I do so?</em></p>
";;0;;2012-07-10T16:56:37.887;6.0;11418192;2017-01-04T23:55:56.943;2017-01-04T23:55:56.943;;2336654.0;;128580.0;;1;36;<pandas>;pandas: complex filter on rows of DataFrame;20153.0
492;492;11495086.0;2.0;"<p>I'm not sure why I'm getting slightly different results for a simple OLS, depending on whether I go through <a href=""http://pandas.pydata.org/pandas-docs/dev/r_interface.html"" rel=""nofollow noreferrer"">panda's experimental rpy interface</a> to do the regression in <code>R</code> or whether I use <a href=""http://statsmodels.sourceforge.net/devel/index.html"" rel=""nofollow noreferrer"">statsmodels</a> in Python.</p>

<pre><code>import pandas
from rpy2.robjects import r

from functools import partial

loadcsv = partial(pandas.DataFrame.from_csv,
                  index_col=""seqn"", parse_dates=False)

demoq = loadcsv(""csv/DEMO.csv"")
rxq = loadcsv(""csv/quest/RXQ_RX.csv"")

num_rx = {}
for seqn, num in rxq.rxd295.iteritems():
    try:
        val = int(num)
    except ValueError:
        val = 0
    num_rx[seqn] = val

series = pandas.Series(num_rx, name=""num_rx"")
demoq = demoq.join(series)

import pandas.rpy.common as com
df = com.convert_to_r_dataframe(demoq)
r.assign(""demoq"", df)
r('lmout &lt;- lm(demoq$num_rx ~ demoq$ridageyr)')  # run the regression
r('print(summary(lmout))')  # print from R
</code></pre>

<p>From <code>R</code>, I get the following summary:</p>

<pre><code>Call:
lm(formula = demoq$num_rx ~ demoq$ridageyr)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9086 -0.6908 -0.2940  0.1358 15.7003 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***
demoq$ridageyr  0.0358161  0.0006232  57.469  &lt; 2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 

Residual standard error: 1.545 on 9963 degrees of freedom
Multiple R-squared: 0.249,  Adjusted R-squared: 0.2489 
F-statistic:  3303 on 1 and 9963 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Using <code>statsmodels.api</code> to do the OLS:</p>

<pre><code>import statsmodels.api as sm
results = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()
results.summary()
</code></pre>

<p>The results are similar to R's output but not the same:</p>

<pre><code>OLS Regression Results
Adj. R-squared:  0.247
Log-Likelihood:  -18488.
No. Observations:    9965    AIC:   3.698e+04
Df Residuals:    9964    BIC:   3.698e+04
             coef   std err  t     P&gt;|t|    [95.0% Conf. Int.]
ridageyr     0.0331  0.000   82.787    0.000        0.032 0.034
</code></pre>

<p>The install process is a a bit cumbersome. But, there is an <em>ipython notebook</em> <a href=""https://github.com/skyl/NHANES-opensource/blob/master/1999-2000/RX-Explore.ipynb"" rel=""nofollow noreferrer"">here</a>, that can reproduce the inconsistency.</p>
";;0;;2012-07-15T19:44:50.263;6.0;11495051;2017-01-04T23:58:27.490;2017-01-04T23:58:27.490;;2336654.0;;177293.0;;1;21;<python><r><pandas><rpy2><statsmodels>;Difference in Python statsmodels OLS and R's lm;4994.0
504;504;11548224.0;2.0;"<p>Is there a preferred way to keep the data type of a <code>numpy</code> array fixed as <code>int</code> (or <code>int64</code> or whatever), while still having an element inside listed as <code>numpy.NaN</code>?</p>

<p>In particular, I am converting an in-house data structure to a Pandas DataFrame. In our structure, we have integer-type columns that still have NaN's (but the dtype of the column is int). It seems to recast everything as a float if we make this a DataFrame, but we'd really like to be <code>int</code>.</p>

<p>Thoughts?</p>

<p><strong>Things tried:</strong></p>

<p>I tried using the <code>from_records()</code> function under pandas.DataFrame, with <code>coerce_float=False</code> and this did not help. I also tried using NumPy masked arrays, with NaN fill_value, which also did not work. All of these caused the column data type to become a float.</p>
";;3;;2012-07-18T18:30:02.887;10.0;11548005;2017-06-27T22:46:16.020;2017-01-05T00:00:02.910;;2336654.0;;567620.0;;1;54;<python><numpy><int><pandas><data-type-conversion>;NumPy or Pandas: Keeping array type as integer while having a NaN value;16797.0
513;513;11589000.0;8.0;"<p>I'm trying to create a series of dummy variables from a categorical variable using pandas in python. I've come across the <code>get_dummies</code> function, but whenever I try to call it I receive an error that the name is not defined. </p>

<p>Any thoughts or other ways to create the dummy variables would be appreciated.</p>

<p><strong>EDIT</strong>: Since others seem to be coming across this, the <code>get_dummies</code> function in pandas now works perfectly fine. This means the following should work:</p>

<pre><code>import pandas as pd

dummies = pd.get_dummies(df['Category'])
</code></pre>

<p>See <a href=""http://blog.yhathq.com/posts/logistic-regression-and-python.html"" rel=""nofollow noreferrer"">http://blog.yhathq.com/posts/logistic-regression-and-python.html</a> for further information.</p>
";;0;;2012-07-20T22:33:29.350;18.0;11587782;2017-05-21T23:28:03.220;2017-01-05T00:02:28.390;;2336654.0;;1074057.0;;1;17;<python><pandas>;Creating dummy variables in pandas for python;39556.0
519;519;11617682.0;2.0;"<p>I have a simple question related with csv files and parsing datetime.</p>

<p>I have a csv file that look like this:</p>

<pre><code>YYYYMMDD, HH,    X
20110101,  1,   10
20110101,  2,   20
20110101,  3,   30
</code></pre>

<p>I would like to read it using pandas (read_csv) and have it in a dataframe indexed by the datetime. So far I've tried to implement the following:</p>

<pre><code>import pandas as pnd
pnd.read_csv(""..\\file.csv"",  parse_dates = True, index_col = [0,1])
</code></pre>

<p>and the result I get is:</p>

<pre><code>                         X
YYYYMMDD    HH            
2011-01-01 2012-07-01   10
           2012-07-02   20
           2012-07-03   30
</code></pre>

<p>As you see the parse_dates in converting the HH into a different date.</p>

<p>Is there a simple and efficient way to combine properly the column ""YYYYMMDD"" with the column ""HH"" in order to have something like this? :</p>

<pre><code>                      X
Datetime              
2011-01-01 01:00:00  10
2011-01-01 02:00:00  20
2011-01-01 03:00:00  30
</code></pre>

<p>Thanks in advance for the help.</p>
";;0;;2012-07-23T15:20:43.083;17.0;11615504;2017-01-05T00:03:47.080;2017-01-05T00:03:47.080;;2336654.0;;1520997.0;;1;21;<python><pandas>;Parse dates when YYYYMMDD and HH are in separate columns using pandas in Python;15326.0
529;529;11622769.0;6.0;"<p>I am exploring switching to python and pandas as a long-time SAS user.  </p>

<p>However, when running some tests today, I was surprised that python ran out of memory when trying to <code>pandas.read_csv()</code> a 128mb csv file.  It had about 200,000 rows and 200 columns of mostly numeric data.</p>

<p>With SAS, I can import a csv file into a SAS dataset and it can be as large as my hard drive. </p>

<p>Is there something analogous in <code>pandas</code>? </p>

<p>I regularly work with large files and do not have access to a distributed computing network.</p>
";;1;;2012-07-24T00:50:49.087;57.0;11622652;2017-06-08T21:54:47.390;2017-01-05T00:05:41.453;;2336654.0;;919872.0;;1;74;<python><pandas><sas>;Large, persistent DataFrame in pandas;45446.0
533;533;11639358.0;2.0;"<p>I am having issues with joins in pandas and I am trying to figure out what is wrong. 
   Say I have a <code>dataframe</code> x:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00
Data columns:
close    1941  non-null values
high     1941  non-null values
low      1941  non-null values
open     1941  non-null values
dtypes: float64(4)
</code></pre>

<p>should I be able to join it with y on index with a simple join command where y = x except colnames have +2.  </p>

<pre><code> &lt;class 'pandas.core.frame.DataFrame'&gt;
 DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00
 Data columns:
 close2    1941  non-null values
 high2     1941  non-null values
 low2      1941  non-null values
 open2     1941  non-null values
 dtypes: float64(4)

 y.join(x) or pandas.DataFrame.join(y,x):
 &lt;class 'pandas.core.frame.DataFrame'&gt;
 DatetimeIndex: 34879 entries, 2004-12-16 00:00:00 to 2012-07-12 00:00:00
 Data columns:
 close2    34879  non-null values
 high2     34879  non-null values
 low2      34879  non-null values
 open2     34879  non-null values
 close     34879  non-null values
 high      34879  non-null values
 low       34879  non-null values
 open      34879  non-null values
 dtypes: float64(8)
</code></pre>

<p>I expect the final to have 1941 non-values for both. I tried merge as well but I have the same issue.</p>

<p>I had thought the right answer was pandas.concat([x,y]), but this does not do what I intend either.</p>

<pre><code>In [83]: pandas.concat([x,y]) 
Out[83]: &lt;class 'pandas.core.frame.DataFrame'&gt; 
DatetimeIndex: 3882 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00 
Data columns: 
close2 3882 non-null values 
high2 3882 non-null values 
low2 3882 non-null values 
open2 3882 non-null values 
dtypes: float64(4) 
</code></pre>

<p>edit: 
If you are having issues with join, read Wes's answer below. I had one time stamp that was duplicated.</p>
";;0;;2012-07-24T18:50:11.920;5.0;11637384;2017-01-05T00:05:58.013;2017-01-05T00:05:58.013;;2336654.0;;1064197.0;;1;18;<python><pandas>;Pandas join/merge/concat two dataframes;29299.0
536;536;11643893.0;1.0;"<p>I know pandas supports a secondary Y axis, but Im curious if anyone knows a way to put a tertiary Y axis on plots... currently I am achieving this with numpy+pyplot ... but it is slow with large data sets.</p>

<p>this is to plot different measurements with distinct units on the same graph for easy comparison (eg Relative Humidity/Temperature/ and Electrical Conductivity)</p>

<p>so really just curious if anyone knows if this is possible in <code>pandas</code> without too much work.</p>

<p>[Edit] I doubt that there is a way to do this(without too much overhead) however I hope to be proven wrong , this may be a limitation of matplotlib...</p>
";;0;;2012-07-24T22:30:16.603;10.0;11640243;2017-01-05T00:06:31.830;2017-01-05T00:06:31.830;;2336654.0;;541038.0;;1;33;<python><pandas>;PANDAS plot multiple Y axes;15502.0
556;556;;4.0;"<p>I am going to convert a Django QuerySet to a pandas <code>DataFrame</code> as follows:</p>

<pre><code>qs = SomeModel.objects.select_related().filter(date__year=2012)
q = qs.values('date', 'OtherField')
df = pd.DataFrame.from_records(q)
</code></pre>

<p>It works, but is there a more efficient way?</p>
";;6;;2012-07-28T02:59:02.687;14.0;11697887;2017-01-05T00:10:04.200;2017-01-05T00:10:04.200;;2336654.0;;1392323.0;;1;43;<python><django><pandas>;Converting Django QuerySet to pandas DataFrame;10165.0
559;559;11711637.0;8.0;"<p>Is there a way to widen the display of output in either interactive or script-execution mode?</p>

<p>Specifically, I am using the describe() function on a Pandas <code>dataframe</code>.  When the <code>dataframe</code> is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the <code>dataframe</code> has any more columns, the statistics are suppressed and something like this is returned:</p>

<pre><code>&gt;Index: 8 entries, count to max  
&gt;Data columns:  
&gt;x1          8  non-null values  
&gt;x2          8  non-null values  
&gt;x3          8  non-null values  
&gt;x4          8  non-null values  
&gt;x5          8  non-null values  
&gt;x6          8  non-null values  
&gt;x7          8  non-null values  
</code></pre>

<p>The ""8"" value is given whether there are 6 or 7 columns.  What does the ""8"" refer to?</p>

<p>I have already tried dragging the IDLE window larger, as well as increasing the ""Configure IDLE"" width options, to no avail.</p>

<p>My purpose in using Pandas and describe() is to avoid using a second program like STATA to do basic data manipulation and investigation.</p>

<p>Thanks.</p>

<p>Python/IDLE 2.7.3<br>
Pandas 0.8.1<br>
Notepad++ 6.1.4 (UNICODE)<br>
Windows Vista SP2  </p>
";;0;;2012-07-29T07:44:51.610;67.0;11707586;2017-01-05T00:10:53.340;2017-01-05T00:10:53.340;;2336654.0;;1560238.0;;1;157;<python><pandas><options><display><column-width>;Python pandas, how to widen output display to see more columns?;96153.0
561;561;11708610.0;1.0;"<p><strong>How do I use scikit-learn to train a model on a large csv data (~75MB) without running into memory problems?</strong></p>

<p>I'm using IPython notebook as the programming environment, and pandas+sklearn packages to analyze data from kaggle's digit recognizer tutorial.</p>

<p>The data is available on the <a href=""http://www.kaggle.com/c/digit-recognizer/data"" rel=""nofollow noreferrer"">webpage</a> , link to <a href=""http://pastie.org/4351884"" rel=""nofollow noreferrer"">my code</a> , and here is the <a href=""http://pastie.org/4351918"" rel=""nofollow noreferrer"">error message</a>: </p>

<p><code>KNeighborsClassifier</code> is used for the prediction.</p>

<p><strong>Problem:</strong></p>

<blockquote>
  <p>""MemoryError"" occurs when loading large dataset using read_csv
  function. To bypass this problem temporarily, I have to restart the
  kernel, which then read_csv function successfully loads the file, but
  the same error occurs when I run the same cell again.</p>
</blockquote>

<p>When the <code>read_csv</code> function loads the file successfully, after making changes to the <code>dataframe</code>, I can pass the features and labels to the KNeighborsClassifier's fit() function. At this point, similar memory error occurs.</p>

<p><strong>I tried the following:</strong></p>

<p>Iterate through the CSV file in chunks, and fit the data accordingly, but the problem is that the predictive model is overwritten every time for a chunk of data.</p>

<p><strong>What do you think I can do to successfully train my model without running into memory problems?</strong></p>
";;3;;2012-07-29T06:11:30.140;4.0;11707805;2017-01-05T00:10:27.430;2017-01-05T00:10:27.430;;2336654.0;ji.;1253437.0;;1;12;<memory><pandas><machine-learning><scikit-learn><classification>;Scikit and Pandas: Fitting Large Data;7090.0
565;565;13593882.0;2.0;"<p>I have two largish (snippets provided) pandas <code>DateFrame</code>s with unequal dates as indexes that I wish to concat into one:</p>

<pre><code>           NAB.AX                                  CBA.AX
            Close    Volume                         Close    Volume
Date                                    Date
2009-06-05  36.51   4962900             2009-06-08  21.95         0
2009-06-04  36.79   5528800             2009-06-05  21.95   8917000
2009-06-03  36.80   5116500             2009-06-04  22.21  18723600
2009-06-02  36.33   5303700             2009-06-03  23.11  11643800
2009-06-01  36.16   5625500             2009-06-02  22.80  14249900
2009-05-29  35.14  13038600   --AND--   2009-06-01  22.52  11687200
2009-05-28  33.95   7917600             2009-05-29  22.02  22350700
2009-05-27  35.13   4701100             2009-05-28  21.63   9679800
2009-05-26  35.45   4572700             2009-05-27  21.74   9338200
2009-05-25  34.80   3652500             2009-05-26  21.64   8502900
</code></pre>

<p>Problem is, if I run this: </p>

<pre><code>keys = ['CBA.AX','NAB.AX']
mv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) 
</code></pre>

<p>the following DateFrame is produced:</p>

<pre><code>                                 CBA.AX          NAB.AX        
                              Close  Volume   Close  Volume
Date                                                      
2200-08-16 04:24:21.460041     NaN     NaN     NaN     NaN
2203-05-13 04:24:21.460041     NaN     NaN     NaN     NaN
2206-02-06 04:24:21.460041     NaN     NaN     NaN     NaN
2208-11-02 04:24:21.460041     NaN     NaN     NaN     NaN
2211-07-30 04:24:21.460041     NaN     NaN     NaN     NaN
2219-10-16 04:24:21.460041     NaN     NaN     NaN     NaN
2222-07-12 04:24:21.460041     NaN     NaN     NaN     NaN
2225-04-07 04:24:21.460041     NaN     NaN     NaN     NaN
2228-01-02 04:24:21.460041     NaN     NaN     NaN     NaN
2230-09-28 04:24:21.460041     NaN     NaN     NaN     NaN
2238-12-15 04:24:21.460041     NaN     NaN     NaN     NaN
</code></pre>

<p>Does anybody have any idea why this might be the case?</p>

<p>On another point: is there any python libraries around that pull data from yahoo and normalise it?</p>

<p>Cheers.</p>

<p>EDIT: For reference: </p>

<pre><code>data = {
'CBA.AX': &lt;class 'pandas.core.frame.DataFrame'&gt;
    DatetimeIndex: 2313 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00
    Data columns:
        Close     2313  non-null values
        Volume    2313  non-null values
    dtypes: float64(1), int64(1),

 'NAB.AX': &lt;class 'pandas.core.frame.DataFrame'&gt;
    DatetimeIndex: 2329 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00
    Data columns:
        Close     2329  non-null values
        Volume    2329  non-null values
    dtypes: float64(1), int64(1)
}
</code></pre>
";;4;;2012-07-30T02:26:41.173;5.0;11714768;2017-01-05T00:11:14.777;2017-01-05T00:11:14.777;;2336654.0;;515311.0;;1;11;<python><numpy><scipy><pandas><yahoo-finance>;concat pandas DataFrame along timeseries indexes;6001.0
568;568;;1.0;"<p>I often need to apply a function to the groups of a very large <code>DataFrame</code> (of mixed data types) and would like to take advantage of multiple cores.</p>

<p>I can create an iterator from the groups and use the multiprocessing module, but it is not efficient because every group and the results of the function must be pickled for messaging between processes.</p>

<p>Is there any way to avoid the pickling or even avoid the copying of the <code>DataFrame</code> completely? It looks like the shared memory functions of the multiprocessing modules are limited to <code>numpy</code> arrays. Are there any other options?</p>
";;7;;2012-07-30T20:08:57.407;17.0;11728836;2017-01-29T22:32:42.920;2017-01-29T22:32:42.920;;5741205.0;;1429196.0;;1;87;<python><pandas><multiprocessing><shared-memory>;Efficiently applying a function to a grouped pandas DataFrame in parallel;5986.0
589;589;11811425.0;5.0;"<p>I have a pandas dataframe object that looks like this:</p>

<pre><code>   one  two  three  four  five
0    1    2      3     4     5
1    1    1      1     1     1
</code></pre>

<p>I'd like to generate a list of lists objects where the first item is the column label and the remaining list values are the column data values:</p>

<pre><code>nested_list = [['one', 1, 1]
               ['two', 2, 1]
               ['three', 3, 1]
               ['four', 4, 1]
               ['five', 5, 1]]
</code></pre>

<p>How can I do this? Thanks for the help.</p>
";;0;;2012-08-04T19:25:34.133;6.0;11811392;2017-01-05T00:14:44.370;2017-01-05T00:14:44.370;;2336654.0;;1255817.0;;1;17;<python><pandas>;How to generate a list from a pandas DataFrame with the column name and column values?;19595.0
606;606;;4.0;"<p>I have a following <code>DataFrame</code>:</p>

<pre><code>from pandas import *
df = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})
</code></pre>

<p>It looks like this:</p>

<pre><code>    bar foo
0    1   a
1    2   b
2    3   c
</code></pre>

<p>Now I want to have something like:</p>

<pre><code>     bar
0    1 is a
1    2 is b
2    3 is c
</code></pre>

<p>How can I achieve this?
I tried the following:</p>

<pre><code>df['foo'] = '%s is %s' % (df['bar'], df['foo'])
</code></pre>

<p>but it gives me a wrong result:</p>

<pre><code>&gt;&gt;&gt;print df.ix[0]

bar                                                    a
foo    0    a
1    b
2    c
Name: bar is 0    1
1    2
2
Name: 0
</code></pre>

<p>Sorry for a dumb question, but this one <a href=""https://stackoverflow.com/questions/10972410/pandas-combine-two-columns-in-a-dataframe"">pandas: combine two columns in a DataFrame</a> wasn't helpful for me.</p>
";;0;;2012-08-08T05:57:33.220;11.0;11858472;2017-04-29T10:56:05.710;2017-05-23T12:34:36.100;;-1.0;;1583620.0;;1;41;<python><numpy><dataframe><pandas>;Pandas: Combine string and int columns;30814.0
608;608;11872393.0;8.0;"<p>Most operations in <code>pandas</code> can be accomplished with operator chaining (<code>groupby</code>, <code>aggregate</code>, <code>apply</code>, etc), but the only way I've found to filter rows is via normal bracket indexing</p>

<pre><code>df_filtered = df[df['column'] == value]
</code></pre>

<p>This is unappealing as it requires I assign <code>df</code> to a variable before being able to filter on its values.  Is there something more like the following?</p>

<pre><code>df_filtered = df.mask(lambda x: x['column'] == value)
</code></pre>
";;0;;2012-08-08T17:25:37.780;56.0;11869910;2017-07-26T03:35:33.333;;;;;128580.0;;1;141;<pandas>;pandas: filter rows of DataFrame with operator chaining;187196.0
614;614;11882354.0;2.0;"<p>I am working with survey data loaded from an h5-file as <code>hdf = pandas.HDFStore('Survey.h5')</code> through the pandas package. Within this <code>DataFrame</code>, all rows are the results of a single survey, whereas the columns are the answers for all questions within a single survey. </p>

<p>I am aiming to reduce this dataset to a smaller <code>DataFrame</code> including only the rows with a certain depicted answer on a certain question, i.e. with all the same value in this column. I am able to determine the index values of all rows with this condition, but I can't find how to <em>delete</em> this rows or make a new df with these rows only.</p>
";;0;;2012-08-09T10:15:27.970;8.0;11881165;2017-01-05T00:19:52.367;2017-01-05T00:19:52.367;;2336654.0;;698207.0;;1;20;<python><pandas><slice>;Slice Pandas DataFrame by Row;20437.0
634;634;11927922.0;2.0;"<p>I just started using pandas/matplotlib as a replacement for Excel to generate stacked bar charts.  I am running into an issue  </p>

<p>(1) there are only 5 colors in the default colormap, so if I have more than 5 categories then the colors repeat.  How can I specify more colors?  Ideally, a gradient with a start color and an end color, and a way to dynamically generate n colors in between?</p>

<p>(2) the colors are not very visually pleasing.  How do I specify a custom set of n colors?  Or, a gradient would also work.</p>

<p>An example which illustrates both of the above points is below:</p>

<pre><code>  4 from matplotlib import pyplot
  5 from pandas import *
  6 import random
  7 
  8 x = [{i:random.randint(1,5)} for i in range(10)]
  9 df = DataFrame(x)
 10 
 11 df.plot(kind='bar', stacked=True)
</code></pre>

<p>And the output is this:</p>

<p><img src=""https://i.stack.imgur.com/SC7g4.png"" alt=""enter image description here""></p>
";;0;;2012-08-13T03:02:57.673;9.0;11927715;2017-01-05T00:25:52.177;2017-01-05T00:25:52.177;;2336654.0;;1058521.0;;1;38;<python><matplotlib><pandas>;How to give a pandas/matplotlib bar graph custom colors;29388.0
638;638;11942697.0;3.0;"<p>I'm suspicious that this is trivial, but I yet to discover the incantation that will let me select rows from a Pandas <code>dataframe</code> based on the values of a hierarchical key. So, for example, imagine we have the following <code>dataframe</code>:</p>

<pre><code>import pandas
df = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],
                       'group2': ['c','c','d','d','d','e'],
                       'value1': [1.1,2,3,4,5,6],
                       'value2': [7.1,8,9,10,11,12]
})
df = df.set_index(['group1', 'group2'])
</code></pre>

<p>df looks as we would expect: </p>

<p><img src=""https://i.stack.imgur.com/VGwOg.png"" alt=""enter image description here""></p>

<p>If df were not indexed on group1 I could do the following:</p>

<pre><code>df['group1' == 'a']
</code></pre>

<p>But that fails on this dataframe with an index. So maybe I should think of this like a Pandas series with a hierarchical index:</p>

<pre><code>df['a','c']
</code></pre>

<p>Nope. That fails as well. </p>

<p>So how do I select out all the rows where:</p>

<ol>
<li>group1 == 'a'</li>
<li>group1 == 'a' &amp; group2 == 'c'</li>
<li>group2 == 'c'</li>
<li>group1 in ['a','b','c']</li>
</ol>
";;0;;2012-08-13T20:07:07.033;26.0;11941492;2017-01-05T00:26:27.120;2017-01-05T00:26:27.120;;2336654.0;;37751.0;;1;30;<python><ipython><pandas>;Selecting rows from a Pandas dataframe with a compound (hierarchical) index;25839.0
652;652;11982843.0;1.0;"<p>I would like to merge two <code>DataFrames</code>, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?</p>

<pre><code>In [441]: a=DataFrame(data={""col1"": [1,2,3], 'to_merge_on' : [1,3,4]}, index=[""a"",""b"",""c""])

In [442]: b=DataFrame(data={""col2"": [1,2,3], 'to_merge_on' : [1,3,5]})
In [443]: a
Out[443]: 
   col1  to_merge_on
a     1            1
b     2            3
c     3            4

In [444]: b
Out[444]: 
   col2  to_merge_on
0     1            1
1     2            3
2     3            5


In [445]: a.merge(b, how=""left"")
Out[445]: 
   col1  to_merge_on  col2
0     1            1     1
1     2            3     2
2     3            4   NaN

In [446]: _.index
Out[447]: Int64Index([0, 1, 2])
</code></pre>

<p>EDIT: Switched to example code that can be easily reproduced</p>
";;1;;2012-08-15T20:10:04.523;16.0;11976503;2017-01-05T00:34:18.360;2017-01-05T00:34:18.360;;2336654.0;;670525.0;;1;38;<python><pandas>;How to keep index when using pandas merge;12679.0
669;669;12022047.0;3.0;"<p>I have a textfile where columns are separated by variable amounts of whitespace. Is it possible to load this file directly as a pandas dataframe without pre-processing the file? In the <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#csv-text-files"">pandas documentation the delimiter section</a> says that I can use a <code>'s*'</code> construct but I couldn't get this to work. </p>

<pre><code>## sample data
head sample.txt

#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord
# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target
#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------
ABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region
ABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter
SMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain
SMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain
AAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain
YceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family
Pyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase

#load data
from pandas import *
data = read_table('sample.txt', skiprows=3, header=None, sep="" "")

ValueError: Expecting 83 columns, got 91 in row 4

#load data part 2
data = read_table('sample.txt', skiprows=3, header=None, sep=""'s*' "")
#this mushes some of the columns into the first column and drops the rest.
    X.1
1    ABC_tran PF00005.22 118 AAF67494.2_
2    SMC_N PF02463.14 220 AAF67494.2_
3    SMC_N PF02463.14 220 AAF67494.2_
4    AAA_16 PF13191.1 166 AAF67494.2_
5    YceG PF02618.11 297 AAF67495.1_
6    Pyr_redox_3 PF13738.1 203 AAF67496.2_
7    Pyr_redox_3 PF13738.1 203 AAF67496.2_
8    FMO-like PF00743.14 532 AAF67496.2_
9    FMO-like PF00743.14 532 AAF67496.2_
</code></pre>

<p>While I can preprocess the files to change the whitespace to commas/tabs it would be nice to load them directly.</p>

<p>(FYI this is the *.hmmdomtblout output from the <a href=""http://hmmer.janelia.org/"">hmmscan program</a>)</p>
";2017-05-22T06:35:29.307;7;;2012-08-18T19:45:34.630;3.0;12021730;2017-05-15T12:02:05.423;2017-03-10T19:52:59.947;;4370109.0;;983191.0;;1;21;<python><pandas>;Can pandas handle variable-length whitespace as column delimiters;8587.0
670;670;12022003.0;3.0;"<p>I have a Pandas Data Frame object that has 1000 rows and 10 columns. I would simply like to slice the Data Frame and take the first 10 rows. How can I do this? I've been trying to use this:</p>

<pre><code>&gt;&gt;&gt; df.shape
(1000,10)
&gt;&gt;&gt; my_slice = df.ix[10,:]
&gt;&gt;&gt; my_slice.shape
(10,)
</code></pre>

<p>Shouldn't my_slice be the first ten rows, ie. a 10 x 10 Data Frame? How can I get the first ten rows, such that <code>my_slice</code> is a 10x10 Data Frame object? Thanks.</p>
";;0;;2012-08-18T19:49:41.783;3.0;12021754;2012-09-09T19:21:23.347;;;;;1255817.0;;1;13;<pandas>;How to slice a Pandas Data Frame by position?;21442.0
684;684;12060886.0;12.0;"<p>Any help on this problem will be greatly appreciated. So basically I want to run a query to my SQL database and store the returned data as Pandas data structure. I have attached code for query. I am reading the documentation on Pandas, but I have problem to identify the return type of my query. I tried to print the query result, but it doesn't give any useful information. 
    Thanks!!!! </p>

<pre><code>from sqlalchemy import create_engine


engine2 = create_engine('mysql://THE DATABASE I AM ACCESSING')
connection2 = engine2.connect()
dataid = 1022
resoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = '%s'""%dataid)
</code></pre>

<p>So I sort of want to understand what's the format/datatype of my variable ""resoverall"" and how to put it with PANDAS data structure.</p>
";;3;;2012-08-21T01:02:02.380;39.0;12047193;2017-08-16T15:59:21.740;;;;;1613017.0;;1;43;<python><mysql><data-structures><pandas>;How to convert SQL Query result to PANDAS Data Structure?;69160.0
686;686;12184679.0;3.0;"<p>Now that pandas provides a data frame structure, is there any need for structured/record arrays in numpy? There are some modifications I need to make to an existing code which requires this structured array type framework, but I am considering using pandas in its place from this point forward. Will I at any point find that I need some functionality of structured/record arrays that pandas does not provide?</p>
";;1;;2012-08-21T09:37:29.400;;12052067;2013-07-09T18:14:44.640;;;;;143476.0;;1;13;<numpy><scipy><pandas>;If I use python pandas, is there any need for structured arrays?;2375.0
695;695;12065904.0;6.0;"<p>I have a Python pandas DataFrame <code>rpt</code>:</p>

<pre><code>rpt
&lt;class 'pandas.core.frame.DataFrame'&gt;
MultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')
Data columns:
STK_ID                    47518  non-null values
STK_Name                  47518  non-null values
RPT_Date                  47518  non-null values
sales                     47518  non-null values
</code></pre>

<p>I can filter the rows whose stock id is <code>'600809'</code> like this: <code>rpt[rpt['STK_ID'] == '600809']</code></p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
MultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')
Data columns:
STK_ID                    25  non-null values
STK_Name                  25  non-null values
RPT_Date                  25  non-null values
sales                     25  non-null values
</code></pre>

<p>and I want to get all the rows of some stocks together, such as <code>['600809','600141','600329']</code>. That means I want a syntax like this: </p>

<pre><code>stk_list = ['600809','600141','600329']

rst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas 
</code></pre>

<p>Since pandas not accept above command, how to achieve the target? </p>
";;0;;2012-08-22T03:16:56.393;83.0;12065885;2017-04-27T13:30:50.053;2017-04-27T13:28:09.233;;3730397.0;;1072888.0;;1;223;<python><pandas><dataframe>;Filter dataframe rows if value in column is in a set list of values;129843.0
709;709;12098586.0;1.0;"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in"">how to filter the dataframe rows of pandas by &ldquo;within&rdquo;/&ldquo;in&rdquo;?</a>  </p>
</blockquote>



<p>Lets say I have the following pandas dataframe:</p>

<pre><code>df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})
df

     A   B
0    5   1
1    6   2
2    3   3
3    4   5
</code></pre>

<p>I can subset based on a specific value:</p>

<pre><code>x = df[df['A'] == 3]
x

     A   B
2    3   3
</code></pre>

<p>But how can I subset based on a list of values? - something like this:</p>

<pre><code>list_of_values = [3,6]

y = df[df['A'] in list_of_values]
</code></pre>
";2012-08-24T10:29:42.517;1;;2012-08-23T16:31:12.237;62.0;12096252;2012-08-23T19:20:12.243;2017-05-23T10:31:37.830;;-1.0;;983191.0;;1;241;<python><pandas>;use a list of values to select rows from a pandas dataframe;178009.0
712;712;;2.0;"<p><code>pandas</code> offers the ability to look up by lists of row and column indices,</p>

<pre><code>In [49]: index = ['a', 'b', 'c', 'd']

In [50]: columns = ['one', 'two', 'three', 'four']

In [51]: M = pandas.DataFrame(np.random.randn(4,4), index=index, columns=columns)

In [52]: M
Out[52]: 
        one       two     three      four
a -0.785841 -0.538572  0.376594  1.316647
b  0.530288 -0.975547  1.063946 -1.049940
c -0.794447 -0.886721  1.794326 -0.714834
d -0.158371  0.069357 -1.003039 -0.807431

In [53]: M.lookup(index, columns) # diagonal entries
Out[53]: array([-0.78584142, -0.97554698,  1.79432641, -0.8074308 ])
</code></pre>

<p>I would like to use this same method of indexing to set <code>M</code>'s elements.  How can I do this?</p>
";;0;;2012-08-23T21:50:36.047;3.0;12100497;2015-11-17T16:57:30.187;;;;;128580.0;;1;16;<python><pandas>;pandas: set values with (row, col) indices;25380.0
715;715;12117333.0;2.0;"<p>I have a #-separated file with three columns: the first is integer, the second looks like a float, but isn't, and the third is a string.  I attempt to load this directly into python with <code>pandas.read_csv</code></p>

<pre><code>In [149]: d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])

In [150]: d
Out[150]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1673 entries, 0 to 1672
Data columns:
int_field          1673  non-null values
floatlike_field    1673  non-null values
str_field          1673  non-null values
dtypes: float64(1), int64(1), object(1)
</code></pre>

<p><code>pandas</code> tries to be smart and automatically convert fields to a useful type.  The issue is that I don't actually want it to do so (if I did, I'd used the <code>converters</code> argument).  How can I prevent <code>pandas</code> from converting types automatically?</p>
";;5;;2012-08-23T22:54:04.330;6.0;12101113;2017-06-18T16:18:26.777;2017-06-18T16:18:26.777;;1033581.0;;128580.0;;1;12;<python><pandas>;Prevent pandas from automatically inferring type in read_csv;5964.0
721;721;12133235.0;1.0;"<p>I am trying to figure out how to sort a results of pandas.DataFrame.groupby.mean() in a smart way.</p>

<p>I generate an aggregation of my DataFrame like this:</p>

<pre><code>means = df.testColumn.groupby(df.testCategory).mean()
</code></pre>

<p>I now try to sort this by value, but get an error:</p>

<pre><code>means.sort()
...
-&gt; Exception: This Series is a view of some other array, to sort in-place you must create a copy
</code></pre>

<p>I then try creating a copy:</p>

<pre><code>meansCopy = Series(means)
meansCopy.sort()
-&gt; Exception: This Series is a view of some other array, to sort in-place you must create a copy
</code></pre>

<p>How can I get this sort working?</p>
";;0;;2012-08-26T19:10:01.447;7.0;12133075;2012-08-26T19:31:37.803;;;;;1058521.0;;1;19;<python><pandas>;pandas - how to sort the result of DataFrame.groupby.mean()?;5843.0
730;730;12152759.0;2.0;"<p>In R, there is a rather useful <code>replace</code> function.
Essentially, it does conditional re-assignment in a given column of a data frame.
It can be used as so:
<code>replace(df$column, df$column==1,'Type 1');</code></p>

<p>What is a good way to achieve the same in pandas?</p>

<p>Should I use a lambda with <code>apply</code>? (If so, how do I get a reference to the given column, as opposed to a whole row).</p>

<p>Should I use <code>np.where</code> on <code>data_frame.values</code>?
It seems like I am missing a very obvious thing here.</p>

<p>Any suggestions are appreciated.</p>
";;0;;2012-08-28T04:18:11.200;6.0;12152716;2017-01-31T03:04:51.180;;;;;1414230.0;;1;20;<python><pandas><equivalent>;Python pandas equivalent for replace;20843.0
735;735;12168857.0;3.0;"<p>The index that I have in the dataframe (with 30 rows) is of the form:</p>

<pre><code>Int64Index([171, 174,173, 172, 199..............
        ....175, 200])
</code></pre>

<p>The index is not strictly increasing because the data frame is the output of a sort().
I want to have add a column which is the series:</p>

<pre><code>[1, 2, 3, 4, 5......................., 30]
</code></pre>

<p>How should I go about doing that?</p>

<p>Thanks.</p>
";;0;;2012-08-28T22:50:21.203;3.0;12168648;2013-10-13T18:57:05.170;;;;;968643.0;;1;12;<python><indexing><dataframe><pandas>;Pandas (python): How to add column to dataframe for index?;37697.0
737;737;12169357.0;1.0;"<p>I have a dataframe with columns <code>A</code>,<code>B</code>. I need to create a column <code>C</code> such that for every record / row:</p>

<p><code>C = max(A, B)</code>.</p>

<p>How should I go about doing this?</p>

<p>Thanks.</p>
";;0;;2012-08-28T23:58:47.137;4.0;12169170;2016-12-04T19:06:43.250;;;;;968643.0;;1;13;<python><dataframe><pandas>;How should I take the max of 2 columns in a dataframe and make it another column?;11405.0
742;742;12183507.0;3.0;"<p>I want to apply a function with arguments to a series in python pandas:</p>

<pre><code>x = my_series.apply(my_function, more_arguments_1)
y = my_series.apply(my_function, more_arguments_2)
...
</code></pre>

<p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html"" rel=""nofollow noreferrer"">documentation</a> describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?</p>
";;1;;2012-08-29T16:46:39.007;18.0;12182744;2017-07-24T11:46:09.757;2017-07-24T11:46:09.757;;6656269.0;;660664.0;;1;49;<python><pandas><apply>;python pandas: apply a function with arguments to a series;45364.0
757;757;12192021.0;5.0;"<p>I'm trying to read a fairly large CSV file with Pandas and split it up into two random chunks, one of which being 10% of the data and the other being 90%.</p>

<p>Here's my current attempt:</p>

<pre><code>rows = data.index
row_count = len(rows)
random.shuffle(list(rows))

data.reindex(rows)

training_data = data[row_count // 10:]
testing_data = data[:row_count // 10]
</code></pre>

<p>For some reason, <code>sklearn</code> throws this error when I try to use one of these resulting DataFrame objects inside of a SVM classifier:</p>

<pre><code>IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis
</code></pre>

<p>I think I'm doing it wrong. Is there a better way to do this?</p>
";2016-10-30T19:11:56.443;1;;2012-08-30T06:12:46.203;24.0;12190874;2016-04-26T15:52:39.280;;;;;464744.0;;1;59;<python><partitioning><pandas>;Pandas: Sampling a DataFrame;57789.0
765;765;12201723.0;5.0;"<p>I have the following data frame in IPython, where each row is a single stock:</p>

<pre><code>In [261]: bdata
Out[261]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 21210 entries, 0 to 21209
Data columns:
BloombergTicker      21206  non-null values
Company              21210  non-null values
Country              21210  non-null values
MarketCap            21210  non-null values
PriceReturn          21210  non-null values
SEDOL                21210  non-null values
yearmonth            21210  non-null values
dtypes: float64(2), int64(1), object(4)
</code></pre>

<p>I want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the ""yearmonth"" column.</p>

<p>This works as expected:</p>

<pre><code>In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())
Out[262]:
yearmonth
201204      -0.109444
201205      -0.290546
</code></pre>

<p>But then I want to sort of ""broadcast"" these values back to the indices in the original data frame, and save them as constant columns where the dates match.</p>

<pre><code>In [263]: dateGrps = bdata.groupby(""yearmonth"")

In [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/&lt;ipython-input-264-4a68c8782426&gt; in &lt;module&gt;()
----&gt; 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())

TypeError: 'DataFrameGroupBy' object does not support item assignment
</code></pre>

<p>I realize this naive assignment should not work. But what is the ""right"" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?</p>

<p>In the end, I want a column called ""MarketReturn"" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.</p>

<p>One hack to achieve this would be the following:</p>

<pre><code>marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())

bdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))

for elem in marketRetsByDate.index.values:
    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]
</code></pre>

<p>But this is slow, bad, and unPythonic.</p>
";;3;;2012-08-30T15:45:26.410;18.0;12200693;2016-08-19T00:08:08.797;2012-08-30T16:22:59.653;;567620.0;;567620.0;;1;34;<python><group-by><dataframe><pandas>;Python Pandas How to assign groupby operation results back to columns in parent dataframe?;28220.0
769;769;12204428.0;1.0;"<p>Very weird bug here: I'm using pandas to merge several dataframes.  As part of the merge, I have to call reset_index several times.  But when I do, it crashes unexpectedly on the second or third use of reset_index.</p>

<p>Here's minimal code to reproduce the error:</p>

<pre><code>import pandas
A = pandas.DataFrame({
    'val' :  ['aaaaa', 'acaca', 'ddddd', 'zzzzz'],
    'extra' : range(10,14),
})
A = A.reset_index()
A = A.reset_index()
A = A.reset_index()
</code></pre>

<p>Here's the relevant part of the traceback:</p>

<pre><code>....
    A = A.reset_index()
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 2393, in reset_index
    new_obj.insert(0, name, _maybe_cast(self.index.values))
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 1787, in insert
    self._data.insert(loc, column, value)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/internals.py"", line 893, in insert
    raise Exception('cannot insert %s, already exists' % item)
Exception: cannot insert level_0, already exists
</code></pre>

<p>Any idea what's going wrong here?  How do I work around it?</p>
";;0;;2012-08-30T19:15:56.350;;12203901;2012-08-30T19:52:44.037;;;;;660664.0;;1;12;<python><pandas>;pandas crashes on repeated DataFrame.reset_index();5744.0
771;771;12207352.0;2.0;"<p>One last newbie pandas question for the day:  How do I generate a table for a single Series?</p>

<p>For example:</p>

<pre><code>my_series = pandas.Series([1,2,2,3,3,3])
pandas.magical_frequency_function( my_series )

&gt;&gt; {
     1 : 1,
     2 : 2, 
     3 : 3
   }
</code></pre>

<p>Lots of googling has led me to Series.describe() and pandas.crosstabs, but neither of these does quite what I need: one variable, counts by categories.  Oh, and it'd be nice if it worked for different data types: strings, ints, etc.</p>
";;0;;2012-08-31T00:10:12.253;13.0;12207326;2017-01-15T21:04:32.933;2017-01-15T21:04:32.933;;2285236.0;;660664.0;;1;60;<python><statistics><pandas><frequency>;Frequency table for a single variable;54196.0
788;788;12250416.0;2.0;"<p>I'm currently using pandas to read an Excel file and present its sheet names to the user, so he can select which sheet he would like to use. The problem is that the files are really big (70 columns x 65k rows), taking up to 14s to load on a notebook (the same data in a CSV file is taking 3s).</p>

<p>My code in panda goes like this:</p>

<pre><code>xls = pandas.ExcelFile(path)
sheets = xls.sheet_names
</code></pre>

<p>I tried xlrd before, but obtained similar results. This was my code with xlrd:</p>

<pre><code>xls = xlrd.open_workbook(path)
sheets = xls.sheet_names
</code></pre>

<p>So, can anybody suggest a faster way to retrieve the sheet names from an Excel file than reading the whole file?</p>
";;2;;2012-09-03T14:44:51.630;7.0;12250024;2017-06-07T08:39:30.267;2016-01-26T19:52:37.443;;1677912.0;;1643926.0;;1;19;<python><excel><pandas><xlrd>;How to obtain sheet names from XLS files without loading the whole file?;16981.0
807;807;;5.0;"<p>I think the title covers the issue, but to elucidate:</p>

<p>The <a href=""http://pandas.pydata.org"" rel=""noreferrer"">pandas</a> python package has a DataFrame data type for holding table data in python. It also has a convenient interface to the <a href=""http://www.hdfgroup.org/HDF5/"" rel=""noreferrer"">hdf5</a> file format, so pandas DataFrames (and other data) can be saved using a simple dict-like interface (assuming you have <a href=""http://pytables.org"" rel=""noreferrer"">pytables</a> installed)</p>

<pre><code>import pandas 
import numpy
d = pandas.HDFStore('data.h5')
d['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})
d.close()
</code></pre>

<p>So far so good. However, if I then try to load that same hdf5 into R I see things aren't so simple:</p>

<pre><code>&gt; library(hdf5)
&gt; hdf5load('data.h5')
NULL
&gt; testdata
$block0_values
         [,1]      [,2]      [,3]       [,4]      [,5]
[1,] 1.498147 0.8843877 -1.081656 0.08717049 -1.302641
attr(,""CLASS"")
[1] ""ARRAY""
attr(,""VERSION"")
[1] ""2.3""
attr(,""TITLE"")
[1] """"
attr(,""FLAVOR"")
[1] ""numpy""

$block0_items
[1] ""N""
attr(,""CLASS"")
[1] ""ARRAY""
attr(,""VERSION"")
[1] ""2.3""
attr(,""TITLE"")
[1] """"
attr(,""FLAVOR"")
[1] ""numpy""
attr(,""kind"")
[1] ""string""
attr(,""name"")
[1] ""N.""

$axis1
[1] 0 1 2 3 4
attr(,""CLASS"")
[1] ""ARRAY""
attr(,""VERSION"")
[1] ""2.3""
attr(,""TITLE"")
[1] """"
attr(,""FLAVOR"")
[1] ""numpy""
attr(,""kind"")
[1] ""integer""
attr(,""name"")
[1] ""N.""

$axis0
[1] ""N""
attr(,""CLASS"")
[1] ""ARRAY""
attr(,""VERSION"")
[1] ""2.3""
attr(,""TITLE"")
[1] """"
attr(,""FLAVOR"")
[1] ""numpy""
attr(,""kind"")
[1] ""string""
attr(,""name"")
[1] ""N.""

attr(,""TITLE"")
[1] """"
attr(,""CLASS"")
[1] ""GROUP""
attr(,""VERSION"")
[1] ""1.0""
attr(,""ndim"")
[1] 2
attr(,""axis0_variety"")
[1] ""regular""
attr(,""axis1_variety"")
[1] ""regular""
attr(,""nblocks"")
[1] 1
attr(,""block0_items_variety"")
[1] ""regular""
attr(,""pandas_type"")
[1] ""frame""
</code></pre>

<p>Which brings me to my question: ideally I would be able to save back and forth from R to pandas. I can obviously write a wrapper from pandas to R (I think... though I think if I use a pandas <a href=""http://pandas.sourceforge.net/indexing.html"" rel=""noreferrer"">MultiIndex</a> that might become trickier), but I don't think I can easily then use that data back in pandas. Any suggestions?</p>

<p>Bonus: what I <em>really</em> want to do is use the <a href=""http://datatable.r-forge.r-project.org/"" rel=""noreferrer"">data.table</a> package in R with a pandas dataframe (the keying approach is suspiciously similar in both packages). Any help on that one greatly appreciated.</p>
";;3;;2012-09-05T09:29:34.570;7.0;12278347;2016-09-30T15:06:28.383;;;;;678486.0;;1;13;<python><r><pandas><data.table><hdf5>;How can I efficiently save a python pandas dataframe in hdf5 and open it as a dataframe in R?;10677.0
810;810;12286958.0;3.0;"<p>I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. </p>

<pre><code>import numpy as np 
from pandas import *

Index= ['aaa','bbb','ccc','ddd','eee']
Cols = ['A', 'B', 'C','D']
df = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)

&gt;&gt;&gt; df
          A         B         C         D
aaa  2.431645  1.248688  0.267648  0.613826
bbb  0.809296  1.671020  1.564420  0.347662
ccc  1.501939  1.126518  0.702019  1.596048
ddd  0.137160  0.147368  1.504663  0.202822
eee  0.134540  3.708104  0.309097  1.641090
&gt;&gt;&gt; 
</code></pre>
";;1;;2012-09-05T17:18:21.077;25.0;12286607;2017-08-23T12:29:16.150;;;;;1649335.0;;1;44;<python><pandas><heatmap>;python Making heatmap from DataFrame;34778.0
819;819;12307162.0;3.0;"<p>Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?</p>

<p>I tried the following</p>

<pre><code>df['A'==0]['B'] = np.nan
</code></pre>

<p>and</p>

<pre><code>df['A'==0]['B'].values.fill(np.nan)
</code></pre>

<p>without success.</p>
";;0;;2012-09-06T19:32:25.827;37.0;12307099;2017-07-04T20:27:54.927;;;;;645212.0;;1;82;<python><pandas>;Modifying a subset of rows in a pandas dataframe;48481.0
822;822;12326113.0;3.0;"<p>kdb+ has an <a href=""http://code.kx.com/wiki/Reference/aj"">aj</a> function that is usually used to join tables along time columns.</p>

<p>Here is an example where I have trade and quote tables and I get the prevailing quote for every trade.</p>

<pre><code>q)5# t
time         sym  price size 
-----------------------------
09:30:00.439 NVDA 13.42 60511
09:30:00.439 NVDA 13.42 60511
09:30:02.332 NVDA 13.42 100  
09:30:02.332 NVDA 13.42 100  
09:30:02.333 NVDA 13.41 100  

q)5# q
time         sym  bid   ask   bsize asize
-----------------------------------------
09:30:00.026 NVDA 13.34 13.44 3     16   
09:30:00.043 NVDA 13.34 13.44 3     17   
09:30:00.121 NVDA 13.36 13.65 1     10   
09:30:00.386 NVDA 13.36 13.52 21    1    
09:30:00.440 NVDA 13.4  13.44 15    17

q)5# aj[`time; t; q]
time         sym  price size  bid   ask   bsize asize
-----------------------------------------------------
09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    
09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    
09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    
09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    
09:30:02.333 NVDA 13.41 100   13.34 13.51 1     1  
</code></pre>

<p>How can I do the same operation using pandas? I am working with trade and quote dataframes where the index is datetime64.</p>

<pre><code>In [55]: quotes.head()
Out[55]: 
                              bid    ask  bsize  asize
2012-09-06 09:30:00.026000  13.34  13.44      3     16
2012-09-06 09:30:00.043000  13.34  13.44      3     17
2012-09-06 09:30:00.121000  13.36  13.65      1     10
2012-09-06 09:30:00.386000  13.36  13.52     21      1
2012-09-06 09:30:00.440000  13.40  13.44     15     17

In [56]: trades.head()
Out[56]: 
                            price   size
2012-09-06 09:30:00.439000  13.42  60511
2012-09-06 09:30:00.439000  13.42  60511
2012-09-06 09:30:02.332000  13.42    100
2012-09-06 09:30:02.332000  13.42    100
2012-09-06 09:30:02.333000  13.41    100
</code></pre>

<p>I see that pandas has an asof function but that is not defined on the DataFrame, only on the Series object. I guess one could loop through each of the Series and align them one by one, but I am wondering if there is a better way?</p>
";;1;;2012-09-07T16:49:21.490;9.0;12322289;2016-10-03T19:29:05.320;2012-09-07T17:01:30.053;;220120.0;;220120.0;;1;11;<python><join><time-series><pandas><kdb>;KDB+ like asof join for timeseries data in pandas?;4799.0
823;823;12323599.0;2.0;"<p>I have a DataFrame that has duplicated rows. I'd like to get a DataFrame with a unique index and no duplicates. It's ok to discard the duplicated values. Is this possible? Would it be a done by <code>groupby</code>?</p>
";;0;;2012-09-07T17:30:38.933;6.0;12322779;2013-08-12T21:29:59.070;2013-08-12T21:29:59.070;;283296.0;;656188.0;;1;22;<pandas>;Pandas: unique dataframe;26767.0
834;834;15658075.0;5.0;"<pre><code>&gt;&gt;&gt; df =DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})
&gt;&gt;&gt; df['x']=df.a + df.b
&gt;&gt;&gt; df['y']=df.a - df.b
&gt;&gt;&gt; df
   a  b   x  y
0  1  2   3 -1
1  2  4   6 -2
2  3  6   9 -3
3  4  8  12 -4
</code></pre>

<p>Now I want to rearrange the column sequence, which makes 'x','y' column to be the first &amp; second columns by :</p>

<pre><code>&gt;&gt;&gt; df = df[['x','y','a','b']]
&gt;&gt;&gt; df
    x  y  a  b
0   3 -1  1  2
1   6 -2  2  4
2   9 -3  3  6
3  12 -4  4  8
</code></pre>

<p>But if I have a long coulmns 'a','b','c','d'....., and I don't want to explictly list the columns. How can I do that ?</p>

<p>Or Does Pandas provide a function like <code>set_column_sequence(dataframe,col_name, seq)</code> so that I can do  :  <code>set_column_sequence(df,'x',0)</code> and <code>set_column_sequence(df,'y',1)</code> ?</p>
";;0;;2012-09-08T10:16:04.243;4.0;12329853;2015-11-03T17:53:25.870;;;;;1072888.0;;1;11;<python><pandas>;How to rearrange Pandas column sequence?;31812.0
852;852;12356541.0;1.0;"<p>I am working with the <a href=""http://pandas.pydata.org/"">pandas</a> library and I want to add two new columns to a dataframe <code>df</code> with n columns (n > 0).<br>
These new columns result from the application of a function to one of the columns in the dataframe.</p>

<p>The function to apply is like:</p>

<pre><code>def calculate(x):
    ...operate...
    return z, y
</code></pre>

<p>One method for creating a new column for a function returning only a value is:</p>

<pre><code>df['new_col']) = df['column_A'].map(a_function)
</code></pre>

<p>So, what I want, and tried unsuccesfully (*), is something like:</p>

<pre><code>(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)
</code></pre>

<p>What the best way to accomplish this could be ? I scanned the <a href=""http://pandas.pydata.org/pandas-docs/stable/"">documentation</a> with no clue. </p>

<p>*<em><code>df['column_A'].map(calculate)</code> returns a panda Series each item consisting of a tuple z, y. And trying to assign this to two dataframe columns produces a ValueError.</em> </p>
";;0;;2012-09-10T17:17:38.683;39.0;12356501;2012-09-10T17:20:49.987;;;;;308903.0;;1;68;<python><pandas>;Pandas: create two new columns in a dataframe with values calculated from a pre-existing column;45846.0
854;854;;3.0;"<p>How do I order columns according to the values of the last row? In the example below, my final df will have columns in the following order: 'ddd' 'aaa' 'ppp' 'fff'.</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(np.random.randn(10, 4), columns=['ddd', 'fff', 'aaa', 'ppp'])
&gt;&gt;&gt; df
        ddd       fff       aaa       ppp
0 -0.177438  0.102561 -1.318710  1.321252
1  0.980348  0.786721  0.374506 -1.411019
2  0.405112  0.514216  1.761983 -0.529482
3  1.659710 -1.017048 -0.737615 -0.388145
4 -0.472223  1.407655 -0.129119 -0.912974
5  1.221324 -0.656599  0.563152 -0.900710
6 -1.816420 -2.898094 -0.232047 -0.648904
7  2.793261  0.568760 -0.850100  0.654704
8 -2.180891  2.054178 -1.050897 -1.461458
9 -1.123756  1.245987 -0.239863  0.359759
</code></pre>
";;0;;2012-09-10T19:39:24.793;5.0;12358360;2016-12-27T18:51:07.303;;;;;1649335.0;;1;13;<python><sorting><pandas>;Python pandas order column according to the values in a row;6706.0
869;869;12377083.0;4.0;"<p>I have an OHLC price data set, that I have parsed from CSV into a Pandas dataframe and resampled to 15 min bars:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00
Freq: 15T
Data columns:
Close    363152  non-null values
High     363152  non-null values
Low      363152  non-null values
Open     363152  non-null values
dtypes: float64(4)
</code></pre>

<p>I would like to add various calculated columns, starting with simple ones such as period Range (H-L) and then booleans to indicate the occurrence of price patterns that I will define - e.g. a hammer candle pattern, for which a sample definition:</p>

<pre><code>def closed_in_top_half_of_range(h,l,c):
    return c &gt; l + (h-1)/2

def lower_wick(o,l,c):
    return min(o,c)-l

def real_body(o,c):
    return abs(c-o)

def lower_wick_at_least_twice_real_body(o,l,c):
    return lower_wick(o,l,c) &gt;= 2 * real_body(o,c)

def is_hammer(row):
    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \
    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])
</code></pre>

<p>Basic problem: how do I map the function to the column, specifically where I would like to reference more than one other column or the whole row or whatever? </p>

<p><a href=""https://stackoverflow.com/questions/12356501/pandas-create-two-new-columns-in-a-dataframe-with-values-calculated-from-a-pre"">This post</a> deals with adding two calculated columns off of a single source column, which is close, but not quite it.</p>

<p>And slightly more advanced: for price patterns that are determined with reference to more than a single bar (T), how can I reference different rows (e.g. T-1, T-2 etc.) from within the function definition?</p>

<p>Many thanks in advance.</p>
";;0;;2012-09-11T19:48:28.923;19.0;12376863;2015-03-06T17:34:27.420;2017-05-23T11:47:11.080;;-1.0;;1583083.0;;1;51;<python><pandas>;Adding calculated column(s) to a dataframe in pandas;70320.0
880;880;12394122.0;1.0;"<p>I want to mark some quantiles in my data, and for each row of the DataFrame, I would like the entry in a new column called e.g. ""xtile"" to hold this value.</p>

<p>For example, suppose I create a data frame like this:</p>

<pre><code>import pandas, numpy as np
dfrm = pandas.DataFrame({'A':np.random.rand(100), 
                         'B':(50+np.random.randn(100)), 
                         'C':np.random.randint(low=0, high=3, size=(100,))})
</code></pre>

<p>And let's say I write my own function to compute the quintile of each element in an array. I have my own function for this, but for example just refer to scipy.stats.mstats.mquantile.</p>

<pre><code>import scipy.stats as st
def mark_quintiles(x, breakpoints):
    # Assume this is filled in, using st.mstats.mquantiles.
    # This returns an array the same shape as x, with an integer for which
    # breakpoint-bucket that entry of x falls into.
</code></pre>

<p>Now, the real question is how to use <code>transform</code> to add a new column to the data. Something like this:</p>

<pre><code>def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):
    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, 
                                              breaks)
    return dataFrame
</code></pre>

<p>And then:</p>

<pre><code>dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))
</code></pre>

<p>The problem is that the above code will not add the new column ""A_xtile"". It just returns my data frame unchanged. If I first add a column full of dummy values, like NaN, called ""A_xtile"", then it <em>does</em> successfully over-write this column to include the correct quintile markings.</p>

<p>But it is extremely inconvenient to have to first write in the column for anything like this that I may want to add on the fly.</p>

<p>Note that a simple <code>apply</code> will not work here, since it won't know how to make sense of the possibly differently-sized result arrays for each group.</p>
";;3;;2012-09-12T13:58:12.403;10.0;12389898;2013-08-09T22:28:15.277;2013-08-09T22:28:15.277;;567620.0;;567620.0;;1;22;<python><group-by><transform><dataframe><pandas>;Python Pandas: how to add a totally new column to a data frame inside of a groupby/transform operation;18845.0
888;888;12449785.0;3.0;"<p>I get a KeyError when I try to plot a slice of a pandas DataFrame column with datetimes in it. Does anybody know what could cause this?</p>

<p>I managed to reproduce the error in a little self contained example (which you can also view here: <a href=""http://nbviewer.ipython.org/3714142/"">http://nbviewer.ipython.org/3714142/</a>):</p>

<pre><code>import numpy as np
from pandas import DataFrame
import datetime
from pylab import *

test = DataFrame({'x' : [datetime.datetime(2012,9,10) + datetime.timedelta(n) for n in range(10)], 
                  'y' : range(10)})
</code></pre>

<p>Now if I plot:</p>

<pre><code>plot(test['x'][0:5])
</code></pre>

<p>there is not problem, but when I plot:</p>

<pre><code>plot(test['x'][5:10])
</code></pre>

<p>I get the KeyError below (and the error message is not very helpfull to me). This <strong>only happens with datetime</strong> columns, not with other columns (as far as I experienced). E.g. <code>plot(test['y'][5:10])</code> is not a problem.</p>

<p>Ther error message:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-7-aa076e3fc4e0&gt; in &lt;module&gt;()
----&gt; 1 plot(test['x'][5:10])

C:\Python27\lib\site-packages\matplotlib\pyplot.pyc in plot(*args, **kwargs)
   2456         ax.hold(hold)
   2457     try:
-&gt; 2458         ret = ax.plot(*args, **kwargs)
   2459         draw_if_interactive()
   2460     finally:

C:\Python27\lib\site-packages\matplotlib\axes.pyc in plot(self, *args, **kwargs)
   3846         lines = []
   3847 
-&gt; 3848         for line in self._get_lines(*args, **kwargs):
   3849             self.add_line(line)
   3850             lines.append(line)

C:\Python27\lib\site-packages\matplotlib\axes.pyc in _grab_next_args(self, *args, **kwargs)
    321                 return
    322             if len(remaining) &lt;= 3:
--&gt; 323                 for seg in self._plot_args(remaining, kwargs):
    324                     yield seg
    325                 return

C:\Python27\lib\site-packages\matplotlib\axes.pyc in _plot_args(self, tup, kwargs)
    298             x = np.arange(y.shape[0], dtype=float)
    299 
--&gt; 300         x, y = self._xy_from_xy(x, y)
    301 
    302         if self.command == 'plot':

C:\Python27\lib\site-packages\matplotlib\axes.pyc in _xy_from_xy(self, x, y)
    215         if self.axes.xaxis is not None and self.axes.yaxis is not None:
    216             bx = self.axes.xaxis.update_units(x)
--&gt; 217             by = self.axes.yaxis.update_units(y)
    218 
    219             if self.command!='plot':

C:\Python27\lib\site-packages\matplotlib\axis.pyc in update_units(self, data)
   1277         neednew = self.converter!=converter
   1278         self.converter = converter
-&gt; 1279         default = self.converter.default_units(data, self)
   1280         #print 'update units: default=%s, units=%s'%(default, self.units)
   1281         if default is not None and self.units is None:

C:\Python27\lib\site-packages\matplotlib\dates.pyc in default_units(x, axis)
   1153         'Return the tzinfo instance of *x* or of its first element, or None'
   1154         try:
-&gt; 1155             x = x[0]
   1156         except (TypeError, IndexError):
   1157             pass

C:\Python27\lib\site-packages\pandas\core\series.pyc in __getitem__(self, key)
    374     def __getitem__(self, key):
    375         try:
--&gt; 376             return self.index.get_value(self, key)
    377         except InvalidIndexError:
    378             pass

C:\Python27\lib\site-packages\pandas\core\index.pyc in get_value(self, series, key)
    529         """"""
    530         try:
--&gt; 531             return self._engine.get_value(series, key)
    532         except KeyError, e1:
    533             if len(self) &gt; 0 and self.inferred_type == 'integer':

C:\Python27\lib\site-packages\pandas\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\src\engines.c:1479)()

C:\Python27\lib\site-packages\pandas\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\src\engines.c:1374)()

C:\Python27\lib\site-packages\pandas\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\src\engines.c:2498)()

C:\Python27\lib\site-packages\pandas\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\src\engines.c:2460)()

KeyError: 0
</code></pre>
";;1;;2012-09-13T12:23:23.853;3.0;12406162;2016-05-02T19:51:03.113;2012-09-14T08:09:11.440;;653364.0;;653364.0;;1;16;<numpy><matplotlib><pandas>;KeyError when plotting a sliced pandas dataframe with datetimes;10015.0
910;910;12433236.0;4.0;"<p>Is there a way to automatically download historical prices of stocks from yahoo finance or google finance (csv format)? Preferably in Python.</p>
";;1;;2012-09-14T23:06:15.723;42.0;12433076;2017-06-09T07:59:18.630;2016-07-27T22:32:32.900;;5741205.0;;464277.0;;1;34;<pandas><finance><yahoo-finance><google-finance><stockquotes>;Download history stock prices automatically from yahoo finance in python;81673.0
914;914;12975518.0;9.0;"<p>I would like to install Python Pandas library (0.8.1) on Mac OS X 10.6.8. This library needs Numpy>=1.6.</p>

<p>I tried this</p>

<pre><code>$ sudo easy_install pandas
Searching for pandas
Reading http://pypi.python.org/simple/pandas/
Reading http://pandas.pydata.org
Reading http://pandas.sourceforge.net
Best match: pandas 0.8.1
Downloading http://pypi.python.org/packages/source/p/pandas/pandas-0.8.1.zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74
Processing pandas-0.8.1.zip
Running pandas-0.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-0.8.1/egg-dist-tmp-0mlL7t
error: Setup script exited with pandas requires NumPy &gt;= 1.6 due to datetime64 dependency
</code></pre>

<p>So I tried to install Numpy</p>

<pre><code>$ sudo easy_install numpy
Searching for numpy
Best match: numpy 1.6.2
Adding numpy 1.6.2 to easy-install.pth file

Using /Library/Python/2.6/site-packages
Processing dependencies for numpy
Finished processing dependencies for numpy
</code></pre>

<p>So I tried again</p>

<pre><code>$ sudo easy_install pandas
</code></pre>

<p>But the problem is still the same !</p>

<pre><code>error: Setup script exited with pandas requires NumPy &gt;= 1.6 due to datetime64 dependency
</code></pre>

<p>I run Python </p>

<pre><code>$ python
Python 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) 
[GCC 4.2.1 (Apple Inc. build 5646)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.__version__
'1.2.1'
</code></pre>

<p>So Numpy 1.6 doesn't seems to be installed correctly !</p>

<p>I tried to install Numpy 1.6 with <code>pip</code> (instead of <code>easy_install</code>)...</p>

<pre><code>$ sudo pip install numpy
Requirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/2.6/site-packages
Cleaning up...
</code></pre>

<p>I added <code>--upgrade</code> flag</p>

<pre><code>$ sudo pip install numpy --upgrade
Requirement already up-to-date: numpy in /Library/Python/2.6/site-packages
Cleaning up...

$ sudo pip install pandas
Downloading/unpacking pandas
  Downloading pandas-0.8.1.zip (1.9MB): 1.9MB downloaded
  Running setup.py egg_info for package pandas
    pandas requires NumPy &gt;= 1.6 due to datetime64 dependency
    Complete output from command python setup.py egg_info:
    pandas requires NumPy &gt;= 1.6 due to datetime64 dependency

----------------------------------------
Command python setup.py egg_info failed with error code 1 in /tmp/pip-build/pandas
Storing complete log in /Users/MyUsername/Library/Logs/pip.log
</code></pre>

<p>I also tried to install binary version of Numpy <a href=""http://sourceforge.net/projects/numpy/files/"" rel=""noreferrer"">http://sourceforge.net/projects/numpy/files/</a>
numpy-1.6.2-py2.6-python.org-macosx10.3.dmg but it fails !!! (installer said me that numpy 1.6.2 can't be install on this disk. Numpy requires python.org Python 2.6 to install.</p>
";;7;;2012-09-15T11:30:35.430;7.0;12436979;2016-04-26T14:21:36.493;2012-09-15T12:24:36.210;;1609077.0;;1609077.0;;1;17;<python><numpy><pip><pandas><easy-install>;How to fix Python Numpy/Pandas installation?;65685.0
930;930;13059751.0;6.0;"<p>I have a dataframe with repeat values in column A.  I want to drop duplicates, keeping the row with the highest value in column B.</p>

<p>So this:</p>

<pre><code>A B
1 10
1 20
2 30
2 40
3 10
</code></pre>

<p>Should turn into this:</p>

<pre><code>A B
1 20
2 40
3 10
</code></pre>

<p>Wes has added some nice functionality to drop duplicates: <a href=""http://wesmckinney.com/blog/?p=340"" rel=""noreferrer"">http://wesmckinney.com/blog/?p=340</a>.  But AFAICT, it's designed for exact duplicates, so there's no mention of criteria for selecting which rows get kept.</p>

<p>I'm guessing there's probably an easy way to do this---maybe as easy as sorting the dataframe before dropping duplicates---but I don't know groupby's internal logic well enough to figure it out.  Any suggestions?</p>
";;1;;2012-09-19T15:01:32.647;17.0;12497402;2017-08-09T16:24:35.543;;;;;660664.0;;1;43;<python><duplicates><pandas>;python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B;38793.0
935;935;12504527.0;1.0;"<p>I'm trying to do what I think is a straight froward operation in pandas but I can't seem to make it work.</p>

<p>I have two pandas Series with different numbers of indices, I would like to add values together if they share an index, otherwise I would just like to pass the values that don't have corresponding indices along.</p>

<p>For example</p>

<pre><code>Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])
Sr2 = pd.Series([5,6], index = ['A', 'C'])
Sr1        Sr2
A     1    A     5
B     2    C     6
C     3
D     4
</code></pre>

<p><code>Sr1 + Sr2</code> or <code>Sr1.add(Sr2)</code> give</p>

<pre><code>A     6
B   NaN
C     9
D   NaN
</code></pre>

<p>But what I want is</p>

<pre><code>A     6
B     2
C     9
D     4
</code></pre>

<p>where the <code>B</code> and <code>D</code> values for <code>Sr1</code> are just passed along.</p>

<p>Any suggestions?</p>
";;0;;2012-09-20T00:02:13.150;3.0;12504493;2012-09-20T00:09:18.543;;;;;1473531.0;;1;14;<python><pandas>;Adding pandas Series with different indices without getting NaNs;5551.0
937;937;;4.0;"<p>It would be useful to save the session variables which could be loaded easily into memory at a later stage.</p>
";;2;;2012-09-20T01:20:19.553;7.0;12504951;2017-03-20T12:01:00.283;2015-08-13T22:30:22.317;;1832942.0;;968643.0;;1;17;<python><ipython><pandas>;Save session in IPython like in MATLAB?;8203.0
938;938;12505089.0;3.0;"<p>I have a column in a pandas DataFrame that I would like to split on a single space. The splitting is simple enough with <code>DataFrame.str.split(' ')</code>, but I can't make a new column from the last entry. When I <code>.str.split()</code> the column I get a list of arrays and I don't know how to manipulate this to get a new column for my DataFrame.</p>

<p>Here is an example. Each entry in the column contains 'symbol data price' and I would like to split off the price (and eventually remove the ""p""... or ""c"" in half the cases).</p>

<pre><code>import pandas as pd
temp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})
temp2 = temp.ticker.str.split(' ')
</code></pre>

<p>which yields</p>

<pre><code>0    ['spx', '5/25/2001', 'p500']
1    ['spx', '5/25/2001', 'p600']
2    ['spx', '5/25/2001', 'p700']
</code></pre>

<p>But <code>temp2[0]</code> just gives one list entry's array and <code>temp2[:][-1]</code> fails. How can I convert the last entry in each array to a new column? Thanks!</p>
";;0;;2012-09-20T01:24:57.557;16.0;12504976;2017-07-07T17:52:33.223;2015-09-21T18:06:26.493;;325565.0;;334755.0;;1;26;<python><string><pandas><split>;"Get last ""column"" after .str.split() operation on column in pandas DataFrame";14984.0
954;954;12525836.0;4.0;"<p>Suppose I have a pandas data frame df: </p>

<p>I want to calculate the column wise mean of a data frame, </p>

<p>This is easy: </p>

<pre><code>df.apply(average) 
</code></pre>

<p>then the column wise range max(col) - min (col). this is easy again: </p>

<pre><code>df.apply(max) - df.apply(min)
</code></pre>

<p>Now for each element I want to subtract its columns mean and divide by its columns range. 
I am not sure how to do that</p>

<p>Any help/pointers much appreciated. </p>
";;0;;2012-09-21T07:04:23.710;17.0;12525722;2017-05-05T18:27:26.403;2016-09-01T15:09:50.440;;100297.0;;218900.0;;1;64;<python><pandas><numpy>;Normalize data in pandas;84877.0
964;964;12555510.0;18.0;"<p>I have the following indexed DataFrame with named columns and rows not- continuous numbers:</p>

<pre><code>          a         b         c         d
2  0.671399  0.101208 -0.181532  0.241273
3  0.446172 -0.243316  0.051767  1.577318
5  0.614758  0.075793 -0.451460 -0.012493
</code></pre>

<p>I would like to add a new column, <code>'e'</code>, to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). </p>

<pre><code>0   -0.335485
1   -1.166658
2   -0.385571
dtype: float64
</code></pre>

<p>I tried different versions of <code>join</code>, <code>append</code>, <code>merge</code>, but I did not get the result I wanted, only errors at most. How can I add column <code>e</code> to the above example? </p>
";;0;;2012-09-23T19:00:01.430;107.0;12555323;2017-06-21T19:35:33.710;2017-06-21T19:35:33.710;;5609221.0;;1661173.0;;1;350;<python><pandas><dataframe>;Adding new column to existing DataFrame in Python pandas;578606.0
967;967;12570410.0;2.0;"<p>I have a dataframe in python pandas with several columns taken from a CSV file.</p>

<p>For instance, data =:</p>

<pre><code>Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3
1   1    2    2    3    1    2
2   2    2    3    5    4    2
</code></pre>

<p>And what I need is to get the sum of all columns which name starts with P1... something like P1* with a wildcard.</p>

<p>Something like the following which gives an error:</p>

<blockquote>
  <p>P1Sum = data[""P1*""]</p>
</blockquote>

<p>Is there any why to do this with pandas?</p>
";;4;;2012-09-24T17:11:53.430;6.0;12569730;2016-09-01T18:20:15.370;2012-09-24T17:17:45.657;;865662.0;;865662.0;;1;31;<python><wildcard><pandas>;Sum all columns with a wildcard name search using Python Pandas;6662.0
977;977;13592901.0;2.0;"<p>Given the following (totally overkill) data frame example</p>

<pre><code>df = pandas.DataFrame({
                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], 
                       ""returns"":0.05*np.random.randn(10), 
                       ""dummy"":np.repeat(1,10) 
                      })
</code></pre>

<p>is there an existing built-in way to apply two different aggregating functions to the same column, without having to call <code>agg</code> multiple times? </p>

<p>The syntactically wrong, but intuitively right, way to do it would be:</p>

<pre><code># Assume `function1` and `function2` are defined for aggregating.
df.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})
</code></pre>

<p>Obviously, Python doesn't allow duplicate keys. Is there any other manner for expressing the input to <code>agg</code>? Perhaps a list of tuples <code>[(column, function)]</code> would work better, to allow multiple functions applied to the same column? But it seems like it only accepts a dictionary.</p>

<p>Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)</p>
";;0;;2012-09-25T19:05:26.550;8.0;12589481;2016-04-08T12:20:20.700;;;;;567620.0;;1;30;<python><aggregate><pandas>;Python Pandas: Multiple aggregations of the same column;11477.0
986;986;;1.0;"<p>I have a data frame with a column called <code>""Date""</code> and want all the values from this column to have the same value (the year only). Example:</p>

<pre><code>City     Date
Paris    01/04/2004
Lisbon   01/09/2004
Madrid   2004
Pekin    31/2004
</code></pre>

<p>What I want is:</p>

<pre><code>City     Date
Paris    2004
Lisbon   2004
Madrid   2004
Pekin    2004
</code></pre>

<p>Here is my code:</p>

<pre><code>fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')

#Here we import the individual sheets and clean the sheets    
years=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])

fr={}

header=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])

for year in years:
    # save every sheet in variable fr['1961'], fr['1962'] and so on
    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)
    fr[year].columns=header
    # drop the entire Legal status date column
    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)
    # drop every row where GUO Name is empty
    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])
    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])
</code></pre>

<p><em>It happens that in my DataFrames, called for example <code>fr['1961']</code> the values of <code>Date_of_incorporation</code> can be anything (strings, integer, and so on), so maybe it would be best to completely erase this column and then attach another column with only the year to the DataFrames?</em></p>
";;2;;2012-09-26T15:12:25.150;9.0;12604909;2014-01-16T00:38:57.740;2013-02-04T18:31:17.280;;1240268.0;;1298051.0;;1;28;<python><database><pandas>;Pandas: how to change all the values of a column?;53708.0
996;996;12627465.0;1.0;"<p>Is there a grep like built-in function in Pandas to drop a row if it has some string or value?
Thanks in advance.</p>
";;0;;2012-09-27T16:10:31.477;3.0;12625650;2012-09-27T18:03:33.140;;;;;1289107.0;;1;15;<grep><row><pandas>;Pandas: grep like function;3264.0
1016;1016;12681217.0;9.0;"<p>I have a <code>pandas dataframe</code> in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, <code>a</code> should become <code>b</code>:</p>

<pre><code>In [7]: a
Out[7]: 
    var1  var2
0  a,b,c     1
1  d,e,f     2

In [8]: b
Out[8]: 
  var1  var2
0    a     1
1    b     1
2    c     1
3    d     2
4    e     2
5    f     2
</code></pre>

<p>So far, I have tried various simple functions, but the <code>.apply</code> method seems to only accept one row as return value when it is used on an axis, and I can't get <code>.transform</code> to work. Any suggestions would be much appreciated!</p>

<p>Example data: </p>

<pre><code>from pandas import DataFrame
import numpy as np
a = DataFrame([{'var1': 'a,b,c', 'var2': 1},
               {'var1': 'd,e,f', 'var2': 2}])
b = DataFrame([{'var1': 'a', 'var2': 1},
               {'var1': 'b', 'var2': 1},
               {'var1': 'c', 'var2': 1},
               {'var1': 'd', 'var2': 2},
               {'var1': 'e', 'var2': 2},
               {'var1': 'f', 'var2': 2}])
</code></pre>

<p>I know this won't work because we lose DataFrame meta-data by going through numpy, but it should give you a sense of what I tried to do: </p>

<pre><code>def fun(row):
    letters = row['var1']
    letters = letters.split(',')
    out = np.array([row] * len(letters))
    out['var1'] = letters
a['idx'] = range(a.shape[0])
z = a.groupby('idx')
z.transform(fun)
</code></pre>
";;0;;2012-10-01T20:42:16.210;19.0;12680754;2017-08-19T09:31:24.483;2017-08-19T09:31:24.483;;5741205.0;;342331.0;;1;38;<python><pandas><numpy><dataframe>;Split pandas dataframe string entry to separate rows;23089.0
1026;1026;12707465.0;3.0;"<pre><code>from pandas import DataFrame
import pyodbc

cnxn = pyodbc.connect(databasez)
cursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")
DF = DataFrame(cursor.fetchall())
</code></pre>

<p>This is fine to populate my pandas DataFrame. But how do I get</p>

<pre><code>DF.columns = ['ID', 'Nickname', 'Residence']
</code></pre>

<p>straight from <em>cursor</em>? Is that information stored in <em>cursor</em> at all?</p>
";;0;;2012-10-03T08:05:48.863;2.0;12704305;2015-06-13T10:48:51.347;2015-06-13T08:47:42.757;;1452002.0;;1479269.0;;1;14;<python><pandas><pyodbc>;return column names from pyodbc execute() statement;12842.0
1036;1036;32399908.0;2.0;"<p>In my application I load text files that are structured as follows:</p>

<ul>
<li>First non numeric column (ID)</li>
<li>A number of non-numeric columns (strings)</li>
<li>A number of numeric columns (floats)</li>
</ul>

<p>The number of the non-numeric columns is variable. Currently I load the data into a DataFrame like this:</p>

<pre><code>source = pandas.read_table(inputfile, index_col=0)
</code></pre>

<p>I would like to drop all non-numeric columns in one fell swoop, without knowing their names or indices, since this could be doable reading their dtype. Is this possible with pandas or do I have to cook up something on my own?</p>
";;2;;2012-10-04T10:36:50.320;9.0;12725417;2017-04-27T11:21:27.257;;;;;241515.0;;1;21;<python><pandas>;Drop non-numeric columns from a pandas DataFrame;9057.0
1037;1037;;4.0;"<p>I am working on time series in python. The libraries which I found useful and promising are </p>

<ul>
<li>pandas;     </li>
<li>statsmodel (for ARIMA);</li>
<li>simple exponential smoothing is provided from pandas.</li>
</ul>

<p>Also for visualization: matplotlib</p>

<p>Does anyone know a library for exponential smoothing?</p>
";2015-01-31T01:23:13.643;1;;2012-10-04T11:38:28.377;11.0;12726432;2015-03-29T09:45:10.180;2015-03-29T09:45:10.180;;1719510.0;;1719510.0;;1;16;<python><pandas><time-series><forecasting><statsmodels>;Package for time series analysis in python;21854.0
1042;1042;12741168.0;1.0;"<p>I can use <code>.map(func)</code> on any column in a df, like:</p>

<pre><code>df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})

df['a']=df['a'].map(lambda x: x &gt; 1)
</code></pre>

<p>I could also:</p>

<pre><code>df['a'],df['b']=df['a'].map(lambda x: x &gt; 1),df['b'].map(lambda x: x &gt; 1)
</code></pre>

<p>Is there a more pythonic way to apply a function to all columns or the entire frame (without a loop)?</p>
";;3;;2012-10-05T06:55:44.403;4.0;12741092;2012-10-05T08:17:32.093;2012-10-05T08:17:32.093;;1491200.0;;1199589.0;;1;22;<python><dataframe><pandas>;Pandas DataFrame: apply function to all columns;13636.0
1067;1067;12846154.0;2.0;"<p>I have a problem with some groupy code which I'm quite sure once ran (on an older pandas version). On 0.9, I get <em>No numeric types to aggregate</em> errors. Any ideas?</p>

<pre><code>In [31]: data
Out[31]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00
Freq: &lt;1 DateOffset&gt;
Columns: 360 entries, -89.75 to 89.75
dtypes: object(360)

In [32]: latedges = linspace(-90., 90., 73)

In [33]: lats_new = linspace(-87.5, 87.5, 72)

In [34]: def _get_gridbox_label(x, bins, labels):
   ....:             return labels[searchsorted(bins, x) - 1]
   ....: 

In [35]: lat_bucket = lambda x: _get_gridbox_label(x, latedges, lats_new)

In [36]: data.T.groupby(lat_bucket).mean()
---------------------------------------------------------------------------
DataError                                 Traceback (most recent call last)
&lt;ipython-input-36-ed9c538ac526&gt; in &lt;module&gt;()
----&gt; 1 data.T.groupby(lat_bucket).mean()

/usr/lib/python2.7/site-packages/pandas/core/groupby.py in mean(self)
    295         """"""
    296         try:
--&gt; 297             return self._cython_agg_general('mean')
    298         except DataError:
    299             raise

/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)
   1415 
   1416     def _cython_agg_general(self, how, numeric_only=True):
-&gt; 1417         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)
   1418         return self._wrap_agged_blocks(new_blocks)
   1419 

/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_blocks(self, how, numeric_only)
   1455 
   1456         if len(new_blocks) == 0:
-&gt; 1457             raise DataError('No numeric types to aggregate')
   1458 
   1459         return new_blocks

DataError: No numeric types to aggregate
</code></pre>
";;0;;2012-10-11T16:45:14.003;1.0;12844529;2015-06-05T08:01:52.057;2012-10-16T10:52:06.750;;1252759.0;;152439.0;;1;11;<python><pandas>;No numeric types to aggregate - change in groupby() behaviour?;14096.0
1072;1072;12850453.0;3.0;"<p>I'm using python pandas data frame , I have a initial data frame say D. I extract two data frames from it like this:</p>

<p><code>A = D[D.label == k]</code></p>

<p><code>B = D[D.label != k]</code></p>

<p>then I change the label in A and B:</p>

<pre><code>A.label = 1
</code></pre>

<p><code>B.label = -1</code></p>

<p>I want to combine A and B so I can have them as one data frame something like union. The order of the data not important , however when we sample A and B from D they retain their indexes from D.</p>
";;1;;2012-10-11T23:53:37.280;6.0;12850345;2016-10-27T13:16:43.383;;;;;445491.0;;1;17;<python><pandas>;how to combine two data frames in python pandas;45587.0
1074;1074;12862196.0;3.0;"<pre><code>df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})

    X   Y   Z
0  X1  Y2  Z3
1  X1  Y1  Z1
2  X1  Y1  Z1
3  X1  Y1  Z2

g=df2.groupby('X')

pd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')
</code></pre>

<blockquote>
  <p>Traceback (most recent call last): ... AttributeError: 'Index' object
  has no attribute 'index'</p>
</blockquote>

<p>How do I get a Pivot Table with <strong>counts of unique values</strong> of one DataFrame column for two other columns?<br>
Is there <code>aggfunc</code> for count unique? Should I be using <code>np.bincount()</code>?</p>

<p>NB. I am aware of 'Series' <code>values_counts()</code> however I need a pivot table.</p>

<hr>

<p>EDIT: The output should be:</p>

<pre><code>Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1
</code></pre>
";;1;;2012-10-12T13:43:47.700;5.0;12860421;2014-02-17T16:29:03.533;2012-10-12T15:06:52.903;;59797.0;;59797.0;;1;24;<python><pandas><pivot-table>;Python Pandas : pivot table with aggfunc = count unique distinct;31611.0
1077;1077;12874054.0;5.0;"<p>In the following, male_trips is a big pandas data frame and stations is a small pandas data frame. For each station id I'd like to know how many male trips took place. The following does the job, but takes a long time:</p>

<pre><code>mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]
</code></pre>

<p>how should I go about this instead?</p>

<hr>

<p>Update! So there were two main approaches: <code>groupby()</code> followed by <code>size()</code>, and the simpler <code>.value_counts()</code>. I did a quick <code>timeit</code>, and the <code>groupby</code> approach wins by quite a large margin! Here is the code:</p>

<pre><code>from timeit import Timer
setup = ""import pandas; male_trips=pandas.load('maletrips')""
a  = ""male_trips.start_station_id.value_counts()""
b = ""male_trips.groupby('start_station_id').size()""
Timer(a,setup).timeit(100)
Timer(b,setup).timeit(100)
</code></pre>

<p>and here is the result:</p>

<pre><code>In [4]: Timer(a,setup).timeit(100) # &lt;- this is value_counts
Out[4]: 9.709594964981079

In [5]: Timer(b,setup).timeit(100) # &lt;- this is groupby / size
Out[5]: 1.5574288368225098
</code></pre>

<p>Note that, at this speed, for exploring data <em>typing</em> value_counts is marginally quicker and less remembering!</p>
";;4;;2012-10-12T21:12:33.627;15.0;12867178;2013-08-14T17:46:28.257;2012-10-14T11:50:43.760;;270572.0;;270572.0;;1;26;<python><pandas>;pandas: count things;42829.0
1083;1083;12882439.0;2.0;"<p>I'm reading a CSV with float numbers like this:</p>

<pre><code>Bob,0.085
Alice,0.005
</code></pre>

<p>And import into a dataframe, and write this dataframe to a new place</p>

<pre><code>df = pd.read_csv(orig)
df.to_csv(pandasfile)
</code></pre>

<p>Now this <code>pandasfile</code> has:</p>

<pre><code>Bob,0.085000000000000006
Alice,0.0050000000000000001
</code></pre>

<p>What happen? maybe I have to cast to a different type like float32 or something? </p>

<p>Im using <strong>pandas 0.9.0</strong> and <strong>numpy 1.6.2</strong>.</p>
";;3;;2012-10-13T21:31:34.397;9.0;12877189;2017-07-14T19:21:43.753;2012-10-15T10:14:28.790;;904365.0;;472866.0;;1;21;<python><numpy><pandas>;float64 with pandas to_csv;18798.0
1100;1100;13674286.0;1.0;"<p>I want to be able to set the major and minor xticks and their labels for a time series graph plotted from a Pandas time series object.  </p>

<p>The Pandas 0.9 ""what's new"" page says: </p>

<blockquote>
  <p>""you can either use to_pydatetime or register a converter for the
  Timestamp type""</p>
</blockquote>

<p>but I can't work out how to do that so that I can use the matplotlib <code>ax.xaxis.set_major_locator</code> and <code>ax.xaxis.set_major_formatter</code> (and minor) commands.</p>

<p>If I use them without converting the pandas times, the x-axis ticks and labels end up wrong.</p>

<p>By using the 'xticks' parameter I can pass the major ticks to pandas.plot, and then set the major tick labels. I can't work out how to do the minor ticks using this approach. (I can set the labels on the default minor ticks set by pandas.plot)</p>

<p>Here is my test code:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas
print 'pandas.__version__ is ', pandas.__version__
print 'matplotlib.__version__ is ', matplotlib.__version__    

dStart = datetime.datetime(2011,5,1) # 1 May
dEnd = datetime.datetime(2011,7,1) # 1 July    

dateIndex = pandas.date_range(start=dStart, end=dEnd, freq='D')
print ""1 May to 1 July 2011"", dateIndex      

testSeries = pandas.Series(data=np.random.randn(len(dateIndex)),
                           index=dateIndex)    

ax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)
testSeries.plot(ax=ax, style='v-', label='first line')    

# using MatPlotLib date time locators and formatters doesn't work with new
# pandas datetime index
ax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),
                                                           interval=1))
ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter('%d\n%a'))
ax.xaxis.grid(True, which=""minor"")
ax.xaxis.grid(False, which=""major"")
ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('\n\n\n%b%Y'))
plt.show()    

# set the major xticks and labels through pandas
ax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)
xticks = pandas.date_range(start=dStart, end=dEnd, freq='W-Tue')
print ""xticks: "", xticks
testSeries.plot(ax=ax2, style='-v', label='second line',
                xticks=xticks.to_pydatetime())
ax2.set_xticklabels([x.strftime('%a\n%d\n%h\n%Y') for x in xticks]);
# set the text of the first few minor ticks created by pandas.plot
#    ax2.set_xticklabels(['a','b','c','d','e'], minor=True)
# remove the minor xtick labels set by pandas.plot 
ax2.set_xticklabels([], minor=True)
# turn the minor ticks created by pandas.plot off 
# plt.minorticks_off()
plt.show()
print testSeries['6/4/2011':'6/7/2011']
</code></pre>

<p>and its output:</p>

<pre><code>pandas.__version__ is  0.9.1.dev-3de54ae
matplotlib.__version__ is  1.1.1
1 May to 1 July 2011 &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]
Length: 62, Freq: D, Timezone: None
</code></pre>

<p><img src=""https://i.stack.imgur.com/8ndjM.png"" alt=""Graph with strange dates on xaxis""></p>

<pre><code>xticks:  &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]
Length: 9, Freq: W-TUE, Timezone: None
</code></pre>

<p><img src=""https://i.stack.imgur.com/hQy8v.png"" alt=""Graph with correct dates""></p>

<pre><code>2011-06-04   -0.199393
2011-06-05   -0.043118
2011-06-06    0.477771
2011-06-07   -0.033207
Freq: D
</code></pre>

<p><strong>Update:</strong> I've been able to get closer to the layout I wanted by using a loop to build the major xtick labels:</p>

<pre><code># only show month for first label in month
month = dStart.month - 1
xticklabels = []
for x in xticks:
    if  month != x.month :
        xticklabels.append(x.strftime('%d\n%a\n%h'))
        month = x.month
    else:
        xticklabels.append(x.strftime('%d\n%a'))
</code></pre>

<p>However, this is a bit like doing the x-axis using <code>ax.annotate</code>: possible but not ideal.</p>
";;3;;2012-10-18T01:51:08.557;40.0;12945971;2016-12-29T05:12:03.763;2016-12-29T05:12:03.763;;283293.0;;1629518.0;;1;70;<python><matplotlib><pandas>;Pandas timeseries plot setting x-axis major and minor ticks and labels;63942.0
1108;1108;12961158.0;2.0;"<p>I am going through the 'Python for Data Analysis' book and having trouble in the 'Example: 2012 Federal Election Commision Database' section reading the data to a DataFrame. The trouble is that one of the columns of data is always being set as the index column, even when the index_col argument is set to None. </p>

<p>Here is the link to the data : <a href=""http://www.fec.gov/disclosurep/PDownload.do"">http://www.fec.gov/disclosurep/PDownload.do</a>.</p>

<p>Here is the loading code (to save time in the checking, I set the nrows=10):</p>

<pre><code>import pandas as pd
fec = pd.read_csv('P00000001-ALL.csv',nrows=10,index_col=None)
</code></pre>

<p>To keep it short I am excluding the data column outputs, but here is my output (please not the Index values):</p>

<pre><code>In [20]: fec

Out[20]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 10 entries, C00410118 to C00410118
Data columns:
...
dtypes: float64(4), int64(3), object(11)
</code></pre>

<p>And here is the book's output (again with data columns excluded):</p>

<pre><code>In [13]: fec = read_csv('P00000001-ALL.csv')
In [14]: fec
Out[14]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1001731 entries, 0 to 1001730
...
dtypes: float64(1), int64(1), object(14)
</code></pre>

<p>The Index values in my output are actually the first column of data in the file, which is then moving all the rest of the data to the left by one. Would anyone know how to prevent this column of data to be listed as an index? I would like to have the index just +1 increasing integers.</p>

<p>I am fairly new to python and pandas, so I apologize for any inconvenience. Thanks.</p>
";;1;;2012-10-18T17:44:44.100;3.0;12960574;2017-07-26T15:29:18.997;;;;;1757088.0;;1;18;<python><pandas>;pandas read_csv index_col=None not working;15871.0
1141;1141;;6.0;"<p>I have a DataFrame with about 25 columns, several of which hold data unsuitable for plotting.  DataFrame.hist() throws errors on those.  How can I specify that those columns should be excluded from the plotting?</p>
";;0;;2012-10-21T23:01:04.740;4.0;13003051;2016-12-03T05:09:13.033;;;;;63401.0;;1;21;<pandas>;How do I exclude a few columns from a DataFrame plot?;15239.0
1152;1152;13020027.0;4.0;"<p>I'm using pandas and I'm wondering what's the easiest way to get the business days between a start and end date using pandas?</p>

<p>There are a lot of posts out there regarding doing this in Python (for <a href=""https://stackoverflow.com/questions/2224742/business-days-in-python"">example</a>), but I would be interested to use directly pandas as I think that pandas can probably handle this quite easy.</p>
";;0;;2012-10-22T20:56:13.003;8.0;13019719;2017-02-10T12:08:03.540;2017-05-23T12:09:59.730;;-1.0;;237690.0;;1;11;<python><pandas>;Get business days between start and end date using pandas;13990.0
1155;1155;13021797.0;3.0;"<p>In R when you need to retrieve a column index based on the name of the column you could do</p>

<pre><code>idx &lt;- which(names(my_data)==my_colum_name)
</code></pre>

<p>Is there a way to do the same with pandas dataframes?</p>
";;0;;2012-10-22T23:48:58.210;14.0;13021654;2016-07-20T19:45:40.957;2012-10-23T00:07:03.167;;487339.0;;55299.0;;1;54;<python><dataframe><pandas>;Retrieving column index from column name in python pandas;40205.0
1180;1180;14900065.0;4.0;"<p>I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame looks something like this:</p>

<pre><code>                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress
Date                                                                                      
2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31
2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30
2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30
2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30
2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28
</code></pre>

<p>The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file. Simple example of such a case is illustrated below:</p>

<pre><code>import pandas 
import datetime
startdate = datetime.datetime(2001, 1, 1, 0, 0)
enddate = datetime.datetime(2001, 1, 1, 5, 0)
index = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')
data = {'A' : range(6), 'B' : range(6)}
data1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}
df1 = pandas.DataFrame(data=data, index=index)
df2 = pandas.DataFrame(data=data1, index=index[:3])
df3 = df1.append(df2)
df3
                       A   B
2001-01-01 00:00:00   20 -50
2001-01-01 01:00:00  -30  60
2001-01-01 02:00:00   40 -70
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
</code></pre>

<p>And so I need <code>df3</code> to evenutally become:</p>

<pre><code>                       A   B
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
</code></pre>

<p>I thought that adding a column of row numbers (<code>df3['rownum'] = range(df3.shape[0])</code>) would help me select out the bottom-most row for any value of the <code>DatetimeIndex</code>, but I am stuck on figuring out the <code>group_by</code> or <code>pivot</code> (or ???) statements to make that work.</p>
";;0;;2012-10-23T17:11:04.333;38.0;13035764;2017-05-04T20:52:58.917;2015-03-24T16:50:31.220;;1552748.0;;1552748.0;;1;90;<python><pandas>;Remove rows with duplicate indices (Pandas DataFrame and TimeSeries);67548.0
1188;1188;13052373.0;3.0;"<p>I am trying to transform DataFrame, such that some of the rows will be replicated a given number of times. For example:</p>

<pre><code>df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})

  class  count
0     A      1
1     B      0
2     C      2
</code></pre>

<p>should be transformed to:</p>

<pre><code>  class 
0     A   
1     C   
2     C 
</code></pre>

<p>This is the reverse of aggregation with count function. Is there an easy way to achieve it in pandas (without using for loops or list comprehensions)?   </p>

<p>One possibility might be to allow <code>DataFrame.applymap</code> function return multiple rows (akin <code>apply</code> method of <code>GroupBy</code>). However, I do not think it is possible in pandas now.</p>
";;1;;2012-10-24T13:14:11.267;6.0;13050003;2015-05-04T21:25:03.923;;;;;74342.0;;1;11;<python><pandas><data-analysis>;pandas: apply function to DataFrame that can return multiple rows;5107.0
1191;1191;13053381.0;3.0;"<p>I'm looking to decrease density of tick labels on differing subplot</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from StringIO import StringIO
data = """"""\
    a   b   c   d
z   54.65   6.27    19.53   4.54
w   -1.27   4.41    11.74   3.06
d   5.51    3.39    22.98   2.29
t   76284.53    -0.20   28394.93    0.28
""""""
df = pd.read_csv(StringIO(data), sep='\s+')
gs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )
ax0 = plt.subplot(gs[0])
ax1 = plt.subplot(gs[1])
ax2 = plt.subplot(gs[2])
df.plot(kind='bar', ax=ax0,color=('Blue','DeepSkyBlue','Red','DarkOrange'))
df.plot(kind='bar', ax=ax1,color=('Blue','DeepSkyBlue','Red','DarkOrange'))
df.plot(kind='bar', ax=ax2,color=('Blue','DeepSkyBlue','Red','DarkOrange'),rot=45)
ax0.set_ylim(69998, 78000)
ax1.set_ylim(19998, 29998)
ax2.set_ylim(-2, 28)
ax0.legend().set_visible(False)
ax1.legend().set_visible(False)
ax2.legend().set_visible(False)
ax0.spines['bottom'].set_visible(False)
ax1.spines['bottom'].set_visible(False)
ax1.spines['top'].set_visible(False)
ax2.spines['top'].set_visible(False)
ax0.xaxis.set_ticks_position('none')
ax1.xaxis.set_ticks_position('none')
ax0.xaxis.set_label_position('top')
ax1.xaxis.set_label_position('top')
ax0.tick_params(labeltop='off')
ax1.tick_params(labeltop='off', pad=15)
ax2.tick_params(pad=15)
ax2.xaxis.tick_bottom()
d = .015
kwargs = dict(transform=ax0.transAxes, color='k', clip_on=False)
ax0.plot((-d,+d),(-d,+d), **kwargs)
ax0.plot((1-d,1+d),(-d,+d), **kwargs)
kwargs.update(transform=ax1.transAxes)
ax1.plot((-d,+d),(1-d,1+d), **kwargs)
ax1.plot((1-d,1+d),(1-d,1+d), **kwargs)
ax1.plot((-d,+d),(-d,+d), **kwargs)
ax1.plot((1-d,1+d),(-d,+d), **kwargs)
kwargs.update(transform=ax2.transAxes)
ax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)
ax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)
plt.show()
</code></pre>

<p>which results in 
<img src=""https://i.stack.imgur.com/d8FL8.png"" alt=""enter image description here""></p>

<p>I would like to decrease tick labels in the two upper subplots. How to do that ? Thanks.</p>

<p>Bonus: 1) how to get rid of the dotted line on y=0 at the basis of the bars?
2) how to get rid of x-trick label between subplot 0 and 1?
3) how to set the back of the plot to transparency? (see the right-bottom broken y-axis line that disappears behind the back of the plot)</p>
";;0;;2012-10-24T15:50:51.210;5.0;13052844;2016-09-01T09:43:38.473;;;;;839119.0;;1;17;<python><plot><matplotlib><pandas>;matplotlib: how to decrease density of tick labels in subplots?;24650.0
1222;1222;13086305.0;1.0;"<p>I'm a beginner in Python and the Pandas library, and I'm rather confused by some basic functionality of DataFrame. I've got a pandas DataFrame as below:  </p>

<pre><code>&gt;&gt;&gt;df.head()  
              X  Y       unixtime
0  652f5e69fcb3  1  1346689910622
1        400292  1  1346614723542
2  1c9d02e4f14e  1  1346862070161
3        610449  1  1346806384518
4        207664  1  1346723370096
</code></pre>

<p>However, after I performed some function:  </p>

<pre><code>def unixTodate(unix):
  day = dt.datetime.utcfromtimestamp(unix/1000).strftime('%Y-%m-%d')
  return day

df['day'] = df['unixtime'].apply(unixTodate)
</code></pre>

<p>I could no longer make use of the df.head() function:  </p>

<pre><code>&gt;&gt;&gt;df.head()  

&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 5 entries, 190648 to 626582
Data columns:
X              5  non-null values
Y              5  non-null values
unixtime       5  non-null values
day            5  non-null values
dtypes: int64(3), object(5)
</code></pre>

<p>I can't see why this is happening. Am I doing something wrong here? Any pointer is welcome! Thanks.</p>
";;0;;2012-10-26T11:02:58.790;2.0;13085709;2017-03-23T14:13:08.840;2017-03-23T14:13:08.840;;1926476.0;;1768115.0;;1;14;<python><pandas>;df.head() sometimes doesn't work in Pandas, Python;14294.0
1234;1234;13115473.0;2.0;"<p>In python, how can I reference previous row and calculate something against it?  Specifically, I am working with <code>dataframes</code> in <code>pandas</code> - I have a data frame full of stock price information that looks like this:</p>

<pre><code>           Date   Close  Adj Close
251  2011-01-03  147.48     143.25
250  2011-01-04  147.64     143.41
249  2011-01-05  147.05     142.83
248  2011-01-06  148.66     144.40
247  2011-01-07  147.93     143.69
</code></pre>

<p>Here is how I created this dataframe:</p>

<pre><code>import pandas

url = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&amp;a=00&amp;b=1&amp;c=2011&amp;d=11&amp;e=31&amp;f=2011&amp;g=d&amp;ignore=.csv'
data = data = pandas.read_csv(url)

## now I sorted the data frame ascending by date 
data = data.sort(columns='Date')
</code></pre>

<p>Starting with row number 2, or in this case, I guess it's 250 (PS - is that the index?), I want to calculate the difference between 2011-01-03 and 2011-01-04, for every entry in this dataframe.  I believe the appropriate way is to write a function that takes the current row, then figures out the previous row, and calculates the difference between them, the use the <code>pandas</code> <code>apply</code> function to update the dataframe with the value.  </p>

<p>Is that the right approach?  If so, should I be using the index to determine the difference?  (note - I'm still in python beginner mode, so index may not be the right term, nor even the correct way to implement this)</p>
";;4;;2012-10-29T00:28:10.363;10.0;13114512;2012-10-29T03:17:30.307;;;;;854739.0;;1;24;<python><pandas>;Calculating difference between two rows in Python / Pandas;31121.0
1242;1242;13130357.0;3.0;"<p>I have some values in a Python Pandas Series (type: pandas.core.series.Series)</p>

<pre><code>In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])

In [2]: series.min()
Out[2]: -100.0

In [3]: series.max()
Out[3]: 950.0
</code></pre>

<p>I would like to get values of histogram (not necessary plotting histogram)... I just need to get the frequency for each interval.</p>

<p>Let's say that my intervals are going from [-200; -150] to [950; 1000]</p>

<p>so lower bounds are</p>

<pre><code>lwb = range(-200,1000,50)
</code></pre>

<p>and upper bounds are</p>

<pre><code>upb = range(-150,1050,50)
</code></pre>

<p>I don't know how to get frequency (the number of values that are inside each interval) now...
I'm sure that defining lwb and upb is not necessary... but I don't know what
function I should use to perform this!
(after diving in Pandas doc, I think <code>cut</code> function can help me because it's a discretization problem... but I'm don't understand how to use it)</p>

<p>After being able to do this, I will have a look at the way to display histogram (but that's an other problem)</p>
";;1;;2012-10-29T21:01:02.027;8.0;13129618;2017-02-10T07:36:25.700;2017-02-10T07:36:25.700;;6207849.0;;1555275.0;;1;33;<python><pandas><numpy><matplotlib>;Histogram values of a Pandas Series;23683.0
1247;1247;;18.0;"<p>I have the following <code>DataFrame</code> (<code>df</code>):</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.rand(10, 5))
</code></pre>

<p>I add more column(s) by assignment:</p>

<pre><code>df['mean'] = df.mean(1)
</code></pre>

<p>How can I move the column <code>mean</code> to the front, i.e. set it as first column leaving the order of the other columns untouched?</p>
";;2;;2012-10-30T22:22:59.293;80.0;13148429;2017-08-23T12:12:57.013;2013-01-04T06:09:36.987;;1479269.0;;1772165.0;;1;286;<python><pandas>;How to change the order of DataFrame columns?;187524.0
1258;1258;;3.0;"<p>What is the best way to multiply all the columns of a Pandas <code>DataFrame</code> by a column vector stored in a <code>Series</code>? I used to do this in Matlab with <code>repmat()</code>, which doesn't exist in Pandas. I can use <code>np.tile()</code>, but it looks ugly to convert the data structure back and forth each time. </p>

<p>Thanks.</p>
";;1;;2012-10-31T20:20:44.700;6.0;13166842;2016-03-29T14:19:28.017;2016-03-29T14:19:28.017;;2087463.0;;1789674.0;;1;16;<dataframe><pandas><multiplying>;pandas dataframe multiply with a series;14469.0
1259;1259;;2.0;"<p>I am creating a <code>groupby</code> object from a Pandas <code>DataFrame</code> and want to select out all the groups with > 1 size.</p>

<p>The following doesn't seem to work:</p>

<pre><code>grouped[grouped.size &gt; 1 ]
</code></pre>

<p>Also, how can one filter out certain values from a grouped <code>DataFrame</code>? For example, how could I remove all the rows from <code>grouped</code> where the column <code>'name'</code> has a value <code>'foo'</code> or <code>'bar'</code>?</p>

<p><strong>Contrived Example:</strong></p>

<pre><code>df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],
                       'B': range(4)})
grouped = df.groupby('A')
</code></pre>

<p>I need the <code>groupby</code> object after removing the groups that have a group size &lt;= 1.</p>

<p>I tried the following, which didn't work:</p>

<pre><code>grouped[grouped.size() &gt; 1]
</code></pre>

<p>I expected:</p>

<pre><code>A
foo 0
    2
    3
</code></pre>

<p>I am not sure how indexing/slicing works for the <code>grouped</code> object.</p>
";;5;;2012-10-31T21:03:56.943;10.0;13167391;2015-03-10T23:44:10.203;2015-03-10T23:44:10.203;;1461210.0;;369541.0;;1;25;<python><pandas>;filtering grouped df in pandas;14412.0
1273;1273;;7.0;"<p>I am interested in knowing how to convert a pandas dataframe into a numpy array, including the index, and set the dtypes.</p>

<p>dataframe:</p>

<pre><code>label   A    B    C
ID                                 
1   NaN  0.2  NaN
2   NaN  NaN  0.5
3   NaN  0.2  0.5
4   0.1  0.2  NaN
5   0.1  0.2  0.5
6   0.1  NaN  0.5
7   0.1  NaN  NaN
</code></pre>

<p>convert df to array returns:</p>

<pre><code>array([[ nan,  0.2,  nan],
       [ nan,  nan,  0.5],
       [ nan,  0.2,  0.5],
       [ 0.1,  0.2,  nan],
       [ 0.1,  0.2,  0.5],
       [ 0.1,  nan,  0.5],
       [ 0.1,  nan,  nan]])
</code></pre>

<p>However, I would like:</p>

<pre><code>array([[ 1, nan,  0.2,  nan],
       [ 2, nan,  nan,  0.5],
       [ 3, nan,  0.2,  0.5],
       [ 4, 0.1,  0.2,  nan],
       [ 5, 0.1,  0.2,  0.5],
       [ 6, 0.1,  nan,  0.5],
       [ 7, 0.1,  nan,  nan]],
     dtype=[('ID', '&lt;i4'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('B', '&lt;f8')])
</code></pre>

<p>(or similar)</p>

<p>Any suggestions on how to accomplish this? (I don't know if I need 1D or 2D array at this point.) I've seen a few posts that touch on this, but nothing dealing specifically with the dataframe.index.</p>

<p>I am writing the dataframe disk using to_csv (and reading it back in to create array) as a workaround, but would prefer something more eloquent than my new-to-pandas kludging. </p>
";;0;;2012-11-02T00:57:33.290;36.0;13187778;2017-06-23T14:28:23.520;2015-06-16T23:06:12.090;;202229.0;;1792954.0;;1;92;<python><arrays><numpy><pandas><type-conversion>;Convert pandas dataframe to numpy array, preserving index;161037.0
1292;1292;13226352.0;1.0;"<p>So I learned that I can use DataFrame.groupby without having a MultiIndex to do subsampling/cross-sections.</p>

<p>On the other hand, when I have a MultiIndex on a DataFrame, I still need to use DataFrame.groupby to do sub-sampling/cross-sections.</p>

<p>So what is a MultiIndex good for apart from the quite helpful and pretty display of the hierarchies when printing?</p>
";;0;;2012-11-05T04:43:47.433;17.0;13226029;2016-10-20T17:52:16.210;;;;;680232.0;;1;29;<python><pandas><multi-index>;Benefits of panda's multiindex?;15935.0
1309;1309;;7.0;"<p>I'm having trouble installing the Python Pandas library on my Mac OSX computer.</p>

<p>I type the following in Terminal:</p>

<pre><code>$ sudo easy_install pandas
</code></pre>

<p>But then I get the following:</p>

<pre><code>Searching for pandas
Reading http://pypi.python.org/simple/pandas/
Reading http://pandas.pydata.org
Reading http://pandas.sourceforge.net
Best match: pandas 0.9.0
Downloading http://pypi.python.org/packages/source/p/pandas/pandas-
0.9.0.zip#md5=04b1d8e11cc0fc30ae777499d89003ec
Processing pandas-0.9.0.zip
Writing /tmp/easy_install-ixjbQO/pandas-0.9.0/setup.cfg
Running pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ixjbQO/pandas-
0.9.0/egg-dist-tmp-EGREoT
warning: no files found matching 'setupegg.py'
no previously-included directories found matching 'doc/build'
warning: no previously-included files matching '*.so' found anywhere in distribution
warning: no previously-included files matching '*.pyd' found anywhere in distribution
warning: no previously-included files matching '*.pyc' found anywhere in distribution
warning: no previously-included files matching '.git*' found anywhere in distribution
warning: no previously-included files matching '.DS_Store' found anywhere in distribution
warning: no previously-included files matching '*.png' found anywhere in distribution
unable to execute gcc: No such file or directory
error: Setup script exited with error: command 'gcc' failed with exit status 1
</code></pre>

<p>I do have Xcode and gcc installed, however, gcc is only found when I type:</p>

<pre><code>$ gcc
-bash: gcc: command not found

$ gcc-4.2
i686-apple-darwin11-gcc-4.2.1: no input files
</code></pre>

<p>What should I do?</p>
";;1;;2012-11-06T10:33:34.617;4.0;13249135;2016-12-03T03:20:37.930;2012-12-10T13:08:31.503;;1240268.0;;1715271.0;;1;12;<python><pandas>;Installing Pandas on Mac OSX;40821.0
1310;1310;;3.0;"<p>I am importing study data into a Pandas data frame using <code>read_csv</code>. </p>

<p>My subject codes are 6 numbers coding, among others, the day of birth. For some of my subjects this results in a code with a leading zero (e.g. ""010816"").</p>

<p>When I import into Pandas, the leading zero is stripped of and the column is formatted as <code>int64</code>.</p>

<p>Is there a way to import this column unchanged maybe as a string? </p>

<p>I tried using a custom converter for the column, but it does not work - it seems as if the custom conversion takes place before Pandas converts to int.</p>
";;1;;2012-11-06T11:27:42.290;8.0;13250046;2016-11-04T18:26:16.163;2012-11-06T11:46:11.847;;1462920.0;;1802883.0;;1;19;<python><pandas>;Pandas csv-import: Keep leading zeros in a column;6682.0
1316;1316;13257677.0;4.0;"<p>With the DataFrame below as an example, </p>

<pre><code>In [83]:
df = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})
df
Out[83]:
   A  B  values
0  1  1      10
1  1  2      15
2  2  1      20
3  2  2      25
</code></pre>

<p>What would be a simple way to generate a new column containing some aggregation of the data over one of the columns?</p>

<p>For example, if I sum <code>values</code> over items in <code>A</code> </p>

<pre><code>In [84]:
df.groupby('A').sum()['values']
Out[84]:
A
1    25
2    45
Name: values
</code></pre>

<p>How can I get </p>

<pre><code>   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45
</code></pre>
";;0;;2012-11-06T18:17:32.320;6.0;13256917;2012-11-06T21:26:44.970;;;;;189418.0;;1;14;<python><pandas>;Pandas: Creating aggregated column in DataFrame;7817.0
1331;1331;13270110.0;4.0;"<p>I have two pandas dataframes:</p>

<pre><code>from pandas import DataFrame
df1 = DataFrame({'col1':[1,2],'col2':[3,4]})
df2 = DataFrame({'col3':[5,6]})     
</code></pre>

<p>What is the best practice to get their cartesian product (of course without writing it explicitly like me)?</p>

<pre><code>#df1, df2 cartesian product
df_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})
</code></pre>
";;3;;2012-11-07T12:33:12.763;9.0;13269890;2017-04-18T14:08:48.620;2012-11-07T12:54:09.847;;308903.0;;1087310.0;;1;30;<python><pandas>;cartesian product in pandas;18300.0
1341;1341;13384494.0;2.0;"<p>I would like to import the following csv as strings not as int64. Pandas read_csv automatically converts it to int64, but I need this column as string.</p>

<pre><code>ID
00013007854817840016671868
00013007854817840016749251
00013007854817840016754630
00013007854817840016781876
00013007854817840017028824
00013007854817840017963235
00013007854817840018860166


df = read_csv('sample.csv')

df.ID
&gt;&gt;

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID
</code></pre>

<p>Unfortunately using converters gives the same result. </p>

<pre><code>df = read_csv('sample.csv', converters={'ID': str})
df.ID
&gt;&gt;

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID
</code></pre>
";;2;;2012-11-08T16:54:59.533;7.0;13293810;2012-12-10T15:59:06.817;;;;;387251.0;;1;32;<pandas>;Import pandas dataframe column as string not int;28863.0
1342;1342;13295801.0;7.0;"<p>I have a dataframe as below</p>

<pre><code>      itm Date                  Amount 
67    420 2012-09-30 00:00:00   65211
68    421 2012-09-09 00:00:00   29424
69    421 2012-09-16 00:00:00   29877
70    421 2012-09-23 00:00:00   30990
71    421 2012-09-30 00:00:00   61303
72    485 2012-09-09 00:00:00   71781
73    485 2012-09-16 00:00:00     NaN
74    485 2012-09-23 00:00:00   11072
75    485 2012-09-30 00:00:00  113702
76    489 2012-09-09 00:00:00   64731
77    489 2012-09-16 00:00:00     NaN
</code></pre>

<p>when I try to .apply a function to the Amount column I get the following error.</p>

<pre><code>ValueError: cannot convert float NaN to integer
</code></pre>

<p>I have tried applying a function using .isnan from the Math Module
I have tried the pandas .replace attribute
I tried the .sparse data attribute from pandas 0.9
I have also tried if NaN == NaN statement in a function.
I have also looked at this article <a href=""https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r"">How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles. 
All the methods I have tried have not worked or do not recognise NaN.
Any Hints or solutions would be appreciated. </p>
";;1;;2012-11-08T18:50:39.703;37.0;13295735;2017-07-28T19:48:43.413;2017-05-23T11:33:24.953;;-1.0;;1733061.0;;1;145;<python><pandas>;How can I replace all the NaN values with Zero's in a column of a pandas dataframe;174458.0
1360;1360;13332682.0;5.0;"<p>How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.</p>

<p>Something along:</p>

<pre><code>&gt;&gt; x = Series()
&gt;&gt; N = 4
&gt;&gt; for i in xrange(N):
&gt;&gt;     x.some_appending_function(i**2)    
&gt;&gt; print x

0 | 0
1 | 1
2 | 4
3 | 9
</code></pre>

<p>also, how can i add a single row to a pandas DataFrame? </p>
";;0;;2012-11-11T13:26:27.600;2.0;13331518;2016-06-23T13:16:57.263;;;;;1724926.0;;1;26;<python><pandas>;How to add a single item to a Pandas Series;38577.0
1361;1361;;8.0;"<p>Suppose I have a <code>df</code> which has columns of <code>'ID', 'col_1', 'col_2'</code>. And I define a function :</p>

<p><code>f = lambda x, y : my_function_expression</code>.</p>

<p>Now I want to apply the <code>f</code> to <code>df</code>'s two columns <code>'col_1', 'col_2'</code> to element-wise calculate a new column <code>'col_3'</code> , somewhat like :</p>

<pre><code>df['col_3'] = df[['col_1','col_2']].apply(f)  
# Pandas gives : TypeError: ('&lt;lambda&gt;() takes exactly 2 arguments (1 given)'
</code></pre>

<p>How to do ?</p>

<p><em><strong></em>**<em></strong> Add detail sample as below <strong></em>***</strong></p>

<pre><code>import pandas as pd

df = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})
mylist = ['a','b','c','d','e','f']

def get_sublist(sta,end):
    return mylist[sta:end+1]

#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)
# expect above to output df as below 

  ID  col_1  col_2            col_3
0  1      0      1       ['a', 'b']
1  2      2      4  ['c', 'd', 'e']
2  3      3      5  ['d', 'e', 'f']
</code></pre>
";;4;;2012-11-11T13:48:53.300;62.0;13331698;2017-05-25T09:36:36.120;2012-11-13T12:59:35.890;;1072888.0;;1072888.0;;1;137;<python><pandas>;How to apply a function to two columns of Pandas dataframe;154207.0
1382;1382;13371090.0;1.0;"<p>I have a dictionary name date_dict keyed by datetime dates with values corresponding to integer counts of observations. I convert this to a sparse series/dataframe with censored observations that I would like to join or convert to a series/dataframe with continuous dates. The nasty list comprehension is my hack to get around the fact that pandas apparently won't automatically covert datetime date objects to an appropriate DateTime index.</p>

<pre><code>df1 = pd.DataFrame(data=date_dict.values(),
                   index=[datetime.datetime.combine(i, datetime.time()) 
                          for i in date_dict.keys()],
                   columns=['Name'])
df1 = df1.sort(axis=0)
</code></pre>

<p>This example has 1258 observations and the DateTime index runs from 2003-06-24 to 2012-11-07.</p>

<pre><code>df1.head()
             Name
Date
2003-06-24   2
2003-08-13   1
2003-08-19   2
2003-08-22   1
2003-08-24   5
</code></pre>

<p>I can create an empty dataframe with a continuous DateTime index, but this introduces an unneeded column and seems clunky. I feel as though I'm missing a more elegant solution involving a join.</p>

<pre><code>df2 = pd.DataFrame(data=None,columns=['Empty'],
                   index=pd.DateRange(min(date_dict.keys()),
                                      max(date_dict.keys())))
df3 = df1.join(df2,how='right')
df3.head()
            Name    Empty
2003-06-24   2   NaN
2003-06-25  NaN  NaN
2003-06-26  NaN  NaN
2003-06-27  NaN  NaN
2003-06-30  NaN  NaN
</code></pre>

<p>Is there a simpler or more elegant way to fill a continuous dataframe from a sparse dataframe so that there is (1) a continuous index, (2) the NaNs are 0s, and (3) there is no left-over empty column in the dataframe?</p>

<pre><code>            Name
2003-06-24   2
2003-06-25   0
2003-06-26   0
2003-06-27   0
2003-06-30   0
</code></pre>
";;0;;2012-11-13T23:14:28.873;5.0;13370525;2012-11-14T00:34:27.837;2012-11-14T00:34:27.837;;1306530.0;;1574687.0;;1;13;<python><python-2.7><pandas>;Filling continuous pandas dataframe from sparse dataframe;6017.0
1390;1390;13386025.0;6.0;"<p>I have the following file named 'data.csv':</p>

<pre><code>    1997,Ford,E350
    1997, Ford , E350
    1997,Ford,E350,""Super, luxurious truck""
    1997,Ford,E350,""Super """"luxurious"""" truck""
    1997,Ford,E350,"" Super luxurious truck ""
    ""1997"",Ford,E350
    1997,Ford,E350
    2000,Mercury,Cougar
</code></pre>

<p>And I would like to parse it into a pandas DataFrame so that the DataFrame looks as follows:</p>

<pre><code>       Year     Make   Model              Description
    0  1997     Ford    E350                     None
    1  1997     Ford    E350                     None
    2  1997     Ford    E350   Super, luxurious truck
    3  1997     Ford    E350  Super ""luxurious"" truck
    4  1997     Ford    E350    Super luxurious truck
    5  1997     Ford    E350                     None
    6  1997     Ford    E350                     None
    7  2000  Mercury  Cougar                     None
</code></pre>

<p>The best I could do was:</p>

<pre><code>    pd.read_table(""data.csv"", sep=r',', names=[""Year"", ""Make"", ""Model"", ""Description""])
</code></pre>

<p>Which gets me:</p>

<pre><code>    Year     Make   Model              Description
 0  1997     Ford    E350                     None
 1  1997    Ford     E350                     None
 2  1997     Ford    E350   Super, luxurious truck
 3  1997     Ford    E350  Super ""luxurious"" truck
 4  1997     Ford    E350   Super luxurious truck 
 5  1997     Ford    E350                     None
 6  1997     Ford    E350                     None
 7  2000  Mercury  Cougar                     None
</code></pre>

<p>How can I get the DataFrame without those whitespaces?</p>
";;0;;2012-11-14T19:25:28.740;15.0;13385860;2017-05-02T18:46:24.057;2012-11-14T19:38:36.557;;982257.0;;1715271.0;;1;28;<python><parsing><pandas>;How can I remove extra whitespace from strings when parsing a csv file in Pandas?;28532.0
1396;1396;13389808.0;4.0;"<p>I have a series with a MultiIndex like this:</p>

<pre><code>import numpy as np
import pandas as pd

buckets = np.repeat(['a','b','c'], [3,5,1])
sequence = [0,1,5,0,1,2,4,50,0]

s = pd.Series(
    np.random.randn(len(sequence)), 
    index=pd.MultiIndex.from_tuples(zip(buckets, sequence))
)

# In [6]: s
# Out[6]: 
# a  0    -1.106047
#    1     1.665214
#    5     0.279190
# b  0     0.326364
#    1     0.900439
#    2    -0.653940
#    4     0.082270
#    50   -0.255482
# c  0    -0.091730
</code></pre>

<p>I'd like to get the s['b'] values where the second index ('<code>sequence</code>') is between 2 and 10.</p>

<p>Slicing on the first index works fine:</p>

<pre><code>s['a':'b']
# Out[109]: 
# bucket  value
# a       0        1.828176
#         1        0.160496
#         5        0.401985
# b       0       -1.514268
#         1       -0.973915
#         2        1.285553
#         4       -0.194625
#         5       -0.144112
</code></pre>

<p>But not on the second, at least by what seems to be the two most obvious ways:</p>

<p>1) This returns elements 1 through 4, with nothing to do with the index values</p>

<pre><code>s['b'][1:10]

# In [61]: s['b'][1:10]
# Out[61]: 
# 1     0.900439
# 2    -0.653940
# 4     0.082270
# 50   -0.255482
</code></pre>

<p>However, if I reverse the index and the first index is integer and the second index is a string, it works:</p>

<pre><code>In [26]: s
Out[26]: 
0   a   -0.126299
1   a    1.810928
5   a    0.571873
0   b   -0.116108
1   b   -0.712184
2   b   -1.771264
4   b    0.148961
50  b    0.089683
0   c   -0.582578

In [25]: s[0]['a':'b']
Out[25]: 
a   -0.126299
b   -0.116108
</code></pre>
";;0;;2012-11-14T23:29:30.823;6.0;13389203;2016-01-28T00:12:42.207;;;;;529841.0;;1;11;<python><pandas>;pandas: slice a MultiIndex by range of secondary index;11982.0
1407;1407;13413842.0;2.0;"<p>If I want to calculate the mean of two categories in Pandas, I can do it like this:</p>

<pre><code>data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],
        'values': [1,2,3,1,2,3,1,2,3,5,1]}
my_data = DataFrame(data)
my_data.groupby('Category').mean()

Category:     values:   
cat1     2.666667
cat2     1.600000
</code></pre>

<p>I have a lot of data formatted this way, and now I need to do a <em>T</em>-test to see if the mean of <em>cat1</em> and <em>cat2</em> are statistically different. How can I do that?</p>
";;0;;2012-11-15T19:11:57.260;11.0;13404468;2017-06-19T07:07:03.307;2017-06-19T06:46:21.680;;1587329.0;;1827631.0;;1;32;<python><pandas><scipy><statistics><hypothesis-test>;T-test in Pandas (Python);27308.0
1410;1410;13485766.0;11.0;"<p>When deleting a column in a DataFrame I use:</p>

<pre><code>del df['column_name']
</code></pre>

<p>and this works great. Why can't I use:</p>

<pre><code>del df.column_name
</code></pre>

<p><em>As you can access the column/Series as <code>df.column_name</code>, I expect this to work.</em></p>
";;0;;2012-11-16T06:26:40.410;176.0;13411544;2017-07-27T17:48:15.380;2016-09-22T13:13:26.303;;3730397.0;;390388.0;;1;643;<python><pandas><design><dataframe><magic-methods>;Delete column from pandas DataFrame;684200.0
1411;1411;13413845.0;8.0;"<p>I have a <code>DataFrame</code>:</p>

<pre><code>&gt;&gt;&gt; df
                 STK_ID  EPS  cash
STK_ID RPT_Date                   
601166 20111231  601166  NaN   NaN
600036 20111231  600036  NaN    12
600016 20111231  600016  4.3   NaN
601009 20111231  601009  NaN   NaN
601939 20111231  601939  2.5   NaN
000001 20111231  000001  NaN   NaN
</code></pre>

<p>Then I just want the records whose <code>EPS</code> is not <code>NaN</code>, that is, <code>df.drop(....)</code> will return the dataframe as below:</p>

<pre><code>                  STK_ID  EPS  cash
STK_ID RPT_Date                   
600016 20111231  600016  4.3   NaN
601939 20111231  601939  2.5   NaN
</code></pre>

<p>How do I do that?</p>
";;2;;2012-11-16T09:17:22.947;114.0;13413590;2017-08-21T09:49:35.780;2017-01-05T17:01:10.563;;604687.0;;1072888.0;;1;272;<python><pandas><dataframe>;How to drop rows of Pandas DataFrame whose value in certain columns is NaN;280310.0
1431;1431;;3.0;"<p>After fighting with NumPy and dateutil for days, I recently discovered the amazing Pandas library. I've been poring through the documentation and source code, but I can't figure out how to get <code>date_range()</code> to generate indices at the right breakpoints.</p>

<pre><code>from datetime import date
import pandas as pd

start = date('2012-01-15')
end = date('2012-09-20')
# 'M' is month-end, instead I need same-day-of-month
date_range(start, end, freq='M')
</code></pre>

<p>What I want:</p>

<pre><code>2012-01-15
2012-02-15
2012-03-15
...
2012-09-15
</code></pre>

<p>What I get:</p>

<pre><code>2012-01-31
2012-02-29
2012-03-31
...
2012-08-31
</code></pre>

<p>I need month-sized chunks that account for the variable number of days in a month. This is possible with dateutil.rrule:</p>

<pre><code>rrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)
</code></pre>

<p>Ugly and illegible, but it works. How can do I this with pandas? I've played with both <code>date_range()</code> and <code>period_range()</code>, so far with no luck.</p>

<p>My actual goal is to use <code>groupby</code>, <code>crosstab</code> and/or <code>resample</code> to calculate values for each period based on sums/means/etc of individual entries within the period. In other words, I want to transform data from:</p>

<pre><code>                total
2012-01-10 00:01    50
2012-01-15 01:01    55
2012-03-11 00:01    60
2012-04-28 00:01    80

#Hypothetical usage
dataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') 
</code></pre>

<p>to</p>

<pre><code>                total
2012-01-09          105 # Values summed
2012-02-09          0   # Missing from dataframe
2012-03-09          60
2012-04-09          0   # Data past end date, not counted
</code></pre>

<p>Given that Pandas originated as a financial analysis tool, I'm virtually certain that there's a simple and fast way to do this. Help appreciated!</p>
";;0;;2012-11-18T22:14:06.220;3.0;13445174;2016-08-23T17:24:20.327;2012-11-19T02:35:01.913;;649167.0;;649167.0;;1;15;<datetime><time-series><pandas>;Date ranges in Pandas;19749.0
1432;1432;21942746.0;3.0;"<p>I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.</p>

<p>Any ideas how this can be improved?</p>

<p>Basically I want to turn this:</p>

<pre><code>                   A    B    C
2000-01-01 -0.532681  foo    0
2000-01-02  1.490752  bar    1
2000-01-03 -1.387326  foo    2
2000-01-04  0.814772  baz     
2000-01-05 -0.222552         4
2000-01-06 -1.176781  qux     
</code></pre>

<p>Into this:</p>

<pre><code>                   A     B     C
2000-01-01 -0.532681   foo     0
2000-01-02  1.490752   bar     1
2000-01-03 -1.387326   foo     2
2000-01-04  0.814772   baz   NaN
2000-01-05 -0.222552   NaN     4
2000-01-06 -1.176781   qux   NaN
</code></pre>

<p>I've managed to do it with the code below, but man is it ugly. It's not Pythonic and I'm sure it's not the most efficient use of pandas either. I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value, matching on whitespace.</p>

<pre><code>for i in df.columns:
    df[i][df[i].apply(lambda i: True if re.search('^\s*$', str(i)) else False)]=None
</code></pre>

<p>It could be optimized a bit by only iterating through fields that could contain empty strings:</p>

<pre><code>if df[i].dtype == np.dtype('object')
</code></pre>

<p>But that's not much of an improvement</p>

<p>And finally, this code sets the target strings to None, which works with Pandas' functions like fillna(), but it would be nice for completeness if I could actually insert a NaN directly instead of None.</p>

<p>Help!</p>
";;2;;2012-11-18T22:22:39.203;15.0;13445241;2016-06-09T11:55:36.213;;;;;221390.0;;1;57;<python><pandas>;Replacing blank values (white space) with NaN in pandas;50114.0
1437;1437;13447176.0;4.0;"<p>I'm trying to remove entries from a data frame which occur less than 100 times. 
The data frame <code>data</code> looks like this:</p>

<pre><code>pid   tag
1     23    
1     45
1     62
2     24
2     45
3     34
3     25
3     62
</code></pre>

<p>Now I count the number of tag occurrences like this:</p>

<pre><code>bytag = data.groupby('tag').aggregate(np.count_nonzero)
</code></pre>

<p>But then I can't figure out how to remove those entries which have low count...</p>
";;1;;2012-11-19T01:20:07.153;7.0;13446480;2017-05-03T01:36:16.687;;;;;1564449.0;;1;17;<python><numpy><python-2.7><pandas>;Python Pandas: remove entries based on the number of occurrences;6714.0
1499;1499;13581730.0;2.0;"<p>Suppose I have a nested dictionary 'user_dict' with structure:</p>

<p><strong>Level 1:</strong> UserId (Long Integer)</p>

<p><strong>Level 2:</strong> Category (String)</p>

<p><strong>Level 3:</strong> Assorted Attributes (floats, ints, etc..)</p>

<p>For example, an entry of this dictionary would be:</p>

<pre><code>user_dict[12] = {
    ""Category 1"": {""att_1"": 1, 
                   ""att_2"": ""whatever""},
    ""Category 2"": {""att_1"": 23, 
                   ""att_2"": ""another""}}
</code></pre>

<p>each item in ""user_dict"" has the same structure and ""user_dict"" contains a large number of items which I want to feed to a pandas DataFrame, constructing the series from the attributes. In this case a hierarchical index would be useful for the purpose.</p>

<p>Specifically, my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the ""level 3"" in the dictionary?</p>

<p>If I try something like:</p>

<pre><code>df = pandas.DataFrame(users_summary)
</code></pre>

<p>The items in ""level 1"" (the user id's) are taken as columns, which is the opposite of what I want to achieve (have user id's as index). </p>

<p>I know I could construct the series after iterating over the dictionary entries, but if there is a more direct way this would be very useful. A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file. </p>
";;0;;2012-11-26T23:41:31.213;14.0;13575090;2017-01-09T23:36:38.933;;;;;1371091.0;;1;29;<python><dataframe><pandas>;Construct pandas DataFrame from items in nested dictionary;27582.0
1510;1510;13583024.0;1.0;"<p>I had a dataframe and did a groupby in FIPS and summed the groups that worked fine.</p>

<pre><code>kl = ks.groupby('FIPS')

kl.aggregate(np.sum)
</code></pre>

<p>I just want a normal Dataframe back but I have a <code>pandas.core.groupby.DataFrameGroupBy</code> object. </p>

<p>There is a question that sounds like this one but it is not the same. </p>
";;0;;2012-11-27T10:43:07.473;6.0;13582449;2014-04-08T17:33:00.047;2012-11-27T10:45:20.317;;502950.0;;1246428.0;;1;21;<python><pandas>;Convert DataFrameGroupBy object to DataFrame pandas;15555.0
1529;1529;13616382.0;3.0;"<p>I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.</p>

<p>The filters should be additive (aka each one applied should narrow results).</p>

<p>I'm currently using <code>reindex()</code> but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.</p>

<p>I'm thinking that using <code>apply()</code>, <code>map()</code>, or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.</p>

<h2>TL;DR</h2>

<p>I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.</p>

<pre><code>relops = {'&gt;=': [1], '&lt;=': [1]}
</code></pre>

<h2>Long Example</h2>

<p>I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:</p>

<pre><code>   def apply_relops(series, relops):
        """"""
        Pass dictionary of relational operators to perform on given series object
        """"""
        for op, vals in relops.iteritems():
            op_func = ops[op]
            for val in vals:
                filtered = op_func(series, val)
                series = series.reindex(series[filtered])
        return series
</code></pre>

<p>The user provides a dictionary with the operations they want to perform:</p>

<pre><code>&gt;&gt;&gt; df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})
&gt;&gt;&gt; print df
&gt;&gt;&gt; print df
   col1  col2
0     0    10
1     1    11
2     2    12

&gt;&gt;&gt; from operator import le, ge
&gt;&gt;&gt; ops ={'&gt;=': ge, '&lt;=': le}
&gt;&gt;&gt; apply_relops(df['col1'], {'&gt;=': [1]})
col1
1       1
2       2
Name: col1
&gt;&gt;&gt; apply_relops(df['col1'], relops = {'&gt;=': [1], '&lt;=': [1]})
col1
1       1
Name: col1
</code></pre>

<p>Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.</p>

<p>Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.</p>
";;1;;2012-11-28T17:34:35.337;17.0;13611065;2016-05-18T10:06:20.270;;;;;1108031.0;;1;46;<python><algorithm><pandas>;Efficient way to apply multiple filters to pandas DataFrame or Series;52152.0
1540;1540;;5.0;"<p>I have the following DataFrame containing song names, their peak chart positions and the number of weeks they spent at position no 1:</p>

<pre><code>                                          Song            Peak            Weeks
76                            Paperback Writer               1               16
117                               Lady Madonna               1                9
118                                   Hey Jude               1               27
22                           Can't Buy Me Love               1               17
29                          A Hard Day's Night               1               14
48                              Ticket To Ride               1               14
56                                       Help!               1               17
109                       All You Need Is Love               1               16
173                The Ballad Of John And Yoko               1               13
85                               Eleanor Rigby               1               14
87                            Yellow Submarine               1               14
20                    I Want To Hold Your Hand               1               24
45                                 I Feel Fine               1               15
60                                 Day Tripper               1               12
61                          We Can Work It Out               1               12
10                               She Loves You               1               36
155                                   Get Back               1                6
8                               From Me To You               1                7
115                              Hello Goodbye               1                7
2                             Please Please Me               2               20
92                   Strawberry Fields Forever               2               12
93                                  Penny Lane               2               13
107                       Magical Mystery Tour               2               16
176                                  Let It Be               2               14
0                                   Love Me Do               4               26
157                                  Something               4                9
166                              Come Together               4               10
58                                   Yesterday               8               21
135                       Back In The U.S.S.R.              19                3
164                         Here Comes The Sun              58               19
96       Sgt. Pepper's Lonely Hearts Club Band              63               12
105         With A Little Help From My Friends              63                7
</code></pre>

<p>I'd like to rank these songs in order of popularity, so I'd like to sort them according to the following criteria: songs that reached the highest position come first, but if there is a tie, the songs that remained in the charts for the longest come first.</p>

<p>I can't seem to figure out how to do this in Pandas.</p>
";;0;;2012-11-29T23:20:44.917;9.0;13636592;2017-04-28T11:57:26.910;;;;;1715271.0;;1;23;<python><pandas>;How to sort a Pandas DataFrame according to multiple criteria?;50220.0
1542;1542;13680953.0;4.0;"<p>I have two DataFrames which I want to merge based on a column. However, due to alternate spellings, different number of spaces, absence/presence of diacritical marks, I would like to be able to merge as long as they are similar to one another.</p>

<p>Any similarity algorithm will do (soundex, Levenshtein, difflib's). </p>

<p>Say one DataFrame has the following data:</p>

<pre><code>df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])

       number
one         1
two         2
three       3
four        4
five        5

df2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])

      letter
one        a
too        b
three      c
fours      d
five       e
</code></pre>

<p>Then I want to get the resulting DataFrame</p>

<pre><code>       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e
</code></pre>
";;2;;2012-11-29T23:44:17.093;10.0;13636848;2017-04-13T22:40:36.910;2012-12-03T10:08:10.873;;1240268.0;;215847.0;;1;24;<python><pandas>;is it possible to do fuzzy match merge with python pandas?;10322.0
1547;1547;13653490.0;2.0;"<p>How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in <code>read_csv</code>.  Am I missing something?</p>

<p>Example: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant.</p>
";;0;;2012-11-30T18:38:41.750;9.0;13651117;2015-11-08T12:09:25.970;;;;;825594.0;;1;39;<pandas>;pandas: filter lines on load in read_csv;17401.0
1555;1555;13655271.0;1.0;"<p>I have a time-series that is not recognized as a DatetimeIndex despite being indexed by standard YYYY-MM-DD strings with valid dates. Coercing them to a valid DatetimeIndex seems to be inelegant enough to make me think I'm doing something wrong.</p>

<p>I read in (someone else's lazily formatted) data that contains invalid datetime values and remove these invalid observations.</p>

<pre><code>In [1]: df = pd.read_csv('data.csv',index_col=0)
In [2]: print df['2008-02-27':'2008-03-02']
Out[2]: 
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-02-30   0
2008-02-31   0
2008-03-01   0
2008-03-02  17

In [3]: def clean_timestamps(df):
    # remove invalid dates like '2008-02-30' and '2009-04-31'
    to_drop = list()
    for d in df.index:
        try:
            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))
        except ValueError:
            to_drop.append(d)
    df2 = df.drop(to_drop,axis=0)
    return df2

In [4]: df2 = clean_timestamps(df)
In [5] :print df2['2008-02-27':'2008-03-02']
Out[5]:
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17
</code></pre>

<p>This new index is still only recognized as a 'object' dtype rather than a DatetimeIndex. </p>

<pre><code>In [6]: df2.index
Out[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,
   2012-11-29], dtype=object)
</code></pre>

<p>Reindexing produces NaNs because they're different dtypes.</p>

<pre><code>In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))
In [8]: df3 = df2.reindex(index=i,columns=['count'])
In [9]: df3['2008-02-27':'2008-03-02']
Out[9]: 
            count
2008-02-27 NaN
2008-02-28 NaN
2008-02-29 NaN
2008-03-01 NaN
2008-03-02 NaN
</code></pre>

<p>I create a fresh dataframe with the appropriate index, drop the data to a dictionary, then populate the new dataframe based on the dictionary values (skipping missing values).</p>

<pre><code>In [10]: df3 = pd.DataFrame(columns=['count'],index=i)
In [11]: values = dict(df2['count'])
In [12]: for d in i:
    try:
        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])
    except KeyError:
        pass
In [13]: print df3['2008-02-27':'2008-03-02']
Out[13]: 

             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17

In [14]: df3.index
Out[14];
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]
Length: 1795, Freq: D, Timezone: None
</code></pre>

<p>This last part of setting values based on lookups to a dictionary keyed by strings seems especially hacky and makes me think I've missed something important.</p>
";;0;;2012-11-30T23:35:15.957;9.0;13654699;2016-11-14T21:38:31.830;2012-12-01T04:26:42.320;;1574687.0;;1574687.0;;1;27;<python><datetime><python-2.7><pandas>;Reindexing pandas timeseries from object dtype to datetime dtype;23824.0
1557;1557;13659944.0;1.0;"<p>I'm trying to figure out how to count by number of rows per unique pair of columns (ip, useragent), e.g.</p>

<pre><code>d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})

     ip              useragent
0    192.168.0.1     a
1    192.168.0.1     a
2    192.168.0.1     b
3    192.168.0.2     b
</code></pre>

<p>To produce:</p>

<pre><code>ip           useragent  
192.168.0.1  a           2
192.168.0.1  b           1
192.168.0.2  b           1
</code></pre>

<p>Ideas?</p>
";;0;;2012-12-01T13:26:10.750;6.0;13659881;2015-07-02T12:36:05.437;;;;;1804520.0;;1;23;<pandas>;Count by unique pair of columns in pandas;12957.0
1564;1564;13682381.0;6.0;"<p>I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.</p>

<p>Data looks like:</p>

<pre><code>    time    result
1    09:00   +52A
2    10:00   +62B
3    11:00   +44a
4    12:00   +30b
5    13:00   -110a
</code></pre>

<p>I need to trim these data to:</p>

<pre><code>    time    result
1    09:00   52
2    10:00   62
3    11:00   44
4    12:00   30
5    13:00   110
</code></pre>

<p>I tried <code>.str.lstrip('+-')</code> and .<code>str.rstrip('aAbBcC')</code>, but got an error:  </p>

<pre><code>TypeError: wrapper() takes exactly 1 argument (2 given)
</code></pre>

<p>Any pointers would be greatly appreciated!</p>
";;0;;2012-12-03T11:11:56.090;24.0;13682044;2016-02-02T23:20:32.193;2016-02-02T23:20:32.193;;2822004.0;;1872240.0;;1;48;<python><dataframe><pandas>;Pandas DataFrame: remove unwanted parts from strings in a column;66744.0
1582;1582;13704307.0;10.0;"<p>How do I convert a <code>numpy.datetime64</code> object to a <code>datetime.datetime</code> (or <code>Timestamp</code>)?</p>

<p>In the following code, I create a datetime, timestamp and datetime64 objects.</p>

<pre><code>import datetime
import numpy as np
import pandas as pd
dt = datetime.datetime(2012, 5, 1)
# A strange way to extract a Timestamp object, there's surely a better way?
ts = pd.DatetimeIndex([dt])[0]
dt64 = np.datetime64(dt)

In [7]: dt
Out[7]: datetime.datetime(2012, 5, 1, 0, 0)

In [8]: ts
Out[8]: &lt;Timestamp: 2012-05-01 00:00:00&gt;

In [9]: dt64
Out[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')
</code></pre>

<p><em>Note: it's easy to get the datetime from the Timestamp:</em></p>

<pre><code>In [10]: ts.to_datetime()
Out[10]: datetime.datetime(2012, 5, 1, 0, 0)
</code></pre>

<p>But how do we extract the <code>datetime</code> or <code>Timestamp</code> from a <code>numpy.datetime64</code> (<code>dt64</code>)?</p>

<p>.</p>

<p>Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:</p>

<pre><code>dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')
</code></pre>

<p>which should be <code>datetime.datetime(2002, 6, 28, 1, 0)</code>, and not a long (!) (<code>1025222400000000000L</code>)...</p>
";;3;;2012-12-04T13:08:29.863;69.0;13703720;2017-08-02T06:59:32.067;2012-12-04T17:53:03.587;;1240268.0;;1240268.0;;1;140;<python><datetime><numpy><pandas>;Converting between datetime, Timestamp and datetime64;163489.0
1600;1600;13741439.0;2.0;"<p>I'd like to filter out weekend data and only look at data for weekdays (mon(0)-fri(4)).  I'm new to pandas, what's the best way to accomplish this in pandas?</p>

<pre><code>import datetime
from pandas import *

data = read_csv(""data.csv"")
data.my_dt 

Out[52]:
0     2012-10-01 02:00:39
1     2012-10-01 02:00:38
2     2012-10-01 02:01:05
3     2012-10-01 02:01:07
4     2012-10-01 02:02:03
5     2012-10-01 02:02:09
6     2012-10-01 02:02:03
7     2012-10-01 02:02:35
8     2012-10-01 02:02:33
9     2012-10-01 02:03:01
10    2012-10-01 02:08:53
11    2012-10-01 02:09:04
12    2012-10-01 02:09:09
13    2012-10-01 02:10:20
14    2012-10-01 02:10:45
...
</code></pre>

<p>I'd like to do something like:</p>

<pre><code>weekdays_only = data[data.my_dt.weekday() &lt; 5]
</code></pre>

<p>AttributeError: 'numpy.int64' object has no attribute 'weekday'</p>

<p>but this doesn't work, I haven't quite grasped how column datetime objects are accessed.</p>

<p>The eventual goal being to arrange hierarchically to weekday hour-range, something like:</p>

<pre><code>monday, 0-6, 7-12, 13-18, 19-23
tuesday, 0-6, 7-12, 13-18, 19-23
</code></pre>
";;1;;2012-12-06T09:36:35.400;6.0;13740672;2014-08-29T03:04:26.423;2014-04-23T02:47:57.230;;24718.0;;24718.0;;1;13;<python><pandas>;in pandas how can I groupby weekday() for a datetime column?;10573.0
1610;1610;13758846.0;2.0;"<p>If I import or create a pandas column that contains no spaces, I can access it as such:</p>

<pre><code>df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
                 'data1': range(7)})

df1.data1
</code></pre>

<p>which would return that series for me.  If, however, that column has a space in its name, it isn't accessible via that method:</p>

<pre><code>df2 = DataFrame({'key': ['a','b','d'],
                 'data 2': range(3)})

df2.data 2      # &lt;--- not the droid i'm looking for.
</code></pre>

<p>I know I can access it using .xs():</p>

<pre><code>df2.xs('data 2', axis=1)
</code></pre>

<p>There's <em>got</em> to be another way.  I've googled it like mad and can't think of any other way to google it.  I've read all 96 entries here on SO that contain ""column"" and ""string"" and ""pandas"" and could find no previous answer.  Is this the only way, or is there something better?</p>

<p>Thanks!</p>
";;0;;2012-12-07T04:49:28.040;2.0;13757090;2015-05-28T18:42:30.483;;;;;1850663.0;;1;16;<string><pandas>;Pandas column access w/column names containing spaces;12685.0
1630;1630;13786327.0;2.0;"<p>I'm starting from the pandas Data Frame docs here: <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p>

<p>I'd like to iteratively fill the Data Frame with values in a time series kind of calculation.
So basically, I'd like to initialize, data frame with columns A,B and timestamp rows, all 0 or all NaN.</p>

<p>I'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.</p>

<p>I'm currently using the code as below, but I feel it's kind of ugly and there must be a  way to do this with a data frame directly or just a better way in general.
Note: I'm using Python 2.7.</p>

<pre><code>import datetime as dt
import pandas as pd
import scipy as s

if __name__ == '__main__':
    base = dt.datetime.today().date()
    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]
    dates.sort()

    valdict = {}
    symbols = ['A','B', 'C']
    for symb in symbols:
        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )

    for thedate in dates:
        if thedate &gt; dates[0]:
            for symb in valdict:
                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]

    print valdict
</code></pre>
";;0;;2012-12-09T02:50:38.060;37.0;13784192;2017-05-23T15:56:27.003;;;;;1707931.0;;1;131;<python><dataframe><pandas>;Creating an empty Pandas DataFrame, then filling it?;326681.0
1632;1632;13788301.0;4.0;"<p>I have a <code>pandas.DatetimeIndex</code>, e.g.:</p>

<pre><code>pd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')
&gt;&gt;&gt; [2012-01-01 02:03:04, ..., 2012-01-01 02:03:04.002000]
</code></pre>

<p>I would like to round the dates (<code>Timestamp</code>s) to the nearest second. How do I do that? The expected result is similar to:</p>

<pre><code>[2012-01-01 02:03:04.000000, ..., 2012-01-01 02:03:04.000000]
</code></pre>

<p>Is it possible to accomplish this by rounding a Numpy <code>datetime64[ns]</code> to seconds without changing the <code>dtype</code> <code>[ns]</code>?</p>

<pre><code>np.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')
</code></pre>
";;4;;2012-12-09T08:42:47.233;1.0;13785932;2016-10-25T18:47:45.147;2012-12-09T14:43:26.967;;1579844.0;;1579844.0;;1;12;<date><datetime><numpy><pandas><date-format>;How to round a Pandas `DatetimeIndex`?;8474.0
1663;1663;13839029.0;3.0;"<p>I have python pandas dataframe, in which a column contains month name.</p>

<p>How can I  do a custom sort using a dictionary, for example:</p>

<pre><code>custom_dict = {'March':0, 'April':1, 'Dec':3}  
</code></pre>
";;1;;2012-12-12T11:09:42.730;8.0;13838405;2015-02-03T04:54:11.257;2012-12-12T11:54:02.947;;1240268.0;;1645853.0;;1;25;<python><pandas>;Custom sorting in pandas dataframe;17015.0
1671;1671;13842286.0;8.0;"<p><br>
I've created a pandas DataFrame</p>

<pre><code>df=DataFrame(index=['A','B','C'], columns=['x','y'])
</code></pre>

<p>and got this</p>

<pre>
    x    y
A  NaN  NaN
B  NaN  NaN
C  NaN  NaN
</pre>

<p><br>
Then I want to assign value to particular cell, for example for row 'C' and column 'x'.
I've expected to get such result:</p>

<pre>
    x    y
A  NaN  NaN
B  NaN  NaN
C  10  NaN
</pre>

<p>with this code:</p>

<pre><code>df.xs('C')['x']=10
</code></pre>

<p>but contents of <b>df</b> haven't changed. It's again only Nan's in dataframe. </p>

<p>Any suggestions?</p>
";;3;;2012-12-12T14:40:45.683;51.0;13842088;2017-06-28T15:39:03.673;;;;;1675248.0;;1;136;<python><pandas>;Set value for particular cell in pandas DataFrame;182280.0
1676;1676;13851602.0;3.0;"<p>I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2. I know I can use <code>df.dropna()</code> to get rid of rows that contain any <code>NaN</code>, but I'm not seeing how to remove rows based on a conditional expression. </p>

<p>The answer for <a href=""https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row"">this question</a> seems very close to what I want -- it seems like I should be able to do something like this:</p>

<pre><code>df[(len(df['column name']) &lt; 2)]
</code></pre>

<p>but I just get the error:</p>

<pre><code>KeyError: u'no item named False'
</code></pre>

<p>Can anyone tell me what I'm doing wrong?</p>
";;0;;2012-12-13T01:28:24.500;23.0;13851535;2017-03-27T15:05:56.163;2017-05-23T12:10:44.930;;-1.0;;1080717.0;;1;48;<python><pandas>;How to delete rows from a pandas DataFrame based on a conditional expression;79345.0
1682;1682;13854901.0;1.0;"<p>Another pandas question.</p>

<p>Reading Wes Mckinney's excellent book about Data Analysis and Pandas, I encountered the following thing that I thought should work:</p>

<p>Suppose I have some info about tips.</p>

<pre><code>In [119]:

tips.head()
Out[119]:
total_bill  tip      sex     smoker    day   time    size  tip_pct
0    16.99   1.01    Female  False   Sun     Dinner  2   0.059447
1    10.34   1.66    Male    False   Sun     Dinner  3   0.160542
2    21.01   3.50    Male    False   Sun     Dinner  3   0.166587
3    23.68   3.31    Male    False   Sun     Dinner  2   0.139780
4    24.59   3.61    Female  False   Sun     Dinner  4   0.146808
</code></pre>

<p>and I want to know the five largest tips in relation to the total bill, that is, <code>tip_pct</code> for smokers and non-smokers separately. So this works:</p>

<pre><code>def top(df, n=5, column='tip_pct'): 
    return df.sort_index(by=column)[-n:]

In [101]:

tips.groupby('smoker').apply(top)
Out[101]:
           total_bill   tip sex smoker  day time    size    tip_pct
smoker                                  
False   88   24.71   5.85    Male    False   Thur    Lunch   2   0.236746
185  20.69   5.00    Male    False   Sun     Dinner  5   0.241663
51   10.29   2.60    Female  False   Sun     Dinner  2   0.252672
149  7.51    2.00    Male    False   Thur    Lunch   2   0.266312
232  11.61   3.39    Male    False   Sat     Dinner  2   0.291990

True    109  14.31   4.00    Female  True    Sat     Dinner  2   0.279525
183  23.17   6.50    Male    True    Sun     Dinner  4   0.280535
67   3.07    1.00    Female  True    Sat     Dinner  1   0.325733
178  9.60    4.00    Female  True    Sun     Dinner  2   0.416667
172  7.25    5.15    Male    True    Sun     Dinner  2   0.710345
</code></pre>

<p>Good enough, but then I wanted to use pandas' transform to do the same like this:</p>

<pre><code>def top_all(df):
    return df.sort_index(by='tip_pct')

tips.groupby('smoker').transform(top_all)
</code></pre>

<p>but instead I get this:</p>

<pre><code>TypeError: Transform function invalid for data types
</code></pre>

<p>Why? I know that transform requires to return an array of the same dimensions that it accepts as input, so I thought I'd be complying with that requirement just sorting both slices (smokers and non-smokers) of the original DataFrame without changing their respective dimensions. Can anyone explain why it failed? </p>
";;0;;2012-12-13T06:46:04.593;11.0;13854476;2012-12-13T07:19:15.917;2012-12-13T06:52:09.670;;460147.0;;460147.0;;1;14;<python><aggregate><pandas>;pandas' transform doesn't work sorting groupby output;4246.0
1698;1698;13876784.0;3.0;"<p>I have a temperature file with many years temperature records, in a format as below:</p>

<pre><code>2012-04-12,16:13:09,20.6
2012-04-12,17:13:09,20.9
2012-04-12,18:13:09,20.6
2007-05-12,19:13:09,5.4
2007-05-12,20:13:09,20.6
2007-05-12,20:13:09,20.6
2005-08-11,11:13:09,20.6
2005-08-11,11:13:09,17.5
2005-08-13,07:13:09,20.6
2006-04-13,01:13:09,20.6
</code></pre>

<p>Every year has different numbers, time of the records, so the pandas datetimeindices are all different.</p>

<p>I want to plot the different year's data in the same figure for comparing . The X-axis is Jan to Dec, the Y-axis is temperature. How should I go about doing this? </p>
";;0;;2012-12-14T04:13:35.073;12.0;13872533;2017-08-02T15:21:45.810;2012-12-14T10:26:01.200;;1240268.0;;1843099.0;;1;31;<python><matplotlib><pandas>;Plot different DataFrames in the same figure;31002.0
1708;1708;13888546.0;2.0;"<p>I know that I can get the unique values of a <code>DataFrame</code> by resetting the index but is there a way to avoid this step and get the unique values directly?</p>

<p>Given I have:</p>

<pre><code>        C
 A B     
 0 one  3
 1 one  2
 2 two  1
</code></pre>

<p>I can do:</p>

<pre><code>df = df.reset_index()
uniq_b = df.B.unique()
df = df.set_index(['A','B'])
</code></pre>

<p>Is there a way built in pandas to do this?</p>
";;3;;2012-12-15T01:29:46.523;6.0;13888468;2014-06-30T11:38:56.107;;;;;8590.0;;1;30;<pandas>;Get unique values from index column in MultiIndex;21242.0
1714;1714;13893632.0;1.0;"<p>I have two pandas dataframes one called 'orders' and another one called 'daily_prices'.
daily_prices is as follows:</p>

<pre><code>              AAPL    GOOG     IBM    XOM
2011-01-10  339.44  614.21  142.78  71.57
2011-01-13  342.64  616.69  143.92  73.08
2011-01-26  340.82  616.50  155.74  75.89
2011-02-02  341.29  612.00  157.93  79.46
2011-02-10  351.42  616.44  159.32  79.68
2011-03-03  356.40  609.56  158.73  82.19
2011-05-03  345.14  533.89  167.84  82.00
2011-06-03  340.42  523.08  160.97  78.19
2011-06-10  323.03  509.51  159.14  76.84
2011-08-01  393.26  606.77  176.28  76.67
2011-12-20  392.46  630.37  184.14  79.97
</code></pre>

<p>orders is as follows:</p>

<pre><code>           direction  size ticker  prices
2011-01-10       Buy  1500   AAPL  339.44
2011-01-13      Sell  1500   AAPL  342.64
2011-01-13       Buy  4000    IBM  143.92
2011-01-26       Buy  1000   GOOG  616.50
2011-02-02      Sell  4000    XOM   79.46
2011-02-10       Buy  4000    XOM   79.68
2011-03-03      Sell  1000   GOOG  609.56
2011-03-03      Sell  2200    IBM  158.73
2011-06-03      Sell  3300    IBM  160.97
2011-05-03       Buy  1500    IBM  167.84
2011-06-10       Buy  1200   AAPL  323.03
2011-08-01       Buy    55   GOOG  606.77
2011-08-01      Sell    55   GOOG  606.77
2011-12-20      Sell  1200   AAPL  392.46
</code></pre>

<p>index of both dataframes is datetime.date.
'prices' column in the 'orders' dataframe was added by using a list comprehension to loop through all the orders and look up the specific ticker for the specific date in the 'daily_prices' data frame and then adding that list as a column to the 'orders' dataframe. I would like to do this using an array operation rather than something that loops. can it be done? i tried to use:</p>

<p>daily_prices.ix[dates,tickers] </p>

<p>but this returns a matrix of cartesian product of the two lists. i want it to return a column vector of only the  price of a specified ticker for a specified date.</p>
";;0;;2012-12-15T14:51:27.933;15.0;13893227;2012-12-15T15:47:22.063;;;;;1167915.0;;1;20;<python><pandas><vectorization>;Vectorized look-up of values in Pandas dataframe;12339.0
1724;1724;13921674.0;2.0;"<p>New to Python.</p>

<p>In R, you can get the dimension of a matrix using dim(...).   What is the corresponding function in Python Pandas for their data frame?</p>
";;0;;2012-12-17T20:27:52.850;2.0;13921647;2017-08-16T11:28:15.860;2015-11-18T11:15:50.567;;99256.0;;1911092.0;;1;43;<python><r><pandas>;Python - Dimension of Data Frame;25371.0
1732;1732;13999234.0;2.0;"<p>How can I retrieve specific columns from a pandas HDFStore?  I regularly work with very large data sets that are too big to manipulate in memory.  I would like to read in a csv file iteratively, append each chunk into HDFStore object, and then work with subsets of the data.  I have read in a simple csv file and loaded it into an HDFStore with the following code:    </p>

<pre><code>tmp = pd.HDFStore('test.h5')
chunker = pd.read_csv('cars.csv', iterator=True, chunksize=10, names=['make','model','drop'])
tmp.append('df', pd.concat([chunk for chunk in chunker], ignore_index=True))
</code></pre>

<p>And the output:</p>

<pre><code>In [97]: tmp
Out[97]:
&lt;class 'pandas.io.pytables.HDFStore'&gt;
File path: test.h5
/df     frame_table (typ-&gt;appendable,nrows-&gt;1930,indexers-&gt;[index])
</code></pre>

<p>My Question is how do I access specific columns from <code>tmp['df']</code>?  The documenation makes mention of a <code>select()</code> method and some <code>Term</code> objects.  The examples provided are applied to Panel data; however, and I'm too much of a novice to extend it to the simpler data frame case.  My guess is that I have to create an index of the columns somehow.  Thanks!</p>
";;0;;2012-12-18T04:08:09.357;14.0;13926089;2012-12-22T01:27:49.173;;;;;919872.0;;1;14;<python><pandas><hdfs>;Selecting columns from pandas.HDFStore table;13798.0
1739;1739;13937141.0;1.0;"<p>Q is similar to this:
<a href=""https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe"">use a list of values to select rows from a pandas dataframe</a></p>

<p>I want to dataframe if either value in two columns are in a list.
Return both columns (combine results of #1 and #4. </p>

<pre><code>import numpy as np
from pandas import *


d = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}

df = DataFrame(d)
print df

checkList = [1,7]

print df[df.one == 1 ]#1
print df[df.one == 7 ]#2
print df[df.two == 1 ]#3
print df[df.two == 7 ]#4

#print df[df.one == 1 or df.two ==7]
print df[df.one.isin(checkList)]
</code></pre>
";;0;;2012-12-18T16:11:22.367;5.0;13937022;2017-05-05T02:19:59.273;2017-05-23T10:31:09.757;;-1.0;;428862.0;;1;11;<python><pandas>;Using pandas to select rows using two different columns from dataframe?;14815.0
1743;1743;13938831.0;1.0;"<p>I'm a newbie to pandas dataframe, and I wanted to apply a function to each column so that it computes for each element x, x/max of column. </p>

<p>I referenced this question, but am having trouble accessing the maximum of each column. Thanks in advance
<a href=""https://stackoverflow.com/questions/12741092/pandas-dataframe-apply-function-to-all-columns"">Pandas DataFrame: apply function to all columns</a></p>

<p>Input:</p>

<pre><code>      A  B  C  D
   0  8  3  5  8
   1  9  4  0  4
   2  5  4  3  8
   3  4  8  5  1
</code></pre>

<p>Output:</p>

<pre><code>      A     B     C    D
   0  8/9  3/8  5/5  8/8
   1  9/9  4/8  0/5  4/8
   2  5/9  4/8  3/5  8/8
   3  4/9  8/8  5/5  1/8
</code></pre>
";;2;;2012-12-18T17:54:06.150;;13938704;2012-12-18T18:03:13.457;2017-05-23T10:26:52.370;;-1.0;;1490464.0;;1;11;<python><pandas>;Apply function on Pandas dataframe;1837.0
1791;1791;13998600.0;3.0;"<p>I have a time series object <code>grouped</code> of the type <code>&lt;pandas.core.groupby.SeriesGroupBy object at 0x03F1A9F0&gt;</code>. <code>grouped.sum()</code> gives the desired result but I cannot get rolling_sum to work with the <code>groupby</code> object. Is there any way to apply rolling functions to <code>groupby</code> objects? For example:</p>

<pre><code>x = range(0, 6)
id = ['a', 'a', 'a', 'b', 'b', 'b']
df = DataFrame(zip(id, x), columns = ['id', 'x'])
df.groupby('id').sum()
id    x
a    3
b   12
</code></pre>

<p>However, I would like to have something like:</p>

<pre><code>  id  x
0  a  0
1  a  1
2  a  3
3  b  3
4  b  7
5  b  12
</code></pre>
";;4;;2012-12-21T19:49:00.067;2.0;13996302;2016-12-16T19:31:54.603;2012-12-21T20:27:57.387;;1642513.0;;1642513.0;;1;15;<python><pandas>;Python - rolling functions for GroupBy object;10359.0
1796;1796;14000420.0;2.0;"<p>The default output format of <code>to_csv()</code> is:</p>

<pre><code>12/14/2012  12:00:00 AM
</code></pre>

<p>I cannot figure out how to output only the date part with specific format:</p>

<pre><code>20121214
</code></pre>

<p>or date and time in two separate columns in the csv file:</p>

<pre><code>20121214,  084530
</code></pre>

<p>The documentation is too brief to give me any clue as to how to do these. Can anyone help?</p>
";;0;;2012-12-22T03:40:35.670;11.0;13999850;2014-04-01T23:30:20.643;;;;;1642513.0;;1;28;<python><pandas>;How to specify date format when using pandas.to_csv?;23394.0
1803;1803;14016590.0;2.0;"<p>I have a pandas DataFrame like this:</p>

<pre><code>                    a         b
2011-01-01 00:00:00 1.883381  -0.416629
2011-01-01 01:00:00 0.149948  -1.782170
2011-01-01 02:00:00 -0.407604 0.314168
2011-01-01 03:00:00 1.452354  NaN
2011-01-01 04:00:00 -1.224869 -0.947457
2011-01-01 05:00:00 0.498326  0.070416
2011-01-01 06:00:00 0.401665  NaN
2011-01-01 07:00:00 -0.019766 0.533641
2011-01-01 08:00:00 -1.101303 -1.408561
2011-01-01 09:00:00 1.671795  -0.764629
</code></pre>

<p>Is there an efficient way to find the ""integer"" index of rows with NaNs? In this case the desired output should be <code>[3, 6]</code>.</p>
";;2;;2012-12-24T01:53:49.930;17.0;14016247;2012-12-25T18:41:23.723;;;;;1642513.0;;1;49;<python><pandas>;Python - find integer index of rows with NaN in pandas;59715.0
1815;1815;14060360.0;1.0;"<p>Another pandas question:</p>

<p>I have this table with hierarchical indexing:</p>

<pre><code>In [51]:
from pandas import DataFrame
f = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})
f.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]
f
Out[51]:
    level1 item1    level1 item2
                    level2 item2
    level3 item1    level3 item2
0         1              2
1         2              3
2         3              4
</code></pre>

<p>It happens that selecting <code>level1 item1</code> produces the following error:</p>

<pre><code>In [58]: f['level1 item1']
AssertionError: Index length did not match values
</code></pre>

<p>However, this seems to be somewhat related to the number of levels. When I reduce the number of levels to only two, there is no such error:</p>

<pre><code>from pandas import DataFrame
f = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})
f.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]
f
Out[59]:
     level1 item1   level1 item2
                    level1 item2
0          1              2
1          2              3
2          3              4
</code></pre>

<p>Instead, the previous DataFrame gives the expected series:</p>

<pre><code>In [63]:
f['level1 item1']
Out[63]:
0    1
1    2
2    3
Name: level1 item1
</code></pre>

<p>Filling the gap below <code>level1 item1</code> with a dummy character ""fixes"" this issue but it's not a good solution.</p>

<p>How can I fix this issue without resorting to fill those columns with dummy names?</p>

<p>Thanks a lot!</p>

<hr>

<h3>Original example:</h3>

<p><img src=""https://i.stack.imgur.com/rUV07.png"" alt=""enter image description here""></p>

<p>This table was produced using the following indexes: </p>

<pre><code>index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),
np.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),
np.array(['', '', '', '', '', '', '', '','', '', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),
np.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]

df.columns = index
</code></pre>

<p>That's an almost perfect copy of some data in a CSV file but you can see that below ""Number of returns"", ""Percent of total"" and ""Adjusted gross income less deficit"" there  is a gap. That gap produces this error when I try to select Number of returns:</p>

<pre><code>In [68]: df['Taxable returns']['Number of returns']
AssertionError: Index length did not match values
</code></pre>

<p>I don't understand this error. So a good explanation would be highly appreciated. In any case, when I fill that gap using this index (notice the first elements in the third numpy array): </p>

<pre><code>index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),
np.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),
np.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),
np.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]

df.columns = index
</code></pre>

<p>I get proper results:</p>

<pre><code>In [71]: df['Taxable returns']['Number of returns']
Out[71]:
7
Average (dollars)
0    90,660,104
1    3,495
...
</code></pre>
";;2;;2012-12-24T21:38:40.420;1.0;14025879;2012-12-27T19:46:05.930;2012-12-26T04:07:28.097;;460147.0;;460147.0;;1;11;<python><pandas><hierarchical>;Assertion Error in columns in DataFrame with hierarchical indexing;1142.0
1831;1831;;4.0;"<p>Sorry just getting into Pandas, this seems like it should be a very straight forward question. How can I use the <code>isin('X')</code> to remove rows that <strong>are in</strong> the list <code>X</code>? In R I would write <code>!which(a %in% b)</code>.</p>
";;0;;2012-12-27T15:24:13.760;11.0;14057007;2017-08-08T10:31:35.703;2012-12-28T15:29:56.460;;1301710.0;;1927088.0;;1;21;<python><filtering><pandas>;Remove rows not .isin('X');20970.0
1834;1834;14060625.0;5.0;"<p>I'm trying to multiply two existing columns in a pandas Dataframe (orders_df) - Prices (stock close price) and Amount (stock quantities) and add the calculation to a new column called 'Value'. For some reason when I run this code, all the rows under the 'Value' column are positive numbers, while some of the rows should be negative. Under the Action column in the DataFrame there are seven rows with the 'Sell' string and seven with the 'Buy' string.</p>

<pre><code>for i in orders_df.Action:
 if i  == 'Sell':
  orders_df['Value'] = orders_df.Prices*orders_df.Amount
 elif i == 'Buy':
  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)
</code></pre>

<p>Please let me know what i'm doing wrong !</p>
";;0;;2012-12-27T18:02:41.717;14.0;14059094;2017-05-25T14:18:35.017;2012-12-28T14:36:58.257;;1301710.0;;1889418.0;;1;24;<python><python-2.7><pandas>;I want to multiply two columns in a pandas DataFrame and add the result into a new column;39480.0
1880;1880;14110955.0;1.0;"<p>I have a <code>df</code> :</p>

<pre><code>&gt;&gt;&gt; df
                   sales     cash
STK_ID RPT_Date                  
000568 20120930   80.093   57.488
000596 20120930   32.585   26.177
000799 20120930   14.784    8.157
</code></pre>

<p>And want to change first row's index value from <code>('000568','20120930')</code> to <code>('000999','20121231')</code> . Final result will be :</p>

<pre><code>&gt;&gt;&gt; df
                   sales     cash
STK_ID RPT_Date                  
000999 20121231   80.093   57.488
000596 20120930   32.585   26.177
000799 20120930   14.784    8.157
</code></pre>

<p>How to do ?</p>
";;0;;2013-01-01T13:05:42.883;2.0;14110721;2017-08-25T18:23:34.540;;;;;1072888.0;;1;12;<python><pandas>;How to change Pandas dataframe index value?;32137.0
1897;1897;14148511.0;1.0;"<p>Can the <a href=""http://pandas.pydata.org/"" rel=""noreferrer"">pandas</a> data analysis module run on Google App Engine?</p>

<p>My first inclination is no: the web page states ""critical code paths <strong>compiled to C</strong>"". So since this is not a purely python package, you cannot simply copy a directory or ZIP file into your app engine project.</p>

<p>Is it possible to ""disable"" the C extensions and have the module run in pure python (albeit slower)?</p>
";;2;;2013-01-03T18:24:00.713;;14144867;2015-08-24T05:42:29.813;;;;;359001.0;;1;16;<python><google-app-engine><pandas>;Can Pandas run on Google App Engine for Python?;2894.0
1902;1902;14163209.0;4.0;"<p>I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.</p>

<p>Any ideas? </p>
";;1;;2013-01-04T18:26:06.627;12.0;14162723;2017-08-02T19:47:59.477;;;;;365781.0;;1;36;<numpy><pandas><mysql-python>;Replacing Pandas or Numpy Nan with a None to use with MysqlDB;22191.0
1910;1910;14179954.0;3.0;"<p>I want to plot multiple lines from a pandas dataframe and setting different options for each line. I would like to do something like</p>

<pre><code>testdataframe=pd.DataFrame(np.arange(12).reshape(4,3))
testdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])
</code></pre>

<p>This will raise some error messages:</p>

<ul>
<li><p>linewidth is not callable with a list</p></li>
<li><p>In style I can't use 's' and 'o' or any other alphabetical symbol, when defining colors in a list</p></li>
</ul>

<p>Also there is some more stuff which seems weird to me</p>

<ul>
<li><p>when I add another plot command to the above code <code>testdataframe[0].plot()</code> it will plot this line in the same plot, if I add the command <code>testdataframe[[0,1]].plot()</code> it will create a new plot</p></li>
<li><p>If i would call <code>testdataframe[0].plot(style=['s-','o-','^-'],color=['b','r','y'])</code> it is fine with a list in style, but not with a list in color</p></li>
</ul>

<p>Hope somebody can help, thanks.</p>
";;0;;2013-01-06T00:58:33.553;9.0;14178194;2017-03-16T18:19:23.437;2013-01-06T02:30:38.327;;1867980.0;;1952043.0;;1;19;<python><plot><pandas>;Python pandas, Plotting options for multiple lines;22674.0
1924;1924;37749078.0;3.0;"<p>Is there a shorter way of dropping a column MultiIndex level (in my case, <code>basic_amt</code>) except transposing it twice?</p>

<pre><code>In [704]: test
Out[704]: 
           basic_amt               
Faculty          NSW  QLD  VIC  All
All                1    1    2    4
Full Time          0    1    0    1
Part Time          1    0    2    3

In [705]: test.reset_index(level=0, drop=True)
Out[705]: 
         basic_amt               
Faculty        NSW  QLD  VIC  All
0                1    1    2    4
1                0    1    0    1
2                1    0    2    3

In [711]: test.transpose().reset_index(level=0, drop=True).transpose()
Out[711]: 
Faculty    NSW  QLD  VIC  All
All          1    1    2    4
Full Time    0    1    0    1
Part Time    1    0    2    3
</code></pre>
";;0;;2013-01-07T03:59:30.153;2.0;14189695;2016-06-10T13:10:57.740;2016-06-10T12:42:27.177;;3730397.0;;1479269.0;;1;17;<python><pandas><dataframe>;Reset a columns MultiIndex levels;9898.0
1926;1926;14193170.0;2.0;"<p>Summary:
This doesn't work:</p>

<pre><code>df[df.key==1]['D'] = 1
</code></pre>

<p>but this does:</p>

<pre><code>df.D[df.key==1] = 1
</code></pre>

<p>Why?</p>

<p>Reproduction:</p>

<pre><code>In [1]: import pandas as pd

In [2]: from numpy.random import randn

In [4]: df = pd.DataFrame(randn(6,3),columns=list('ABC'))

In [5]: df
Out[5]: 
          A         B         C
0  1.438161 -0.210454 -1.983704
1 -0.283780 -0.371773  0.017580
2  0.552564 -0.610548  0.257276
3  1.931332  0.649179 -1.349062
4  1.656010 -1.373263  1.333079
5  0.944862 -0.657849  1.526811

In [6]: df['D']=0.0

In [7]: df['key']=3*[1]+3*[2]

In [8]: df
Out[8]: 
          A         B         C  D  key
0  1.438161 -0.210454 -1.983704  0    1
1 -0.283780 -0.371773  0.017580  0    1
2  0.552564 -0.610548  0.257276  0    1
3  1.931332  0.649179 -1.349062  0    2
4  1.656010 -1.373263  1.333079  0    2
5  0.944862 -0.657849  1.526811  0    2
</code></pre>

<p>This doesn't work:</p>

<pre><code>In [9]: df[df.key==1]['D'] = 1

In [10]: df
Out[10]: 
          A         B         C  D  key
0  1.438161 -0.210454 -1.983704  0    1
1 -0.283780 -0.371773  0.017580  0    1
2  0.552564 -0.610548  0.257276  0    1
3  1.931332  0.649179 -1.349062  0    2
4  1.656010 -1.373263  1.333079  0    2
5  0.944862 -0.657849  1.526811  0    2
</code></pre>

<p>but this does:</p>

<pre><code>In [11]: df.D[df.key==1] = 3.4

In [12]: df
Out[12]: 
          A         B         C    D  key
0  1.438161 -0.210454 -1.983704  3.4    1
1 -0.283780 -0.371773  0.017580  3.4    1
2  0.552564 -0.610548  0.257276  3.4    1
3  1.931332  0.649179 -1.349062  0.0    2
4  1.656010 -1.373263  1.333079  0.0    2
5  0.944862 -0.657849  1.526811  0.0    2
</code></pre>

<p><a href=""http://nbviewer.ipython.org/4473426/"" rel=""nofollow noreferrer"">Link to notebook</a></p>

<p>My question is: Why does only the 2nd way work? I can't seem to see a difference in selection/indexing logic?</p>

<p>Version is 0.10.0</p>

<p>Edit: This should not be done like this anymore. Since 0.11 there is <code>.loc</code>, see here: <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html</a></p>
";;2;;2013-01-07T09:03:57.777;7.0;14192741;2014-07-11T23:59:31.700;2014-07-11T23:59:31.700;;680232.0;;680232.0;;1;12;<python><pandas>;Understanding pandas dataframe indexing;5916.0
1946;1946;14224489.0;6.0;"<p>As part of a unit test, I need to test two DataFrames for equality.  The order of the columns in the DataFrames is not important to me.  However, it seems to matter to Pandas:</p>

<pre><code>import pandas
df1 = pandas.DataFrame(index = [1,2,3,4])
df2 = pandas.DataFrame(index = [1,2,3,4])
df1['A'] = [1,2,3,4]
df1['B'] = [2,3,4,5]
df2['B'] = [2,3,4,5]
df2['A'] = [1,2,3,4]
df1 == df2
</code></pre>

<p>Results in:</p>

<pre><code>Exception: Can only compare identically-labeled DataFrame objects
</code></pre>

<p>I believe the expression <code>df1 == df2</code> should evaluate to a DataFrame containing all <code>True</code> values.  Obviously it's debatable what the correct functionality of <code>==</code> should be in this context.  My question is: Is there a Pandas method that does what I want?  That is, is there a way to do equality comparison that ignores column order?</p>
";;2;;2013-01-08T21:16:05.603;5.0;14224172;2017-06-01T19:16:01.673;;;;;1572508.0;;1;20;<python><pandas>;Equality in Pandas DataFrames - Column Order Matters?;14439.0
1949;1949;14225838.0;1.0;"<p>How can I export a list of DataFrames into one Excel spreadsheet?<br>
The docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html""><code>to_excel</code></a> state:</p>

<blockquote>
  <p>Notes<br>
  If passing an existing ExcelWriter object, then the sheet will be added
  to the existing workbook.  This can be used to save different
  DataFrames to one workbook</p>
  
  <p><code>writer = ExcelWriter('output.xlsx')</code><br>
  <code>df1.to_excel(writer, 'sheet1')</code><br>
  <code>df2.to_excel(writer, 'sheet2')</code><br>
  <code>writer.save()</code></p>
</blockquote>

<p>Following this, I thought I could write a function which saves a list of DataFrames to one spreadsheet as follows:</p>

<pre><code>from openpyxl.writer.excel import ExcelWriter
def save_xls(list_dfs, xls_path):
    writer = ExcelWriter(xls_path)
    for n, df in enumerate(list_dfs):
        df.to_excel(writer,'sheet%s' % n)
    writer.save()
</code></pre>

<p>However (with a list of two small DataFrames, each of which can save <code>to_excel</code> individually), an exception is raised <em>(Edit: traceback removed)</em>:</p>

<pre><code>AttributeError: 'str' object has no attribute 'worksheets'
</code></pre>

<p>Presumably I am not calling <a href=""http://www.nullege.com/codes/search?cq=openpyxl.writer.excel.ExcelWriter""><code>ExcelWriter</code></a> correctly, how should I be in order to do this?</p>
";;0;;2013-01-08T23:12:25.383;16.0;14225676;2013-02-04T21:55:52.057;2013-01-08T23:42:42.827;;1240268.0;;1240268.0;;1;43;<python><pandas><openpyxl>;Save list of DataFrames to multisheet Excel spreadsheet;23952.0
1962;1962;;2.0;"<p>I have a dataframe with ~300K rows and ~40 columns.
I want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily.</p>

<p>I can create a mask explicitly:</p>

<pre><code>mask=False
for col in df.columns: mask = mask | df[col].isnull()
dfnulls = df[mask]
</code></pre>

<p>Or I can do something like:</p>

<pre><code>df.ix[df.index[(df.T == np.nan).sum() &gt; 1]]
</code></pre>

<p>Is there a more elegant way of doing it (locating rows with nulls in them)?</p>
";;0;;2013-01-09T22:22:05.290;20.0;14247586;2017-06-22T14:15:37.523;2016-11-17T10:43:25.713;;202229.0;;1442475.0;;1;60;<python><pandas><null><nan>;Python Pandas How to select rows with one or more nulls from a DataFrame without listing columns explicitly?;66974.0
1966;1966;14248948.0;1.0;"<p>Given:</p>

<pre><code>ser = Series(['one', 'two', 'three', 'two', 'two'])
</code></pre>

<p>How do I plot a basic histogram of these values?</p>

<p>Here is an ASCII version of what I'd want to see in matplotlib:</p>

<pre><code>     X
 X   X   X
-------------
one two three
</code></pre>

<p>I'm tired of seeing:</p>

<pre><code>TypeError: cannot concatenate 'str' and 'float' objects
</code></pre>
";;0;;2013-01-10T00:05:42.593;2.0;14248706;2013-01-10T00:43:33.317;;;;;26002.0;;1;18;<pandas><histogram>;How can I plot a histogram in pandas using nominal values?;8485.0
1973;1973;14268804.0;11.0;"<p>I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.</p>

<p>One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I'm not talking about ""big data"" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.</p>

<p>My first thought is to use <code>HDFStore</code> to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:</p>

<p>What are some best-practice workflows for accomplishing the following:</p>

<ol>
<li>Loading flat files into a permanent, on-disk database structure</li>
<li>Querying that database to retrieve data to feed into a pandas data structure</li>
<li>Updating the database after manipulating pieces in pandas</li>
</ol>

<p>Real-world examples would be much appreciated, especially from anyone who uses pandas on ""large data"".</p>

<p>Edit -- an example of how I would like this to work:</p>

<ol>
<li>Iteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.</li>
<li>In order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.</li>
<li>I would create new columns by performing various operations on the selected columns.</li>
<li>I would then have to append these new columns into the database structure.</li>
</ol>

<p>I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.</p>

<p>Edit -- Responding to Jeff's questions specifically:</p>

<ol>
<li>I am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.</li>
<li>Typical operations involve combining several columns using conditional logic into a new, compound column. For example, <code>if var1 &gt; 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'</code>.  The result of these operations is a new column for every record in my dataset.</li>
<li>Finally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.</li>
<li>A typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.</li>
<li>It's pretty rare that I would subset by rows when creating a new column.  However, it's pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.</li>
<li>The modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What I'm doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.</li>
</ol>

<p>It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).</p>
";;4;;2013-01-10T16:20:32.017;570.0;14262433;2017-03-26T05:59:45.757;2014-05-23T11:40:37.257;;759866.0;;919872.0;;1;577;<python><mongodb><pandas><large-data><hdf5>;"""Large data"" work flows using pandas";161470.0
2015;2015;14306902.0;1.0;"<p>What is the best way to make a series of scatter plots using <code>matplotlib</code> from a <code>pandas</code> dataframe in Python? </p>

<p>For example, if I have a dataframe <code>df</code> that has some columns of interest, I find myself typically converting everything to arrays:</p>

<pre><code>import matplotlib.pylab as plt
# df is a DataFrame: fetch col1 and col2 
# and drop na rows if any of the columns are NA
mydata = df[[""col1"", ""col2""]].dropna(how=""any"")
# Now plot with matplotlib
vals = mydata.values
plt.scatter(vals[:, 0], vals[:, 1])
</code></pre>

<p>The problem with converting everything to array before plotting is that it forces you to break out of dataframes.</p>

<p>Consider these two use cases where having the full dataframe is essential to plotting:</p>

<ol>
<li><p>For example, what if you wanted to now look at all the values of <code>col3</code> for the corresponding values that you plotted in the call to <code>scatter</code>, and color each point (or size) it by that value? You'd have to go back, pull out the non-na values of <code>col1,col2</code> and check what their corresponding values.</p>

<p>Is there a way to plot while preserving the dataframe? For example:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])
# plot a scatter of col1 by col2, with sizes according to col3
scatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])
</code></pre></li>
<li><p>Similarly, imagine that you wanted to filter or color each point differently depending on the values of some of its columns. E.g. what if you wanted to automatically plot the labels of the points that meet a certain cutoff on <code>col1, col2</code> alongside them (where the labels are stored in another column of the df), or color these points differently, like people do with dataframes in R.  For example:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) 
myscatter = scatter(mydata[[""col1"", ""col2""]], s=1)
# Plot in red, with smaller size, all the points that 
# have a col2 value greater than 0.5
myscatter.replot(mydata[""col2""] &gt; 0.5, color=""red"", s=0.5)
</code></pre></li>
</ol>

<p>How can this be done?</p>

<p><strong>EDIT</strong> Reply to crewbum:</p>

<p>You say that the best way is to plot each condition (like <code>subset_a</code>, <code>subset_b</code>) separately. What if you have many conditions, e.g. you want to split up the scatters into 4 types of points or even more, plotting each in different shape/color. How can you elegantly apply condition a, b, c, etc. and make sure you then plot ""the rest"" (things not in any of these conditions) as the last step? </p>

<p>Similarly in your example where you plot <code>col1,col2</code> differently based on <code>col3</code>, what if there are NA values that break the association between <code>col1,col2,col3</code>? For example if you want to plot all <code>col2</code> values based on their <code>col3</code> values, but some rows have an NA value in either <code>col1</code> or <code>col3</code>, forcing you to use <code>dropna</code> first. So you would do:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")
</code></pre>

<p>then you can plot using <code>mydata</code> like you show -- plotting the scatter between <code>col1,col2</code> using the values of <code>col3</code>. But <code>mydata</code> will be missing some points that have values for <code>col1,col2</code> but are NA for <code>col3</code>, and those still have to be plotted... so how would you basically plot ""the rest"" of the data, i.e. the points that are <em>not</em> in the filtered set <code>mydata</code>?</p>
";;2;;2013-01-13T02:38:30.010;43.0;14300137;2017-06-27T22:28:09.407;2015-08-31T21:46:32.590;;1832942.0;;248237.0;;1;52;<python><matplotlib><plot><dataframe><pandas>;making matplotlib scatter plots from dataframes in Python's pandas;43938.0
2016;2016;14307460.0;3.0;"<p>I'm looking for a way to do something like the various <code>rolling_*</code> functions of <code>pandas</code>, but I want the window of the rolling computation to be defined by a range of values (say, a range of values of a column of the DataFrame), not by the number of rows in the window.</p>

<p>As an example, suppose I have this data:</p>

<pre><code>&gt;&gt;&gt; print d
   RollBasis  ToRoll
0          1       1
1          1       4
2          1      -5
3          2       2
4          3      -4
5          5      -2
6          8       0
7         10     -13
8         12      -2
9         13      -5
</code></pre>

<p>If I do something like <code>rolling_sum(d, 5)</code>, I get a rolling sum in which each window contains 5 rows.  But what I want is a rolling sum in which each window contains a certain range of values of <code>RollBasis</code>.  That is, I'd like to be able to do something like <code>d.roll_by(sum, 'RollBasis', 5)</code>, and get a result where the first window contains all rows whose <code>RollBasis</code> is between 1 and 5, then the second window contains all rows whose <code>RollBasis</code> is between 2 and 6, then the third window contains all rows whose <code>RollBasis</code> is between 3 and 7, etc.  The windows will not have equal numbers of rows, but the range of <code>RollBasis</code> values selected in each window will be the same.  So the output should be like:</p>

<pre><code>&gt;&gt;&gt; d.roll_by(sum, 'RollBasis', 5)
    1    -4    # sum of elements with 1 &lt;= Rollbasis &lt;= 5
    2    -4    # sum of elements with 2 &lt;= Rollbasis &lt;= 6
    3    -6    # sum of elements with 3 &lt;= Rollbasis &lt;= 7
    4    -2    # sum of elements with 4 &lt;= Rollbasis &lt;= 8
    # etc.
</code></pre>

<p>I can't do this with <code>groupby</code>, because <code>groupby</code> always produces disjoint groups.  I can't do it with the rolling functions, because their windows always roll by number of rows, not by values.  So how can I do it?</p>
";;0;;2013-01-13T04:50:56.850;13.0;14300768;2014-01-17T15:22:13.390;2013-01-13T05:02:41.370;;1427416.0;;1427416.0;;1;16;<python><pandas>;pandas rolling computation with window based on values instead of counts;9850.0
2018;2018;14306921.0;1.0;"<p>If I have the following Dataframe</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'Name': ['Bob'] * 3 + ['Alice'] * 3, \
'Destination': ['Athens', 'Rome'] * 3, 'Length': np.random.randint(1, 6, 6)}) 
&gt;&gt;&gt; df    
  Destination  Length   Name
0      Athens       3    Bob
1        Rome       5    Bob
2      Athens       2    Bob
3        Rome       1  Alice
4      Athens       3  Alice
5        Rome       5  Alice
</code></pre>

<p>I can goup by name and destination...</p>

<pre><code>&gt;&gt;&gt; grouped = df.groupby(['Name', 'Destination'])
&gt;&gt;&gt; for nm, gp in grouped:
&gt;&gt;&gt;     print nm
&gt;&gt;&gt;     print gp
('Alice', 'Athens')
  Destination  Length   Name
4      Athens       3  Alice
('Alice', 'Rome')
  Destination  Length   Name
3        Rome       1  Alice
5        Rome       5  Alice
('Bob', 'Athens')
  Destination  Length Name
0      Athens       3  Bob
2      Athens       2  Bob
('Bob', 'Rome')
  Destination  Length Name
1        Rome       5  Bob
</code></pre>

<p>but I would like a new multi-indexed dataframe out of it that looks something like</p>

<pre><code>                Length
Alice   Athens       3
        Rome         1
        Rome         5
Bob     Athens       3
        Athens       2
        Rome         5
</code></pre>

<p>It seems there should be a way to do something like <code>Dataframe(grouped)</code> to get my multi-indexed Dataframe, but instead I get a <code>PandasError</code> (""DataFrame constructor not properly called!"").</p>

<p>What is the easiest way to get this? Also, anyone know if there will ever be an option to pass a groupby object to the constructor, or if I'm just doing it wrong?</p>

<p>Thanks</p>
";;0;;2013-01-13T08:36:21.320;5.0;14301913;2013-01-13T18:51:41.580;2013-01-13T18:51:41.580;;243434.0;;386279.0;;1;11;<python><group-by><dataframe><pandas><multi-index>;Convert pandas group by object to multi-indexed Dataframe;4483.0
2044;2044;14345875.0;2.0;"<p>I have a similar problem to the one posted here: </p>

<p><a href=""https://stackoverflow.com/questions/13682044/pandas-dataframe-remove-unwanted-parts-from-strings-in-a-column"">Pandas DataFrame: remove unwanted parts from strings in a column</a></p>

<p>I need to remove newline characters from within a string in a DataFrame. Basically, I've accessed an api using python's json module and that's all ok. Creating the DataFrame works amazingly, too. However, when I want to finally output the end result into a csv, I get a bit stuck, because there are newlines that are creating false 'new rows' in the csv file.</p>

<p>So basically I'm trying to turn this: </p>

<p>'...this is a paragraph.</p>

<p>And this is another paragraph...'</p>

<p>into this:</p>

<p>'...this is a paragraph. And this is another paragraph...'</p>

<p>I don't care about preserving any kind of '\n' or any special symbols for the paragraph break. So it can be stripped right out.</p>

<p>I've tried a few variations:</p>

<pre><code>misc['product_desc'] = misc['product_desc'].strip('\n')

AttributeError: 'Series' object has no attribute 'strip'
</code></pre>

<p>here's another</p>

<pre><code>misc['product_desc'] = misc['product_desc'].str.strip('\n')

TypeError: wrapper() takes exactly 1 argument (2 given)

misc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\n'))
misc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\n\t'))
</code></pre>

<p>There is no error message, but the newline characters don't go away, either. Same thing with this:</p>

<pre><code>misc = misc.replace('\n', '')
</code></pre>

<p>The write to csv line is this:</p>

<pre><code>misc_id.to_csv('C:\Users\jlalonde\Desktop\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')
</code></pre>

<p>Version of Pandas is 0.9.1</p>

<p>Thanks! :)</p>
";;0;;2013-01-15T19:54:38.240;7.0;14345739;2015-11-30T07:03:08.363;2017-05-23T12:34:47.163;;-1.0;;1819380.0;;1;20;<python><csv><pandas>;Replacing part of string in python pandas dataframe;41978.0
2046;2046;;7.0;"<p>Is there a way to make <code>matplotlib</code> behave identically to R, or almost like R, in terms of plotting defaults? For example R treats its axes pretty differently from <code>matplotlib</code>. The following histogram
<img src=""https://i.stack.imgur.com/PC9A7.png"" alt=""enter image description here""></p>

<p>has ""floating axes"" with outward ticks, such that there are no inner ticks (unlike <code>matplotlib</code>) and the axes do not cross ""near"" the origin. Also, the histogram can ""spillover"" to values that are not marked by the tick - e.g. the x-axis ends at 3 but the histograms extends slightly beyond it. How can this be achieved automatically for all histograms in <code>matplotlib</code>?</p>

<p>Related question: scatter plots and line plots have different default axes settings in R, for example:
<img src=""https://i.stack.imgur.com/Ln7QH.png"" alt=""enter image description here""></p>

<p>There no inner ticks again and the ticks face outward. Also, the ticks start slightly after the origin point (where the y and x axes cross at the bottom left of the axes) and the ticks end slightly before the axes end. This way the labels of the lowest x-axis tick and lowest y-axis tick can't really cross, because there's a space between them and this gives the plots a very elegant clean look.  Note that there's also considerably more space between the axes ticklabels and the ticks themselves.</p>

<p>Also, by default there are no ticks on the non-labeled x or y axes, meaning the y-axis on the left that is parallel to the labeled y-axis on the right has no ticks, and same for the x-axis, again removing clutter from the plots.</p>

<p>Is there a way to make matplotlib look like this? And in general to look by default as much as default R plots?  I like <code>matplotlib</code> a lot but I think the R defaults / out-of-the-box plotting behavior really have gotten things right and its default settings rarely lead to overlapping tick labels, clutter or squished data, so I  would like the defaults to be as much like that as possible.</p>
";;5;;2013-01-15T23:45:53.873;24.0;14349055;2015-01-21T13:42:40.760;;;;;248237.0;;1;53;<python><r><matplotlib><plot><pandas>;making matplotlib graphs look like R by default?;21800.0
2053;2053;14360423.0;2.0;"<p>I have a pandas.DataFrame with measurements taken at consecutive points in time. Along with each measurement the system under observation had a distinct state at each point in time. Hence, the DataFrame also contains a column with the state of the system at each measurement. State changes are much slower than the measurement interval. As a result, the column indicating the states might look like this (index: state):</p>

<pre><code>1:  3
2:  3
3:  3
4:  3
5:  4
6:  4
7:  4
8:  4
9:  1
10: 1
11: 1
12: 1
13: 1
</code></pre>

<p>Is there an easy way to retrieve the indices of each segment of consecutively equal states. That means I would like to get something like this:</p>

<pre><code>[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]
</code></pre>

<p>The result might also be in something different than plain lists.</p>

<p>The only solution I could think of so far is manually iterating over the rows, finding segment change points and reconstructing the indices from these change points, but I have the hope that there is an easier solution.</p>
";;0;;2013-01-16T12:31:42.877;12.0;14358567;2016-12-05T21:34:29.980;;;;;283649.0;;1;14;<python><pandas>;Finding consecutive segments in a pandas data frame;9861.0
2061;2061;;3.0;"<p>In short ... I have a Python Pandas data frame that is read in from an Excel file using 'read_table'.  I would like to keep a handful of the series from the data, and purge the rest.  I know that I can just delete what I don't want one-by-one using 'del data['SeriesName']', but what I'd rather do is specify what to keep instead of specifying what to delete.</p>

<p>If the simplest answer is to copy the existing data frame into a new data frame that only contains the series I want, and then delete the existing frame in its entirety, I would satisfied with that solution ... but if that is indeed the best way, can someone walk me through it?</p>

<p>TIA ... I'm a newb to Pandas.  :)</p>
";;0;;2013-01-16T16:57:30.890;2.0;14363640;2017-06-16T15:21:14.740;2013-01-16T17:20:35.227;;344160.0;;344160.0;;1;17;<python><pandas>;Python Pandas - Deleting multiple series from a data frame in one command;25631.0
2065;2065;14365647.0;4.0;"<p>I have a CSV file, <code>""value.txt""</code> with the following content: 
the first few rows of the file are :</p>

<pre><code>Date,""price"",""factor_1"",""factor_2""
2012-06-11,1600.20,1.255,1.548
2012-06-12,1610.02,1.258,1.554
2012-06-13,1618.07,1.249,1.552
2012-06-14,1624.40,1.253,1.556
2012-06-15,1626.15,1.258,1.552
2012-06-16,1626.15,1.263,1.558
2012-06-17,1626.15,1.264,1.572
</code></pre>

<p>In R we can read this file in using </p>

<pre><code>price &lt;- read.csv(""value.txt"")  
</code></pre>

<p>and that will return a data.frame which I can use for statistical operations:</p>

<pre><code>&gt; price &lt;- read.csv(""value.txt"")
&gt; price
     Date   price factor_1 factor_2
1  2012-06-11 1600.20    1.255    1.548
2  2012-06-12 1610.02    1.258    1.554
3  2012-06-13 1618.07    1.249    1.552
4  2012-06-14 1624.40    1.253    1.556
5  2012-06-15 1626.15    1.258    1.552
6  2012-06-16 1626.15    1.263    1.558
7  2012-06-17 1626.15    1.264    1.572
</code></pre>

<p>Is there a Pythonic way to get the same functionality?</p>
";;3;;2013-01-16T18:50:15.153;11.0;14365542;2014-03-30T20:51:24.777;2014-03-30T20:51:24.777;;656912.0;;1815281.0;;1;35;<python><r><csv><pandas>;read csv file and return data.frame in Python;64795.0
2073;2073;14383654.0;2.0;"<p>Is there an easy way to export a data frame (or even a part of it) to LaTeX?  </p>

<p><em>I searched in google and was only able to find solutions using asciitables.</em></p>
";;1;;2013-01-17T13:40:24.257;11.0;14380371;2017-05-11T14:33:35.433;2013-01-17T18:07:44.777;;1240268.0;;1898534.0;;1;30;<python><latex><dataframe><pandas>;Export a LaTeX table from pandas DataFrame;10522.0
2097;2097;;4.0;"<p>I have installed pandas on python 3.3, and coded like this:</p>

<pre><code>import csv
import pandas
from pandas import DataFrame

csvdata = pandas.read_csv('datafile.csv')
df = DataFrame(csvdata)
</code></pre>

<p>It comes with the following error message:</p>

<pre><code>cannot import name hashtable
Traceback (most recent call last):
  File ""C:\Users\document\test4.py"", line 5, in &lt;module&gt;
    import pandas
  File ""C:\Python33\lib\site-packages\pandas\__init__.py"", line 6, in &lt;module&gt;
    from . import hashtable, tslib, lib
ImportError: cannot import name hashtable
</code></pre>

<p>Could anyone help me figure out how to solve this error?  Python and pandas were successfully installed.</p>
";;2;;2013-01-20T08:48:56.137;5.0;14422976;2014-08-19T02:10:08.143;2013-06-15T23:50:01.543;;1240268.0;;1994194.0;;1;17;<python-3.x><pandas>;Importing pandas shows ImportError: cannot import name hashtable;29985.0
2103;2103;14432914.0;2.0;"<p>I have a list of stockmarket data pulled from Yahoo in a pandas DataFrame (see format below). The date is serving as the index in the DataFrame. I want to write the data (including the index) out to a SQLite database. </p>

<pre><code>             AAPL     GE
Date
2009-01-02  89.95  14.76
2009-01-05  93.75  14.38
2009-01-06  92.20  14.58
2009-01-07  90.21  13.93
2009-01-08  91.88  13.95
</code></pre>

<p>Based on my reading of the write_frame code for Pandas, it <a href=""https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L163"">does not currently support writing the index</a>. I've attempted to use to_records instead, but ran into the <a href=""https://github.com/pydata/pandas/issues/1908"">issue with Numpy 1.6.2 and datetimes</a>. Now I'm trying to write tuples using .itertuples, but SQLite throws an error that the data type isn't supported (see code and result below). I'm relatively new to Python, Pandas and Numpy, so it is entirely possible I'm missing something obvious. I think I'm running into a problem trying to write a datetime to SQLite, but I think I might be overcomplicating this. </p>

<p>I think I <em>may</em> be able to fix the issue by upgrading to Numpy 1.7 or the development version of Pandas, which has a fix posted on GitHub. I'd prefer to develop using release versions of software - I'm new to this and I don't want stability issues confusing matters further. </p>

<p>Is there a way to accomplish this using Python 2.7.2, Pandas 0.10.0, and Numpy 1.6.2? Perhaps cleaning the datetimes somehow? I'm in a bit over my head, any help would be appreciated. </p>

<p><strong>Code:</strong></p>

<pre><code>import numpy as np
import pandas as pd
from pandas import DataFrame, Series
import sqlite3 as db

# download data from yahoo
all_data = {}

for ticker in ['AAPL', 'GE']:
    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, '1/1/2009','12/31/2012')

# create a data frame
price = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})

# get output ready for database export
output = price.itertuples()
data = tuple(output)

# connect to a test DB with one three-column table titled ""Demo""
con = db.connect('c:/Python27/test.db')
wildcards = ','.join(['?'] * 3)
insert_sql = 'INSERT INTO Demo VALUES (%s)' % wildcards
con.executemany(insert_sql, data)
</code></pre>

<p><strong>Result:</strong></p>

<pre><code>---------------------------------------------------------------------------
InterfaceError                            Traceback (most recent call last)
&lt;ipython-input-15-680cc9889c56&gt; in &lt;module&gt;()
----&gt; 1 con.executemany(insert_sql, data)

InterfaceError: Error binding parameter 0 - probably unsupported type.
</code></pre>
";;1;;2013-01-21T02:19:42.023;18.0;14431646;2017-04-06T15:56:38.727;2013-01-21T05:09:10.997;;1240268.0;;1995577.0;;1;19;<python><sqlite3><pandas>;How to write Pandas dataframe to sqlite with Index;16793.0
2127;2127;14451264.0;1.0;"<p>I've got a data frame and want to filter or bin by a range of values and then get the counts of values in each bin. </p>

<p>Currently, I'm doing this:</p>

<pre><code>x = 5
y = 17
z = 33
filter_values = [x, y, z]
filtered_a = df[df.filtercol &lt;= x]
a_count = filtered_a.filtercol.count()

filtered_b = df[df.filtercol &gt; x]
filtered_b = filtered_b[filtered_b &lt;= y]
b_count = filtered_b.filtercol.count()

filtered_c = df[df.filtercol &gt; y]
c_count = filtered_c.filtercol.count()
</code></pre>

<p>But is there a more concise way to accomplish the same thing?</p>
";;0;;2013-01-22T03:37:04.203;5.0;14451185;2016-06-05T14:29:22.167;;;;;24718.0;;1;16;<python><pandas><binning>;Better binning in pandas;15236.0
2157;2157;14508355.0;10.0;"<p>I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):</p>

<pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                                     sum   sum   sum    sum   amax   amin
0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94
</code></pre>

<p>I want to flatten it, so that it looks like this (names aren't critical - I could rename):</p>

<pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   
0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94
</code></pre>

<p>How do I do this? (I've tried a lot, to no avail.) </p>

<p>Per a suggestion, here is the head in dict form</p>

<pre><code>{('USAF', ''): {0: '702730',
  1: '702730',
  2: '702730',
  3: '702730',
  4: '702730'},
 ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},
 ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},
 ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},
 ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},
 ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},
 ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},
 ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},
 ('tempf', 'amax'): {0: 30.920000000000002,
  1: 32.0,
  2: 23.0,
  3: 10.039999999999999,
  4: 19.939999999999998},
 ('tempf', 'amin'): {0: 24.98,
  1: 24.98,
  2: 6.9799999999999969,
  3: 3.9199999999999982,
  4: 10.940000000000001},
 ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}
</code></pre>
";;2;;2013-01-24T18:03:11.447;44.0;14507794;2017-08-10T20:35:18.507;2013-05-14T11:53:47.653;;61974.0;;1400991.0;;1;89;<python><pandas>;Python Pandas - How to flatten a hierarchical index in columns;47981.0
2181;2181;14530027.0;1.0;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once"">docs</a> show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:</p>

<pre><code>In [563]: grouped['D'].agg({'result1' : np.sum,
   .....:                   'result2' : np.mean})
   .....:
Out[563]: 
      result2   result1
A                      
bar -0.579846 -1.739537
foo -0.280588 -1.402938
</code></pre>

<p>However, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.</p>

<p>What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, <em>some functions will depend on other columns in the groupby object</em> (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). I'll have to change it so that I iterate through the whole groupby object in a single run, but I'm wondering if there's a built in way in pandas to do this somewhat cleanly.</p>

<p>For example, I've tried something like </p>

<pre><code>grouped.agg({'C_sum' : lambda x: x['C'].sum(),
             'C_std': lambda x: x['C'].std(),
             'D_sum' : lambda x: x['D'].sum()},
             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)
</code></pre>

<p>but as expected I get a KeyError (since the keys have to be a column if <code>agg</code> is called from a DataFrame).</p>

<p>Is there any built in way to do what I'd like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?</p>

<p>Thanks</p>
";;0;;2013-01-25T20:26:45.787;30.0;14529838;2016-04-20T23:16:20.603;;;;;386279.0;;1;59;<python><group-by><aggregate-functions><pandas>;Apply multiple functions to multiple groupby columns;34437.0
2188;2188;14540509.0;2.0;"<p>I am trying to go through every row in a DataFrame index and remove all rows that are not between a certain time.</p>

<p>I have been looking for solutions but none of them separate the Date from the Time, and all I want to do is drop the rows that are outside of a Time range.</p>
";;0;;2013-01-26T18:15:35.187;5.0;14539992;2014-06-10T12:15:06.320;;;;;546624.0;;1;13;<python><pandas>;Pandas Drop Rows Outside of Time Range;10697.0
2205;2205;14568392.0;1.0;"<p>I would like to build pandas from source rather than use a package manager because I am interested in contributing. <strong>The first time</strong> I tried to build pandas, these were the steps I took:</p>

<p>1) created the virtualenv
<code>mkvirtualenv --no-site-packages pandas</code></p>

<p>2) activated the virtualenv</p>

<p>3) installed Anaconda CE. However, this was installed in ~/anaconda.</p>

<p>4) cloned pandas</p>

<p>5) built C extensions in place</p>

<p><code>(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas&gt; ~/anaconda/bin/python setup.py build_ext --inplace</code></p>

<p>6) built pandas</p>

<p><code>(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas&gt; ~/anaconda/bin/python setup.py build</code></p>

<p>7) ran nosetests on master branch</p>

<p>Tests failed:
(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> nosetests pandas
    E
    ======================================================================
    ERROR: Failure: ValueError (numpy.dtype has the wrong size, try recompiling)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
    File ""/Users/EmilyChen/.virtualenvs/pandas/lib/python2.7/site-packages/nose/loader.py"",     line 390, in loadTestsFromName
    addr.filename, addr.module)
    File ""/Users/EmilyChen/.virtualenvs/pandas/lib/python2.7/site-packages/nose/importer.py"", line 39, in importFromPath
    return self.importFromDir(dir_path, fqname)
    File ""/Users/EmilyChen/.virtualenvs/pandas/lib/python2.7/site-packages/nose/importer.py"", line 86, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
    File ""/Users/EmilyChen/.virtualenvs/pandas/localrepo/pandas/pandas/<strong>init</strong>.py"", line 6, in 
    from . import hashtable, tslib, lib
    File ""numpy.pxd"", line 156, in init pandas.hashtable (pandas/hashtable.c:20354)
    ValueError: numpy.dtype has the wrong size, try recompiling</p>

<hr>

<p>Ran 1 test in 0.001s</p>

<p>FAILED (errors=1)</p>

<p>Someone on the PyData mailing list said:</p>

<blockquote>
  <p>It looks like you have NumPy installed someplace else on your machine and AnacondaCE is not playing nicely in the virtualenv. The error you are getting is a Cython error message which occurs when the NumPy version it built against doesn't match the installed version on your system-- I had thought that 1.7.x was supposed to be ABI compatible with 1.6.x (so this would not happen) but I guess not. Sigh</p>
</blockquote>

<p>The numpy version in Anaconda CE library is 1.7.0b2 and my system numpy installation is version 1.5.1. Setup.py linked to the numpy in the Anaconda distribution's libraries when it built pandas but my guess is it's linking to my system version when nosetests runs /pandas/<strong>init</strong>.py</p>

<p><strong>Next</strong>, I repeated the steps outside a virtualenv, but got the same error. <strong>Finally</strong>, I decided to install all the dependencies in a new virtualenv instead of using the Anaconda distribution to build pandas. This way, I can see that dependencies like numpy reside in the lib directory of the virtualenv python installation, which takes precedent when pandas.<strong>init</strong> runs import statements. This is what I did:</p>

<p>1) installed numpy, dateutil, pytz, cython, scipy, matplotlib and openpyxl using pip</p>

<p>2) built c extensions in place</p>

<p>3) pandas install output here: <a href=""http://pastebin.com/3CKf1f9i"" rel=""nofollow noreferrer"">http://pastebin.com/3CKf1f9i</a></p>

<p>4) pandas did not install correctly</p>

<pre><code>(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas&gt; python
Python 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) 
[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import pandas
 cannot import name hashtable
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""pandas/__init__.py"", line 6, in &lt;module&gt;
from . import hashtable, tslib, lib
ImportError: cannot import name hashtable
</code></pre>

<p>I took a look at <a href=""https://stackoverflow.com/questions/14422976/python3-3-importerror-cannot-import-name-hashtable"">this question</a> but cython installed in my case, and I am trying to build successfully from source rather than using pip like the answer recommended..</p>

<pre><code>(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas&gt; which cython
/Users/EmilyChen/.virtualenvs/pandas/bin/cython
</code></pre>
";;2;;2013-01-28T17:51:55.877;2.0;14568070;2013-01-28T18:12:24.997;2017-05-23T12:09:20.883;;-1.0;;1644251.0;;1;13;<python><hashtable><pandas><cython><importerror>;Pandas installation on Mac OS X: ImportError (cannot import name hashtable);12158.0
2207;2207;;2.0;"<p>I use <code>TimeGrouper</code> from <code>pandas.tseries.resample</code> to sum monthly return to 6M as follows:</p>

<pre><code>6m_return = monthly_return.groupby(TimeGrouper(freq='6M')).aggregate(numpy.sum)
</code></pre>

<p>where <code>monthly_return</code> is like:</p>

<pre><code>2008-07-01    0.003626
2008-08-01    0.001373
2008-09-01    0.040192
2008-10-01    0.027794
2008-11-01    0.012590
2008-12-01    0.026394
2009-01-01    0.008564
2009-02-01    0.007714
2009-03-01   -0.019727
2009-04-01    0.008888
2009-05-01    0.039801
2009-06-01    0.010042
2009-07-01    0.020971
2009-08-01    0.011926
2009-09-01    0.024998
2009-10-01    0.005213
2009-11-01    0.016804
2009-12-01    0.020724
2010-01-01    0.006322
2010-02-01    0.008971
2010-03-01    0.003911
2010-04-01    0.013928
2010-05-01    0.004640
2010-06-01    0.000744
2010-07-01    0.004697
2010-08-01    0.002553
2010-09-01    0.002770
2010-10-01    0.002834
2010-11-01    0.002157
2010-12-01    0.001034
</code></pre>

<p>The 6m_return is like:</p>

<pre><code>2008-07-31    0.003626
2009-01-31    0.116907
2009-07-31    0.067688
2010-01-31    0.085986
2010-07-31    0.036890
2011-01-31    0.015283
</code></pre>

<p>However I want to get the <code>6m_return</code> starting 6m from 7/2008 like the following:</p>

<pre><code>2008-12-31    ...
2009-06-31    ...
2009-12-31    ...
2010-06-31    ...
2010-12-31    ...
</code></pre>

<p>Tried the different input options (i.e. loffset) in TimeGrouper but doesn't work.
Any suggestion will be really appreciated!</p>
";;2;;2013-01-28T19:03:20.737;3.0;14569223;2014-03-20T09:27:55.310;2013-01-28T19:07:14.387;;1240268.0;;2019264.0;;1;18;<group-by><dataframe><pandas>;TimeGrouper, pandas;20925.0
2221;2221;;3.0;"<p>I created a file by using:</p>

<pre><code>store = pd.HDFStore('/home/.../data.h5')
</code></pre>

<p>and stored some tables using:</p>

<pre><code>store['firstSet'] = df1
store.close()
</code></pre>

<p>I closed down python and reopened in a fresh environment.</p>

<p>How do I reopen this file?</p>

<p>When I go:</p>

<pre><code>store = pd.HDFStore('/home/.../data.h5')
</code></pre>

<p>I get the following error.</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 207, in __init__
    self.open(mode=mode, warn=False)
  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 302, in open
    self.handle = _tables().openFile(self.path, self.mode)
  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 230, in openFile
    return File(filename, mode, title, rootUEP, filters, **kwargs)
  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 495, in __init__
    self._g_new(filename, mode, **params)
  File ""hdf5Extension.pyx"", line 317, in tables.hdf5Extension.File._g_new (tables/hdf5Extension.c:3039)
tables.exceptions.HDF5ExtError: HDF5 error back trace

  File ""H5F.c"", line 1582, in H5Fopen
    unable to open file
  File ""H5F.c"", line 1373, in H5F_open
    unable to read superblock
  File ""H5Fsuper.c"", line 334, in H5F_super_read
    unable to find file signature
  File ""H5Fsuper.c"", line 155, in H5F_locate_signature
    unable to find a valid file signature

End of HDF5 error back trace

Unable to open/create file '/home/.../data.h5'
</code></pre>

<p>What am I doing wrong here?  Thank you.</p>
";;7;;2013-01-29T20:41:42.183;3.0;14591855;2015-07-08T18:17:55.530;2013-01-29T20:44:32.353;;1240268.0;;1911092.0;;1;17;<python><pandas>;pandas HDFStore - how to reopen?;6267.0
2233;2233;14630250.0;1.0;"<p>I am trying to format a table, such that data in each column are formatted in a style depending on their values (similar to conditional formatting in spreadsheet programs). How can I achieve that in pandas using the HTML formatter?</p>

<p>A typical use case is highlighting significant values in a table. For example:</p>

<pre><code>    correlation  p-value
0   0.5          0.1
1   0.1          0.8
2   0.9          *0.01*
</code></pre>

<p>pandas allows to define custom formatters for HTML output - to obtain above output one could use:</p>

<pre><code>import pandas as pd
from pandas.core import format
from StringIO import StringIO
buf = StringIO()
df = pd.DataFrame({'correlation':[0.5, 0.1,0.9], 'p_value':[0.1,0.8,0.01]})
fmt = format.DataFrameFormatter(df, 
          formatters={'p_value':lambda x: ""*%f*"" % x if x&lt;0.05 else str(x)})
format.HTMLFormatter(fmt).write_result(buf)
</code></pre>

<p>However, I would like to change the style for significant values (for example, by using bold font).</p>

<p>A possible solution would be to attach a CSS class to <code>&lt;td&gt;</code> tags in the HTML output, which could be then formatted using CSS stylesheet. The above would then become:</p>

<pre><code>&lt;table border=""1"" class=""dataframe""&gt;
  &lt;thead&gt;
    &lt;tr style=""text-align: right;""&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;correlation&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt; 0.5&lt;/td&gt;
      &lt;td&gt; 0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; 0.1&lt;/td&gt;
      &lt;td&gt; 0.80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt; 0.9&lt;/td&gt;
      &lt;td class='significant'&gt; 0.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>

<p><strong>Edit</strong>: As suggested by @Andy-Hayden I can add formatting by simply replacing stars with <code>&lt;span class=""signifcant""&gt;...&lt;/span&gt;</code> in my example:</p>

<pre><code>import pandas as pd
from StringIO import StringIO
buf = StringIO()
significant = lambda x: '&lt;span class=""significant""&gt;%f&lt;/span&gt;' % x if x&lt;0.05 else str(x)
df = pd.DataFrame({'correlation':[0.5, 0.1,0.9], 'p_value':[0.1,0.8,0.01]})
df.to_html(buf, formatters={'p_value': significant})
</code></pre>

<p>Newer versions of pandas escape the tags. To avoid it replace last line with:</p>

<pre><code>df.to_html(buf, formatters={'p_value': significant}, escape=False)
</code></pre>
";;1;;2013-01-31T13:52:00.740;5.0;14627380;2017-06-09T18:17:21.440;2013-12-06T13:17:59.333;;74342.0;;74342.0;;1;21;<python><html><css><dataframe><pandas>;pandas: HTML output with conditional formatting;9977.0
2252;2252;14789513.0;3.0;"<p>I'd like to use pandas for all my analysis along with numpy but use Rpy2 for plotting my data. I want to do all analyses using pandas dataframes and then use full plotting of R via rpy2 to plot these. py2, and am using ipython to plot. What's the correct way to do this?  </p>

<p>Nearly all commands I try fail. For example:</p>

<ul>
<li>I'm trying to plot a scatter between two columns of a pandas DataFrame <code>df</code>. I'd like the labels of <code>df</code> to be used in x/y axis just like would be used if it were an R dataframe. Is there a way to do this? When I try to do it with <code>r.plot</code>, I get this gibberish plot:</li>
</ul>

<p><code>In: r.plot(df.a, df.b) # df is pandas DataFrame</code></p>

<p>yields:</p>

<p><code>Out: rpy2.rinterface.NULL</code></p>

<p>resulting in the plot:</p>

<p><img src=""https://i.stack.imgur.com/FvJdf.png"" alt=""enter image description here""></p>

<p>As you can see, the axes labels are messed up and it's not reading the axes labels from the DataFrame like it should (the X axis is column <code>a</code> of <code>df</code> and the Y axis is column <code>b</code>).</p>

<ul>
<li><p>If I try to make a histogram with <code>r.hist</code>, it doesn't work at all, yielding the error:</p>

<pre><code>In: r.hist(df.a)
Out: 
...
vectors.pyc in &lt;genexpr&gt;((x,))
    293         if l &lt; 7:
    294             s = '[' + \
--&gt; 295                 ', '.join((p_str(x, max_width = math.floor(52 / l)) for x in self[ : 8])) +\
    296                 ']'
    297         else:

vectors.pyc in p_str(x, max_width)
    287                     res = x
    288                 else:
--&gt; 289                     res = ""%s..."" % (str(x[ : (max_width - 3)]))
    290             return res
    291 

TypeError: slice indices must be integers or None or have an __index__ method
</code></pre></li>
</ul>

<p>And resulting in this plot:</p>

<p><img src=""https://i.stack.imgur.com/0klT0.png"" alt=""enter image description here""></p>

<p>Any idea what the error means?  And again here, the axes are all messed up and littered with gibberish data. </p>

<p><strong>EDIT</strong>: This error occurs only when using ipython. When I run the command from a script, it still produces the problematic plot, but at least runs with no errors. It must be something wrong with calling these commands from ipython.</p>

<ul>
<li><p>I also tried to convert the pandas DataFrame <code>df</code> to an R DataFrame as recommended by the poster below, but that fails too with this error:</p>

<pre><code>com.convert_to_r_dataframe(mydf) # mydf is a pandas DataFrame
----&gt; 1 com.convert_to_r_dataframe(mydf)
in convert_to_r_dataframe(df, strings_as_factors)
    275     # FIXME: This doesn't handle MultiIndex
    276 
--&gt; 277     for column in df:
    278         value = df[column]
    279         value_type = value.dtype.type

TypeError: iteration over non-sequence
</code></pre></li>
</ul>

<p>How can I get these basic plotting features to work on Pandas DataFrame (with labels of plots read from the labels of the Pandas DataFrame), and also get the conversion between a Pandas DF to an R DF to work? </p>

<p><strong>EDIT2</strong>: Here is a complete example of a csv file ""test.txt"" (<a href=""http://pastebin.ca/2311928"" rel=""noreferrer"">http://pastebin.ca/2311928</a>) and my code to answer @dale's comment:</p>

<pre><code>import rpy2
from rpy2.robjects import r
import rpy2.robjects.numpy2ri
import pandas.rpy.common as com
from rpy2.robjects.packages import importr
from rpy2.robjects.lib import grid
from rpy2.robjects.lib import ggplot2
rpy2.robjects.numpy2ri.activate()
from numpy import *
import scipy

# load up pandas df
import pandas
data = pandas.read_table(""./test.txt"")
# plotting a column fails
print ""data.c2: "", data.c2
r.plot(data.c2)
# Conversion and then plotting also fails
r_df = com.convert_to_r_dataframe(data)
r.plot(r_df)
</code></pre>

<p>The call to plot the column of ""data.c2"" fails, even though data.c2 is a column of a pandas df and therefore for all intents and purposes should be a numpy array.  I use the <code>activate()</code> call so I thought it would handle this column as a numpy array and plot it.</p>

<p>The second call to plot the dataframe <code>data</code> after conversion to an R dataframe also fails. Why is that? If I load up <code>test.txt</code> from R as a dataframe, I'm able to <code>plot()</code> it and since my dataframe was converted from pandas to R, it seems like it should work here too.  </p>

<p>When I do try <code>rmagic</code> in ipython, it does not fire up a plot window for some reason, though it does not error. I.e. if I do:</p>

<pre><code>In [12]: X = np.array([0,1,2,3,4])

In [13]: Y = np.array([3,5,4,6,7])
In [14]: import rpy2

In [15]: from rpy2.robjects import r

In [16]: import rpy2.robjects.numpy2ri

In [17]: import pandas.rpy.common as com

In [18]: from rpy2.robjects.packages import importr

In [19]: from rpy2.robjects.lib import grid

In [20]: from rpy2.robjects.lib import ggplot2


In [21]: rpy2.robjects.numpy2ri.activate()

In [22]: from numpy import *

In [23]: import scipy

In [24]: r.assign(""x"", X)
Out[24]: 
&lt;Array - Python:0x592ad88 / R:0x6110850&gt;
[       0,        1,        2,        3,        4]

In [25]: r.assign(""y"", Y)
&lt;Array - Python:0x592f5f0 / R:0x61109b8&gt;
[       3,        5,        4,        6,        7]

In [27]: %R plot(x,y)
</code></pre>

<p>There's no error, but no plot window either. In any case, I'd like to stick to rpy2 and not rely on <code>rmagic</code> if possible.</p>

<p>Thanks.</p>
";;3;;2013-02-01T23:32:46.617;5.0;14656852;2015-03-20T00:06:35.013;2013-02-08T22:20:47.700;;248237.0;;248237.0;;1;12;<python><numpy><pandas><ipython><rpy2>;How to use pandas dataframes and numpy arrays in Rpy2?;4058.0
2253;2253;14657511.0;4.0;"<p>I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.duplicated.html"">duplicated method</a>, it only returns the first duplicate.  Is there a a way to get all of the duplicates and not just the first one?</p>

<p>A small subsection of my dataset looks like this:</p>

<pre><code>ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE
1536D,12-Feb-12,""06DA1B3-Lebanon NH"",,15-Feb-12
F15D,18-May-12,""06405B2-Lebanon NH"",,25-Jul-12
8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12
A036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12
8944,19-Feb-12,""06D26AD-Hanover NH"",,4-Feb-12
1004E,8-Jun-12,""06388B2-Lebanon NH"",,24-Dec-11
11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12
30D7,11-Nov-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",30-Nov-11
3AE2,21-Feb-12,""06405B2-Lebanon NH"",,26-Oct-12
B0FE,17-Feb-12,""06D1B9D-Hartland VT"",,16-Feb-12
127A1,11-Dec-11,""064456E-Hanover NH"",""064456E-Hanover NH"",11-Nov-12
161FF,20-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",3-Jul-12
A036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",
475B,25-Sep-12,""06D26AD-Hanover NH"",,5-Nov-12
151A3,7-Mar-12,""06388B2-Lebanon NH"",,16-Nov-12
CA62,3-Jan-12,,,
D31B,18-Dec-11,""06405B2-Lebanon NH"",,9-Jan-12
20F5,8-Jul-12,""0669C50-Randolph VT"",,3-Feb-12
8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12
14E48,1-Aug-12,""06D3206-Hanover NH"",,
177F8,20-Aug-12,""063B208-Randolph VT"",""063B208-Randolph VT"",5-May-12
553E,11-Oct-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",8-Mar-12
12D5F,18-Jul-12,""0649597-White River VT"",""0649597-White River VT"",2-Nov-12
C6DC,13-Apr-12,""06388B2-Lebanon NH"",,
11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12
17B43,11-Aug-12,,,22-Oct-12
A036,11-Aug-12,""06D3206-Hanover NH"",,19-Jun-12
</code></pre>

<p>My code looks like this currently:</p>

<pre><code>df_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols='ID')]
</code></pre>

<p>There area a couple duplicate items. But, when I use the above code, I only get the first item.  In the API reference, I see how I can get the last item, but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy.  So, in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries, instead of the just first one.  Any help is most appreciated.</p>
";;0;;2013-02-02T00:22:08.413;6.0;14657241;2017-01-22T02:50:32.567;2013-02-02T00:29:22.420;;1467553.0;;1467553.0;;1;14;<python><pandas>;How do I get a list of all the duplicate items using pandas in python?;15478.0
2258;2258;14661768.0;5.0;"<p>I have a dataframe df :</p>

<pre><code>&gt;&gt;&gt; df
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20060630   6.590       NaN      6.590   5.291
       20060930  10.103       NaN     10.103   7.981
       20061231  15.915       NaN     15.915  12.686
       20070331   3.196       NaN      3.196   2.710
       20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>Then I want to drop rows with certain sequence numbers which indicated in a list, suppose here is <code>[1,2,4],</code> then left:</p>

<pre><code>                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20061231  15.915       NaN     15.915  12.686
       20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>How or what function can do that ?</p>
";;0;;2013-02-02T12:03:46.323;40.0;14661701;2017-04-15T01:57:42.187;2013-02-02T12:25:47.507;;733291.0;;1072888.0;;1;133;<python><pandas>;How to drop a list of rows from Pandas dataframe?;202455.0
2260;2260;14760930.0;2.0;"<p>I have pandas dataframe <code>df1</code> and <code>df2</code> (df1 is vanila dataframe, df2 is indexed by 'STK_ID' &amp; 'RPT_Date') :</p>

<pre><code>&gt;&gt;&gt; df1
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    3.69   5.975       NaN
1   000568  20060630    9.14  10.143       NaN
2   000568  20060930    9.49  13.854       NaN
3   000568  20061231   15.84  19.262       NaN
4   000568  20070331   17.00   6.803       NaN
5   000568  20070630   26.31  12.940       NaN
6   000568  20070930   39.12  19.977       NaN
7   000568  20071231   45.94  29.269       NaN
8   000568  20080331   38.75  12.668       NaN
9   000568  20080630   30.09  21.102       NaN
10  000568  20080930   26.00  30.769       NaN

&gt;&gt;&gt; df2
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20060331    3.69   5.975       NaN      5.975   2.591
       20060630    9.14  10.143       NaN     10.143   4.363
       20060930    9.49  13.854       NaN     13.854   5.901
       20061231   15.84  19.262       NaN     19.262   8.407
       20070331   17.00   6.803       NaN      6.803   2.815
       20070630   26.31  12.940       NaN     12.940   5.418
       20070930   39.12  19.977       NaN     19.977   8.452
       20071231   45.94  29.269       NaN     29.269  12.606
       20080331   38.75  12.668       NaN     12.668   3.958
       20080630   30.09  21.102       NaN     21.102   7.431
</code></pre>

<p>I can get the last 3 rows of df2 by:</p>

<pre><code>&gt;&gt;&gt; df2.ix[-3:]
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20071231   45.94  29.269       NaN     29.269  12.606
       20080331   38.75  12.668       NaN     12.668   3.958
       20080630   30.09  21.102       NaN     21.102   7.431
</code></pre>

<p>while <code>df1.ix[-3:]</code> give all the rows: </p>

<pre><code>&gt;&gt;&gt; df1.ix[-3:]
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    3.69   5.975       NaN
1   000568  20060630    9.14  10.143       NaN
2   000568  20060930    9.49  13.854       NaN
3   000568  20061231   15.84  19.262       NaN
4   000568  20070331   17.00   6.803       NaN
5   000568  20070630   26.31  12.940       NaN
6   000568  20070930   39.12  19.977       NaN
7   000568  20071231   45.94  29.269       NaN
8   000568  20080331   38.75  12.668       NaN
9   000568  20080630   30.09  21.102       NaN
10  000568  20080930   26.00  30.769       NaN
</code></pre>

<p>Why ? How to get the last 3 rows of <code>df1</code> (dataframe without index) ?
Pandas 0.10.1</p>
";;4;;2013-02-02T14:40:31.973;6.0;14663004;2014-09-15T23:36:50.183;;;;;1072888.0;;1;53;<pandas>;How to get the last n row of pandas dataframe?;73792.0
2270;2270;14688398.0;5.0;"<p>Is it possible to add some meta-information/metadata to a pandas DataFrame?</p>

<p>For example, the instrument's name used to measure the data, the instrument responsible, etc.</p>

<p><em>One workaround would be to create a column with that information, but it seems wasteful to store a single piece of information in every row!</em></p>
";;0;;2013-02-04T13:59:18.650;6.0;14688306;2016-11-09T19:35:55.480;2013-02-04T18:35:20.490;;1240268.0;;639650.0;;1;35;<python><pandas>;Adding meta-information/metadata to pandas DataFrame;9816.0
2285;2285;14717374.0;4.0;"<p>How to apply conditional logic to a Pandas DataFrame. </p>

<p>See DataFrame shown below,</p>

<pre><code>   data desired_output
0     1          False
1     2          False
2     3           True
3     4           True
</code></pre>

<p>My original data is show in the 'data' column and the desired_output is shown next to it. If the number in 'data' is below 2.5, the desired_output is False.</p>

<p>I could apply a loop and do re-construct the DataFrame... but that would be 'un-pythonic'</p>
";;2;;2013-02-05T18:17:37.657;10.0;14714181;2017-03-17T02:47:57.750;2014-05-10T22:23:58.453;;428862.0;;1257953.0;;1;14;<python><pandas>;Conditional Logic on Pandas DataFrame;37489.0
2313;2313;14734148.0;1.0;"<p>I have a dataset with multi-index columns in a pandas df that I would like to sort by values in a specific column.  I have tried using sortindex and sortlevel but haven't been able get the results I am looking for.  My dataset looks like:</p>

<pre><code>    Group1    Group2
    A B C     A B C
1   1 0 3     2 5 7
2   5 6 9     1 0 0
3   7 0 2     0 3 5 
</code></pre>

<p>I want to sort all data and the index by column C in Group 1 in descending order so my results look like:</p>

<pre><code>    Group1    Group2
    A B C     A B C
 2  5 6 9     1 0 0
 1  1 0 3     2 5 7
 3  7 0 2     0 3 5 
</code></pre>

<p>Is it possible to do this sort with the structure that my data is in, or should I be swapping Group1 to the index side?</p>
";;0;;2013-02-06T16:24:31.443;8.0;14733871;2013-05-13T22:28:25.470;2013-05-13T22:28:25.470;;183172.0;;1769851.0;;1;42;<python><sorting><pandas><multi-index>;Multi Index Sorting in Pandas;17418.0
2315;2315;14734627.0;5.0;"<p>How do I access the corresponding groupby dataframe in a groupby object by the key? With the following groupby:</p>

<pre><code>rand = np.random.RandomState(1)
df = pd.DataFrame({'A': ['foo', 'bar'] * 3,
                   'B': rand.randn(6),
                   'C': rand.randint(0, 20, 6)})
gb = df.groupby(['A'])
</code></pre>

<p>I can iterate through it to get the keys and groups:</p>

<pre><code>In [11]: for k, gp in gb:
             print 'key=' + str(k)
             print gp
key=bar
     A         B   C
1  bar -0.611756  18
3  bar -1.072969  10
5  bar -2.301539  18
key=foo
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p>I would like to be able to do something like</p>

<pre><code>In [12]: gb['foo']
Out[12]:  
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p>But when I do that (well, actually I have to do <code>gb[('foo',)]</code>), I get this weird <code>pandas.core.groupby.DataFrameGroupBy</code> thing which doesn't seem to have any methods that correspond to the DataFrame I want.</p>

<p>The best I can think of is</p>

<pre><code>In [13]: def gb_df_key(gb, key, orig_df):
             ix = gb.indices[key]
             return orig_df.ix[ix]

         gb_df_key(gb, 'foo', df)
Out[13]:
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14  
</code></pre>

<p>but this is kind of nasty, considering how nice pandas usually is at these things.<br>
What's the built-in way of doing this?</p>
";;0;;2013-02-06T16:55:54.167;23.0;14734533;2016-06-14T21:45:14.487;2013-09-01T09:36:43.630;;1240268.0;;386279.0;;1;71;<python><group-by><dataframe><pandas>;How to access pandas groupby dataframe by key;77177.0
2324;2324;14746845.0;2.0;"<p>I have a DataFrame with a MultiIndex created after some grouping:</p>

<pre><code>import numpy as np
import pandas as p
from numpy.random import randn

df = p.DataFrame({
    'A' : ['a1', 'a1', 'a2', 'a3']
  , 'B' : ['b1', 'b2', 'b3', 'b4']
  , 'Vals' : randn(4)
}).groupby(['A', 'B']).sum()

df

Output&gt;            Vals
Output&gt; A  B           
Output&gt; a1 b1 -1.632460
Output&gt;    b2  0.596027
Output&gt; a2 b3 -0.619130
Output&gt; a3 b4 -0.002009
</code></pre>

<p>How do I prepend a level to the MultiIndex so that I turn it into something like:</p>

<pre><code>Output&gt;                       Vals
Output&gt; FirstLevel A  B           
Output&gt; Foo        a1 b1 -1.632460
Output&gt;               b2  0.596027
Output&gt;            a2 b3 -0.619130
Output&gt;            a3 b4 -0.002009
</code></pre>
";;0;;2013-02-07T05:16:07.423;9.0;14744068;2017-02-07T16:11:06.527;;;;;20371.0;;1;22;<python><pandas>;Prepend a level to a pandas MultiIndex;16301.0
2325;2325;14745484.0;4.0;"<p>I have a data frame with one column and i'd like to split it into two columns, with one column header as '<code>fips'</code> and the other <code>'row'</code></p>

<p>My datFrame df looks like this currently</p>

<pre><code>          row
0    00000 UNITED STATES
1    01000 ALABAMA
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL
</code></pre>

<p>I am new to python and do not understand how using <code>df.row.str[:]</code> would help me achieve my goal of splitting the row cell.  I can use <code>df['fips'] = hello</code> to add a new column and populate it with hello. Any ideas? </p>

<pre><code>         fips       row
0    00000 UNITED STATES
1    01000 ALABAMA 
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL
</code></pre>
";;1;;2013-02-07T06:30:11.817;23.0;14745022;2017-05-31T15:04:18.730;2013-02-07T07:07:02.210;;1199589.0;;2049102.0;;1;42;<python><dataframe><pandas>;Pandas DataFrame, how do i split a column into two;56622.0
2370;2370;14809149.0;2.0;"<p>when my function f is called with a variable I want to check if var is a pandas dataframe:</p>

<pre><code>def f(var):
if var == pd.DataFrame():
    print ""do stuff""
</code></pre>

<p>I guess the solution might be quite simple but even with </p>

<pre><code>def f(var):
if var.values != None:
    print ""do stuff""
</code></pre>

<p>I can't get it to work like expected. </p>
";;1;;2013-02-11T09:10:07.870;3.0;14808945;2013-02-12T09:14:32.203;;;;;439693.0;;1;34;<python><pandas>;check if variable is dataframe;21988.0
2375;2375;14813733.0;2.0;"<p>I'm using the excellent <code>read_csv()</code>function from pandas, which gives:</p>

<pre><code>In [31]: data = pandas.read_csv(""lala.csv"", delimiter="","")

In [32]: data
Out[32]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 12083 entries, 0 to 12082
Columns: 569 entries, REGIONC to SCALEKER
dtypes: float64(51), int64(518)
</code></pre>

<p>but when i apply a function from scikit-learn i loose the informations about columns:</p>

<pre><code>from sklearn import preprocessing
preprocessing.scale(data)
</code></pre>

<p>gives numpy array.</p>

<p>Is there a way to apply scikit or numpy function to DataFrames without loosing the information?</p>
";;0;;2013-02-11T13:46:22.263;3.0;14813289;2017-02-02T02:37:22.227;2015-11-10T14:23:23.663;;121704.0;;395239.0;;1;11;<python><pandas><scikit-learn>;Keep pandas structure with numpy/scikit functions;2305.0
2384;2384;;2.0;"<p>I have a Python dataFrame with multiple columns. </p>

<pre><code>  LogBlk    Page                                    BayFail       
  0          0                                 [0, 1, 8, 9]  
  1          16           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  
  2          32           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  
  3          48           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  
</code></pre>

<p>I want to find BayFails that is associated with LogBlk=0, and Page=0. </p>

<pre><code>df2 = df[ (df['Page'] == 16) &amp; (df['LogBlk'] == 0) ]['BayFail']
</code></pre>

<p>This will return [0,1,8,9]</p>

<p>What I want to do is to convert this pandas.series into a list. Does anyone know how to do that?</p>
";;0;;2013-02-11T23:11:37.207;3.0;14822680;2013-11-11T19:39:50.217;2013-02-12T02:10:23.943;;190597.0;;1452849.0;;1;14;<python><python-3.x><python-2.7><pandas>;convert python dataframe to list;54043.0
2388;2388;14843650.0;2.0;"<p>I am making a stacked bar plot using:</p>

<pre><code>DataFrame.plot(kind='bar',stacked=True)
</code></pre>

<p>I want to control width of bars so that the bars are connected to each other like a histogram.</p>

<p><em>I've looked through the documentation but to no avail - any suggestions? Is it possible to do it this way?</em></p>
";;2;;2013-02-12T02:23:01.880;4.0;14824456;2014-12-05T17:46:24.480;2013-05-29T22:20:23.903;;1819479.0;;1819479.0;;1;17;<python><matplotlib><pandas><histogram><bar-chart>;Edit the width of bars using dataframe.plot() function in matplotlib;7537.0
2407;2407;14861132.0;1.0;"<p>I have minute based OHLCV data for the opening range/first hour (9:30-10:30 AM EST).  I'm looking to resample this data so I can get one 60-minute value and then calculate the range.</p>

<p>When I call the dataframe.resample() function on the data I get two rows and the initial row starts at 9:00 AM.  I'm looking to get only one row which starts at 9:30 AM. </p>

<p>Note: the initial data begins at 9:30. </p>

<p><img src=""https://i.stack.imgur.com/LQTZ6.png"" alt=""enter image description here""></p>

<p>Edit:  Adding code:</p>

<pre><code># Extract data for regular trading hours (rth) from the 24 hour data set
rth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)

# Extract data for extended trading hours (eth) from the 24 hour data set
eth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)

# Extract data for initial balance (rth) from the 24 hour data set
initial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)
</code></pre>

<p>Got stuck tried to separate the opening range by individual date and get the Initial Balance</p>

<pre><code>conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}
sample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)
sample = sample.ix['2007-05-07']
sample.tail()

sample.resample('60Min', how = conversion) 
</code></pre>

<p>By default resample starts at the beggining of the hour.  I would like it to start from where the data starts.</p>
";;1;;2013-02-13T19:03:18.797;6.0;14861023;2013-02-13T19:17:11.987;2013-02-13T19:17:11.987;;1569815.0;;1569815.0;;1;14;<pandas>;Resampling Minute data;17128.0
2418;2418;14887119.0;3.0;"<p>One of my favorite aspects of using the <code>ggplot2</code> library in R is the ability to easily specify aesthetics. I can quickly make a scatterplot and apply color associated with a specific column and I would love to be able to do this with python/pandas/matplotlib.  I'm wondering if there are there any convenience functions that people use to map colors to values using pandas dataframes and Matplotlib?</p>

<pre><code>##ggplot scatterplot example with R dataframe, `df`, colored by col3
ggplot(data = df, aes(x=col1, y=col2, color=col3)) + geom_point()

##ideal situation with pandas dataframe, 'df', where colors are chosen by col3
df.plot(x=col1,y=col2,color=col3)
</code></pre>

<p>EDIT:
Thank you for your responses but I want to include a sample dataframe to clarify what I am asking. Two columns contain numerical data and the third is a categorical variable. The script I am thinking of will assign colors based on this value.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Height':np.random.normal(10),
                   'Weight':np.random.normal(10),
                   'Gender': [""Male"",""Male"",""Male"",""Male"",""Male"",
                              ""Female"",""Female"",""Female"",""Female"",""Female""]})
</code></pre>
";;0;;2013-02-14T23:23:12.467;6.0;14885895;2016-07-21T09:38:20.887;2013-02-15T22:26:02.527;;983191.0;;983191.0;;1;13;<matplotlib><pandas>;Color by Column Values in Matplotlib;16024.0
2423;2423;;2.0;"<p>I would like to produce a subplot from data 4 column DataFrame into 2 rows and 2 columns</p>

<pre><code>df =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))
</code></pre>

<p>However below will give a 4 row and 1 column plot</p>

<pre><code> df.plot(use_index=False, title=f, subplots=True, sharey=True, figsize=(8, 6))
</code></pre>

<p>Thanks.</p>
";;2;;2013-02-15T04:54:57.203;;14888473;2015-05-02T07:21:27.757;;;;;1569058.0;;1;11;<python><dataframe><pandas><subplot>;python pandas DataFrame subplot in columns and rows;6555.0
2443;2443;14917572.0;1.0;"<p>What is the best way to reshape the following dataframe in pandas? This DataFrame <code>df</code> has <code>x,y</code> values for each sample (<code>s1</code> and <code>s2</code> in this case) and looks like this:</p>

<pre><code>In [23]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})
In [24]: df
Out[24]: 
       s1_x      s1_y      s2_x      s2_y
0  0.913462  0.525590 -0.377640  0.700720
1  0.723288 -0.691715  0.127153  0.180836
2  0.181631 -1.090529 -1.392552  1.530669
3  0.997414 -1.486094  1.207012  0.376120
4 -0.319841  0.195289 -1.034683  0.286073
5  1.085154 -0.619635  0.396867  0.623482
6  1.867816 -0.928101 -0.491929 -0.955295
7  0.920658 -1.132057  1.701582 -0.110299
8 -0.241853 -0.129702 -0.809852  0.014802
9 -0.019523 -0.578930  0.803688 -0.881875
</code></pre>

<p><code>s1_x</code> and <code>s1_y</code> are the x/y values for sample 1, <code>s2_x, s2_y</code> are the sample values for sample 2, etc.  How can this be reshaped into a DataFrame containing only <code>x</code>, <code>y</code> columns but that contains an additional column <code>sample</code> that says for each row in the DataFrame whether it's from <code>s1</code> or <code>s2</code>?  E.g.</p>

<pre><code>          x         y      sample
0  0.913462  0.525590          s1
1  0.723288 -0.691715          s1
2  0.181631 -1.090529          s1
3  0.997414 -1.486094          s1
...
5  0.396867  0.623482          s2
...
</code></pre>

<p>This is useful for plotting things with Rpy2 later on, since many R plotting features can make use of this grouping variable, so that's my motivation for reshaping the dataframe. </p>

<p>I think the answer given by Chang She doesn't translate to dataframes that have a unique index, like this one:</p>

<pre><code>In [636]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})
In [637]: df
Out[637]: 
   names      s1_x      s1_y      s2_x      s2_y
0      0  0.672298  0.415366  1.034770  0.556209
1      1  0.067087 -0.851028  0.053608 -0.276461
2      2 -0.674174 -0.099015  0.864148 -0.067240
3      3  0.542996 -0.813018  2.283530  2.793727
4      4  0.216633 -0.091870 -0.746411 -0.421852
5      5  0.141301 -1.537721 -0.371601 -1.594634
6      6  1.267148 -0.833120  0.369516 -0.671627
7      7 -0.231163 -0.557398  1.123155  0.865140
8      8  1.790570 -0.428563  0.668987  0.632409
9      9 -0.820315 -0.894855  0.673247 -1.195831
In [638]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])

In [639]: df.stack(0).reset_index(1)
Out[639]: 
  level_1         x         y
0      s1  0.672298  0.415366
0      s2  1.034770  0.556209
1      s1  0.067087 -0.851028
1      s2  0.053608 -0.276461
2      s1 -0.674174 -0.099015
2      s2  0.864148 -0.067240
3      s1  0.542996 -0.813018
3      s2  2.283530  2.793727
4      s1  0.216633 -0.091870
4      s2 -0.746411 -0.421852
5      s1  0.141301 -1.537721
5      s2 -0.371601 -1.594634
6      s1  1.267148 -0.833120
6      s2  0.369516 -0.671627
7      s1 -0.231163 -0.557398
7      s2  1.123155  0.865140
8      s1  1.790570 -0.428563
8      s2  0.668987  0.632409
9      s1 -0.820315 -0.894855
9      s2  0.673247 -1.195831
</code></pre>

<p>The transformation worked but in the process the column <code>""names""</code> was lost. How can I keep the <code>""names""</code> column in the df while still doing the melting transformation on the columns that have <code>_</code> in their names? The <code>""names""</code> column just assigns a unique name to each row in the dataframe. It's numeric here for example but in my data they are string identifiers.</p>

<p>thanks.</p>
";;0;;2013-02-16T23:37:04.970;8.0;14916358;2016-04-01T15:28:39.757;2016-04-01T15:28:39.757;;2230844.0;;248237.0;;1;11;<python><numpy><pandas><scipy><multi-index>;Reshaping dataframes in pandas based on column labels;11117.0
2451;2451;29319200.0;6.0;"<p>I would like to create views or dataframes from an existing dataframe based on column selections.</p>

<p>For example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work:</p>

<pre><code>import numpy as np
import pandas as pd

# Create a dataframe with columns A,B,C and D
df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))

# Try to create a second dataframe df2 from df with all columns except 'B' and D
my_cols = set(df.columns)
my_cols.remove('B').remove('D')

# This returns an error (""unhashable type: set"")
df2 = df[my_cols]
</code></pre>

<p>What am I doing wrong? Perhaps more generally, what mechanisms does Panda have to support the picking and <strong>exclusions</strong> of arbitrary sets of columns from a dataframe?</p>
";;1;;2013-02-18T16:22:38.697;23.0;14940743;2016-08-16T08:34:48.393;2016-02-18T04:15:43.297;;283296.0;;283296.0;;1;84;<python><pandas>;Selecting/Excluding sets of columns in Pandas;77656.0
2454;2454;14941170.0;5.0;"<p>I'm simply trying to access named pandas columns by an integer. </p>

<p>You can select a row by location using <code>df.ix[3]</code>.</p>

<p>But how to select a column by integer?</p>

<p>My dataframe:</p>

<pre><code>df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})
</code></pre>
";;1;;2013-02-18T16:41:15.757;5.0;14941097;2017-08-16T04:09:02.597;2015-08-29T05:09:27.753;;202229.0;;687739.0;;1;19;<python><pandas><indexing>;Selecting pandas column by location;26383.0
2457;2457;14946246.0;3.0;"<p>Given the following dataframe</p>

<pre><code>In [31]: rand = np.random.RandomState(1)
         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,
                            'B': rand.randn(6),
                            'C': rand.rand(6) &gt; .5})

In [32]: df
Out[32]:      A         B      C
         0  foo  1.624345  False
         1  bar -0.611756   True
         2  baz -0.528172  False
         3  foo -1.072969   True
         4  bar  0.865408  False
         5  baz -2.301539   True 
</code></pre>

<p>I would like to sort it in groups (<code>A</code>) by the aggregated sum of <code>B</code>, and then by the value in <code>C</code> (not aggregated). So basically get the order of the <code>A</code> groups with</p>

<pre><code>In [28]: df.groupby('A').sum().sort('B')
Out[28]:             B  C
         A               
         baz -2.829710  1
         bar  0.253651  1
         foo  0.551377  1
</code></pre>

<p>And then by True/False, so that it ultimately looks like this:</p>

<pre><code>In [30]: df.ix[[5, 2, 1, 4, 3, 0]]
Out[30]: A         B      C
    5  baz -2.301539   True
    2  baz -0.528172  False
    1  bar -0.611756   True
    4  bar  0.865408  False
    3  foo -1.072969   True
    0  foo  1.624345  False
</code></pre>

<p>How can this be done?                         </p>
";;0;;2013-02-18T16:55:54.100;26.0;14941366;2016-04-27T13:38:41.860;;;;;386279.0;;1;43;<python><sorting><group-by><dataframe><pandas>;Pandas sort by group aggregate and column;70881.0
2486;2486;14964637.0;2.0;"<p>If I define a hierarchically-indexed dataframe like this:</p>

<pre><code>import itertools
import pandas as pd
import numpy as np
a = ('A', 'B')
i = (0, 1, 2)
b = (True, False)
idx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),
                                names=('Alpha', 'Int', 'Bool'))
df = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,
                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))
</code></pre>

<p>the contents look like this:</p>

<pre><code>In [19]: df
Out[19]: 
                        I        II       III        IV         V        VI       VII
Alpha Int Bool                                                                       
A     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865
          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833
      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738
          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957
      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844
          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906
B     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128
          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612
      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053
          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555
      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513
          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215
</code></pre>

<p>I know how to extract the data corresponding to a given column.  E.g. for column <code>'VII'</code>:</p>

<pre><code>In [20]: df['VII']
Out[20]: 
Alpha  Int  Bool 
A      0    True     0.892865
            False   -1.133833
       1    True     0.171738
            False   -0.485957
       2    True     0.100844
            False   -0.350906
B      0    True     0.009128
            False   -1.125612
       1    True    -0.009053
            False   -0.329555
       2    True    -0.212513
            False   -2.414215
Name: VII
</code></pre>

<p>How do I extract the data matching the following sets of criteria:</p>

<ol>
<li><code>Alpha=='B'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, column <code>'I'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, columns <code>'I'</code> and <code>'III'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, columns <code>'I'</code>, <code>'III'</code>, and all columns from <code>'V'</code> onwards</li>
<li><code>Int</code> is even</li>
</ol>

<p>(BTW, I did rtfm, more than once even, but I really find it incomprehensible.)</p>
";;0;;2013-02-19T18:13:30.553;9.0;14964493;2016-04-14T05:29:19.000;;;;;559827.0;;1;21;<pandas>;MultiIndex-based indexing in pandas;10792.0
2496;2496;14985695.0;4.0;"<p>What is the easiest way to remove duplicate columns from a dataframe?</p>

<p>I am reading a text file that has duplicate columns via:</p>

<pre><code>import pandas as pd

df=pd.read_table(fname)
</code></pre>

<p>The column names are:</p>

<pre><code>Time, Time Relative, N2, Time, Time Relative, H2, etc...
</code></pre>

<p>All the Time and Time Relative columns contain the same data. I want:</p>

<pre><code>Time, Time Relative, N2, H2
</code></pre>

<p>All my attempts at dropping, deleting, etc  such as:</p>

<pre><code>df=df.T.drop_duplicates().T
</code></pre>

<p>Result in uniquely valued index errors:</p>

<pre><code>Reindexing only valid with uniquely valued index objects
</code></pre>

<p>Sorry for being a Pandas noob. Any Suggestions would be appreciated.</p>

<hr>

<p><strong>Additional Details</strong></p>

<p>Pandas version: 0.9.0<br>
Python Version: 2.7.3<br>
Windows 7<br>
(installed via Pythonxy 2.7.3.0)</p>

<p>data file (note: in the real file, columns are separated by tabs, here they are separated by 4 spaces):</p>

<pre><code>Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]
2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    
2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    
2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    
... etc ...
2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    
2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005
2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005
</code></pre>
";;3;;2013-02-20T15:49:17.780;9.0;14984119;2017-08-24T17:34:44.620;2013-02-20T17:53:59.323;;979203.0;;979203.0;;1;24;<python><pandas>;python pandas remove duplicate columns;23542.0
2505;2505;14988913.0;2.0;"<p>In R, you can combine two dataframes by sticking the columns of one onto the bottom of the columns of the other using rbind. In pandas, how do you accomplish the same thing? It seems bizarrely difficult. </p>

<p>Using append results in a horrible mess including NaNs and things for reasons I don't understand. I'm just trying to ""rbind"" two identical frames that look like this:</p>

<p>EDIT: I was creating the DataFrames in a stupid way, which was causing issues. Append=rbind to all intents and purposes. See answer below.</p>

<pre><code>        0         1       2        3          4          5        6                    7
0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43
4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44
5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44
6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45
</code></pre>

<p>But I'm getting something horrible a la this: </p>

<pre><code>        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7
0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43
4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44
5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44
6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45
0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  
</code></pre>

<p>And I don't understand why. I'm starting to miss R :(</p>
";;2;;2013-02-20T19:38:34.887;;14988480;2017-05-06T21:12:06.290;2017-05-06T21:12:06.290;;1319312.0;;1319312.0;;1;26;<python><r><dataframe><pandas>;Pandas version of rbind;17614.0
2509;2509;14992237.0;1.0;"<p>I need to remove all rows in which elements from column 3 onwards are all NaN</p>

<pre><code>df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])

df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
df2.ix[1][0] = 111
df2.ix[1][1] = 222
</code></pre>

<p>In the example above, my final data frame would not be having rows 'b' and 'c'. </p>

<p>How to use <code>df.dropna()</code> in this case?</p>
";;0;;2013-02-20T22:31:29.910;9.0;14991195;2015-01-22T20:38:01.767;2015-01-22T20:38:01.767;;1140126.0;;1140126.0;;1;23;<python><pandas>;How to remove rows with null values from kth column onward in python;33921.0
2517;2517;15002718.0;3.0;"<p>I have a data frame and I would like to group it by a particular column (or, in other words, by values from a particular column). I can do it in the following way: <code>grouped = df.groupby(['ColumnName'])</code>.</p>

<p>I imagine the result of this operation as a table in which some cells can contain sets of values instead of single values. To get a usual table (i.e. a table in which every cell contains only one a single value) I need to indicate what function I want to use to transform the sets of values in the cells into single values.</p>

<p>For example I can replace sets of values by their sum, or by their minimal or maximal value. I can do it in the following way: <code>grouped.sum()</code> or <code>grouped.min()</code> and so on.</p>

<p>Now I want to use different functions for different columns. I figured out that I can do it in the following way: <code>grouped.agg({'ColumnName1':sum, 'ColumnName2':min})</code>.</p>

<p>However, because of some reasons I cannot use <code>first</code>. In more details, <code>grouped.first()</code> works, but <code>grouped.agg({'ColumnName1':first, 'ColumnName2':first})</code> does not work. As a result I get a NameError: <code>NameError: name 'first' is not defined</code>. So, my question is: Why does it happen and how to resolve this problem.</p>

<p><strong>ADDED</strong></p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html"" rel=""noreferrer"">Here</a> I found the following example:</p>

<pre><code>grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})
</code></pre>

<p>May be I also need to use <code>np</code>? But in my case python does not recognize ""np"". Should I import it? </p>
";;1;;2013-02-21T11:33:56.173;6.0;15001237;2016-11-18T19:10:46.457;2013-02-21T12:20:38.573;;245549.0;;245549.0;;1;16;<group-by><pandas>;"How to apply ""first"" and ""last"" functions to columns while using group by in pandas?";12399.0
2522;2522;;1.0;"<p>I'm trying to wrap my head around Pandas groupby methods. I'd like to write a function that does some aggregation functions and then returns a Pandas DataFrame. Here's a grossly simplified example using sum(). I know there are easier ways to do simple sums, in real life my function is more complex:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'col1': ['A', 'A', 'B', 'B'], 'col2':[1.0, 2, 3, 4]})

In [3]: df
Out[3]: 
  col1  col2
0    A     1
1    A     2
2    B     3
3    B     4

def func2(df):
    dfout = pd.DataFrame({ 'col1' : df['col1'].unique() ,
                           'someData': sum(df['col2']) })
    return  dfout

t = df.groupby('col1').apply(func2)

In [6]: t
Out[6]: 
       col1  someData
col1                 
A    0    A         3
B    0    B         7
</code></pre>

<p>I did not expect to have <code>col1</code> in there twice nor did I expect that mystery index looking thing. I really thought I would just get <code>col1</code> &amp; <code>someData</code>. </p>

<p>In my real life application I'm grouping by more than one column and really would like to get back a DataFrame and not a Series object.<br>
Any ideas for a solution or an explanation on what Pandas is doing in my example above?</p>

<p><strong>----- added info -----</strong></p>

<p>I should have started with this example, I think:</p>

<pre><code>In [13]: import pandas as pd

In [14]: df = pd.DataFrame({'col1':['A','A','A','B','B','B'], 'col2':['C','D','D','D','C','C'], 'col3':[.1,.2,.4,.6,.8,1]})

In [15]: df
Out[15]: 
  col1 col2  col3
0    A    C   0.1
1    A    D   0.2
2    A    D   0.4
3    B    D   0.6
4    B    C   0.8
5    B    C   1.0

In [16]: def func3(df):
   ....:         dfout =  sum(df['col3']**2)
   ....:         return  dfout
   ....: 

In [17]: t = df.groupby(['col1', 'col2']).apply(func3)

In [18]: t
Out[18]: 
col1  col2
A     C       0.01
      D       0.20
B     C       1.64
      D       0.36
</code></pre>

<p>In the above illustration the result of the <code>apply()</code> function is a Pandas Series. And it lacks the groupby columns from the <code>df.groupby</code>. The essence of what I'm struggling with is how do I create a function which I apply to a groupby which returns both the result of the function AND the columns on which it was grouped? </p>

<p><strong>----- yet another update ------</strong></p>

<p>It appears that if I then do this:</p>

<pre><code> pd.DataFrame(t).reset_index()
</code></pre>

<p>I get back a dataframe which is really close to what I was after. </p>
";;5;;2013-02-21T13:44:51.463;1.0;15003828;2013-02-21T15:39:30.933;2013-02-21T15:39:30.933;;37751.0;;37751.0;;1;15;<python><group-by><pandas>;returning aggregated dataframe from pandas groupby;28764.0
2531;2531;15006495.0;7.0;"<p>I am just getting started with pandas in the IPython Notebook and encountering the following problem: When a <code>DataFrame</code> read from a CSV file is small, the IPython Notebook displays it in a nice table view. When the <code>DataFrame</code> is large, something like this is ouput:</p>

<pre><code>In [27]:

evaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])

In [37]:

evaluation

Out[37]:

&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 333 entries, 0 to 332
Data columns:
solver       333  non-null values
instance     333  non-null values
runtime      333  non-null values
objective    333  non-null values
dtypes: int64(1), object(3)
</code></pre>

<p>I would like to see a small portion of the data frame as a table just to make sure it is in the right format. What options do I have?</p>
";;4;;2013-02-21T15:42:05.437;5.0;15006298;2017-05-15T13:48:57.747;2014-01-22T17:40:19.477;;28035.0;;626537.0;;1;23;<python><pandas><dataframe><ipython><ipython-notebook>;How to preview a part of a large pandas DataFrame?;40523.0
2535;2535;15009160.0;2.0;"<p>Is there a built-in way to use <code>read_csv</code> to read only the first <code>n</code> lines of a file without knowing the length of the lines ahead of time? I have a large file that takes a long time to read, and occasionally only want to use the first, say, 20 lines to get a sample of it (and prefer not to load the full thing and take the head of it).</p>

<p>If I knew the total number of lines I could do something like <code>footer_lines = total_lines - n</code> and pass this to the <code>skipfooter</code> keyword arg. My current solution is to manually grab the first <code>n</code> lines with python and StringIO it to pandas:</p>

<pre><code>import pandas as pd
from StringIO import StringIO

n = 20
with open('big_file.csv', 'r') as f:
    head = ''.join(f.readlines(n))

df = pd.read_csv(StringIO(head))
</code></pre>

<p>It's not that bad, but is there a more concise, 'pandasic' (?) way to do it with keywords or something?</p>
";;1;;2013-02-21T17:50:12.057;2.0;15008970;2017-06-18T14:43:49.740;;;;;386279.0;;1;24;<csv><dataframe><pandas>;Way to read first few lines for pandas dataframe;24174.0
2551;2551;15100193.0;4.0;"<p>I have a csv file which isn't coming in correctly with <code>pandas.read_csv</code> when I  filter the columns with <code>usecols</code> and use multiple indexes.<br>
</p>

<pre><code>import pandas as pd
csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""
f = open('foo.csv', 'w')
f.write(csv)
f.close()

df1 = pd.read_csv('foo.csv', 
        index_col=[""date"", ""loc""], 
        usecols=[""dummy"", ""date"", ""loc"", ""x""],
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
print df1

# Ignore the dummy columns
df2 = pd.read_csv('foo.csv', 
        index_col=[""date"", ""loc""], 
        usecols=[""date"", ""loc"", ""x""], # &lt;----------- Changed
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
print df2
</code></pre>

<p>I expect that df1 and df2 should be the same except for the missing dummy column, but the columns come in mislabeled.  Also the date is getting parsed as a date.  </p>

<pre><code>In [118]: %run test.py
               dummy  x
date       loc
2009-01-01 a     bar  1
2009-01-02 a     bar  3
2009-01-03 a     bar  5
2009-01-01 b     bar  1
2009-01-02 b     bar  3
2009-01-03 b     bar  5
              date
date loc
a    1    20090101
     3    20090102
     5    20090103
b    1    20090101
     3    20090102
     5    20090103
</code></pre>

<p>Using column numbers instead of names give me the same problem.  I can workaround the issue by dropping the dummy column after the read_csv step, but I'm trying to understand what is going wrong.  I'm using pandas 0.10.1.</p>

<p>edit: fixed bad header usage.</p>
";;4;;2013-02-22T04:50:55.607;11.0;15017072;2015-01-06T18:31:35.627;2013-02-22T17:24:46.450;;653689.0;;653689.0;;1;37;<python><pandas>;pandas read_csv and filter columns with usecols;74330.0
2557;2557;15026839.0;2.0;"<p>I need to created a data frame using data stored in a file. For that I want to use <code>read_csv</code> method. However, the separator is not very regular. Some columns are separated by tabs (<code>\t</code>), other are separated by spaces. Moreover, some columns can be separated by 2 or 3 or more spaces or even by a combination of spaces and tabs (for example 3 spaces, two tabs and then 1 space).</p>

<p>Is there a way to tell pandas to treat these files properly?</p>

<p>By the way, I do not have this problem if I use Python. I use:</p>

<pre><code>for line in file(file_name):
   fld = line.split()
</code></pre>

<p>And it works perfect. It does not care if there are 2 or 3 spaces between the fields. Even combinations of spaces and tabs do not cause any problem. Can pandas do the same?</p>
";;0;;2013-02-22T14:43:51.307;7.0;15026698;2016-12-25T11:32:28.703;2014-09-18T00:30:01.747;;202229.0;;245549.0;;1;30;<python><csv><pandas><dataframe><whitespace>;How to make separator in read_csv more flexible wrt whitespace?;25215.0
2591;2591;15070110.0;1.0;"<p>I've read <a href=""https://stackoverflow.com/questions/14380371/export-a-latex-table-from-pandas-dataframe/14383654#14383654"">about the <code>to_latex</code></a> method, but it's not clear <strong>how to use the formatters argument</strong>.</p>

<p>I have some numbers which are <strong>too long</strong> and some which I want <strong>thousand separators</strong>.</p>

<p><em>A side <a href=""https://github.com/pydata/pandas/issues/2924"" rel=""nofollow noreferrer"">issue</a> for the <code>to_latex</code> method on multi-indexed tables, the indices are parsed together and it issues some <code>&amp;</code>s in the latex output.</em></p>
";;1;;2013-02-25T14:55:21.653;5.0;15069814;2013-05-17T14:48:44.550;2017-05-23T12:32:05.060;;-1.0;;2107677.0;;1;11;<python><latex><pandas>;Formatting latex (to_latex) output;4939.0
2595;2595;15073977.0;3.0;"<p>I know about these column slice methods:</p>

<p><code>df2 = df[[""col1"", ""col2"", ""col3""]]</code> and <code>df2 = df.ix[:,0:2]</code></p>

<p>but I'm wondering if there is a way to slice columns from the front/middle/end of a dataframe in the same slice without specifically listing each one.</p>

<p>For example, a dataframe <code>df</code> with columns: col1, col2, col3, col4, col5 and col6.</p>

<p>Is there a way to do something like this?</p>

<p><code>df2 = df.ix[:, [0:2, ""col5""]]</code></p>

<p>I'm in the situation where I have hundreds of columns and routinely need to slice specific ones for different requests.  I've checked through the documentation and haven't seen something like this.  Have I overlooked something? </p>

<p>Thanks!</p>

<p>*Edited to be more clear about what I'm looking for.</p>
";;0;;2013-02-25T16:48:56.673;6.0;15072005;2013-02-27T08:35:37.573;2013-02-25T18:16:18.350;;1730674.0;;1649780.0;;1;14;<python><pandas>;keep/slice specific columns in pandas;20039.0
2596;2596;15074395.0;1.0;"<p>For dataframe</p>

<pre><code>In [2]: df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,
   ...:                    'Rank': np.random.randint(0,3,6),
   ...:                    'Val': np.random.rand(6)})
   ...: df
Out[2]: 
  Name  Rank       Val
0  foo     0  0.299397
1  bar     0  0.909228
2  foo     0  0.517700
3  bar     0  0.929863
4  foo     1  0.209324
5  bar     2  0.381515
</code></pre>

<p>I'm interested in grouping by Name and Rank and possibly getting aggregate values</p>

<pre><code>In [3]: group = df.groupby(['Name', 'Rank'])
In [4]: agg = group.agg(sum)
In [5]: agg
Out[5]: 
                Val
Name Rank          
bar  0     1.839091
     2     0.381515
foo  0     0.817097
     1     0.209324
</code></pre>

<p>But I would like to get a field in the original <code>df</code> that contains the group number for that row, like</p>

<pre><code>In [13]: df['Group_id'] = [2, 0, 2, 0, 3, 1]
In [14]: df
Out[14]: 
  Name  Rank       Val  Group_id
0  foo     0  0.299397         2
1  bar     0  0.909228         0
2  foo     0  0.517700         2
3  bar     0  0.929863         0
4  foo     1  0.209324         3
5  bar     2  0.381515         1
</code></pre>

<p>Is there a good way to do this in pandas?</p>

<p>I can get it with python, </p>

<pre><code>In [16]: from itertools import count
In [17]: c = count()
In [22]: group.transform(lambda x: c.next())
Out[22]: 
   Val
0    2
1    0
2    2
3    0
4    3
5    1
</code></pre>

<p>but it's pretty slow on a large dataframe, so I figured there may be a better built in pandas way to do this. </p>
";;0;;2013-02-25T17:20:46.357;10.0;15072626;2015-09-21T18:08:33.503;2015-09-21T18:08:33.503;;325565.0;;386279.0;;1;13;<python><pandas><group-by>;Get group id back into pandas dataframe;5077.0
2605;2605;30087487.0;4.0;"<p>I want to use pandas dataFrames with dataTables.  I cannot figure out how to initialize the table without an id.   </p>

<p>Is there any way to set the id in the table tag when I call df.to_html()?</p>
";;0;;2013-02-26T00:20:22.273;3.0;15079118;2017-01-08T19:30:39.693;2013-02-26T00:25:15.433;;1064197.0;;1064197.0;;1;11;<python><datatables><pandas>;JS dataTables from pandas;1890.0
2626;2626;15112264.0;1.0;"<p>I have a Python dataFrame with multiple columns.</p>

<pre><code>2u    2s    4r     4n     4m   7h   7v
0     1     1      0      0     0    1
0     1     0      1      0     0    1
1     0     0      1      0     1    0
1     0     0      0      1     1    0
1     0     1      0      0     1    0
0     1     1      0      0     0    1
</code></pre>

<p>What I want to do is to convert this pandas.DAtaFrame into a list like following</p>

<pre><code>X= [
 [0,0,1,1,1,0],
 [1,1,0,0,0,1],
 [1,0,0,0,1,1],
 [0,1,1,0,0,0],
 [0,0,0,1,0,0],
 [0,0,1,1,1,0],
 [1,1,0,0,0,1]
 ]
</code></pre>

<p>(2u    2s    4r     4n     4m   7h   7v   are column headings.It will change in different situations, so don't bother about it.)</p>
";;0;;2013-02-27T12:33:19.430;6.0;15112234;2013-02-27T12:39:57.463;;;;;2005075.0;;1;17;<python><dataframe><pandas>;converting dataframe into a list;30212.0
2635;2635;15125793.0;4.0;"<p>I have a pandas DataFrame, <code>st</code> containing multiple columns:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23
Data columns:
Date(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values
Julian_Day                          53732  non-null values
AOT_1020                            53716  non-null values
AOT_870                             53732  non-null values
AOT_675                             53188  non-null values
AOT_500                             51687  non-null values
AOT_440                             53727  non-null values
AOT_380                             51864  non-null values
AOT_340                             52852  non-null values
Water(cm)                           51687  non-null values
%TripletVar_1020                    53710  non-null values
%TripletVar_870                     53726  non-null values
%TripletVar_675                     53182  non-null values
%TripletVar_500                     51683  non-null values
%TripletVar_440                     53721  non-null values
%TripletVar_380                     51860  non-null values
%TripletVar_340                     52846  non-null values
440-870Angstrom                     53732  non-null values
380-500Angstrom                     52253  non-null values
440-675Angstrom                     53732  non-null values
500-870Angstrom                     53732  non-null values
340-440Angstrom                     53277  non-null values
Last_Processing_Date(dd/mm/yyyy)    53732  non-null values
Solar_Zenith_Angle                  53732  non-null values
dtypes: datetime64[ns](1), float64(22), object(1)
</code></pre>

<p>I want to create two new columns for this dataframe based on applying a function to each row of the dataframe. I don't want to have to call the function multiple times (eg. by doing two separate <code>apply</code> calls) as it is rather computationally intensive. I have tried doing this in two ways, and neither of them work:</p>

<hr>

<p><strong>Using <code>apply</code>:</strong></p>

<p>I have written a function which takes a <code>Series</code> and returns a tuple of the values I want:</p>

<pre><code>def calculate(s):
    a = s['path'] + 2*s['row'] # Simple calc for example
    b = s['path'] * 0.153
    return (a, b)
</code></pre>

<p>Trying to apply this to the DataFrame gives an error:</p>

<pre><code>st.apply(calculate, axis=1)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-248-acb7a44054a7&gt; in &lt;module&gt;()
----&gt; 1 st.apply(calculate, axis=1)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)
   4191                     return self._apply_raw(f, axis)
   4192                 else:
-&gt; 4193                     return self._apply_standard(f, axis)
   4194             else:
   4195                 return self._apply_broadcast(f, axis)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _apply_standard(self, func, axis, ignore_failures)
   4274                 index = None
   4275 
-&gt; 4276             result = self._constructor(data=results, index=index)
   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),
   4278                           inplace=True)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in __init__(self, data, index, columns, dtype, copy)
    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)
    391         elif isinstance(data, dict):
--&gt; 392             mgr = self._init_dict(data, index, columns, dtype=dtype)
    393         elif isinstance(data, ma.MaskedArray):
    394             mask = ma.getmaskarray(data)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _init_dict(self, data, index, columns, dtype)
    521 
    522         return _arrays_to_mgr(arrays, data_names, index, columns,
--&gt; 523                               dtype=dtype)
    524 
    525     def _init_ndarray(self, values, index, columns, dtype=None,

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   5411 
   5412     # consolidate for now
-&gt; 5413     mgr = BlockManager(blocks, axes)
   5414     return mgr.consolidate()
   5415 

C:\Python27\lib\site-packages\pandas\core\internals.pyc in __init__(self, blocks, axes, do_integrity_check)
    802 
    803         if do_integrity_check:
--&gt; 804             self._verify_integrity()
    805 
    806         self._consolidate_check()

C:\Python27\lib\site-packages\pandas\core\internals.pyc in _verify_integrity(self)
    892                                      ""items"")
    893             if block.values.shape[1:] != mgr_shape[1:]:
--&gt; 894                 raise AssertionError('Block shape incompatible with manager')
    895         tot_items = sum(len(x.items) for x in self.blocks)
    896         if len(self.items) != tot_items:

AssertionError: Block shape incompatible with manager
</code></pre>

<p>I was then going to assign the values returned from <code>apply</code> to two new columns using the method shown in <a href=""https://stackoverflow.com/questions/12356501/pandas-create-two-new-columns-in-a-dataframe-with-values-calculated-from-a-pre"">this question</a>. However, I can't even get to this point! This all works fine if I just return one value.</p>

<hr>

<p><strong>Using a loop:</strong></p>

<p>I first created two new columns of the dataframe and set them to <code>None</code>:</p>

<pre><code>st['a'] = None
st['b'] = None
</code></pre>

<p>Then looped over all of the indices and tried to modify these <code>None</code> values that I'd got in there, but the modifications I did didn't seem to work. That is, no error was generated, but the DataFrame didn't seem to be modified.</p>

<pre><code>for i in st.index:
    # do calc here
    st.ix[i]['a'] = a
    st.ix[i]['b'] = b
</code></pre>

<hr>

<p>I thought that both of these methods would work, but neither of them did. So, what am I doing wrong here? And what is the best, most 'pythonic' and 'pandaonic' way to do this?</p>
";;0;;2013-02-27T17:13:54.930;11.0;15118111;2016-07-19T10:25:21.350;2017-05-23T10:31:27.110;;-1.0;;1912.0;;1;24;<python><pandas>;Apply function to each row of pandas dataframe to create two new columns;61486.0
2644;2644;27023500.0;3.0;"<p>What is the closest equivalent to an <a href=""http://www.stat.berkeley.edu/classes/s133/factors.html"">R Factor variable</a> in <a href=""http://pandas.pydata.org/"">Python pandas</a>?</p>
";;2;;2013-02-27T23:16:34.673;4.0;15124439;2015-05-04T06:47:14.160;2013-02-28T00:23:56.193;;283296.0;;283296.0;;1;18;<python><r><pandas>;Closest equivalent of a factor variable in Python Pandas;4996.0
2666;2666;15139677.0;3.0;"<p>I have a data frame and I would like to know how many times a given column has the most frequent value.</p>

<p>I try to do it in the following way:</p>

<pre><code>items_counts = df['item'].value_counts()
max_item = items_counts.max()
</code></pre>

<p>As a result I get:</p>

<pre><code>ValueError: cannot convert float NaN to integer
</code></pre>

<p>As far as I understand, with the first line I get series in which the values from a column are used as key and frequency of these values are used as values. So, I just need to find the largest value in the series and, because of some reason, it does not work. Does anybody know how this problem can be solved?</p>
";;1;;2013-02-28T15:11:19.177;2.0;15138973;2017-05-11T05:05:00.153;;;;;245549.0;;1;13;<pandas><frequency><series>;How to get the number of the most frequent value in a column?;16451.0
2674;2674;15144847.0;3.0;"<p>Got a large dataframe that I want to take slices of (according to multiple boolean criteria), and then modify the entries in those slices in order to change the original dataframe -- i.e. I need a <code>view</code> to the original. Problem is, fancy indexing always returns a <code>copy</code>. Thought of the <code>.ix</code> method, but boolean indexing with the <code>df.ix[]</code> method also returns a copy. </p>

<p>Essentially if <code>df</code> is my dataframe, I'd like a view to column C such that <code>C!=0, A==10, B&lt;30,...</code> etc. Is there a fast way to do this in pandas? </p>
";;0;;2013-02-28T19:17:19.917;4.0;15143842;2015-09-18T02:27:47.257;2013-02-28T19:56:34.530;;1730674.0;;2120889.0;;1;13;<python><dataframe><pandas>;boolean indexing that can produce a view to a large pandas dataframe?;1892.0
2703;2703;15204235.0;2.0;"<p>What is the idiomatic way of converting a pandas DateTimeIndex to (an iterable of) Unix Time? 
This is probably not the way to go:</p>

<pre><code>[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]
</code></pre>
";;0;;2013-03-04T14:17:41.270;8.0;15203623;2013-03-04T15:03:48.133;;;;;2131903.0;;1;30;<python><pandas>;Convert pandas DateTimeIndex to Unix Time?;20077.0
2714;2714;15213171.0;2.0;"<p>I'm attempting to read a simple space-separated file with pandas <code>read_csv</code> method.  However, pandas doesn't seem to be obeying my <code>dtype</code> argument.  Maybe I'm incorrectly specifying it?</p>

<p>I've distilled down my somewhat complicated call to <code>read_csv</code> to this simple test case.  I'm actually using the <code>converters</code> argument in my 'real' scenario but I removed this for simplicity.</p>

<p>Below is my ipython session:</p>

<pre><code>&gt;&gt;&gt; cat test.out
a b
0.76398 0.81394
0.32136 0.91063
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)
&gt;&gt;&gt; x
         a        b
0  0.76398  0.81394
1  0.32136  0.91063
&gt;&gt;&gt; x.a.dtype
dtype('float64')
</code></pre>

<p>I've also tried this using this with a <code>dtype</code> of <code>numpy.int32</code> or <code>numpy.int64</code>.  These choices result in an exception:</p>

<pre><code>AttributeError: 'NoneType' object has no attribute 'dtype'
</code></pre>

<p>I'm assuming the <code>AttributeError</code> is because pandas will not automatically try to convert/truncate the float values into an integer?</p>

<p>I'm running on a 32-bit machine with a 32-bit version of Python.</p>

<pre><code>&gt;&gt;&gt; !uname -a
Linux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux
&gt;&gt;&gt; import platform
&gt;&gt;&gt; platform.architecture()
('32bit', 'ELF')
&gt;&gt;&gt; pandas.__version__
'0.10.1'
</code></pre>
";;2;;2013-03-04T20:56:12.807;2.0;15210962;2016-11-02T01:52:12.253;2016-11-02T01:52:12.253;;3621464.0;;1108031.0;;1;34;<numpy><pandas>;Specifying dtype float32 with pandas.read_csv on pandas 0.10.1;52865.0
2720;2720;15223034.0;3.0;"<p>I have a data frame with three string columns. I know that the only one value in the 3rd column is valid for every combination of the first two. To clean the data I have to group by data frame by first two columns and select most common value of the third column for each combination.</p>

<p>My code:</p>

<pre><code>import pandas as pd
from scipy import stats

source = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], 
                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],
                  'Short name' : ['NY','New','Spb','NY']})

print source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])
</code></pre>

<p>Last line of code doesn't work, it says ""Key error 'Short name'"" and if I try to group only by City, then I got an AssertionError. What can I do fix it?</p>
";;0;;2013-03-05T11:34:38.150;6.0;15222754;2017-02-13T21:07:36.490;;;;;1956483.0;;1;23;<python><pandas>;Group by pandas dataframe and select most common string factor;15902.0
2736;2736;15252012.0;3.0;"<p>I have a data set that looks like this (at most 5 columns - but can be less)</p>

<pre><code>1,2,3
1,2,3,4
1,2,3,4,5
1,2
1,2,3,4
....
</code></pre>

<p>I am trying to use pandas read_table to read this into a 5 column data frame. I would like to read this in without additional massaging. </p>

<p>If I try</p>

<pre><code>import pandas as pd
my_cols=['A','B','C','D','E']
my_df=pd.read_table(path,sep=',',header=None,names=my_cols)
</code></pre>

<p>I get an error - ""column names have 5 fields, data has 3 fields"". </p>

<p><strong>Is there any way to make pandas fill in NaN for the missing columns while reading the data?</strong></p>
";;0;;2013-03-06T08:52:55.007;10.0;15242746;2013-03-06T15:55:05.310;;;;;2139058.0;;1;38;<python><pandas>;Handling Variable Number of Columns with Pandas - Python;15050.0
2741;2741;15248239.0;5.0;"<p>I have a <code>pandas.DataFrame</code> with a column called <code>name</code> containing strings.
I would like to get a list of the names which occur more than once in the column. How do I do that?</p>

<p>I tried:</p>

<pre><code>funcs_groups = funcs.groupby(funcs.name)
funcs_groups[(funcs_groups.count().name&gt;1)]
</code></pre>

<p>But it doesn't filter out the singleton names.</p>
";;0;;2013-03-06T12:40:38.490;6.0;15247628;2016-09-18T23:19:48.407;2013-03-06T12:55:03.413;;1579844.0;;1579844.0;;1;19;<python><group-by><pandas>;How to find duplicate names using pandas?;25826.0
2755;2755;15262146.0;2.0;"<p>I just recently made the switch from R to python and have been having some trouble getting used to data frames again as opposed to using R's data.table. The problem I've been having is that I'd like to take a list of strings, check for a value, then sum the count of that string- broken down by user. So I would like to take this data:</p>

<pre><code>   A_id       B    C
1:   a1    ""up""  100
2:   a2  ""down""  102
3:   a3    ""up""  100
3:   a3    ""up""  250
4:   a4  ""left""  100
5:   a5 ""right""  102
</code></pre>

<p>And return:</p>

<pre><code>   A_id_grouped   sum_up   sum_down  ...  over_200_up
1:           a1        1          0  ...            0
2:           a2        0          1                 0
3:           a3        2          0  ...            1
4:           a4        0          0                 0
5:           a5        0          0  ...            0
</code></pre>

<p>Before I did it with the R code (using data.table)</p>

<pre><code>&gt;DT[ ,list(A_id_grouped, sum_up = sum(B == ""up""),
+  sum_down = sum(B == ""down""), 
+  ...,
+  over_200_up = sum(up == ""up"" &amp; &lt; 200), by=list(A)];
</code></pre>

<p>However all of my recent attempts with Python have failed me:</p>

<pre><code>DT.agg({""D"": [np.sum(DT[DT[""B""]==""up""]),np.sum(DT[DT[""B""]==""up""])], ...
    ""C"": np.sum(DT[(DT[""B""]==""up"") &amp; (DT[""C""]&gt;200)])
    })
</code></pre>

<p>Thank you in advance! it seems like a simple question however I couldn't find it anywhere.</p>
";;0;;2013-03-06T22:39:42.590;6.0;15259547;2013-03-07T02:45:52.397;;;;;1529734.0;;1;12;<python><r><pandas><data.table>;conditional sums for pandas aggregate;4486.0
2785;2785;15300930.0;2.0;"<p>I have the following 15 minute data as a <code>dataframe</code> for 3 years. With the first two columns being the index.</p>

<pre><code>2014-01-01 00:15:00  1269.6      
2014-01-01 00:30:00  1161.6      
2014-01-01 00:45:00  1466.4      
2014-01-01 01:00:00  1365.6      
2014-01-01 01:15:00  1362.6      
2014-01-01 01:30:00  1064.0      
2014-01-01 01:45:00  1171.2      
2014-01-01 02:00:00  1171.0      
2014-01-01 02:15:00  1330.4      
2014-01-01 02:30:00  1309.6      
2014-01-01 02:45:00  1308.4      
2014-01-01 03:00:00  1494.0    
</code></pre>

<p>I have used <code>resample</code> to get a second series with monthly averages.</p>

<pre><code>data_Monthly = data.resample('1M', how='mean')
</code></pre>

<p>How can I divide the values in the last column by their monthly average with the result being still a time series on 15 minute granularity?</p>
";;0;;2013-03-08T15:06:39.983;9.0;15297053;2017-07-05T17:46:58.923;2013-03-08T15:34:38.003;;61378.0;;2148845.0;;1;15;<pandas><time-series><average><resampling>;How can I divide single values of a dataframe by monthly averages?;9735.0
2791;2791;15315507.0;1.0;"<p>For example I have simple DF:</p>

<pre><code>import pandas as pd
from random import randint

df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                   'B': [randint(1, 9)*10 for x in xrange(10)],
                   'C': [randint(1, 9)*100 for x in xrange(10)]})
</code></pre>

<p>Can I select values from 'A' for which corresponding values for 'B' will be greater than 50, and for 'C' - not equal 900, using methods and idioms of Pandas?</p>
";;0;;2013-03-09T20:17:49.170;44.0;15315452;2017-07-10T11:48:15.470;2017-05-28T13:55:00.227;;4008657.0;;1818608.0;;1;77;<python><pandas>;Selecting with complex criteria from pandas.DataFrame;153283.0
2793;2793;15322715.0;2.0;"<p>On a concrete problem, say I have a DataFrame DF</p>

<pre><code>     word  tag count
0    a     S    30
1    the   S    20
2    a     T    60
3    an    T    5
4    the   T    10 
</code></pre>

<p>I want to find, <strong>for every ""word"", the ""tag"" that has the most ""count""</strong>. So the return would be something like</p>

<pre><code>     word  tag count
1    the   S    20
2    a     T    60
3    an    T    5
</code></pre>

<p>I don't care about the count column or if the order/Index is original or messed up. Returning a dictionary {<strong>'the' : 'S'</strong>, ...} is just fine.</p>

<p>I hope I can do</p>

<pre><code>DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )
</code></pre>

<p>but it doesn't work. I can't access column information.</p>

<p>More abstractly, <strong>what does the <em>function</em> in agg(<em>function</em>) see as its argument</strong>?</p>

<p>btw, is .agg() the same as .aggregate() ?</p>

<p>Many thanks.</p>
";;0;;2013-03-10T13:16:20.917;18.0;15322632;2015-08-24T12:39:17.967;2013-12-31T01:41:47.487;;680232.0;;1568919.0;;1;37;<python><group-by><pandas>;python pandas, DF.groupby().agg(), column reference in agg();68108.0
2796;2796;;4.0;"<p>I would like to cleanly filter a dataframe using regex on one of the columns.</p>

<p>For a contrived example:</p>

<pre><code>In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})
In [211]: foo
Out[211]: 
   a    b
0  1   hi
1  2  foo
2  3  fat
3  4  cat
</code></pre>

<p>I want to filter the rows to those that start with <code>f</code> using a regex. First go:</p>

<pre><code>In [213]: foo.b.str.match('f.*')
Out[213]: 
0    []
1    ()
2    ()
3    []
</code></pre>

<p>That's not too terribly useful. However this will get me my boolean index:</p>

<pre><code>In [226]: foo.b.str.match('(f.*)').str.len() &gt; 0
Out[226]: 
0    False
1     True
2     True
3    False
Name: b
</code></pre>

<p>So I could then do my restriction by:</p>

<pre><code>In [229]: foo[foo.b.str.match('(f.*)').str.len() &gt; 0]
Out[229]: 
   a    b
1  2  foo
2  3  fat
</code></pre>

<p>That makes me artificially put a group into the regex though, and seems like maybe not the clean way to go. Is there a better way to do this?</p>
";;2;;2013-03-10T17:23:39.850;10.0;15325182;2017-06-02T19:23:05.903;2016-04-09T18:08:38.510;;1492145.0;;1704581.0;;1;56;<python><regex><pandas>;How to filter rows in pandas by regex;38880.0
2811;2811;15362700.0;4.0;"<p>I tried:</p>

<pre><code>x=pandas.DataFrame(...)
s = x.take([0], axis=1)
</code></pre>

<p>And <code>s</code> gets a DataFrame, not a Series.</p>
";;0;;2013-03-12T12:14:22.037;12.0;15360925;2017-06-17T17:43:52.507;2013-03-12T13:34:57.423;;567989.0;;1579844.0;;1;55;<python><dataframe><pandas><series>;How to get the first column of a pandas DataFrame as a Series?;81214.0
2829;2829;15375176.0;3.0;"<p>I have a pandas dataframe with a column called <code>my_labels</code> which contains strings: <code>'A', 'B', 'C', 'D', 'E'</code>. I would like to count the number of occurances of each of these strings then divide the number of counts by the sum of all the counts. I'm trying to do this in Pandas like this:</p>

<pre><code>func = lambda x: x.size() / x.sum()
data = frame.groupby('my_labels').apply(func)
</code></pre>

<p>This code throws an error, 'DataFrame object has no attribute 'size'. How can I apply a function to calculate this in Pandas?</p>
";;0;;2013-03-13T00:01:13.620;6.0;15374597;2015-07-04T16:29:38.607;;;;;1255817.0;;1;16;<python><pandas>;Apply function to pandas groupby;30097.0
2855;2855;15411596.0;4.0;"<p>I am using pandas as a db substitute as I have multiple databases (oracle, mssql, etc) and I am unable to make a sequence of commands to a SQL equivalent.</p>

<p>I have a table loaded in a DataFrame with some columns:</p>

<pre><code>YEARMONTH, CLIENTCODE, SIZE, .... etc etc
</code></pre>

<p>In SQL, to count the amount of different clients per year would be:</p>

<pre><code>SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;
</code></pre>

<p>And the result would be </p>

<pre><code>201301    5000
201302    13245
</code></pre>

<p>How can I do that in pandas? </p>
";;2;;2013-03-14T13:50:03.070;22.0;15411158;2017-08-21T11:33:19.787;2015-12-20T07:31:56.753;;1240268.0;;1879940.0;;1;87;<python><pandas><count><group-by><distinct>;Pandas count(distinct) equivalent;100056.0
2866;2866;;1.0;"<p>I have created a package using the encoding utf-8. </p>

<p>When calling a function, it returns a <code>DataFrame</code>, with a column coded in utf-8.</p>

<p>When using IPython at the command line, I don't have any problems showing the content of this table. When using the Notebook, it crashes with the error <code>'utf8' codec can't decode byte 0xe7</code>. I've attached a full traceback below.</p>

<p>What is the proper encoding to work with Notebook?</p>

<pre><code>UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-13-92c0011919e7&gt; in &lt;module&gt;()
      3 ver = verif.VerificacaoNA()
      4 comp, total = ver.executarCompRealFisica(DT_INI, DT_FIN)
----&gt; 5 comp

c:\Python27-32\lib\site-packages\ipython-0.13.1-py2.7.egg\IPython\core\displayhook.pyc in __call__(self, result)
    240             self.update_user_ns(result)
    241             self.log_output(format_dict)
--&gt; 242             self.finish_displayhook()
    243 
    244     def flush(self):

c:\Python27-32\lib\site-packages\ipython-0.13.1-py2.7.egg\IPython\zmq\displayhook.pyc in finish_displayhook(self)
     59         sys.stdout.flush()
     60         sys.stderr.flush()
---&gt; 61         self.session.send(self.pub_socket, self.msg, ident=self.topic)
     62         self.msg = None
     63 

c:\Python27-32\lib\site-packages\ipython-0.13.1-py2.7.egg\IPython\zmq\session.pyc in send(self, stream, msg_or_type, content, parent, ident, buffers, subheader, track, header)
    557 
    558         buffers = [] if buffers is None else buffers
--&gt; 559         to_send = self.serialize(msg, ident)
    560         flag = 0
    561         if buffers:

c:\Python27-32\lib\site-packages\ipython-0.13.1-py2.7.egg\IPython\zmq\session.pyc in serialize(self, msg, ident)
    461             content = self.none
    462         elif isinstance(content, dict):
--&gt; 463             content = self.pack(content)
    464         elif isinstance(content, bytes):
    465             # content is already packed, as in a relayed message

c:\Python27-32\lib\site-packages\ipython-0.13.1-py2.7.egg\IPython\zmq\session.pyc in &lt;lambda&gt;(obj)
     76 
     77 # ISO8601-ify datetime objects
---&gt; 78 json_packer = lambda obj: jsonapi.dumps(obj, default=date_default)
     79 json_unpacker = lambda s: extract_dates(jsonapi.loads(s))
     80 

c:\Python27-32\lib\site-packages\pyzmq-13.0.0-py2.7-win32.egg\zmq\utils\jsonapi.pyc in dumps(o, **kwargs)
     70         kwargs['separators'] = (',', ':')
     71 
---&gt; 72     return _squash_unicode(jsonmod.dumps(o, **kwargs))
     73 
     74 def loads(s, **kwargs):

c:\Python27-32\lib\json\__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, encoding=encoding, default=default,
--&gt; 238         **kw).encode(obj)
    239 
    240 

c:\Python27-32\lib\json\encoder.pyc in encode(self, o)
    199         # exceptions aren't as detailed.  The list call should be roughly
    200         # equivalent to the PySequence_Fast that ''.join() would do.
--&gt; 201         chunks = self.iterencode(o, _one_shot=True)
    202         if not isinstance(chunks, (list, tuple)):
    203             chunks = list(chunks)

c:\Python27-32\lib\json\encoder.pyc in iterencode(self, o, _one_shot)
    262                 self.key_separator, self.item_separator, self.sort_keys,
    263                 self.skipkeys, _one_shot)
--&gt; 264         return _iterencode(o, 0)
    265 
    266 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

UnicodeDecodeError: 'utf8' codec can't decode byte 0xe7 in position 199: invalid continuation byte
</code></pre>
";;3;;2013-03-14T21:36:12.817;7.0;15420672;2013-09-30T08:01:28.067;2013-03-15T15:46:16.413;;1253428.0;;1879940.0;;1;16;<pandas><ipython><ipython-notebook>;IPython Notebook: What is the default encoding?;9150.0
2872;2872;15433426.0;2.0;"<p>I have the following dataframe read in from a .csv file with the ""Date"" column being the index. The days are in the rows and the columns show the values for the hours that day.</p>

<pre><code>&gt; Date           h1 h2  h3  h4 ... h24
&gt; 14.03.2013    60  50  52  49 ... 73
</code></pre>

<p>I would like to arrange it like this, so that there is one index column with the date/time and one column with the values in a sequence</p>

<pre><code>&gt;Date/Time            Value
&gt;14.03.2013 00:00:00  60
&gt;14.03.2013 01:00:00  50
&gt;14.03.2013 02:00:00  52
&gt;14.03.2013 03:00:00  49
&gt;.
&gt;.
&gt;.
&gt;14.03.2013 23:00:00  73
</code></pre>

<p>I was trying it by using two loops to go through the dataframe.
Is there an easier way to do this in pandas?</p>
";;0;;2013-03-15T12:36:02.737;6.0;15432659;2013-03-17T20:29:03.637;2013-03-15T12:44:41.263;;507423.0;;2148845.0;;1;13;<python><row><pandas><sequence><dataframe>;How to rearrange a python pandas dataframe?;5962.0
2883;2883;15455455.0;1.0;"<p>I'd like to store JSON data in a Python Pandas DataFrame</p>

<p>my JSON data is a dict of dicts of dicts like this</p>

<pre><code>d = {
  ""col1"": {
    ""row1"": {
      ""data1"": ""0.87"", 
      ""data2"": ""Title col1"", 
      ""data3"": ""14.4878"", 
      ""data4"": ""Title row1""
    }, 
    ""row2"": {
      ""data1"": ""15352.3"", 
      ""data2"": ""Title col1"", 
      ""data3"": ""14.9561"", 
      ""data4"": ""Title row2""
    }, 
    ""row3"": {
      ""data1"": ""0"", 
      ""data2"": ""Title col1"", 
      ""data3"": ""16.8293"", 
      ""data4"": ""Title row3""
    }
  }, 
  ""col2"": {
    ""row1"": {
      ""data1"": ""0.87"", 
      ""data2"": ""Title col2"", 
      ""data3"": ""24.4878"", 
      ""data4"": ""Title row1""
    }, 
    ""row2"": {
      ""data1"": ""15352.3"", 
      ""data2"": ""Title col2"", 
      ""data3"": ""24.9561"", 
      ""data4"": ""Title row2""
    }, 
    ""row3"": {
      ""data1"": ""0"", 
      ""data2"": ""Title col2"", 
      ""data3"": ""26.8293"", 
      ""data4"": ""Title row3""
    }
  }
}
</code></pre>

<p>I did this to put my data in a DataFrame</p>

<pre><code>import pandas as pd
df=pd.DataFrame(d)
</code></pre>

<p>I get this</p>

<pre><code>In [1]: df
Out[1]: 
                                                   col1                                               col2
row1  {'data4': 'Title col1', 'data1': '0.87', 'data3':  {'data4': 'Title col1', 'data1': '0.87', 'data3':
row2  {'data4': 'Title col2', 'data1': '15352.3', 'data  {'data4': 'Title col2', 'data1': '15352.3', 'data
row3  {'data4': 'Title col3', 'data1': '0', 'data3': '1  {'data4': 'Title col3', 'data1': '0', 'data3': '2
</code></pre>

<p>My problem is that my DataFrame contains dicts instead of values.</p>

<p>I wonder how I can manage multidimensionnal data (more than 2 dimensions... 3 dimensions here) with a Pandas DataFrame.</p>

<p>Each dict inside DataFrame have the same keys.</p>
";;0;;2013-03-16T22:21:12.190;2.0;15455388;2014-01-09T18:20:02.137;2013-03-16T22:26:15.257;;1609077.0;;1609077.0;;1;12;<python><json><dictionary><pandas>;Dict of dicts of dicts to DataFrame;4558.0
2897;2897;15478827.0;4.0;"<p>I have a DataFrame ""df"" with (time,ticker) Multiindex and bid/ask/etc data columns: </p>

<pre>

                          tod    last     bid      ask      volume
    time        ticker                  
    2013-02-01  SPY       1600   149.70   150.14   150.17   1300
                SLV       1600   30.44    30.38    30.43    3892
                GLD       1600   161.20   161.19   161.21   3860

</pre>

<p>I would like to select a second-level (level=1) cross section using multiple keys. Right now, I can do it using one key, i.e.</p>

<pre>

    df.xs('SPY', level=1)

</pre>

<p>which gives me a timeseries of SPY. What is the best way to select a multi-key cross section, i.e. a combined cross-section of both SPY and GLD, something like:</p>

<pre>

    df.xs(['SPY', 'GLD'], level=1)

</pre>

<p>?</p>
";;0;;2013-03-17T17:19:33.427;3.0;15463729;2016-12-07T23:22:12.347;2014-07-28T23:53:13.860;;283296.0;;2179806.0;;1;14;<pandas>;Select a multiple-key cross section from a DataFrame;2361.0
2901;2901;15466103.0;1.0;"<p>I'm starting to learn Pandas and am trying to find the most Pythonic (or panda-thonic?) ways to do certain tasks.</p>

<p>Suppose we have a DataFrame with columns A, B, and C.</p>

<ul>
<li>Column A contains boolean values: each row's A value is either true or false.</li>
<li>Column B has some important values we want to plot.</li>
</ul>

<p>What we want to discover is the subtle distinctions between B values for rows that have A set to false, vs. B values for rows that have A is true.</p>

<p>In other words, <strong>how can I group by the value of column A (either true or false), then plot the values of column B for both groups on the same graph?</strong> The two datasets should be colored differently to be able to distinguish the points.</p>

<hr>

<p>Next, let's add another feature to this program: before graphing, we want to compute another value for each row and store it in column D. This value is the mean of all data stored in B for the entire five minutes before a record - but we only include rows that have the same boolean value stored in A.</p>

<p>In other words, <strong>if I have a row where <code>A=True</code> and <code>time=t</code>, I want to compute a value for column D that is the mean of B for all records from time <code>t-5</code> to <code>t</code> that have the same <code>A=True</code>.</strong></p>

<p>In this case, how can we execute the groupby on values of A, then apply this computation to each individual group, and finally plot the D values for the two groups?</p>
";;1;;2013-03-17T20:12:23.337;15.0;15465645;2013-03-17T20:52:57.310;;;;;130164.0;;1;35;<python><matplotlib><group-by><pandas><data-analysis>;Plotting results of Pandas GroupBy;31249.0
2974;2974;15557663.0;1.0;"<p>I have a data frame. Then I have a logical condition using which I create another data frame by removing some rows. The new data frame however skips indices for removed rows. How can I get it to reindex sequentially without skipping? Here's a sample coded to clarify</p>

<pre><code>import pandas as pd
import numpy as np

jjarray = np.array(range(5))
eq2 = jjarray == 2
neq2 = np.logical_not(eq2)

jjdf = pd.DataFrame(jjarray)
jjdfno2 = jjdf[neq2]

jjdfno2
</code></pre>

<p>Out: </p>

<pre><code>  0
0 0
1 1
3 3
4 4
</code></pre>

<p>I want it to look like this:</p>

<pre><code>  0
0 0
1 1
2 3
3 4
</code></pre>

<p>Thanks.</p>
";;0;;2013-03-21T20:34:43.270;4.0;15557542;2013-03-21T20:41:07.893;;;;;2133151.0;;1;20;<pandas><dataframe>;Reindexing dataframes;22266.0
2983;2983;15574875.0;4.0;"<p>I'm using Pandas 0.10.1</p>

<p>Considering this Dataframe:</p>

<pre><code>Date       State   City    SalesToday  SalesMTD  SalesYTD
20130320     stA    ctA            20       400      1000
20130320     stA    ctB            30       500      1100
20130320     stB    ctC            10       500       900
20130320     stB    ctD            40       200      1300
20130320     stC    ctF            30       300       800
</code></pre>

<p>How can i group subtotals per state?</p>

<pre><code>State   City  SalesToday  SalesMTD  SalesYTD
  stA    ALL          50       900      2100
  stA    ctA          20       400      1000
  stA    ctB          30       500      1100
</code></pre>

<p>I tried with a pivot table but i only can have subtotals in columns</p>

<pre><code>table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\
                     rows=['State','City'], aggfunc=np.sum, margins=True)
</code></pre>

<p>I can achieve this on excel, with a pivot table.</p>
";;0;;2013-03-22T12:16:10.503;12.0;15570099;2017-04-20T21:49:50.963;;;;;584332.0;;1;26;<python><pandas><pivot-table>;Pandas Pivot tables row subtotals;17499.0
2996;2996;15590006.0;4.0;"<p>My problem is how to calculate frequencies on multiple variables in pandas . 
I have from this dataframe : </p>

<pre><code>d1 = pd.DataFrame( {'StudentID': [""x1"", ""x10"", ""x2"",""x3"", ""x4"", ""x5"", ""x6"",   ""x7"",     ""x8"", ""x9""],
                       'StudentGender' : ['F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M'],
                 'ExamenYear': ['2007','2007','2007','2008','2008','2008','2008','2009','2009','2009'],
                 'Exam': ['algebra', 'stats', 'bio', 'algebra', 'algebra', 'stats', 'stats', 'algebra', 'bio', 'bio'],
                 'Participated': ['no','yes','yes','yes','no','yes','yes','yes','yes','yes'],
                  'Passed': ['no','yes','yes','yes','no','yes','yes','yes','no','yes']},
                  columns = ['StudentID', 'StudentGender', 'ExamenYear', 'Exam', 'Participated', 'Passed'])
</code></pre>

<p>To the following result</p>

<pre><code>             Participated  OfWhichpassed
 ExamenYear                             
2007                   3              2
2008                   4              3
2009                   3              2
</code></pre>

<p>(1) One possibility I tried is to compute two dataframe and bind them </p>

<pre><code>t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)
t2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)
tx = pd.concat([t1, t2] , axis = 1)

Res1 = tx['yes']
</code></pre>

<p>(2) The second possibility is to use an aggregation function . </p>

<pre><code>import collections
dg = d1.groupby('ExamenYear')
Res2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})

 Res2.columns = ['Participated', 'OfWhichpassed']
</code></pre>

<p>Both ways are awckward to say the least. 
<strong>How is this done properly in pandas ?</strong> </p>

<p>P.S: I also tried <em>value_counts</em> instead of <em>collections.Counter</em> but could not get it to work </p>

<p>For reference: Few months ago, I asked similar question for R  <a href=""https://stackoverflow.com/questions/11852408/"">here</a> and <strong>plyr</strong> could help </p>

<p><strong>---- UPDATE ------</strong></p>

<p>user <strong>DSM</strong> is right. there was a mistake in the desired table result. </p>

<p>(1) The code for option one is </p>

<pre><code> t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], aggfunc = len)
 t2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)
 t3 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)

 Res1 = pd.DataFrame( {'All': t1,
                       'OfWhichParticipated': t2['yes'],
                     'OfWhichPassed': t3['yes']})
</code></pre>

<p>It will produce the result </p>

<pre><code>             All  OfWhichParticipated  OfWhichPassed
ExamenYear                                         
2007          3                    2              2
2008          4                    3              3
2009          3                    3              2
</code></pre>

<p>(2) For Option 2, thanks to user <strong>herrfz</strong>, I figured out how to use value_count and the code will be </p>

<pre><code>Res2 = d1.groupby('ExamenYear').agg({'StudentID': len,
                                 'Participated': lambda x: x.value_counts()['yes'],
                                 'Passed': lambda x: x.value_counts()['yes']})

Res2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']
</code></pre>

<p>which will produce the same result as Res1</p>

<p>My question remains though: </p>

<p>Using Option 2, will it be possible to use the same Variable twice (for another operation ?) can one pass a custom name for the resulting variable ? </p>

<p><strong>---- A NEW UPDATE ----</strong> </p>

<p>I have finally decided to use <strong>apply</strong> which I understand is more flexible. </p>
";;3;;2013-03-23T16:47:03.057;7.0;15589354;2016-08-19T05:59:41.527;2017-05-23T12:17:30.793;;-1.0;;1043144.0;;1;11;<python><group-by><pandas>;Frequency tables in pandas (like plyr in R);10305.0
3045;3045;16958464.0;3.0;"<p>Is there a way to preserve the order of the columns in a csv file when read and the write with Python Pandas? For example, in this code</p>

<pre><code>import pandas as pd

data = pd.read_csv(filename)
data.to_csv(filename)
</code></pre>

<p>the output files might be different because the columns are not preserved.</p>
";;1;;2013-03-27T07:27:37.053;2.0;15653688;2017-05-14T19:44:26.577;;;;;482819.0;;1;15;<python><pandas>;Preserving column order in Python Pandas DataFrame;18663.0
3090;3090;15705958.0;5.0;"<p>I hope I can find help for my question. I am searching for a solution for the following problem:</p>

<p>I have a dataFrame like:</p>

<pre><code> Sp  Mt Value  count
0  MM1  S1   a      **3**
1  MM1  S1   n      2
2  MM1  S3   cb     5
3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10**
5  MM2  S4   dgd      1
6  MM4  S2  rd     2
7  MM4  S2   cb      2
8  MM4  S2   uyi      **7**
</code></pre>

<p>My objective is to get the result rows whose count is max between the groups, like :</p>

<pre><code>0  MM1  S1   a      **3**
1 3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10** 
8  MM4  S2   uyi      **7**
</code></pre>

<p>Somebody knows how can I do it in pandas or in python?</p>

<p><strong>UPDATE</strong></p>

<p>I didn't give more details for my question. For my problem, I want to group by ['Sp','Mt'].  Let take a second example like this :</p>

<pre><code>   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8
</code></pre>

<p>For the above example, I want to get ALL the rows where count equals max in each group e.g :</p>

<pre><code>MM2  S4   bg     10
MM4  S2   cb     8
MM4  S2   uyi    8
</code></pre>
";;5;;2013-03-29T14:48:13.023;27.0;15705630;2017-07-07T00:40:35.037;2017-03-26T00:44:18.697;;778533.0;;2216109.0;;1;40;<python><pandas>;Python : Getting the Row which has the max value in groups using groupby;46906.0
3106;3106;15723905.0;3.0;"<p>In pandas, how can I convert a column of a DataFrame into dtype object?
Or better yet, into a factor? (For those who speak R, in Python, how do I <code>as.factor()</code>?)</p>

<p>Also, what's the difference between <code>pandas.Factor</code> and <code>pandas.Categorical</code>?</p>
";;0;;2013-03-30T21:21:57.247;6.0;15723628;2015-04-30T23:33:54.007;2015-02-09T21:03:46.680;;3923281.0;;1319312.0;;1;30;<python><pandas>;Pandas - make a column dtype object or Factor;21102.0
3116;3116;15742147.0;3.0;"<p><img src=""https://i.stack.imgur.com/a34it.png"" alt=""Structure of data;""></p>

<p>Using Python Pandas I am trying to find the 'Country' &amp; 'Place' with the maximum value.</p>

<p>This returns the maximum value:</p>

<pre><code>data.groupby(['Country','Place'])['Value'].max()
</code></pre>

<p>But how do I get the corresponding 'Country' and 'Place' name?</p>
";;0;;2013-04-01T10:31:01.700;9.0;15741759;2017-07-07T01:55:56.137;2013-04-01T10:38:03.497;;1219006.0;;1948860.0;;1;26;<python><pandas>;Find maximum value of a column and return the corresponding row values using Pandas;51057.0
3126;3126;;3.0;"<p>I have a table of data imported from a CSV file into a DataFrame.</p>

<p>The data contains around 10 categorical fields, 1 month column (in date time format) and the rest are data series. </p>

<p>How do I convert the date column into an index across the the column axis?</p>
";;1;;2013-04-01T21:43:42.900;5.0;15752422;2015-02-02T16:51:37.530;2013-04-01T22:47:25.127;;243434.0;;1918822.0;;1;13;<python><dataframe><pandas>;Python Pandas - Date Column to Column index;18886.0
3131;3131;15756128.0;2.0;"<p>From a Pandas newbie: I have data that looks essentially like this -</p>

<pre><code> data1=pd.DataFrame({'Dir':['E','E','W','W','E','W','W','E'], 'Bool':['Y','N','Y','N','Y','N','Y','N'], 'Data':[4,5,6,7,8,9,10,11]}, index=pd.DatetimeIndex(['12/30/2000','12/30/2000','12/30/2000','1/2/2001','1/3/2001','1/3/2001','12/30/2000','12/30/2000']))
data1
Out[1]: 
           Bool  Data Dir
2000-12-30    Y     4   E
2000-12-30    N     5   E
2000-12-30    Y     6   W
2001-01-02    N     7   W
2001-01-03    Y     8   E
2001-01-03    N     9   W
2000-12-30    Y    10   W
2000-12-30    N    11   E
</code></pre>

<p>And I want to group it by multiple levels, then do a cumsum():</p>

<p>E.g., like <code>running_sum=data1.groupby(['Bool','Dir']).cumsum()</code> &lt;-(Doesn't work)</p>

<p>with output that would look something like:</p>

<pre><code>Bool Dir Date        running_sum
N    E   2000-12-30           16
     W   2001-01-02            7
         2001-01-03           16
Y    E   2000-12-30            4
         2001-01-03           12
     W   2000-12-30           16
</code></pre>

<p>My ""like"" code is clearly not even close. I have made a number of attempts and learned many new things about how not to do this. </p>

<p>Thanks for any help you can give.</p>
";;0;;2013-04-02T02:28:06.497;7.0;15755057;2015-09-30T13:23:39.147;;;;;2209601.0;;1;11;<python><group-by><pandas>;Using cumsum in pandas on group();9433.0
3141;3141;15772263.0;7.0;"<p>I'm new to Pandas.... I've got a bunch of polling data; I want to compute a rolling mean to get an estimate for each day based on a three-day window. As I understand from <a href=""https://stackoverflow.com/questions/9762193/pandas-rolling-median-for-duplicate-time-series-data"">this question</a>, the rolling_* functions compute the window based on a specified number of values, and not a specific datetime range. </p>

<p>Is there a different function that implements this functionality? Or am I stuck writing my own?</p>

<p>EDIT: </p>

<p>Sample input data: </p>

<pre><code>polls_subset.tail(20)
Out[185]: 
            favorable  unfavorable  other

enddate                                  
2012-10-25       0.48         0.49   0.03
2012-10-25       0.51         0.48   0.02
2012-10-27       0.51         0.47   0.02
2012-10-26       0.56         0.40   0.04
2012-10-28       0.48         0.49   0.04
2012-10-28       0.46         0.46   0.09
2012-10-28       0.48         0.49   0.03
2012-10-28       0.49         0.48   0.03
2012-10-30       0.53         0.45   0.02
2012-11-01       0.49         0.49   0.03
2012-11-01       0.47         0.47   0.05
2012-11-01       0.51         0.45   0.04
2012-11-03       0.49         0.45   0.06
2012-11-04       0.53         0.39   0.00
2012-11-04       0.47         0.44   0.08
2012-11-04       0.49         0.48   0.03
2012-11-04       0.52         0.46   0.01
2012-11-04       0.50         0.47   0.03
2012-11-05       0.51         0.46   0.02
2012-11-07       0.51         0.41   0.00
</code></pre>

<p>Output would have only one row for each date. </p>

<p>EDIT x2: fixed typo </p>
";;3;;2013-04-02T18:22:45.433;21.0;15771472;2017-03-08T21:14:29.967;2017-05-23T12:34:22.307;;-1.0;;1451298.0;;1;41;<python><pandas><time-series>;Pandas: rolling mean by time interval;47293.0
3142;3142;15772356.0;9.0;"<p>What's a simple and efficient way to shuffle a dataframe in pandas, by rows or by columns? I.e. how to write a function <code>shuffle(df, n, axis=0)</code> that takes a dataframe, a number of shuffles <code>n</code>, and an axis (<code>axis=0</code> is rows, <code>axis=1</code> is columns) and returns a copy of the dataframe that has been shuffled <code>n</code> times. </p>

<p><strong>Edit</strong>: key is to do this without destroying the row/column labels of the dataframe. If you just shuffle <code>df.index</code> that loses all that information. I want the resulting <code>df</code> to be the same as the original except with the order of rows or order of columns different.</p>

<p><strong>Edit2</strong>: My question was unclear. When I say shuffle the rows, I mean shuffle each row independently. So if you have two columns <code>a</code> and <code>b</code>, I want each row shuffled on its own, so that you don't have the same associations between <code>a</code> and <code>b</code> as you do if you just re-order each row as a whole. Something like: </p>

<pre><code>for 1...n:
  for each col in df: shuffle column
return new_df
</code></pre>

<p>But hopefully more efficient than naive looping. This does not work for me:</p>

<pre><code>def shuffle(df, n, axis=0):
        shuffled_df = df.copy()
        for k in range(n):
            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)
        return shuffled_df

df = pandas.DataFrame({'A':range(10), 'B':range(10)})
shuffle(df, 5)
</code></pre>
";;0;;2013-04-02T18:50:12.687;21.0;15772009;2017-06-21T21:18:47.780;2015-02-10T16:30:50.627;;3923281.0;;248237.0;;1;45;<python><numpy><pandas>;shuffling/permutating a DataFrame in pandas;40329.0
3149;3149;15778297.0;1.0;"<p>When I run the program, Pandas gives 'Future warning' like below every time.</p>

<pre><code>D:\Python\lib\site-packages\pandas\core\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward
  "" from pandas 0.11 onward"", FutureWarning) 
</code></pre>

<p>I got the msg, but I just want to stop Pandas showing such msg again and again, is there any buildin parameter that I can set to let Pandas not pop up the 'Future warning' ?</p>
";;0;;2013-04-03T02:37:48.730;6.0;15777951;2017-04-26T03:59:49.850;;;;;1072888.0;;1;25;<pandas><suppress-warnings>;How to suppress Pandas Future warning ?;7622.0
3175;3175;15800314.0;1.0;"<p>I have about 7 million rows in an <code>HDFStore</code> with more than 60 columns. The data is more than I can fit into memory. I'm looking to aggregate the data into groups based on the value of a column ""A"". The documentation for pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"">splitting/aggregating/combining</a> assumes that I have all my data in a <code>DataFrame</code> already, however I can't read the entire store into an in-memory <code>DataFrame</code>. What is the correct approach for grouping data in an <code>HDFStore</code>?</p>
";;6;;2013-04-03T21:11:03.723;10.0;15798209;2014-07-03T00:46:46.740;;;;;6173.0;;1;17;<python><pandas><pytables>;"Pandas ""Group By"" Query on Large Data in HDFStore?";5672.0
3176;3176;15799355.0;6.0;"<p>I have some hierarchical data which bottoms out into time series data which looks something like this:</p>

<pre><code>df = pandas.DataFrame(
    {'value_a': values_a, 'value_b': values_b},
    index=[states, cities, dates])
df.index.names = ['State', 'City', 'Date']
df

                               value_a  value_b
State   City       Date                        
Georgia Atlanta    2012-01-01        0       10
                   2012-01-02        1       11
                   2012-01-03        2       12
                   2012-01-04        3       13
        Savanna    2012-01-01        4       14
                   2012-01-02        5       15
                   2012-01-03        6       16
                   2012-01-04        7       17
Alabama Mobile     2012-01-01        8       18
                   2012-01-02        9       19
                   2012-01-03       10       20
                   2012-01-04       11       21
        Montgomery 2012-01-01       12       22
                   2012-01-02       13       23
                   2012-01-03       14       24
                   2012-01-04       15       25
</code></pre>

<p>I'd like to perform time resampling per city, so something like</p>

<pre><code>df.resample(""2D"", how=""sum"")
</code></pre>

<p>would output</p>

<pre><code>                             value_a  value_b
State   City       Date                        
Georgia Atlanta    2012-01-01        1       21
                   2012-01-03        5       25
        Savanna    2012-01-01        9       29
                   2012-01-03       13       33
Alabama Mobile     2012-01-01       17       37
                   2012-01-03       21       41
        Montgomery 2012-01-01       25       45
                   2012-01-03       29       49
</code></pre>

<p>as is, <code>df.resample('2D', how='sum')</code> gets me</p>

<pre><code>TypeError: Only valid with DatetimeIndex or PeriodIndex
</code></pre>

<p>Fair enough, but I'd sort of expect this to work:</p>

<pre><code>&gt;&gt;&gt; df.swaplevel('Date', 'State').resample('2D', how='sum')
TypeError: Only valid with DatetimeIndex or PeriodIndex
</code></pre>

<p>at which point I'm really running out of ideas... is there some way stack and unstack might be able to help me?</p>
";;0;;2013-04-03T22:12:51.567;9.0;15799162;2017-07-14T14:32:11.583;2014-06-07T14:39:36.403;;690430.0;;2242374.0;;1;15;<python><pandas><time-series><hierarchical-data>;Resampling Within a Pandas MultiIndex;8485.0
3192;3192;15822811.0;1.0;"<p>I have a list of 4 pandas dataframes containing a day of tick data that I want to merge into a single data frame. I cannot understand the behavior of concat on my timestamps. See details below:</p>

<pre><code>data

[&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00
Data columns:
Price       4040  non-null values
Volume      4040  non-null values
BidQty      35228  non-null values
BidPrice    35228  non-null values
AskPrice    35228  non-null values
AskQty      35228  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00
Data columns:
Price       3969  non-null values
Volume      3969  non-null values
BidQty      33088  non-null values
BidPrice    33088  non-null values
AskPrice    33088  non-null values
AskQty      33088  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00
Data columns:
Price       7326  non-null values
Volume      7326  non-null values
BidQty      50740  non-null values
BidPrice    50740  non-null values
AskPrice    50740  non-null values
AskQty      50740  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00
Data columns:
Price       8258  non-null values
Volume      8258  non-null values
BidQty      60799  non-null values
BidPrice    60799  non-null values
AskPrice    60799  non-null values
AskQty      60799  non-null values
dtypes: float64(6)]
</code></pre>

<p>Using <code>append</code> I get:</p>

<pre><code>pd.DataFrame().append(data)

&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00
Data columns:
AskPrice    179855  non-null values
AskQty      179855  non-null values
BidPrice    179855  non-null values
BidQty      179855  non-null values
Price       23593  non-null values
Volume      23593  non-null values
dtypes: float64(6)
</code></pre>

<p>Using <code>concat</code> I get:</p>

<pre><code>pd.concat(data)

&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00
Data columns:
Price       23593  non-null values
Volume      23593  non-null values
BidQty      179855  non-null values
BidPrice    179855  non-null values
AskPrice    179855  non-null values
AskQty      179855  non-null values
dtypes: float64(6)
</code></pre>

<p>Notice how the index changes when using <code>concat</code>. Why is that happening and how would I go about using <code>concat</code> to reproduce the results obtained using <code>append</code>? (Since <code>concat</code> seems so much faster; 24.6 ms per loop vs 3.02 s per loop)</p>
";;0;;2013-04-04T18:37:25.263;6.0;15819050;2017-01-20T15:46:32.183;2017-01-20T15:46:32.183;;1007939.0;;2246074.0;;1;22;<python><pandas>;Pandas DataFrame concat vs append;33712.0
3220;3220;15855998.0;1.0;"<p>I'm pretty new to pandas, so I guess I'm doing something wrong - </p>

<p>I have a DataFrame:</p>

<pre><code>     a     b
0  0.5  0.75
1  0.5  0.75
2  0.5  0.75
3  0.5  0.75
4  0.5  0.75
</code></pre>

<p><code>df.corr()</code> gives me: </p>

<pre><code>    a   b
a NaN NaN
b NaN NaN
</code></pre>

<p>but <code>np.correlate(df[""a""], df[""b""])</code> gives: <code>1.875</code></p>

<p>Why is that? 
I want to have the correlation matrix for my DataFrame and thought that <code>corr()</code> does that (at least according to the documentation). Why does it return <code>NaN</code>?</p>

<p>What's the correct way to calculate?</p>

<p>Many thanks!</p>
";;0;;2013-04-06T19:05:50.520;17.0;15854878;2014-11-07T22:06:28.837;;;;;1643257.0;;1;28;<python><pandas>;Correlation between columns in DataFrame;45152.0
3227;3227;15863028.0;2.0;"<p>I've looking around for this but I can't seem to find it (though it must be extremely trivial).</p>

<p>The problem that I have is that I would like to retrieve the value of a column for the first and last entries of a data frame. But if I do:</p>

<pre><code>df.ix[0]['date']
</code></pre>

<p>I get:</p>

<pre><code>datetime.datetime(2011, 1, 10, 16, 0)
</code></pre>

<p>but if I do:</p>

<pre><code>df[-1:]['date']
</code></pre>

<p>I get:</p>

<pre><code>myIndex
13         2011-12-20 16:00:00
Name: mydate
</code></pre>

<p>with a different format. Ideally, I would like to be able to access the value of the last index of the data frame, but I can't find how.</p>

<p>I even tried to create a column (IndexCopy) with the values of the index and try:</p>

<pre><code>df.ix[df.tail(1)['IndexCopy']]['mydate']
</code></pre>

<p>but this also yields a different format (since df.tail(1)['IndexCopy'] does not output a simple integer). </p>

<p>Any ideas? </p>
";;0;;2013-04-07T11:51:50.360;5.0;15862034;2016-05-12T08:39:42.983;2013-04-07T14:34:53.017;;487339.0;;1225744.0;;1;21;<python><pandas>;Access index of last element in data frame;31919.0
3252;3252;15889056.0;1.0;"<p>I have a DataFrame object similar to this one:</p>

<pre><code>       onset    length
1      2.215    1.3
2     23.107    1.3
3     41.815    1.3
4     61.606    1.3
...
</code></pre>

<p>What I would like to do is insert a row at a position specified by some index value and update the following indices accordingly. E.g.:</p>

<pre><code>       onset    length
1      2.215    1.3
2     23.107    1.3
3     30.000    1.3  # new row
4     41.815    1.3
5     61.606    1.3
...
</code></pre>

<p>What would be the best way to do this?</p>
";;0;;2013-04-08T20:45:01.703;1.0;15888648;2013-04-08T21:28:32.083;;;;;1304147.0;;1;16;<python><pandas>;Is it possible to insert a row at an arbitrary position in a dataframe using pandas?;6025.0
3255;3255;28648923.0;4.0;"<p>I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:</p>

<pre><code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
df = pd.DataFrame(a)
</code></pre>

<p>What is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I don't want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.</p>
";;5;;2013-04-08T23:53:30.657;93.0;15891038;2017-07-29T10:40:53.817;2017-01-08T11:56:21.240;;3923281.0;;1642513.0;;1;257;<python><pandas><dataframe><types><casting>;Pandas: change data type of columns;406291.0
3281;3281;15911372.0;4.0;"<p>I would like to annotate the data points with their values next to the points on the plot. The examples I found only deal with x and y as vectors. However, I would like to do this for a pandas DataFrame that contains multiple columns. </p>

<pre><code>ax = plt.figure().add_subplot(1, 1, 1)
df.plot(ax = ax)
plt.show()
</code></pre>

<p>What is the best way to annotate all the points for a multi-column DataFrame?</p>
";;0;;2013-04-09T18:43:16.617;9.0;15910019;2016-10-07T16:57:11.007;;;;;1642513.0;;1;13;<matplotlib><pandas>;Annotate data points while plotting from Pandas DataFrame;8786.0
3289;3289;15916686.0;1.0;"<p>I have a dataframe:</p>

<pre><code>&gt;&gt;&gt; dt
                   COL000   COL001   QT
STK_ID  RPT_Date                       
STK000  20120331   2.6151   2.1467    1
        20120630   4.0589   2.3442    2
        20120930   4.4547   3.9204    3
        20121231   4.1360   3.8559    4
STK001  20120331  -0.2178   0.9184    1
        20120630  -1.9639   0.7900    2
        20120930  -2.9147   1.0189    3
        20121231  -2.5648   2.3743    4
STK002  20120331  -0.6426   0.9543    1
        20120630  -0.3575   1.6085    2
        20120930  -2.3549   0.7174    3
        20121231  -3.4860   1.6324    4
</code></pre>

<p>And I want the columns values divided by 'QT' column, somewhat like this:</p>

<pre><code>dt =  dt/dt.QT     # pandas does not accept this syntax
</code></pre>

<p>The desired output is:</p>

<pre><code>STK_ID  RPT_Date        COL000       COL001  QT
STK000  20120331   2.615110188  2.146655745   1
        20120630   2.029447265  1.172093561   1
        20120930   1.484909881  1.306795608   1
        20121231   1.034008443  0.963970609   1
STK001  20120331  -0.217808111  0.918355842   1
        20120630  -0.981974837  0.394977675   1
        20120930  -0.97157148   0.339633733   1
        20121231  -0.641203355  0.593569537   1
STK002  20120331  -0.642567516  0.954323016   1
        20120630  -0.178759288  0.804230898   1
        20120930  -0.784982521  0.239117442   1
        20121231  -0.871501505  0.408094317   1
</code></pre>

<p>How to do that?</p>
";;0;;2013-04-10T03:52:09.493;9.0;15916612;2017-04-01T13:06:00.987;2017-04-01T13:06:00.987;;5108509.0;;1072888.0;;1;20;<pandas><division>;How to divide the value of pandas columns by the other column;25171.0
3293;3293;15923878.0;5.0;"<p>Is there a way to select random rows from a DataFrame in Pandas.</p>

<p>In R, using the car package, there is a useful function <code>some(x, n)</code> which is similar to head but selects, in this example, 10 rows at random from x.</p>

<p>I have also looked at the slicing documentation and there seems to be nothing equivalent.</p>

<h1>Update</h1>

<p>Now using version 20. There is a sample method.</p>

<p><code>df.sample(n)</code></p>
";;0;;2013-04-10T10:52:47.243;6.0;15923826;2017-06-29T18:08:39.910;2017-06-29T18:08:39.910;;390388.0;;390388.0;;1;33;<python><pandas>;Random row selection in Pandas dataframe;26646.0
3300;3300;15939457.0;3.0;"<p>I have series of measurements which are time stamped and irregularly spaced. Values in these series always represent changes of the measurement -- i.e. without a change no new value. A simple example of such a series would be:</p>

<pre><code>23:00:00.100     10
23:00:01.200      8
23:00:01.600      0
23:00:06.300      4
</code></pre>

<p>What I want to reach is an equally spaced series of time-weighted averages. For the given example I might aim at a frequency based on seconds and hence a result like the following:</p>

<pre><code>23:00:01     NaN ( the first 100ms are missing )
23:00:02     5.2 ( 10*0.2 + 8*0.4 + 0*0.4 )
23:00:03       0
23:00:04       0
23:00:05       0
23:00:06     2.8 ( 0*0.3 + 4*0.7 )
</code></pre>

<p>I am searching for a Python library solving that problem. For me, this seems to be a standard problem, but I couldn't find such a functionality so far in standard libraries like pandas.</p>

<p>The algorithm needs to take two things into account:</p>

<ul>
<li>time-weighted averaging</li>
<li>considering values ahead of the current interval ( and possibly even ahead of the lead ) when forming the average</li>
</ul>

<h2>Using pandas</h2>

<pre><code>data.resample('S', fill_method='pad')          # forming a series of seconds
</code></pre>

<p>does parts of the work. Providing a user-defined function for aggregation will allow <a href=""https://stackoverflow.com/questions/10839701/time-weighted-average-with-pandas/10856106#10856106"">to form time-weighted averages</a>, but because the beginning of the interval is ignored, this average will be incorrect too. Even worse: the holes in the series are filled with the average values, leading in the example from above to the values of seconds 3, 4 and 5 to be non zero.</p>

<pre><code>data = data.resample('L', fill_method='pad')   # forming a series of milliseconds
data.resample('S')
</code></pre>

<p>does the trick with a certain accurateness, but is -- depending on the accurateness -- very expensive. In my case, too expensive.</p>

<h2>Edit: Solution</h2>

<pre><code>import pandas as pa
import numpy as np
from datetime import datetime
from datetime import timedelta

time_stamps=[datetime(2013,04,11,23,00,00,100000), 
             datetime(2013,04,11,23,00,1,200000),
             datetime(2013,04,11,23,00,1,600000),
             datetime(2013,04,11,23,00,6,300000)]
values = [10, 8, 0, 4]
raw = pa.TimeSeries(index=time_stamps, data=values)

def round_down_to_second(dt):
    return datetime(year=dt.year, month=dt.month, day=dt.day, 
                    hour=dt.hour, minute=dt.minute, second=dt.second)

def round_up_to_second(dt):
    return round_down_to_second(dt) + timedelta(seconds=1)

def time_weighted_average(data):
    end = pa.DatetimeIndex([round_up_to_second(data.index[-1])])
    return np.average(data, weights=np.diff(data.index.append(end).asi8))

start = round_down_to_second(time_stamps[0])
end = round_down_to_second(time_stamps[-1])
range = pa.date_range(start, end, freq='S')
data = raw.reindex(raw.index + range)
data = data.ffill()

data = data.resample('S', how=time_weighted_average)
</code></pre>
";;0;;2013-04-10T16:03:08.267;4.0;15930885;2016-09-27T17:14:41.630;2017-05-23T12:06:52.560;;-1.0;;1856079.0;;1;12;<python><pandas><time-series>;Converting irregularly time stamped measurements into equally spaced, time-weighted averages;1459.0
3311;3311;15943975.0;7.0;"<p>I'm trying to get the number of rows of dataframe df with Pandas, and here is my code.</p>

<h3>Method 1:</h3>

<pre><code>total_rows = df.count
print total_rows +1
</code></pre>

<h3>Method 2:</h3>

<pre><code>total_rows = df['First_columnn_label'].count
print total_rows +1
</code></pre>

<p>Both the code snippets give me this error:</p>

<blockquote>
  <p>TypeError: unsupported operand type(s) for +: 'instancemethod' and 'int'</p>
</blockquote>

<p>What am I doing wrong?</p>

<p>According to <a href=""https://stackoverflow.com/a/15943975/4230591"">the answer</a> given by <a href=""https://stackoverflow.com/users/1199589/root"">@root</a> the best (the fastest) way to check df length is to call:</p>

<pre><code>len(df.index)
</code></pre>
";;3;;2013-04-11T08:14:08.400;51.0;15943769;2017-05-07T16:03:10.837;2017-05-23T11:47:31.583;;-1.0;;2006977.0;;1;260;<python><pandas><dataframe>;How do I get the row count of a Pandas dataframe?;373069.0
3341;3341;15990537.0;1.0;"<p>I have two dataframes which look like this:</p>

<pre><code>&gt;&gt;&gt; df1
              A    B
2000-01-01  1.4  1.4
2000-01-02  1.7 -1.9
2000-01-03 -0.2 -0.8

&gt;&gt;&gt; df2
              A    B
2000-01-01  0.6 -0.3
2000-01-02 -0.4  0.6
2000-01-03  1.1 -1.0
</code></pre>

<p>How can I make one dataframe out of this two with hierarchical column index like below?</p>

<pre><code>            df1       df2
              A    B    A    B
2000-01-01  1.4  1.4  0.6 -0.3
2000-01-02  1.7 -1.9 -0.4  0.6
2000-01-03 -0.2 -0.8  1.1 -1.0
</code></pre>
";;1;;2013-04-13T15:13:34.390;5.0;15989281;2016-06-07T08:10:34.717;2013-04-13T17:21:55.830;;625914.0;;625914.0;;1;15;<python><pandas>;python/pandas: how to combine two dataframes into one with hierarchical column index?;3076.0
3349;3349;15998993.0;4.0;"<p>I have a relatively straightforward question, today.  I have a pandas <code>Series</code> object containing boolean values. How can I get a series containing the logical <code>NOT</code> of each value?</p>

<p>For example, consider a series containing:</p>

<pre><code>True
True
True
False
</code></pre>

<p>The series I'd like to get would contain:</p>

<pre><code>False
False
False
True
</code></pre>

<p>This seems like it should be reasonably simple, but apparently I've misplaced my mojo today =(</p>

<p>Thanks!</p>
";;0;;2013-04-14T10:44:28.963;13.0;15998188;2017-03-10T15:10:11.237;;;;;1156707.0;;1;80;<python><pandas><boolean-logic>;How can I obtain the element-wise logical NOT of a pandas Series?;30530.0
3385;3385;16068497.0;2.0;"<p>I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 205482 entries, 0 to 209018
Data columns:
Month           205482  non-null values
Reported by     205482  non-null values
Falls within    205482  non-null values
Easting         205482  non-null values
Northing        205482  non-null values
Location        205482  non-null values
Crime type      205482  non-null values
long            205482  non-null values
lat             205482  non-null values
dtypes: float64(4), object(5)
</code></pre>

<p>The code I tried to use was:</p>

<pre><code>def merge_two_cols(series): 
    return (series['lat'], series['long'])

sample['lat_long'] = sample.apply(merge_two_cols, axis=1)
</code></pre>

<p>However, this returned the following error:</p>

<pre><code>---------------------------------------------------------------------------
 AssertionError                            Traceback (most recent call last)
&lt;ipython-input-261-e752e52a96e6&gt; in &lt;module&gt;()
      2     return (series['lat'], series['long'])
      3 
----&gt; 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)
      5
</code></pre>

<p>...</p>

<pre><code>AssertionError: Block shape incompatible with manager 
</code></pre>

<p>How can I solve this problem?</p>
";;0;;2013-04-16T07:21:52.430;9.0;16031056;2016-11-07T16:58:03.140;;;;;386861.0;;1;38;<python><dataframe><pandas><tuples>;How to form tuple column from two columns in Pandas;23199.0
3409;3409;16074407.0;3.0;"<p>I want to get both horizontal and vertical grid lines on my plot but only the horizontal grid lines are appearing by default. I am using a <code>pandas.DataFrame</code> from an sql query in python to generate a line plot with dates on the x-axis. I'm not sure why they do not appear on the dates and I have tried to search for an answer to this but couldn't find one.</p>

<p>All I have used to plot the graph is the simple code below. </p>

<pre><code>data.plot()
grid('on')
</code></pre>

<p>data is the DataFrame which contains the dates and the data from the sql query. </p>

<p>I have also tried adding the code below but I still get the same output with no vertical grid lines.</p>

<pre><code>ax = plt.axes()        
ax.yaxis.grid() # horizontal lines
ax.xaxis.grid() # vertical lines
</code></pre>

<p>Any suggestions?</p>

<p><img src=""https://i.stack.imgur.com/nsdzO.png"" alt=""enter image description here""></p>
";;1;;2013-04-18T04:17:10.157;4.0;16074392;2016-11-16T05:18:08.047;2015-04-20T17:47:51.360;;1510289.0;;1819479.0;;1;38;<python><matplotlib><pandas>;Getting vertical gridlines to appear in line plot in matplotlib;50935.0
3416;3416;16089219.0;2.0;"<p>I would like to add a column to the second level of a multiindex column dataframe.</p>

<pre><code>In [151]: df
Out[151]: 
first        bar                 baz           
second       one       two       one       two 
A       0.487880 -0.487661 -1.030176  0.100813 
B       0.267913  1.918923  0.132791  0.178503
C       1.550526 -0.312235 -1.177689 -0.081596 
</code></pre>

<p>The usual trick of direct assignment does not work:</p>

<pre><code>In [152]: df['bar']['three'] = [0, 1, 2]

In [153]: df
Out[153]: 
first        bar                 baz           
second       one       two       one       two 
A       0.487880 -0.487661 -1.030176  0.100813
B       0.267913  1.918923  0.132791  0.178503
C       1.550526 -0.312235 -1.177689 -0.081596
</code></pre>

<p>How can I add the third row to under ""bar""?</p>
";;0;;2013-04-18T16:46:56.630;6.0;16088741;2017-07-19T11:45:55.550;;;;;1642513.0;;1;22;<pandas><multi-index>;Pandas: add a column to a multiindex column dataframe;10377.0
3420;3420;16104482.0;5.0;"<p>I am curious as to why <code>df[2]</code> is not supported, while <code>df.ix[2]</code> and <code>df[2:3]</code> both work. </p>

<pre><code>In [26]: df.ix[2]
Out[26]: 
A    1.027680
B    1.514210
C   -1.466963
D   -0.162339
Name: 2000-01-03 00:00:00

In [27]: df[2:3]
Out[27]: 
                  A        B         C         D
2000-01-03  1.02768  1.51421 -1.466963 -0.162339
</code></pre>

<p>I would expect <code>df[2]</code> to work the same way as <code>df[2:3]</code> to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?</p>
";;1;;2013-04-19T03:14:00.170;36.0;16096627;2017-05-02T12:18:33.137;;;;;1642513.0;;1;166;<pandas>;Pandas select row of data frame by integer index;227892.0
3423;3423;16104567.0;5.0;"<p>I have a dataframe in pandas called 'munged_data' with two columns 'entry_date' and 'dob' which i have converted to Timestamps using pd.to_timestamp.I am trying to figure out how to calculate ages of people based on the time difference between 'entry_date' and 'dob' and to do this i need to get the difference in days between the two columns ( so that i can then do somehting like round(days/365.25). I do not seem to be able to find a way to do this using a vectorized operation. When I do munged_data.entry_date-munged_data.dob i get the following : </p>

<pre><code>internal_quote_id
2                    15685977 days, 23:54:30.457856
3                    11651985 days, 23:49:15.359744
4                     9491988 days, 23:39:55.621376
7                     11907004 days, 0:10:30.196224
9                    15282164 days, 23:30:30.196224
15                  15282227 days, 23:50:40.261632  
</code></pre>

<p>However i do not seem to be able to extract the days as an integer so that i can continue with my calculation. 
Any help appreciated.</p>
";;0;;2013-04-19T11:09:34.510;3.0;16103238;2016-02-15T18:54:13.853;;;;;1167915.0;;1;11;<python><numpy><timestamp><pandas>;Pandas Timedelta in Days;26655.0
3474;3474;16168245.0;1.0;"<pre><code>In [37]: df = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])

In [38]: df2 = pd.concat([df, df])

In [39]: df2.reset_index()
Out[39]: 
   index  0  1  2  3
0      0  1  2  3  4
1      1  2  3  4  5
2      2  3  4  5  6
3      0  1  2  3  4
4      1  2  3  4  5
5      2  3  4  5  6
</code></pre>

<p>My problem is that how can I <code>reset_index</code> without adding a new column <code>index</code>?</p>
";;0;;2013-04-23T11:10:32.200;4.0;16167829;2015-09-07T22:27:47.277;;;;;1426056.0;;1;17;<python><pandas>;In pandas, how can I reset index without adding a new column?;20242.0
3481;3481;16176457.0;3.0;"<p>I am able to read and slice pandas dataframe using python datetime objects, however I am forced to use only <em>existing dates</em> in index.  For example, this works:</p>

<pre><code>&gt;&gt;&gt; data
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00
Data columns:
Adj Close    252  non-null values
dtypes: float64(1)

&gt;&gt;&gt; st = datetime.datetime(2010, 12, 31, 0, 0)
&gt;&gt;&gt; en = datetime.datetime(2010, 12, 28, 0, 0)

&gt;&gt;&gt; data[st:en]
            Adj Close
Date                 
2010-12-31     593.97
2010-12-30     598.86
2010-12-29     601.00
2010-12-28     598.92
</code></pre>

<p>However if I use a start or end date that is not present in the DF, I get python KeyError.</p>

<p>My Question : How do I query the dataframe object for a date range; even when the start and end dates are not present in the DataFrame.  Does pandas allow for range based slicing?</p>

<p>I am using pandas version 0.10.1</p>
";;0;;2013-04-23T17:48:05.480;19.0;16175874;2017-05-02T02:12:40.827;2014-06-23T09:38:35.780;;454466.0;;454466.0;;1;29;<python><dataframe><pandas>;python pandas dataframe slicing by date conditions;43773.0
3483;3483;;4.0;"<p>I use <code>pandas.to_datetime</code> to parse the dates in my data. Pandas by default represents the dates with <code>datetime64[ns]</code> even though the dates are all daily only.
I wonder whether there is an elegant/clever way to convert the dates to <code>datetime.date</code> or <code>datetime64[D]</code> so that, when I write the data to CSV, the dates are not appended with <code>00:00:00</code>. I know I can convert the type manually element-by-element:</p>

<pre><code>[dt.to_datetime().date() for dt in df.dates]
</code></pre>

<p>But this is really slow since I have many rows and it sort of defeats the purpose of using <code>pandas.to_datetime</code>. Is there a way to convert the <code>dtype</code> of the entire column at once? Or alternatively, does <code>pandas.to_datetime</code> support a precision specification so that I can get rid of the time part while working with daily data?</p>
";;6;;2013-04-23T18:50:36.500;9.0;16176996;2017-07-04T10:27:46.457;2017-05-19T13:47:11.380;;5276797.0;;1642513.0;;1;33;<python><datetime><pandas>;Keep only date part when using pandas.to_datetime;31231.0
3494;3494;16202796.0;2.0;"<p>I'm working through the ""Python For Data Analysis"" and I don't understand a particular functionality. Adding two pandas series objects will automatically align the indexed data but if one object does not contain that index it is returned as NaN. For example from book:</p>

<pre><code>    a = Series([35000,71000,16000,5000],index=['Ohio','Texas','Oregon','Utah'])
    b = Series([NaN,71000,16000,35000],index=['California', 'Texas', 'Oregon', 'Ohio'])
</code></pre>

<p>Result:</p>

<pre><code>    In [63]: a
    Out[63]: Ohio          35000
             Texas         71000
             Oregon        16000
             Utah           5000
    In [64]: b
    Out[64]: California      NaN
             Texas         71000
             Oregon        16000
             Ohio          35000
</code></pre>

<p>When I add them together I get this...</p>

<pre><code>    In [65]: a+b
    Out[65]: California       NaN
             Ohio           70000
             Oregon         32000
             Texas         142000
             Utah             NaN
</code></pre>

<p>So why is the Utah value NaN and not 500? It seems that 500+NaN=500. What gives? I'm missing something, please explain.</p>

<p><b>Update:</b></p>

<pre><code>    In [92]: # fill NaN with zero
             b = b.fillna(0)
             b
    Out[92]: California        0
             Texas         71000
             Oregon        16000
             Ohio          35000

    In [93]: a
    Out[93]: Ohio      35000
             Texas     71000
             Oregon    16000
             Utah       5000

    In [94]: # a is still good
             a+b
    Out[94]: California       NaN
             Ohio           70000
             Oregon         32000
             Texas         142000 
             Utah             NaN
</code></pre>

<p><b>Update:</b>
Thanks for the solution!</p>

<pre><code>In [95]: a.add(b, fill_value=0)
Out[95]: California         0
         Ohio           70000
         Oregon         32000
         Texas         142000
         Utah            5000
</code></pre>
";;1;;2013-04-24T21:41:04.443;2.0;16202711;2013-04-24T22:22:58.963;2013-04-24T22:22:58.963;;1827947.0;;1827947.0;;1;14;<python><pandas>;Adding two pandas.series objects;6014.0
3505;3505;16245109.0;5.0;"<p>How to do this in pandas:</p>

<p>I have a function <code>extract_text_features</code> on a single text column, returning multiple output columns. Specifically, the function returns 6 values.</p>

<p>The function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned <code>df.ix[: ,10:16] = df.textcol.map(extract_text_features)</code></p>

<p>So I think I need to drop back to iterating with <code>df.iterrows()</code>, as per <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">this</a>?</p>

<p>UPDATE: 
Iterating with <code>df.iterrows()</code> is at least 20x slower, so I surrendered and split out the function into six distinct <code>.map(lambda ...)</code> calls.</p>
";;4;;2013-04-26T12:38:33.757;38.0;16236684;2017-07-26T14:28:24.953;2017-05-23T11:47:30.917;;-1.0;;202229.0;;1;75;<merge><pandas><multiple-columns><return-type>;Apply pandas function to column to create multiple new columns?;33681.0
3518;3518;16255680.0;8.0;"<p>I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?</p>

<p>I am new to pandas and numpy.</p>

<p>EDIT:
The mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. </p>

<p>Sample Data:</p>

<pre><code>{
""_cls"" : ""SensorReport"",
""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),
""_types"" : [
    ""SensorReport""
],
""Readings"" : [
    {
        ""a"" : 0.958069536790466,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),
        ""b"" : 6.296118156595,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.95574014778624,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),
        ""b"" : 6.29651468650064,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.953648289182713,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),
        ""b"" : 7.29679823731148,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.955931884300997,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),
        ""b"" : 6.29642922525632,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.95821381,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),
        ""b"" : 7.28956613,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 4.95821335,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),
        ""b"" : 6.28956574,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 9.95821341,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),
        ""b"" : 0.28956488,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 1.95667927,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),
        ""b"" : 0.29115237,
        ""_cls"" : ""Reading""
    }
],
""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),
""sensorName"" : ""56847890-0"",
""reportCount"" : 8
}
</code></pre>
";;1;;2013-04-27T07:59:07.333;23.0;16249736;2017-01-20T06:32:38.327;2017-01-20T06:32:38.327;;2096752.0;;1351876.0;;1;38;<python><mongodb><pandas><pymongo>;How to import data from mongodb to pandas?;24496.0
3524;3524;;3.0;"<p>This seems like it would be fairly straight forward but after nearly an entire day I have not found the solution. I've loaded my dataframe with read_csv and easily parsed, combined and indexed a date and a time column into one column but now I want to be able to just reshape and perform calculations based on hour and minute groupings similar to what you can do in excel pivot. </p>

<p>I know how to resample to hour or minute but it maintains the date portion associated with each hour/minute whereas I want to aggregate the data set ONLY to hour and minute similar to grouping in excel pivots and selecting ""hour"" and ""minute"" but not selecting anything else. </p>

<p>Any help would be greatly appreciated. </p>
";;2;;2013-04-28T18:07:33.097;11.0;16266019;2017-08-23T18:48:23.643;;;;;2329714.0;;1;24;<python><date><pandas>;Python Pandas: Group datetime column into hour and minute aggregations;20761.0
3566;3566;16314480.0;1.0;"<p>I want to use groupby().transform() to do a custom (cumulative) transform of each block of records in a (sorted) dataset.  Unless I ensure I have a unique key, it doesn't work.  Why?</p>

<p>Here's a toy example:</p>

<pre><code>df = pd.DataFrame([[1,1],
                  [1,2],
                  [2,3],
                  [3,4],
                  [3,5]], 
                  columns='a b'.split())
df['partials'] = df.groupby('a')['b'].transform(np.cumsum)
df
</code></pre>

<p>gives the expected:</p>

<pre><code>     a   b   partials
0    1   1   1
1    1   2   3
2    2   3   3
3    3   4   4
4    3   5   9
</code></pre>

<p>but if 'a' is a key, it all goes wrong:</p>

<pre><code>df = df.set_index('a')
df['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)
df

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;ipython-input-146-d0c35a4ba053&gt; in &lt;module&gt;()
      3 
      4 df = df.set_index('a')
----&gt; 5 df.groupby(level=0)['b'].transform(np.cumsum)

/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)
   1542             res = wrapper(group)
   1543             # result[group.index] = res
-&gt; 1544             indexer = self.obj.index.get_indexer(group.index)
   1545             np.put(result, indexer, res)
   1546 

/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit)
    847 
    848         if not self.is_unique:
--&gt; 849             raise Exception('Reindexing only valid with uniquely valued Index '
    850                             'objects')
    851 

Exception: Reindexing only valid with uniquely valued Index objects
</code></pre>

<p>Same error if you select column 'b' before grouping, ie.</p>

<pre><code>df['b'].groupby(level=0).transform(np.cumsum)
</code></pre>

<p>but you can make it work if you transform the entire dataframe, like:</p>

<pre><code>df.groupby(level=0).transform(np.cumsum)
</code></pre>

<p>or even a one-column dataframe (rather than series):</p>

<pre><code>df.groupby(level=0)[['b']].transform(np.cumsum)
</code></pre>

<p>I feel like there's some still some deep part of <a href=""http://wesmckinney.com/blog/?p=125"" rel=""noreferrer"">GroupBy-fu</a> that I'm missing.  Can someone set me straight?</p>
";;1;;2013-05-01T02:14:38.120;3.0;16311793;2015-02-20T16:22:55.107;2013-05-01T10:20:20.087;;2336373.0;;2336373.0;;1;11;<python><pandas>;Why does pandas groupby().transform() require a unique index?;10302.0
3579;3579;16327135.0;5.0;"<p>What's the easiest way to add an empty column to a pandas <code>DataFrame</code> object?  The best I've stumbled upon is something like</p>

<pre><code>df['foo'] = df.apply(lambda _: '', axis=1)
</code></pre>

<p>Is there a less perverse method?</p>
";;1;;2013-05-01T21:46:47.383;18.0;16327055;2017-08-25T04:39:35.623;2015-09-19T22:53:31.833;;325565.0;;559827.0;;1;73;<python><pandas>;How to add an empty column to a dataframe?;65645.0
3597;3597;16345735.0;1.0;"<p>I am dealing with pandas DataFrames like this:</p>

<pre><code>   id    x
0   1   10
1   1   20
2   2  100
3   2  200
4   1  NaN
5   2  NaN
6   1  300
7   1  NaN
</code></pre>

<p>I would like to replace each NAN 'x' with the previous non-NAN 'x' from a row with the same 'id' value:</p>

<pre><code>   id    x
0   1   10
1   1   20
2   2  100
3   2  200
4   1   20
5   2  200
6   1  300
7   1  300
</code></pre>

<p>Is there some slick way to do this without manually looping over rows?</p>
";;0;;2013-05-02T18:51:25.003;4.0;16345583;2017-07-18T20:13:55.623;;;;;1332492.0;;1;11;<python><pandas><nan><missing-data><data-cleansing>;Fill in missing pandas data with previous non-missing value, grouped by key;4190.0
3601;3601;16359854.0;3.0;"<p>I have a csv file that shows parts on order. The columns include days late, qty and commodity.</p>

<p>I need to group the data by days late and commodity with a sum of the qty. However the days late needs to be grouped into ranges.</p>

<pre><code>&gt;56
&gt;35 and &lt;= 56
&gt;14 and &lt;= 35
&gt;0 and &lt;=14
</code></pre>

<p>I was hoping I could use a dict some how. Something like this </p>

<pre><code>{'Red':'&gt;56,'Amber':'&gt;35 and &lt;= 56','Yellow':'&gt;14 and &lt;= 35','White':'&gt;0 and &lt;=14'}
</code></pre>

<p>I am looking for a result like this</p>

<pre><code>        Red  Amber  Yellow  White
STRSUB  56   60     74      40
BOTDWG  20   67     87      34
</code></pre>

<p>I am new to pandas so I don't know if this is possible at all. Could anyone provide some advice.</p>

<p>Thanks</p>
";;0;;2013-05-02T23:30:49.320;12.0;16349389;2017-07-10T10:16:11.490;2016-03-31T13:17:34.467;;190597.0;;501827.0;;1;15;<python-2.7><pandas>;Grouping data by value ranges;7593.0
3606;3606;16354730.0;2.0;"<p>I have some problems with the Pandas apply function, when using multiple columns with the following dataframe</p>

<pre><code>df = DataFrame ({'a' : np.random.randn(6),
             'b' : ['foo', 'bar'] * 3,
             'c' : np.random.randn(6)})
</code></pre>

<p>and the following function</p>

<pre><code>def my_test(a, b):
    return a % b
</code></pre>

<p>When I try to apply this function with :</p>

<pre><code>df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)
</code></pre>

<p>I get the error message:</p>

<pre><code>NameError: (""global name 'a' is not defined"", u'occurred at index 0')
</code></pre>

<p>I do not understand this message, I defined the name properly. </p>

<p>I would highly appreciate any help on this issue</p>

<p>Update</p>

<p>Thanks for your help. I made indeed some syntax mistakes with the code, the index should be put ''. However I have still the same issue using a more complex function such as:</p>

<pre><code>def my_test(a):
    cum_diff = 0
    for ix in df.index():
        cum_diff = cum_diff + (a - df['a'][ix])
    return cum_diff 
</code></pre>

<p>Thank you</p>
";;0;;2013-05-03T07:25:48.870;41.0;16353729;2014-03-08T12:00:13.610;2014-03-08T12:00:13.610;;2255305.0;;2331506.0;;1;84;<python><python-2.7><pandas><dataframe><apply>;Pandas: How to use apply function to multiple columns;136478.0
3631;3631;16377383.0;2.0;"<p>I am new to python and pandas, and have the following <code>DataFrame</code>. </p>

<p>How can I plot the <code>DataFrame</code> where each <code>ModelID</code> is a separate plot, <code>saledate</code> is the x-axis and <code>MeanToDate</code> is the y-axis?</p>

<p><strong>Attempt</strong></p>

<pre><code>data[40:76].groupby('ModelID').plot()
</code></pre>

<p><img src=""https://i.stack.imgur.com/oa7Vp.png"" alt=""enter image description here""></p>

<p><strong>DataFrame</strong></p>

<p><img src=""https://i.stack.imgur.com/2qbhA.png"" alt=""enter image description here""></p>
";;0;;2013-05-04T15:56:27.510;7.0;16376159;2017-07-25T07:52:13.690;2013-05-04T16:03:01.283;;741099.0;;741099.0;;1;12;<python><python-2.7><numpy><scipy><pandas>;Plotting a Pandas DataSeries.GroupBy;27727.0
3649;3649;16393023.0;4.0;"<p>I have started my IPython Notebook with </p>

<pre><code>ipython notebook --pylab inline
</code></pre>

<p>This is my code in one cell</p>

<pre><code>df['korisnika'].plot()
df['osiguranika'].plot()
</code></pre>

<p>This is working fine, it will draw two lines, but on the same chart.</p>

<p>I would like to draw each line on a separate chart.
And it would be great if the charts would be next to each other, not one after the other.</p>

<p>I know that I can put the second line in the next cell, and then I would get two charts. But I would like the charts close to each other, because they represent the same logical unit.</p>
";;0;;2013-05-06T06:10:36.307;10.0;16392921;2016-08-23T14:29:58.873;2013-05-11T07:57:14.340;;2022086.0;;2006674.0;;1;47;<python><pandas><ipython><ipython-notebook>;Make more than one chart in same IPython Notebook cell;38376.0
3653;3653;16398361.0;5.0;"<p>I need to delete the first three rows of a dataframe in pandas.</p>

<p>I know <code>df.ix[:-1]</code> would remove the last row, but I can't figure out how to remove first n rows.</p>
";;0;;2013-05-06T10:35:58.180;13.0;16396903;2017-08-21T16:38:49.460;2017-03-21T21:40:17.060;;2208505.0;;461436.0;;1;58;<pandas>;Delete the first three rows of a dataframe in pandas;65041.0
3668;3668;16420213.0;4.0;"<p>I have a <code>DataFrame</code> with column named <code>date</code>. How can we convert/parse the 'date' column to a <code>DateTime</code> object?</p>

<p>I loaded the date column from a Postgresql database using <code>sql.read_frame()</code>. An example of the <code>date</code> column is <code>2013-04-04</code>.</p>

<p>What I am trying to do is to select all rows in a dataframe that has their date columns within a certain period, like after <code>2013-04-01</code> and before <code>2013-04-04</code>.</p>

<p>My attempt below gives the error <code>'Series' object has no attribute 'read'</code></p>

<p><strong>Attempt</strong></p>

<pre><code>import dateutil

df['date'] = dateutil.parser.parse(df['date'])
</code></pre>

<p><strong>Error</strong></p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-636-9b19aa5f989c&gt; in &lt;module&gt;()
     15 
     16 # Parse 'Date' Column to Datetime
---&gt; 17 df['date'] = dateutil.parser.parse(df['date'])
     18 
     19 # SELECT RECENT SALES

C:\Python27\lib\site-packages\dateutil\parser.pyc in parse(timestr, parserinfo, **kwargs)
    695         return parser(parserinfo).parse(timestr, **kwargs)
    696     else:
--&gt; 697         return DEFAULTPARSER.parse(timestr, **kwargs)
    698 
    699 

C:\Python27\lib\site-packages\dateutil\parser.pyc in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)
    299             default = datetime.datetime.now().replace(hour=0, minute=0,
    300                                                       second=0, microsecond=0)
--&gt; 301         res = self._parse(timestr, **kwargs)
    302         if res is None:
    303             raise ValueError, ""unknown string format""

C:\Python27\lib\site-packages\dateutil\parser.pyc in _parse(self, timestr, dayfirst, yearfirst, fuzzy)
    347             yearfirst = info.yearfirst
    348         res = self._result()
--&gt; 349         l = _timelex.split(timestr)
    350         try:
    351 

C:\Python27\lib\site-packages\dateutil\parser.pyc in split(cls, s)
    141 
    142     def split(cls, s):
--&gt; 143         return list(cls(s))
    144     split = classmethod(split)
    145 

C:\Python27\lib\site-packages\dateutil\parser.pyc in next(self)
    135 
    136     def next(self):
--&gt; 137         token = self.get_token()
    138         if token is None:
    139             raise StopIteration

C:\Python27\lib\site-packages\dateutil\parser.pyc in get_token(self)
     66                 nextchar = self.charstack.pop(0)
     67             else:
---&gt; 68                 nextchar = self.instream.read(1)
     69                 while nextchar == '\x00':
     70                     nextchar = self.instream.read(1)

AttributeError: 'Series' object has no attribute 'read'
</code></pre>

<hr>

<p><code>df['date'].apply(dateutil.parser.parse)</code> gives me the error <code>AttributeError: 'datetime.date' object has no attribute 'read'</code></p>

<p><code>df['date'].truncate(after='2013/04/01')</code> gives the error <code>TypeError: can't compare datetime.datetime to long</code></p>

<p><code>df['date'].dtype</code> returns <code>dtype('O')</code>. Is it already a <code>datetime</code> object?</p>
";;3;;2013-05-07T05:54:24.243;4.0;16412099;2014-06-27T14:27:37.627;2013-05-07T13:19:45.607;;741099.0;;741099.0;;1;20;<python><python-2.7><numpy><scipy><pandas>;Parse a Pandas column to Datetime;29568.0
3683;3683;16433953.0;3.0;"<p>I have a problem viewing the following <code>DataFrame</code>: </p>

<pre><code>n = 100
foo = DataFrame(index=range(n))
foo['floats'] = np.random.randn(n)
foo
</code></pre>

<p>The problem is that it does not print all rows per default in ipython notebook, but I have to slice to view the resulting rows. Even the following option does not change the output:</p>

<pre><code>pd.set_option('display.max_rows', 500)
</code></pre>

<p>Does anyone know how to display the whole array?</p>
";;8;;2013-05-07T16:52:02.127;11.0;16424493;2017-05-08T00:39:45.240;2017-01-05T14:49:23.253;;604687.0;;2331506.0;;1;44;<python><formatting><pandas><ipython-notebook>;Pandas: Setting no. of max rows;28777.0
3722;3722;16477603.0;3.0;"<p>I can connect to my local mysql database from python, and I can create, select from, and insert individual rows.</p>

<p>My question is: can I directly instruct mysqldb to take an entire dataframe and insert it into an existing table, or do I need to iterate over the rows? </p>

<p>In either case, what would the python script look like for a very simple table with ID and two data columns, and a matching dataframe?</p>
";;0;;2013-05-10T06:29:10.827;14.0;16476413;2016-10-09T15:55:02.890;;;;;1494637.0;;1;13;<python><mysql><pandas><mysql-python>;How to insert pandas dataframe via mysqldb into database?;33749.0
3724;3724;16476974.0;8.0;"<p>I have a DataFrames from pandas:</p>

<pre><code>import pandas as pd
inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]
df = pd.DataFrame(inp)
print df
</code></pre>

<p>Output:</p>

<pre><code>   c1   c2
0  10  100
1  11  110
2  12  120
</code></pre>

<p>Now I want to iterate over the rows of the above frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. So, for example, I would like to have something like that:</p>

<pre><code>for row in df.rows:
   print row['c1'], row['c2']
</code></pre>

<p>Is it possible to do that in pandas?</p>

<p>I found <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">similar question</a>. But it does not give me the answer I need. For example, it is suggested there to use:</p>

<pre><code>for date, row in df.T.iteritems():
</code></pre>

<p>or</p>

<pre><code>for row in df.iterrows():
</code></pre>

<p>But I do not understand what the <code>row</code> object is and how I can work with it.</p>
";;2;;2013-05-10T07:04:49.897;110.0;16476924;2017-08-25T16:17:33.927;2017-05-23T12:10:45.110;;-1.0;;245549.0;;1;379;<python><pandas><rows><dataframe>;How to iterate over rows in a DataFrame in Pandas?;384371.0
3758;3758;16522626.0;2.0;"<p>When I run this code</p>

<pre><code>import pandas as pd
import numpy as np
def add_prop(group):
    births = group.births.astype(float)
    group['prop'] = births/births.sum()
    return group

pieces = []
columns = ['name', 'sex', 'births']

for year in range(1880, 2012):
    path = 'yob%d.txt' % year
    frame = pd.read_csv(path, names = columns)
    frame['year'] = year
    pieces.append(frame)
    names = pd.concat(pieces, ignore_index = True)

total_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)
total_births.plot(title = 'Total Births by sex and year')
</code></pre>

<p>I get no plot.  This is from Wes McKinney's book on using Python for data analysis.
Can anyone point me in the right direction?</p>
";;2;;2013-05-13T12:44:17.230;10.0;16522380;2017-08-01T19:07:10.380;2017-08-01T19:07:10.380;;190597.0;;467379.0;;1;52;<python><matplotlib><pandas>;Matplotlib plot is a no-show;30493.0
3794;3794;16576030.0;1.0;"<p>I am analyzing a data set that is similar in shape to the following example. I have two different types of data (<em>abc</em> data and <em>xyz</em> data):</p>

<pre><code>   abc1  abc2  abc3  xyz1  xyz2  xyz3
0     1     2     2     2     1     2
1     2     1     1     2     1     1
2     2     2     1     2     2     2
3     1     2     1     1     1     1
4     1     1     2     1     2     1
</code></pre>

<p>I want to create a function that adds a categorizing column for each <em>abc</em> column that exists in the dataframe. Using lists of column names and a category mapping dictionary, I was able to get my desired result.</p>

<pre><code>abc_columns = ['abc1', 'abc2', 'abc3']
xyz_columns = ['xyz1', 'xyz2', 'xyz3']
abc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']
categories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}

for i in range(len(abc_category_columns)):
    df3[abc_category_columns[i]] = df3[abc_columns[i]].map(categories)

print df3
</code></pre>

<p>The end result:</p>

<pre><code>   abc1  abc2  abc3  xyz1  xyz2  xyz3 abc1_category abc2_category abc3_category
0     1     2     2     2     1     2          Good           Bad           Bad
1     2     1     1     2     1     1           Bad          Good          Good
2     2     2     1     2     2     2           Bad           Bad          Good
3     1     2     1     1     1     1          Good           Bad          Good
4     1     1     2     1     2     1          Good          Good           Bad
</code></pre>

<p>While the <code>for</code> loop at the end works fine, I feel like I should be using Python's <code>lambda</code> function, but can't seem to figure it out. </p>

<p>Is there a more efficient way to map in a dynamic number of <em>abc</em>-type columns?</p>
";;0;;2013-05-15T22:12:30.203;6.0;16575868;2016-09-01T15:44:34.620;2016-09-01T15:44:34.620;;100297.0;;2188652.0;;1;14;<python><pandas><dataframe>;Efficiently creating additional columns in a pandas DataFrame using .map();11896.0
3815;3815;16597375.0;3.0;"<p>Is it possible to append to an empty data frame that doesn't contain any indices or columns?</p>

<p>I have tried to do this, but keep getting an empty dataframe at the end.</p>

<p>e.g.</p>

<pre><code>df = pd.DataFrame()
data = ['some kind of data here' --&gt; I have checked the type already, and it is a dataframe]
df.append(data)
</code></pre>

<p>The result looks like this:</p>

<pre><code>Empty DataFrame
Columns: []
Index: []
</code></pre>
";;1;;2013-05-16T20:52:29.333;20.0;16597265;2016-08-12T12:15:25.147;;;;;1274908.0;;1;84;<python><pandas>;Appending to an empty data frame in Pandas?;97800.0
3841;3841;16616454.0;2.0;"<p>Say I have a data table</p>

<pre><code>    1  2  3  4  5  6 ..  n
A   x  x  x  x  x  x ..  x
B   x  x  x  x  x  x ..  x
C   x  x  x  x  x  x ..  x
</code></pre>

<p>And I want to slim it down so that I only have, say, columns 3 and 5 deleting all other and maintaining the structure. How could I do this with pandas? I think I understand how to delete a single column, but I don't know how to save a select few and delete all others.</p>

<p>Thanks.</p>
";;0;;2013-05-17T19:00:47.867;3.0;16616141;2016-07-28T00:42:28.220;;;;;2388218.0;;1;15;<python><pandas>;Deleting all columns except a few python-pandas;8381.0
3853;3853;16629125.0;1.0;"<p>From the pandas documentation, I've gathered that unique-valued indices make certain operations efficient, and that non-unique indices are occasionally tolerated.</p>

<p>From the outside, it doesn't look like non-unique indices are taken advantage of in any way. For example, the following <code>ix</code> query is slow enough that it seems to be scanning the entire dataframe</p>

<pre><code>In [23]: import numpy as np
In [24]: import pandas as pd
In [25]: x = np.random.randint(0, 10**7, 10**7)
In [26]: df1 = pd.DataFrame({'x':x})
In [27]: df2 = df1.set_index('x', drop=False)
In [28]: %timeit df2.ix[0]
1 loops, best of 3: 402 ms per loop
In [29]: %timeit df1.ix[0]
10000 loops, best of 3: 123 us per loop
</code></pre>

<p>(I realize the two <code>ix</code> queries don't return the same thing -- it's just an example that calls to <code>ix</code> on a non-unique index appear much slower)</p>

<p>Is there any way to coax pandas into using faster lookup methods like binary search on non-unique and/or sorted indices?</p>
";;0;;2013-05-18T15:44:31.040;4.0;16626058;2017-07-26T15:38:25.260;2013-08-16T08:58:03.517;;202229.0;;1332492.0;;1;18;<python><performance><indexing><pandas><binary-search>;What is the performance impact of non-unique indexes in pandas?;3430.0
3855;3855;16629243.0;2.0;"<p>I have the following questions about HDF5 performance and concurrency:</p>

<ol>
<li>Does HDF5 support concurrent write access? </li>
<li>Concurrency considerations aside, how is HDF5 performance in terms of <strong>I/O performance</strong> (does <strong>compression rates</strong> affect the performance)?</li>
<li>Since I use HDF5 with Python I wonder wow does it's performance compare to Sqlite.</li>
</ol>

<p>References:</p>

<ul>
<li><a href=""http://www.sqlite.org/faq.html#q5"" rel=""nofollow noreferrer"">http://www.sqlite.org/faq.html#q5</a></li>
<li><a href=""https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible"">Locking sqlite file on NFS filesystem possible?</a></li>
<li><a href=""http://pandas.pydata.org/"" rel=""nofollow noreferrer"">http://pandas.pydata.org/</a></li>
</ul>
";2013-05-19T00:32:45.603;2;;2013-05-18T19:46:20.520;19.0;16628329;2016-10-30T15:46:57.150;2017-05-23T11:54:56.330;;-1.0;;283296.0;;1;39;<python><sqlite><pandas><hdf5>;HDF5 - concurrency, compression & I/O performance;17032.0
3856;3856;34687479.0;4.0;"<p>You can use the function <code>tz_localize</code> to make a Timestamp or DateTimeIndex timezone aware, but how can you do the opposite: how can you convert a timezone aware Timestamp to a naive one, while preserving its timezone?</p>

<p>An example:</p>

<pre><code>In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq='s', tz=""Europe/Brussels"")

In [83]: t
Out[83]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: S, Timezone: Europe/Brussels
</code></pre>

<p>I could remove the timezone by setting it to None, but then the result is converted to UTC (12 o'clock became 10):</p>

<pre><code>In [86]: t.tz = None

In [87]: t
Out[87]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]
Length: 10, Freq: S, Timezone: None
</code></pre>

<p>Is there another way I can convert a DateTimeIndex to timezone naive, but while preserving the timezone it was set in?</p>

<hr>

<p>Some <strong>context</strong> on the reason I am asking this: I want to work with timezone naive timeseries (to avoid the extra hassle with timezones, and I do not need them for the case I am working on).<br>
But for some reason, I have to deal with a timezone-aware timeseries in my local timezone (Europe/Brussels). As all my other data are timezone naive (but represented in my local timezone), I want to convert this timeseries to naive to further work with it, but it also has to be represented in my local timezone (so just remove the timezone info, without converting the <em>user-visible</em> time to UTC).  </p>

<p>I know the time is actually internal stored as UTC and only converted to another timezone when you represent it, so there has to be some kind of conversion when I want to ""delocalize"" it. For example, with the python datetime module you can ""remove"" the timezone like this:</p>

<pre><code>In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")

In [120]: d
Out[120]: &lt;Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels&gt;

In [121]: d.replace(tzinfo=None)
Out[121]: &lt;Timestamp: 2013-05-18 12:00:00&gt; 
</code></pre>

<p>So, based on this, I could do the following, but I suppose this will not be very efficient when working with a larger timeseries:</p>

<pre><code>In [124]: t
Out[124]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: S, Timezone: Europe/Brussels

In [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
Out[125]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: None, Timezone: None
</code></pre>
";;4;;2013-05-18T20:51:01.090;7.0;16628819;2016-01-08T22:57:13.760;2013-05-18T21:29:06.800;;653364.0;;653364.0;;1;22;<python><pandas>;Convert pandas timezone-aware DateTimeIndex to naive timestamp, but in certain timezone;16527.0
3865;3865;16637607.0;1.0;"<p>I have a pandas Series which presently looks like this:</p>

<pre><code>14    [Yellow, Pizza, Restaurants]
...
160920                  [Automotive, Auto Parts &amp; Supplies]
160921       [Lighting Fixtures &amp; Equipment, Home Services]
160922                 [Food, Pizza, Candy Stores]
160923           [Hair Removal, Nail Salons, Beauty &amp; Spas]
160924           [Hair Removal, Nail Salons, Beauty &amp; Spas]
</code></pre>

<p>And I want to radically reshape it into a dataframe that looks something like this...</p>

<pre><code>      Yellow  Automotive  Pizza
14       1         0        1
           
160920   0         1        0
160921   0         0        0
160922   0         0        1
160923   0         0        0
160924   0         0        0
</code></pre>

<p>ie. a logical construction noting which categories each observation(row) falls into.</p>

<p>I'm capable of writing for loop based code to tackle the problem, but given the large number of rows I need to handle, that's going to be very slow. </p>

<p>Does anyone know a vectorised solution to this kind of problem? I'd be very grateful.</p>

<p>EDIT: there are 509 categories, which I do have a list of.</p>
";;0;;2013-05-19T17:03:17.570;8.0;16637171;2013-05-19T17:47:42.400;2013-05-19T17:25:11.740;;1319312.0;;1319312.0;;1;11;<python><pandas><category><vectorization>;Pandas: reshaping data;1544.0
3866;3866;16637572.0;2.0;"<p><a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#notes-caveats"">Pandas</a> has the following examples for how to store <code>Series</code>, <code>DataFrames</code> and <code>Panels</code>in HDF5 files:</p>

<h2>Prepare some data:</h2>

<pre><code>In [1142]: store = HDFStore('store.h5')

In [1143]: index = date_range('1/1/2000', periods=8)

In [1144]: s = Series(randn(5), index=['a', 'b', 'c', 'd', 'e'])

In [1145]: df = DataFrame(randn(8, 3), index=index,
   ......:                columns=['A', 'B', 'C'])
   ......:

In [1146]: wp = Panel(randn(2, 5, 4), items=['Item1', 'Item2'],
   ......:            major_axis=date_range('1/1/2000', periods=5),
   ......:            minor_axis=['A', 'B', 'C', 'D'])
   ......:
</code></pre>

<h2>Save it in a store:</h2>

<pre><code>In [1147]: store['s'] = s

In [1148]: store['df'] = df

In [1149]: store['wp'] = wp
</code></pre>

<h2>Inspect what's in the store:</h2>

<pre><code>In [1150]: store
Out[1150]: 
&lt;class 'pandas.io.pytables.HDFStore'&gt;
File path: store.h5
/df            frame        (shape-&gt;[8,3])  
/s             series       (shape-&gt;[5])    
/wp            wide         (shape-&gt;[2,5,4])
</code></pre>

<h1>Close the store:</h1>

<pre><code>In [1151]: store.close()
</code></pre>

<h2>Questions:</h2>

<ol>
<li><p>In the code above, <strong>when is the data actually written to disk</strong>? </p></li>
<li><p>Say I want to add thousands of large dataframes living in <code>.csv</code> files to a single <code>.h5</code> file. I would need to load them and add them to the <code>.h5</code> file one by one since I <strong>cannot</strong> afford to have them all in memory at once as they would take too much memory. Is this possible with HDF5? What would be the correct way to do it?</p></li>
<li><p>The Pandas documentation says the following: </p>

<blockquote>
  <p>""These stores are not appendable <strong>once written</strong> (though you simply
  remove them and rewrite). Nor are they <strong>queryable</strong>; they must be
  retrieved in their entirety.""</p>
</blockquote>

<p>What does it mean by <strong>not appendable nor queryable</strong>? Also, shouldn't it say once <strong>closed</strong> instead of <strong>written</strong>?</p></li>
</ol>
";;0;;2013-05-19T17:14:26.350;18.0;16637271;2016-08-16T13:42:42.040;2013-05-19T17:28:09.770;;283296.0;;283296.0;;1;20;<python><io><pandas><hdf5><pytables>;Iteratively writing to HDF5 Stores in Pandas;12287.0
3874;3874;16641346.0;1.0;"<p>Consider the following example:</p>

<h2>Prepare the data:</h2>

<pre><code>import string
import random
import pandas as pd

matrix = np.random.random((100, 3000))
my_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]
mydf = pd.DataFrame(matrix, columns=my_cols)
mydf['something'] = 'hello_world'
</code></pre>

<h2>Set the highest compression possible for HDF5:</h2>

<pre><code>store = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')
store['mydf'] = mydf
store.close()
</code></pre>

<h2>Save also to CSV:</h2>

<pre><code>mydf.to_csv('myfile.csv', sep=':')
</code></pre>

<p>The result is:</p>

<ul>
<li><code>myfile.csv</code> is 5.6 MB big</li>
<li><code>myfile.h5</code> is 11 MB big</li>
</ul>

<p>The difference grows bigger as the datasets get larger. </p>

<p>I have tried with other compression methods and levels. Is this a bug? (I am using Pandas 0.11 and the latest stable version of HDF5 and Python).</p>
";;5;;2013-05-19T21:57:11.980;6.0;16639877;2013-05-20T01:37:27.203;2013-05-19T23:18:36.993;;283296.0;;283296.0;;1;20;<python><pandas><hdf5><pytables>;HDF5 taking more space than CSV?;6173.0
3877;3877;16648510.0;3.0;"<p>I'm new to pandas and trying to figure out how to convert multiple columns which are formatted as strings to float64's.  Currently I'm doing the below, but it seems like apply() or applymap() should be able to accomplish this task even more efficiently...unfortunately I'm a bit too much of a rookie to figure out how.  Currently the values are percentages formatted as strings like '15.5%'</p>

<pre><code>for column in ['field1', 'field2', 'field3']:
    data[column] = data[column].str.rstrip('%').astype('float64') / 100
</code></pre>
";;0;;2013-05-20T06:23:34.707;4.0;16643695;2015-05-05T19:38:14.020;;;;;1507844.0;;1;11;<python><pandas>;pandas convert strings to float for multiple columns in dataframe;20295.0
3880;3880;22365284.0;2.0;"<p>Is there any way to merge on a single level of a MultiIndex without resetting the index?</p>

<p>I have a ""static"" table of time-invariant values, indexed by an ObjectID, and I have a ""dynamic"" table of time-varying fields, indexed by ObjectID+Date. I'd like to join these tables together.</p>

<p>Right now, the best I can think of is:</p>

<pre><code>dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)
</code></pre>

<p>However, the dynamic table is very big, and I don't want to have to muck around with its index in order to combine the values.</p>
";;3;;2013-05-20T13:45:50.743;7.0;16650945;2014-03-12T22:31:00.293;2013-05-20T14:35:30.227;;250839.0;;250839.0;;1;18;<python><pandas>;Merge on single level of MultiIndex;8389.0
3890;3890;16672514.0;2.0;"<p>I have data in a csv file with dates stored as strings in a standard UK format - <code>%d/%m/%Y</code> - meaning they look like:</p>

<pre><code>12/01/2012
30/01/2012
</code></pre>

<p>The examples above represent 12 January 2012 and 30 January 2012.</p>

<p>When I import this data with pandas version 0.11.0 I applied the following transformation:</p>

<pre><code>import pandas as pd
...
cpts.Date = cpts.Date.apply(pd.to_datetime)
</code></pre>

<p>but it converted dates inconsistently. To use my existing example, 12/01/2012 would convert as a datetime object representing 1 December 2012 but 30/01/2012 converts as 30 January 2012, which is what I want.</p>

<p>After looking at <a href=""https://stackoverflow.com/questions/15929861/pandas-to-datetime-inconsistent-time-string-format"">this question</a> I tried:</p>

<pre><code>cpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')
</code></pre>

<p>but the results are exactly the same. The <a href=""https://github.com/pydata/pandas/blob/v0.11.0/pandas/tseries/tools.py"" rel=""nofollow noreferrer"">source code</a> suggests I'm doing things right so I'm at a loss. Does anyone know what I'm doing wrong?</p>
";;7;;2013-05-21T14:12:21.823;2.0;16672237;2017-04-12T06:35:08.323;2017-05-23T12:32:23.910;;-1.0;;1280784.0;;1;16;<python><datetime><pandas>;Specifying date format when converting with pandas.to_datetime;36583.0
3900;3900;;3.0;"<p>If we have a known value in a column, how can we get its index-value? For example:  </p>

<pre><code>In [148]: a = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])
In [149]: a
Out[149]:   
   c1  c2
0   0   1
1   2   3
2   4   5
........
</code></pre>

<p>As we know, we can get a value by the index corresponding to it, like this.</p>

<pre><code>In [151]: a.ix[0,1]    In [152]: a.c2[0]   In [154]: a.c2.ix[0]   &lt;--  use index
Out[151]: 1            Out[152]: 1         Out[154]: 1            &lt;--  get value
</code></pre>

<p>But how to get the index by value?</p>
";;0;;2013-05-22T04:52:24.463;8.0;16683701;2017-08-01T12:54:11.357;2016-12-19T05:52:06.283;;6655984.0;;2407991.0;;1;12;<indexing><pandas>;In PANDAS, how to get the index of a known value?;28655.0
3908;3908;16689573.0;1.0;"<pre><code>                    A        B
DATE                 
2013-05-01        473077    71333
2013-05-02         35131    62441
2013-05-03           727    27381
2013-05-04           481     1206
2013-05-05           226     1733
2013-05-06           NaN     4064
2013-05-07           NaN    41151
2013-05-08           NaN     8144
2013-05-09           NaN       23
2013-05-10           NaN       10
</code></pre>

<p>say i have the dataframe above.  what is the easiest way to get a series with the same index which is the average of the columns A and B?  the average needs to ignore NaN values. the twist is that this solution needs to be flexible to the addition of new columns to the dataframe.</p>

<p>the closest i have come was </p>

<pre><code>df.sum(axis=1) / len(df.columns)
</code></pre>

<p>however, this does not seem to ignore the NaN values</p>

<p>(note:  i am still a bit new to the pandas library, so i'm guessing there's an obvious way to do this that my limited brain is simply not seeing)</p>
";;2;;2013-05-22T10:32:38.217;4.0;16689514;2016-09-05T19:30:12.690;;;;;152018.0;;1;21;<python><pandas><dataframe>;how to get the average of dataframe column values;52385.0
3941;3941;;4.0;"<p>How to covert a DataFrame column containing strings and <code>NaN</code> values to floats. And there is another column whose values are strings and floats; how to convert this entire column to floats. </p>
";;0;;2013-05-24T07:10:54.877;11.0;16729483;2016-07-09T06:13:02.790;2013-05-24T07:34:23.497;;1199589.0;;2393747.0;;1;39;<python><pandas>;Converting strings to floats in a DataFrame;88771.0
3942;3942;16729808.0;5.0;"<p>I have constructed a condition that extract exactly one row from my data frame:</p>

<pre><code>d2 = df[(df['l_ext']==l_ext) &amp; (df['item']==item) &amp; (df['wn']==wn) &amp; (df['wd']==1)]
</code></pre>

<p>Now I would like to take a value from a particular column:</p>

<pre><code>val = d2['col_name']
</code></pre>

<p>But as a result I get a data frame that contains one row and one column (<em>i.e.</em> one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?</p>
";;0;;2013-05-24T07:17:06.607;18.0;16729574;2017-03-10T19:52:09.913;2017-03-10T19:52:09.913;;4370109.0;;245549.0;;1;86;<python><pandas><dataframe>;How to get a value from a cell of a data frame?;161920.0
3961;3961;;2.0;"<p>Which is the most recommended/pythonic way of handling live incoming data with pandas?</p>

<p>Every few seconds I'm receiving a data point in the format below:</p>

<pre><code>{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',
 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}
</code></pre>

<p>I would like to append it to an existing DataFrame and then run some analysis on it.</p>

<p>The problem is, just appending rows with DataFrame.append can lead to performance issues with all that copying.</p>

<h3>Things I've tried:</h3>

<p>A few people suggested preallocating a big DataFrame and updating it as data comes in:</p>

<pre><code>In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)

In [2]: columns = ['high', 'low', 'open', 'close']

In [3]: df = pd.DataFrame(index=t, columns=columns)

In [4]: df
Out[4]: 
                    high  low open close
2013-01-01 00:00:00  NaN  NaN  NaN   NaN
2013-01-01 00:00:01  NaN  NaN  NaN   NaN
2013-01-01 00:00:02  NaN  NaN  NaN   NaN
2013-01-01 00:00:03  NaN  NaN  NaN   NaN
2013-01-01 00:00:04  NaN  NaN  NaN   NaN

In [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}

In [6]: data_ = pd.Series(data)

In [7]: df.loc[data['time']] = data_

In [8]: df
Out[8]: 
                    high  low open close
2013-01-01 00:00:00  NaN  NaN  NaN   NaN
2013-01-01 00:00:01  NaN  NaN  NaN   NaN
2013-01-01 00:00:02    4    3    2     1
2013-01-01 00:00:03  NaN  NaN  NaN   NaN
2013-01-01 00:00:04  NaN  NaN  NaN   NaN
</code></pre>

<p>The other alternative is building a list of dicts. Simply appending the incoming data to a list and slicing it into smaller DataFrames to do the work.</p>

<pre><code>In [9]: ls = []

In [10]: for n in range(5):
   .....:     # Naive stuff ahead =)
   .....:     time = '2013-01-01 00:00:0' + str(n)
   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}
   .....:     ls.append(d)

In [11]: df = pd.DataFrame(ls[1:3]).set_index('time')

In [12]: df
Out[12]: 
                        close      high       low      open stock
time                                                             
2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH
2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH
</code></pre>

<p>or something like that, maybe processing the input a little bit more.</p>
";;6;;2013-05-24T17:53:51.780;22.0;16740887;2015-12-15T17:12:50.963;2015-12-15T06:27:42.613;;1240268.0;;42040.0;;1;46;<python><pandas>;How to handle incoming real time data with python pandas;7890.0
3998;3998;;2.0;"<p>I am trying to add a column of deltaT to a dataframe where deltaT is the time difference between the successive rows (as indexed in the timeseries).</p>

<pre><code>time                 value

2012-03-16 23:50:00      1
2012-03-16 23:56:00      2
2012-03-17 00:08:00      3
2012-03-17 00:10:00      4
2012-03-17 00:12:00      5
2012-03-17 00:20:00      6
2012-03-20 00:43:00      7
</code></pre>

<p>Desired result is something like the following (deltaT units shown in minutes):</p>

<pre><code>time                 value  deltaT

2012-03-16 23:50:00      1       0
2012-03-16 23:56:00      2       6
2012-03-17 00:08:00      3      12
2012-03-17 00:10:00      4       2
2012-03-17 00:12:00      5       2
2012-03-17 00:20:00      6       8
2012-03-20 00:43:00      7      23
</code></pre>
";;1;;2013-05-27T17:01:30.943;15.0;16777570;2017-03-28T07:12:33.243;2017-03-28T07:12:33.243;;2179729.0;;2425759.0;;1;22;<python><dataframe><pandas>;Calculate time difference between Pandas Dataframe indices;19835.0
4004;4004;16789254.0;1.0;"<p>When selecting a single column from a pandas DataFrame(say <code>df.iloc[:, 0]</code>, <code>df['A']</code>, or <code>df.A</code>, etc), the resulting vector is automatically converted to a Series instead of a single-column DataFrame. However, I am writing some functions that takes a DataFrame as an input argument. Therefore, I prefer to deal with single-column DataFrame instead of Series so that the function can assume say df.columns is accessible. Right now I have to explicitly convert the Series into a DataFrame by using something like <code>pd.DataFrame(df.iloc[:, 0])</code>. This doesn't seem like the most clean method. Is there a more elegant way to index from a DataFrame directly so that the result is a single-column DataFrame instead of Series?</p>
";;1;;2013-05-28T00:48:52.993;7.0;16782323;2013-05-28T10:03:16.580;;;;;1642513.0;;1;33;<python><pandas>;Python pandas: Keep selected column as DataFrame instead of Series;15889.0
4036;4036;;2.0;"<p>I am looking for a way to convert a DataFrame to a TimeSeries without splitting the index and value columns. Any ideas? Thanks.</p>

<pre><code>In [20]: import pandas as pd

In [21]: import numpy as np

In [22]: dates = pd.date_range('20130101',periods=6)

In [23]: df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))

In [24]: df
Out[24]:
                   A         B         C         D
2013-01-01 -0.119230  1.892838  0.843414 -0.482739
2013-01-02  1.204884 -0.942299 -0.521808  0.446309
2013-01-03  1.899832  0.460871 -1.491727 -0.647614
2013-01-04  1.126043  0.818145  0.159674 -1.490958
2013-01-05  0.113360  0.190421 -0.618656  0.976943
2013-01-06 -0.537863 -0.078802  0.197864 -1.414924

In [25]: pd.Series(df)
Out[25]:
0    A
1    B
2    C
3    D
dtype: object
</code></pre>
";;4;;2013-05-29T20:12:57.137;6.0;16822996;2015-10-26T05:29:53.780;;;;;1035112.0;;1;14;<python><pandas><time-series>;How to convert a pandas DataFrame into a TimeSeries?;15070.0
4038;4038;16824696.0;2.0;"<p>Is there any way to specify the index that I want for a new row, when appending the row to a dataframe?</p>

<p>The original documentation provides <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"">the following example</a>:</p>

<pre><code>In [1301]: df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])

In [1302]: df
Out[1302]: 
          A         B         C         D
0 -1.137707 -0.891060 -0.693921  1.613616
1  0.464000  0.227371 -0.496922  0.306389
2 -2.290613 -1.134623 -1.561819 -0.260838
3  0.281957  1.523962 -0.902937  0.068159
4 -0.057873 -0.368204 -1.144073  0.861209
5  0.800193  0.782098 -1.069094 -1.099248
6  0.255269  0.009750  0.661084  0.379319
7 -0.008434  1.952541 -1.056652  0.533946

In [1303]: s = df.xs(3)

In [1304]: df.append(s, ignore_index=True)
Out[1304]: 
          A         B         C         D
0 -1.137707 -0.891060 -0.693921  1.613616
1  0.464000  0.227371 -0.496922  0.306389
2 -2.290613 -1.134623 -1.561819 -0.260838
3  0.281957  1.523962 -0.902937  0.068159
4 -0.057873 -0.368204 -1.144073  0.861209
5  0.800193  0.782098 -1.069094 -1.099248
6  0.255269  0.009750  0.661084  0.379319
7 -0.008434  1.952541 -1.056652  0.533946
8  0.281957  1.523962 -0.902937  0.068159
</code></pre>

<p>where the new row gets the index label automatically. Is there any way to control the new label?</p>
";;0;;2013-05-29T21:59:14.313;5.0;16824607;2017-01-13T07:44:59.290;2013-05-29T22:46:49.513;;283296.0;;283296.0;;1;18;<python><pandas>;Pandas: Appending a row to a dataframe and specify its index label;23799.0
4041;4041;16827257.0;2.0;"<p>I am producing some plots in matplotlib and would like to add explanatory text for some of the data. I want to have a string inside my legend as a separate legend item above the '0-10' item. Does anyone know if there is a possible way to do this?</p>

<p><img src=""https://i.stack.imgur.com/TDuGh.png"" alt=""enter image description here""></p>

<p>This is the code for my legend:<br>
<code>ax.legend(['0-10','10-100','100-500','500+'],loc='best')</code></p>
";;3;;2013-05-30T01:51:44.533;5.0;16826711;2017-07-20T16:39:33.797;2013-05-30T02:29:32.600;;1819479.0;;1819479.0;;1;19;<python><matplotlib><pandas><legend><legend-properties>;Is it possible to add a string as a legend item in matplotlib;12597.0
4043;4043;16834949.0;1.0;"<p>I would like to modify some values from a column in my DataFrame. At the moment I have  a <em>view</em> from select via the multi index of my original <code>df</code> (and modifying does change <code>df</code>).</p>

<p>Here's an example: </p>

<pre><code>In [1]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),
                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),
                  np.arange(0, 6, 1)]
In [2]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])

In [3]: df
                  A         B         C
bar one 0 -0.088671  1.902021 -0.540959
    two 1  0.782919 -0.733581 -0.824522
baz one 2 -0.827128 -0.849712  0.072431
qux one 3 -0.328493  1.456945  0.587793
    two 4 -1.466625  0.720638  0.976438
bar one 5 -0.456558  1.163404  0.464295
</code></pre>

<p>I try to modify a slice of <code>df</code> to a scalar value:</p>

<pre><code>In [4]: df.ix['bar', 'two', :]['A']
Out[4]:
1    0.782919
Name: A, dtype: float64

In [5]: df.ix['bar', 'two', :]['A'] = 9999
# df is unchanged
</code></pre>

<p>I really want to modify <em>several</em> values in the column (and since indexing returns a vector, not a scalar value, I think this would make more sense):</p>

<pre><code>In [6]: df.ix['bar', 'one', :]['A'] = [999, 888]
# again df remains unchanged
</code></pre>

<p>I'm using pandas 0.11. Is there is a simple way to do this?</p>

<p><em>The current solution is to recreate df from a new one and modify values I want to. But it's not elegant and can be very heavy on complex dataframe. In my opinion the problem should come from .ix and .loc not returning a view but a copy.</em></p>
";;5;;2013-05-30T10:32:20.057;4.0;16833842;2013-05-30T15:32:40.930;2013-05-30T15:32:40.930;;1240268.0;;458130.0;;1;15;<python><pandas><multi-index><dataframe>;Assign new values to slice from MultiIndex DataFrame;7430.0
4064;4064;16853161.0;4.0;"<p>I have a Pandas data frame, one of the columns of which contains date strings in the format 'YYYY-MM-DD' e.g. '2013-10-28'.</p>

<p>At the moment the dtype of the column is 'object'.</p>

<p>How do I convert the column values to Pandas date format?</p>
";;0;;2013-05-31T08:23:02.937;13.0;16852911;2015-11-07T10:25:42.517;2015-11-07T10:25:42.517;;4370109.0;;213216.0;;1;42;<python><date><pandas>;How do I convert dates in a Pandas data frame to a 'date' data type?;54111.0
4079;4079;16896091.0;1.0;"<p>I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table. <br/></p>

<p>All I could do up until now is:<br/></p>

<pre><code>import pandas as pd
data = pd.ExcelFile(""*File Name*"")
</code></pre>

<p>
Now I know that the step got executed successfully, but I want to know how i can parse the excel file that has been read so that I can understand how the data in the excel maps to the data in the variable data. <br/>
I learnt that data is a Dataframe object if I'm not wrong. So How do i parse this dataframe object to extract each line row by row.</p>
";;1;;2013-06-03T01:20:08.113;6.0;16888888;2017-01-26T05:16:10.080;2014-07-18T18:09:42.353;;1044792.0;;1024273.0;;1;28;<python><pandas><ipython><ipython-notebook><dataframe>;How to read a .xlsx file using the pandas Library in iPython?;25755.0
4098;4098;16923367.0;5.0;"<p>I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:</p>

<pre><code>df.to_csv('out.csv')
</code></pre>

<p>And getting the error:</p>

<pre><code>UnicodeEncodeError: 'ascii' codec can't encode character u'\u03b1' in position 20: ordinal not in range(128)
</code></pre>

<p>Is there any way to get around this easily (i.e. I have unicode characters in my data frame)? And is there a way to write to a tab delimited file instead of a CSV using e.g. a 'to-tab' method (that I dont think exists)?</p>
";;0;;2013-06-04T16:46:56.137;41.0;16923281;2017-07-17T10:27:18.440;2015-08-31T03:13:13.823;;2681088.0;;213216.0;;1;214;<python><csv><pandas><dataframe>;Pandas writing dataframe to CSV file;296632.0
4126;4126;16949498.0;3.0;"<p>given the following dataframe in pandas:</p>

<pre><code>import numpy as np
df = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})
</code></pre>

<p>where <code>id</code> is an id for each point consisting of an <code>a</code> and <code>b</code> value, how can I bin <code>a</code> and <code>b</code> into a specified set of bins (so that I can then take the median/average value of <code>a</code> and <code>b</code> in each bin)?  <code>df</code> might have <code>NaN</code> values for <code>a</code> or <code>b</code> (or both) for any given row in <code>df</code>. thanks.</p>

<p>Here's a better example using Joe Kington's solution with a more realistic df. The thing I'm unsure about is how to access the df.b elements for each df.a group below:</p>

<pre><code>a = np.random.random(20)
df = pandas.DataFrame({""a"": a, ""b"": a + 10})
# bins for df.a
bins = np.linspace(0, 1, 10)
# bin df according to a
groups = df.groupby(np.digitize(df.a,bins))
# Get the mean of a in each group
print groups.mean()
## But how to get the mean of b for each group of a?
# ...
</code></pre>
";;0;;2013-06-05T18:33:51.607;27.0;16947336;2016-08-18T11:38:47.443;2013-06-05T22:10:58.080;;248237.0;;248237.0;;1;31;<python><numpy><pandas>;binning a dataframe in pandas in Python;34913.0
4142;4142;;3.0;"<p>I have a pandas data frame and I want to sort column('Bytes') in Descending order and print highest 10 values and its related ""Client IP"" column value. Suppose following is a part of my dataframe. I have many different methods and failed? </p>

<pre><code>0       Bytes    Client Ip                
0       1000      192.168.10.2    
1       2000      192.168.10.12    
2       500       192.168.10.4     
3       159       192.168.10.56 
</code></pre>

<p>Following prints only the raw which has the highest value. </p>

<pre><code>print df['Bytes'].argmax()
</code></pre>
";;0;;2013-06-06T09:30:01.890;2.0;16958499;2016-04-19T05:50:14.513;;;;;461436.0;;1;11;<pandas><dataframe>;Sort Pandas dataframe and print highest n values;20978.0
4181;4181;16988624.0;1.0;"<p>I have a data frame with alpha-numeric keys which I want to save as a csv and read back later. For various reasons I need to explicitly read this key column as a string format, I have keys which are strictly numeric or even worse, things like: 1234E5 which Pandas interprets as a float. This obviously makes the key completely useless.</p>

<p>The problem is when I specify a string dtype for the data frame or any column of it I just get garbage back. I have some example code here:</p>

<pre><code>df = pd.DataFrame(np.random.rand(2,2),
                  index=['1A', '1B'],
                  columns=['A', 'B'])
df.to_csv(savefile)
</code></pre>

<p>The data frame looks like:</p>

<pre><code>           A         B
1A  0.209059  0.275554
1B  0.742666  0.721165
</code></pre>

<p>Then I read it like so:</p>

<pre><code>df_read = pd.read_csv(savefile, dtype=str, index_col=0)
</code></pre>

<p>and the result is:</p>

<pre><code>   A  B
B  (  &lt;
</code></pre>

<p>Is this a problem with my computer, or something I'm doing wrong here, or just a bug?</p>
";;0;;2013-06-07T16:09:32.663;3.0;16988526;2013-06-07T21:53:12.667;;;;;192266.0;;1;16;<python><pandas>;Pandas reading csv as string type;25336.0
4186;4186;;2.0;"<p>I am having trouble finding a way to do an efficient element-wise minimum of two Series objects in pandas. For example I can add two Series easily enough:</p>

<pre><code>In [1]:
import pandas as pd
s1 = pd.Series(data=[1,1,1], index=[1,2,3])
s2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])
s1.add(s2)    
Out[1]:
1     2
2     3
3     3
4   NaN
dtype: float64
</code></pre>

<p>But I cannot find an efficient way to do an element-wise minimum between two Series (along with aligning the indices and handling NaN values).</p>

<p><em>Nevermind. There is an escape hatch with the combine function so you can put in any element-wise function:</em></p>

<pre><code>In [2]:
s1 = pd.Series(data=[1,1,1], index=[1,2,3])
s2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])
s1.combine(s2, min, 0)
Out[2]:
1    1
2    1
3    1
4    0
dtype: int64
</code></pre>
";;1;;2013-06-07T17:38:38.383;3.0;16989946;2015-07-31T12:55:31.750;2013-06-07T21:45:16.587;;1240268.0;;2464433.0;;1;15;<python><pandas>;Creating an element-wise minimum Series from two other Series in Python Pandas;9408.0
4188;4188;;2.0;"<p>I'm trying out multidimensional scaling with sklearn, pandas and numpy.  The data file Im using has 10 numerical columns and no missing values.  I am trying to take this ten dimensional data and visualize it in 2 dimensions with sklearn.manifold's multidimensional scaling as follows:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn import manifold
from sklearn.metrics import euclidean_distances

seed = np.random.RandomState(seed=3)
data = pd.read_csv('data/big-file.csv')

#  start small dont take all the data, 
#  its about 200k records
subset = data[:10000]
similarities = euclidean_distances(subset)

mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, 
      random_state=seed, dissimilarity=""precomputed"", n_jobs=1)

pos = mds.fit(similarities).embedding_
</code></pre>

<p>But I get this value error:</p>

<pre><code>Traceback (most recent call last):
  File ""demo/mds-demo.py"", line 18, in &lt;module&gt;
    pos = mds.fit(similarities).embedding_
  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 360, in fit
    self.fit_transform(X, init=init)
  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 395, in fit_transform
eps=self.eps, random_state=self.random_state)
  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 242, in smacof
eps=eps, random_state=random_state)
  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 73, in _smacof_single
raise ValueError(""similarities must be symmetric"")
ValueError: similarities must be symmetric
</code></pre>

<p>I thought euclidean_distances returned a symmetric matrix.  What am I doing wrong and how do I fix it?</p>
";;6;;2013-06-07T18:50:18.373;3.0;16990996;2014-10-24T08:40:49.500;2013-11-09T19:56:35.993;;2831353.0;;1698695.0;;1;12;<python><numpy><pandas><scikit-learn>;Multidimensional Scaling Fitting in Numpy, Pandas and Sklearn (ValueError);7435.0
4194;4194;;1.0;"<p>I'm importing large amounts of http logs (80GB+) into a Pandas HDFStore for statistical processing. Even within a single import file I need to batch the content as I load it. My tactic thus far has been to read the parsed lines into a DataFrame then store the DataFrame into the HDFStore. My goal is to have the index key unique for a single key in the DataStore but each DataFrame restarts it's own index value again. I was anticipating HDFStore.append() would have some mechanism to tell it to ignore the DataFrame index values and just keep adding to my HDFStore key's existing index values but cannot seem to find it. How do I import DataFrames and ignore the index values contained therein while having the HDFStore increment its existing index values? Sample code below batches every 10 lines. Naturally the real thing would be larger.</p>

<pre class=""lang-py prettyprint-override""><code>if hd_file_name:
        """"""
        HDF5 output file specified.
        """"""

        hdf_output = pd.HDFStore(hd_file_name, complib='blosc')
        print hdf_output

        columns = ['source', 'ip', 'unknown', 'user', 'timestamp', 'http_verb', 'path', 'protocol', 'http_result', 
                   'response_size', 'referrer', 'user_agent', 'response_time']

        source_name = str(log_file.name.rsplit('/')[-1])   # HDF5 Tables don't play nice with unicode so explicit str(). :(

        batch = []

        for count, line in enumerate(log_file,1):
            data = parse_line(line, rejected_output = reject_output)

            # Add our source file name to the beginning.
            data.insert(0, source_name )    
            batch.append(data)

            if not (count % 10):
                df = pd.DataFrame( batch, columns = columns )
                hdf_output.append(KEY_NAME, df)
                batch = []

        if (count % 10):
            df = pd.DataFrame( batch, columns = columns )
            hdf_output.append(KEY_NAME, df)
</code></pre>
";;1;;2013-06-08T07:09:21.403;7.0;16997048;2014-03-19T11:56:07.743;2014-03-19T11:56:07.743;;1900149.0;;457288.0;;1;12;<python><indexing><pandas><dataframe><hdfstore>;How does one append large amounts of data to a Pandas HDFStore and get a natural unique index?;9423.0
4197;4197;17001474.0;3.0;"<p>So I completely understand how to use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html"">resample</a>, but the documentation does not do a good job explaining the options.</p>

<p>So most options in the <code>resample</code> function are pretty straight forward except for these two:</p>

<ul>
<li>rule : the offset string or object representing target conversion</li>
<li>how : string, method for down- or re-sampling, default to mean</li>
</ul>

<p>So from looking at as many examples as I found online I can see for rule you can do <code>'D'</code> for day, <code>'xMin'</code> for minutes, <code>'xL'</code> for milliseconds, but that is all I could find.</p>

<p>for how I have seen the following: <code>'first'</code>, <code>np.max</code>, <code>'last'</code>, <code>'mean'</code>, and <code>'n1n2n3n4...nx'</code> where nx is the first letter of each column index.</p>

<p>So is there somewhere in the documentation that I am missing that displays every option for <code>pandas.resample</code>'s rule and how inputs? If yes, where because I could not find it. If no, <strong>what are all the options for them?</strong></p>
";;2;;2013-06-08T16:09:43.760;75.0;17001389;2016-11-19T21:12:25.620;;;;;2127988.0;;1;113;<python><documentation><pandas>;pandas resample documentation;59119.0
4208;4208;17005204.0;1.0;"<p>Simple example:</p>

<pre><code>&gt;&gt;&gt; from collections import namedtuple
&gt;&gt;&gt; import pandas

&gt;&gt;&gt; Price = namedtuple('Price', 'ticker date price')
&gt;&gt;&gt; a = Price('GE', '2010-01-01', 30.00)
&gt;&gt;&gt; b = Price('GE', '2010-01-02', 31.00)
&gt;&gt;&gt; l = [a, b]
&gt;&gt;&gt; df = pandas.DataFrame.from_records(l, index='ticker')
Traceback (most recent call last)
...
KeyError: 'ticker'
</code></pre>

<p>Harder example:</p>

<pre><code>&gt;&gt;&gt; df2 = pandas.DataFrame.from_records(l, index=['ticker', 'date'])
&gt;&gt;&gt; df2

         0           1   2
ticker  GE  2010-01-01  30
date    GE  2010-01-02  31
</code></pre>

<p>Now it thinks that <code>['ticker', 'date']</code> is the index itself, rather than the columns I want to use as the index.</p>

<p>Is there a way to do this without resorting to an intermediate numpy ndarray or using <code>set_index</code> after the fact?</p>
";;0;;2013-06-08T23:37:23.210;1.0;17004985;2013-06-09T11:23:26.057;2013-06-09T00:13:12.083;;290443.0;;290443.0;;1;14;<python><pandas>;How do I create pandas DataFrame (with index or multiindex) from list of namedtuple instances?;17744.0
4281;4281;17063653.0;4.0;"<p>I am trying to read an excel file this way :</p>

<pre><code>newFile = pd.ExcelFile(PATH\FileName.xlsx)
ParsedData = pd.io.parsers.ExcelFile.parse(newFile)
</code></pre>

<p>which throws an error that says two arguments expected, I don't know what the second argument is and also what I am trying to achieve here is to convert an Excel file to a DataFrame, Am I doing it the right way? or is there any other way to do this using pandas?</p>
";;0;;2013-06-12T10:42:11.967;24.0;17063458;2016-11-30T14:10:24.533;;;;;4168397.0;;1;56;<python><python-2.7><pandas>;Reading an Excel file in python using pandas;114839.0
4286;4286;17068439.0;3.0;"<p>I want to replace negative values in a pandas DataFrame column with zero.</p>

<p>Is there a more concise way to construct this expression?</p>

<pre><code>df['value'][df['value'] &lt; 0] = 0
</code></pre>
";;2;;2013-06-12T14:35:47.560;2.0;17068269;2015-10-14T18:54:42.613;2013-06-12T17:04:06.233;;1240268.0;;2478647.0;;1;18;<python><pandas>;Return max of zero or value for a pandas DataFrame column;6301.0
4290;4290;17071908.0;9.0;"<p>How to select rows from a DataFrame based on values in some column in pandas?<br>
In SQL I would use: </p>

<pre><code>select * from table where colume_name = some_value. 
</code></pre>

<p><em>I tried to look at pandas documentation but did not immediately find the answer.</em></p>
";;0;;2013-06-12T17:42:05.903;226.0;17071871;2017-07-05T16:34:57.100;2016-04-17T21:49:06.030;;2826011.0;;458429.0;;1;375;<python><pandas><dataframe>;Select rows from a DataFrame based on values in a column in pandas;420943.0
4292;4292;;1.0;"<p>I have the following data imported from a csv file using pandas <code>read_csv</code>:</p>

<pre><code> instrument         type   from_date  to_date   
0   96000001    W/D &amp; V/L  19951227  19960102
1   96000002   DEED TRUST  19951227  19960102
2   96000003  WARNTY DEED  19951228  19960102
3   96000004   DEED TRUST  19951228  19960102
4   96000005    W/D &amp; V/L  19951228  19960102
</code></pre>

<p>I would like to select those rows that fit a date or date range. For instance I want to 
select only those rows with the date <code>19951227</code> in the <code>from_date</code> column or select days that range from <code>from_date</code> of <code>19951227</code> to <code>to_date</code> <code>19960102</code>.</p>

<p>How would I do this?</p>
";;0;;2013-06-12T18:54:16.920;2.0;17073150;2013-06-12T19:24:42.417;2013-06-12T19:19:24.220;;1240268.0;;2280768.0;;1;11;<date><pandas>;filtering a pandas Dataframe based on date value;5012.0
4299;4299;17085044.0;2.0;"<p>For example, I have:</p>

<pre><code>In [1]: df = pd.DataFrame([8, 9],
                          index=pd.MultiIndex.from_tuples([(1, 1, 1),
                                                           (1, 3, 2)]),
                          columns=['A'])

In [2] df
Out[2]: 
       A
1 1 1  8
  3 2  9
</code></pre>

<p>Is there a better way to remove the last level from the index than this:</p>

<pre><code>In [3]: pd.DataFrame(df.values,
                     index=df.index.droplevel(2),
                     columns=df.columns)
Out[3]: 
     A
1 1  8
  3  9
</code></pre>
";;0;;2013-06-13T10:17:57.387;5.0;17084579;2013-06-13T10:47:04.530;2013-06-13T10:47:04.530;;1240268.0;;1579844.0;;1;19;<pandas>;How to remove levels from a multi-indexed dataframe?;18302.0
4305;4305;17092113.0;4.0;"<p>The simple task of adding a row to a <code>pandas.DataFrame</code> object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.</p>

<p>Here is what I'm trying to do. I have a DataFrame of which I already know the shape as well as the names of the rows and columns.</p>

<pre><code>&gt;&gt;&gt; df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])
&gt;&gt;&gt; df
     a    b    c    d
x  NaN  NaN  NaN  NaN
y  NaN  NaN  NaN  NaN
z  NaN  NaN  NaN  NaN
</code></pre>

<p>Now, I have a function to compute the values of the rows iteratively. How can I fill in one of the rows with either a dictionary or a <code>pandas.Series</code> ? Here are various attempts that have failed:</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df['y'] = y
AssertionError: Length of values does not match length of index
</code></pre>

<p>Apparently it tried to add a column instead of a row.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.join(y)
AttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'
</code></pre>

<p>Very uninformative error message.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.set_value(index='y', value=y)
TypeError: set_value() takes exactly 4 arguments (3 given)
</code></pre>

<p>Apparently that is only for setting individual values in the dataframe.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.append(y)
Exception: Can only append a Series if ignore_index=True
</code></pre>

<p>Well, I don't want to ignore the index, otherwise here is the result:</p>

<pre><code>&gt;&gt;&gt; df.append(y, ignore_index=True)
     a    b    c    d
0  NaN  NaN  NaN  NaN
1  NaN  NaN  NaN  NaN
2  NaN  NaN  NaN  NaN
3    1    5    2    3
</code></pre>

<p>It did align the column names with the values, but lost the row labels.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.ix['y'] = y
&gt;&gt;&gt; df
                                  a                                 b  \
x                               NaN                               NaN
y  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}
z                               NaN                               NaN

                                  c                                 d
x                               NaN                               NaN
y  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}
z                               NaN                               NaN
</code></pre>

<p>That also failed miserably.</p>

<p>So how do you do it ?</p>
";;0;;2013-06-13T16:02:32.103;16.0;17091769;2017-08-03T21:46:55.563;2013-06-24T09:38:30.290;;287297.0;;287297.0;;1;54;<python><dataframe><row><pandas>;Python pandas: fill a dataframe row by row;53561.0
4307;4307;18588980.0;3.0;"<p>I have a <code>pandas.DataFrame</code> that I wish to export to a CSV file. However, pandas seems to write some of the values as <code>float</code> instead of <code>int</code> types. I couldn't not find how to change this behavior.</p>

<p>Building a data frame:</p>

<pre><code>df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)
x = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)
y = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)
z = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)
df.loc['x']=x; df.loc['y']=y; df.loc['z']=z
</code></pre>

<p>View it:</p>

<pre><code>&gt;&gt;&gt; df
    a   b    c   d
x  10  10  NaN  10
y   1   5    2   3
z   1   2    3   4
</code></pre>

<p>Export it:</p>

<pre><code>&gt;&gt;&gt; df.to_csv('test.csv', sep='\t', na_rep='0', dtype=int)
&gt;&gt;&gt; for l in open('test.csv'): print l.strip('\n')
        a       b       c       d
x       10.0    10.0    0       10.0
y       1       5       2       3
z       1       2       3       4
</code></pre>

<p>Why do the tens have a dot zero ?</p>

<p>Sure, I could just stick this function into my pipeline to reconvert the whole CSV file, but it seems unnecessary:</p>

<pre><code>def lines_as_integer(path):
    handle = open(path)
    yield handle.next()
    for line in handle:
        line = line.split()
        label = line[0]
        values = map(float, line[1:])
        values = map(int, values)
        yield label + '\t' + '\t'.join(map(str,values)) + '\n'
handle = open(path_table_int, 'w')
handle.writelines(lines_as_integer(path_table_float))
handle.close()
</code></pre>
";;5;;2013-06-13T16:47:35.687;7.0;17092671;2013-09-03T09:42:55.590;2013-06-13T16:53:43.990;;287297.0;;287297.0;;1;14;<python><csv><dataframe><pandas>;Python pandas: output dataframe to csv with integers;16184.0
4312;4312;17095620.0;8.0;"<p>I am trying to highlight exactly what changed between two dataframes.</p>

<p>Suppose I have two Python Pandas dataframes:</p>

<pre><code>""StudentRoster Jan-1"":
id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.11                     False                Graduated
113  Zoe    4.12                     True       

""StudentRoster Jan-2"":
id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.21                     False                Graduated
113  Zoe    4.12                     False                On vacation
</code></pre>

<p>My goal is to output an HTML table that:</p>

<ol>
<li>Identifies rows that have changed (could be int, float, boolean, string)</li>
<li><p>Outputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: </p>

<pre><code>""StudentRoster Difference Jan-1 - Jan-2"":  
id   Name   score                    isEnrolled           Comment
112  Nick   was 1.11| now 1.21       False                Graduated
113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""
</code></pre></li>
</ol>

<p>I suppose I could do a row by row and column by column comparison, but is there an easier way?</p>
";;1;;2013-06-13T19:08:16.757;28.0;17095101;2017-06-07T06:46:20.287;2017-05-09T09:17:52.077;;3478852.0;;1625152.0;;1;48;<python><html><dataframe><pandas>;Outputting difference in two Pandas dataframes side by side - highlighting the difference;51910.0
4317;4317;17097397.0;2.0;"<p>Is there any method to replace values with <code>None</code> in Pandas in Python?</p>

<p>You can use <code>df.replace('pre', 'post')</code> and can replace a value with another, but this can't be done if you want to replace with <code>None</code> value, which if you try, you get a strange result.</p>

<p>So here's an example:</p>

<pre><code>df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])
df.replace('-', 0)
</code></pre>

<p>which returns a successful result.</p>

<p>But,</p>

<pre><code>df.replace('-', None)
</code></pre>

<p>which returns a following result:</p>

<pre><code>0
0   - // this isn't replaced
1   3
2   2
3   5
4   1
5  -5
6  -1
7  -1 // this is changed to `-1`...
8   9
</code></pre>

<p>Why does such a strange result be returned?</p>

<p>Since I want to pour this data frame into MySQL database, I can't put <code>NaN</code> values into any element in my data frame and instead want to put <code>None</code>. Surely, you can first change <code>'-'</code> to <code>NaN</code> and then convert <code>NaN</code> to <code>None</code>, but I want to know why the dataframe acts in such a terrible way.</p>
";;5;;2013-06-13T21:17:31.310;6.0;17097236;2014-04-09T21:38:12.773;2014-03-29T19:58:48.193;;2360798.0;;2360798.0;;1;29;<python><replace><pandas><nan><nonetype>;How to replace values with None in Pandas data frame in Python?;53211.0
4319;4319;17097777.0;3.0;"<p>I've done some searching and can't figure out how to filter a dataframe by <code>df[""col""].str.contains(word)</code>, however I'm wondering if there is a way to do the reverse: filter a dataframe by that set's compliment. eg: to the effect of <code>!(df[""col""].str.contains(word))</code>. </p>

<p>Can this be done through a <code>DataFrame</code> method?</p>
";;0;;2013-06-13T21:43:26.767;5.0;17097643;2016-12-15T21:10:34.647;;;;;1529734.0;;1;23;<python><pandas><contains>;"search for ""does-not-contain"" on a dataframe in pandas";13033.0
4322;4322;17098736.0;5.0;"<p>Right now I'm importing a fairly large <code>CSV</code> as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?</p>
";;0;;2013-06-13T23:05:36.977;50.0;17098654;2017-08-22T07:10:19.143;2017-08-22T07:10:19.143;;4909087.0;;972381.0;;1;117;<python><pandas><dataframe>;How to store a dataframe using Pandas;78448.0
4340;4340;17115229.0;4.0;"<p>Is there anyway to use the mapping function or something better to replace values in an entire dataframe?</p>

<p>I only know how to perform the mapping on series.</p>

<p>I would like to replace the strings in the 'tesst' and 'set' column with a number
for example set = 1, test =2</p>

<p>Here is a example of my dataset: (Original dataset is very large)</p>

<pre><code>ds_r
  respondent  brand engine  country  aware  aware_2  aware_3  age tesst   set
0          a  volvo      p      swe      1        0        1   23   set   set
1          b  volvo   None      swe      0        0        1   45   set   set
2          c    bmw      p       us      0        0        1   56  test  test
3          d    bmw      p       us      0        1        1   43  test  test
4          e    bmw      d  germany      1        0        1   34   set   set
5          f   audi      d  germany      1        0        1   59   set   set
6          g  volvo      d      swe      1        0        0   65  test   set
7          h   audi      d      swe      1        0        0   78  test   set
8          i  volvo      d       us      1        1        1   32   set   set
</code></pre>

<p>Final result should be </p>

<pre><code> ds_r
  respondent  brand engine  country  aware  aware_2  aware_3  age  tesst  set
0          a  volvo      p      swe      1        0        1   23      1    1
1          b  volvo   None      swe      0        0        1   45      1    1
2          c    bmw      p       us      0        0        1   56      2    2
3          d    bmw      p       us      0        1        1   43      2    2
4          e    bmw      d  germany      1        0        1   34      1    1
5          f   audi      d  germany      1        0        1   59      1    1
6          g  volvo      d      swe      1        0        0   65      2    1
7          h   audi      d      swe      1        0        0   78      2    1
8          i  volvo      d       us      1        1        1   32      1    1
</code></pre>

<p>grateful for advise,</p>
";;0;;2013-06-14T18:20:02.803;3.0;17114904;2017-07-12T20:31:34.303;2015-06-30T22:36:43.770;;1649780.0;;2219369.0;;1;11;<python><replace><dataframe><pandas>;python pandas replacing strings in dataframe with numbers;14060.0
4346;4346;17116976.0;3.0;"<p>I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter. I was wondering if there is a simple way to do this using pandas or python?</p>

<pre><code>CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt
32363    McCartney, Paul      3     F04    2:218:10:4,6                   60
31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300
</code></pre>

<p>I want to split by the space<code>(' ')</code> and then the colon<code>(':')</code> in the <code>Seatblocks</code> column, but each cell would result in a different number of columns. I have a function to rearrange the columns so the <code>Seatblocks</code> column is at the end of the sheet, but I'm not sure what to do from there. I can do it in excel with the built in <code>text-to-columns</code> function and a quick macro, but my dataset has too many records for excel to handle.</p>

<p>Ultimately, I want to take records such John Lennon's and create multiple lines, with the info from each set of seats on a separate line.</p>
";;0;;2013-06-14T20:32:46.103;63.0;17116814;2016-09-14T08:47:08.010;2015-12-29T20:43:05.957;;2901002.0;;1955794.0;;1;95;<python><pandas><dataframe>;pandas: How do I split text in a column into multiple rows?;70528.0
4358;4358;17128356.0;1.0;"<p>After performing calculations on an entire pandas dataframe, I need to go back and override variable calculations (often setting to zero) based on the value of another variable(s). Is there a more succinct/idiomatic way to perform this kind of operation?</p>

<pre><code>df['var1000'][df['type']==7] = 0
df['var1001'][df['type']==7] = 0
df['var1002'][df['type']==7] = 0
...
df['var1099'][df['type']==7] = 0
</code></pre>

<p>Is there a pandas-y way to do something like this?</p>

<pre><code>if (df['type']==7):
    df['var1000'] = 0
    df['var1001'] = 0
    df['var1002'] = 0
    ...
    df['var1099'] = 0
</code></pre>
";;0;;2013-06-15T22:00:32.330;6.0;17128302;2013-08-22T18:05:58.623;;;;;2478647.0;;1;14;<python><pandas>;Python/pandas idiom for if/then/else;17587.0
4367;4367;17134750.0;2.0;"<p>How can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetimes?</p>
";;0;;2013-06-16T15:14:58.850;16.0;17134716;2017-06-26T16:11:55.207;2013-06-16T15:40:24.107;;1240268.0;;857130.0;;1;92;<python><pandas><dataframe>;Convert DataFrame column type from string to datetime;85223.0
4369;4369;17135044.0;4.0;"<p>I wonder how to add new DataFrame data onto the end of an existing csv file? The to_csv doesn't mention such functionality. Thank you in advance. </p>
";;0;;2013-06-16T15:40:40.683;4.0;17134942;2015-05-17T22:17:49.893;2014-03-19T11:54:38.033;;1900149.0;;857130.0;;1;12;<python><csv><pandas><dataframe>;pandas DataFrame output end of csv;16440.0
4379;4379;17141755.0;2.0;"<p>Suppose I have a data-Frame with columns a b &amp; c, I want to sort the data-Frame by column b in ascending, and by column c in descending, how do I do this?</p>
";;1;;2013-06-17T06:28:47.340;18.0;17141558;2017-05-18T19:10:08.750;;;;;4168397.0;;1;65;<python><python-2.7><pandas><data-analysis>;How to sort a dataFrame in python pandas by two or more columns?;73105.0
4384;4384;17142595.0;2.0;"<p>I have a very large dataset were I want to replace strings with numbers. I would like to operate on the dataset without typing a mapping function for each key (column) in the dataset. (similar to the fillna method, but replace specific string with assosiated value).
Is there anyway to do this?</p>

<p>Here is an example of my dataset</p>

<pre><code>data
   resp          A          B          C
0     1       poor       poor       good
1     2       good       poor       good
2     3  very good  very good  very good
3     4       bad        poor       bad 
4     5   very bad   very bad   very bad
5     6       poor       good   very bad
6     7       good       good       good
7     8  very good  very good  very good
8     9       bad        bad    very bad
9    10   very bad   very bad   very bad
</code></pre>

<p>The desired result:</p>

<pre><code> data
   resp  A  B  C
0      1  3  3  4
1     2  4  3  4
2     3  5  5  5
3     4  2  3  2
4     5  1  1  1
5     6  3  4  1
6     7  4  4  4
7     8  5  5  5
8     9  2  2  1
9    10  1  1  1
</code></pre>

<p>very bad=1, bad=2, poor=3, good=4, very good=5</p>

<p>//Jonas</p>
";;0;;2013-06-17T07:23:17.123;11.0;17142304;2017-07-11T10:30:39.047;;;;;2219369.0;;1;33;<python><replace><dataframe><pandas>;replace string/value in entire dataframe;35525.0
4386;4386;17148934.0;3.0;"<p>There is a method to <strong>plot</strong> Series histograms, but is there a function to retrieve the histogram counts to do further calculations on top of it? </p>

<p>I keep using numpy's functions to do this and converting the result to a DataFrame or Series when I need this. It would be nice to stay with pandas objects the whole time. </p>
";;0;;2013-06-17T13:32:57.023;2.0;17148787;2016-12-23T08:50:35.147;2015-01-28T02:15:30.870;;202229.0;;114388.0;;1;15;<pandas><histogram><series>;Are there functions to retrieve the histogram counts of a Series in pandas?;6490.0
4395;4395;17156233.0;3.0;"<p>Suppose I have a select roughly like this:</p>

<pre><code>select instrument, price, date from my_prices;
</code></pre>

<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>

<p>To be clear: I'm looking for:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: ...
Data columns (total 2 columns):
inst_1    ...
inst_2    ...
dtypes: float64(1), object(1) 
</code></pre>

<p>I'm NOT looking for:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: ...
Data columns (total 2 columns):
instrument    ...
price         ...
dtypes: float64(1), object(1)
</code></pre>

<p>...which is easy ;-)</p>
";;0;;2013-06-17T20:17:47.487;6.0;17156084;2016-10-12T18:27:02.160;2013-06-17T20:49:58.557;;216229.0;;216229.0;;1;12;<sql><pandas>;unpacking a sql select into a pandas dataframe;6474.0
4402;4402;17158735.0;2.0;"<p>I want to have the x-tick date labels centered between the tick marks, instead of centered about the tick marks as shown in the photo below.</p>

<p>I have read the documentation but to no avail - does anyone know a way to do this? </p>

<p><img src=""https://i.stack.imgur.com/Ea81A.png"" alt=""enter image description here""></p>

<p>Here is everything that I've used for my x-axis tick formatting if it helps:    </p>

<pre><code>day_fmt = '%d'   
myFmt = mdates.DateFormatter(day_fmt)
ax.xaxis.set_major_formatter(myFmt)    
ax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     

for tick in ax.xaxis.get_major_ticks():
    tick.tick1line.set_markersize(0)
    tick.tick2line.set_markersize(0)
    tick.label1.set_horizontalalignment('center')
</code></pre>
";;0;;2013-06-17T23:17:09.050;3.0;17158382;2016-09-10T12:50:01.683;2013-06-17T23:45:15.453;;1819479.0;;1819479.0;;1;13;<python><matplotlib><pandas>;Centering x-tick labels between tick marks in matplotlib;5901.0
4404;4404;38658669.0;4.0;"<p>I have data with a time-stamp in UTC. I'd like to convert the timezone of this timestamp to 'US/Pacific' and add it as a hierarchical index to a pandas DataFrame. I've been able to convert the timestamp as an Index, but it loses the timezone formatting when I try to add it back into the DataFrame, either as a column or as an index.</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; dat = pd.DataFrame({'label':['a', 'a', 'a', 'b', 'b', 'b'], 'datetime':['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00'], 'value':range(6)})
&gt;&gt;&gt; dat.dtypes
#datetime    object
#label       object
#value        int64
#dtype: object
</code></pre>

<p>Now if I try to convert the Series directly I run into an error.</p>

<pre><code>&gt;&gt;&gt; times = pd.to_datetime(dat['datetime'])
&gt;&gt;&gt; times.tz_localize('UTC')
#Traceback (most recent call last):
#  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize
#    raise Exception('Cannot tz-localize non-time series')
#Exception: Cannot tz-localize non-time series
</code></pre>

<p>If I convert it to an Index then I can manipulate it as a timeseries. Notice that the index now has the Pacific timezone.</p>

<pre><code>&gt;&gt;&gt; times_index = pd.Index(times)
&gt;&gt;&gt; times_index_pacific = times_index.tz_localize('UTC').tz_convert('US/Pacific')
&gt;&gt;&gt; times_index_pacific
#&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]
#Length: 6, Freq: None, Timezone: US/Pacific
</code></pre>

<p>However, now I run into problems adding the index back to the dataframe as it loses its timezone formatting:</p>

<pre><code>&gt;&gt;&gt; dat_index = dat.set_index([dat['label'], times_index_pacific])
&gt;&gt;&gt; dat_index
#                                      datetime label  value
#label                                                      
#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0
#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1
#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2
#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3
#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4
#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5
</code></pre>

<p>You'll notice the index is back on the UTC timezone instead of the converted Pacific timezone.</p>

<p>How can I change the timezone and add it as an index to a DataFrame?</p>
";;3;;2013-06-18T01:11:03.323;2.0;17159207;2016-07-29T12:18:11.103;;;;;599139.0;;1;17;<python><timezone><dataframe><pandas><multi-index>;Change timezone of date-time column in pandas and add as hierarchical index;18265.0
4406;4406;17169776.0;1.0;"<p>I'm using Pandas library for remote sensing time series analysis. Eventually I would like to save my DataFrame to csv by using chunk-sizes, but I run into a little issue. My code generates 6 NumPy arrays that I convert to Pandas Series. Each of these Series contains a lot of items</p>

<pre><code>&gt;&gt;&gt; prcpSeries.shape
(12626172,)
</code></pre>

<p>I would like to add the Series into a Pandas DataFram (df) so I can save them chunk by chunk to a csv file.</p>

<pre><code>d = {'prcp': pd.Series(prcpSeries),
     'tmax': pd.Series(tmaxSeries),
     'tmin': pd.Series(tminSeries),
     'ndvi': pd.Series(ndviSeries),
     'lstm': pd.Series(lstmSeries),
     'evtm': pd.Series(evtmSeries)}

df = pd.DataFrame(d)
outFile ='F:/data/output/run1/_'+str(i)+'.out'
df.to_csv(outFile, header = False, chunksize = 1000)
d = None
df = None
</code></pre>

<p>But my code get stuck at following line giving a Memory Error</p>

<pre><code>df = pd.DataFrame(d)
</code></pre>

<p>Any suggestions? Is it possible to fill the Pandas DataFrame chunk by chunk?</p>
";;7;;2013-06-18T09:32:02.833;3.0;17165340;2015-01-04T18:52:44.037;;;;;2459096.0;;1;15;<python><numpy><pandas>;Using Pandas to create DataFrame with Series, resulting in memory error;47971.0
4427;4427;17194149.0;2.0;"<p>What's the difference between:</p>

<pre><code>Maand['P_Sanyo_Gesloten']
Out[119]: 
Time
2012-08-01 00:00:11    0
2012-08-01 00:05:10    0
2012-08-01 00:10:11    0
2012-08-01 00:20:10    0
2012-08-01 00:25:10    0
2012-08-01 00:30:09    0
2012-08-01 00:40:10    0
2012-08-01 00:50:09    0
2012-08-01 01:05:10    0
2012-08-01 01:10:10    0
2012-08-01 01:15:10    0
2012-08-01 01:25:10    0
2012-08-01 01:30:10    0
2012-08-01 01:35:09    0
2012-08-01 01:40:10    0
...
2012-08-30 22:35:09    0
2012-08-30 22:45:10    0
2012-08-30 22:50:09    0
2012-08-30 22:55:10    0
2012-08-30 23:00:09    0
2012-08-30 23:05:10    0
2012-08-30 23:10:09    0
2012-08-30 23:15:10    0
2012-08-30 23:20:09    0
2012-08-30 23:25:10    0
2012-08-30 23:35:09    0
2012-08-30 23:40:10    0
2012-08-30 23:45:09    0
2012-08-30 23:50:10    0
2012-08-30 23:55:11    0
Name: P_Sanyo_Gesloten, Length: 7413, dtype: int64
</code></pre>

<p>And</p>

<pre><code>Maand[[1]]
Out[120]: 
&amp;ltclass 'pandas.core.frame.DataFrame'&amp;gt
DatetimeIndex: 7413 entries, 2012-08-01 00:00:11 to 2012-08-30 23:55:11
Data columns (total 1 columns):
P_Sanyo_Gesloten    7413  non-null values
dtypes: int64(1)
</code></pre>

<p>How can I get data by column-indexnumber?
And not by an Index-string?</p>
";;0;;2013-06-19T14:24:22.453;3.0;17193850;2013-06-19T14:44:12.147;2013-06-19T14:29:59.093;;489590.0;;2474581.0;;1;11;<python><pandas>;How to get column by number in Pandas?;21660.0
4448;4448;17211698.0;3.0;"<p>I'm still kinda new with Python, using Pandas, and I've got some issues debugging my Python script.</p>

<p>I've got the following warning message :</p>

<pre><code>[...]\pandas\core\index.py:756: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
return self._engine.get_loc(key)
</code></pre>

<p>And can't find where it's from.</p>

<p>After some research, I tried to do that in the Pandas lib file (index.py):</p>

<pre><code>try:
    return self._engine.get_loc(key)
except UnicodeWarning:
    warnings.warn('Oh Non', stacklevel=2)
</code></pre>

<p>But that didn't change anything about the warning message.</p>
";;1;;2013-06-20T08:15:00.040;3.0;17208567;2016-10-26T10:58:38.857;;;;;2452521.0;;1;12;<python><debugging><warnings><pandas>;How to find out where a Python Warning is from;2105.0
4457;4457;17216674.0;2.0;"<p>despite there being at least <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""noreferrer"">two</a> <a href=""http://manishamde.github.io/blog/2013/03/07/pandas-and-python-top-10/#indexing"" rel=""noreferrer"">good</a> tutorials on how to index a DataFrame in Python's <code>pandas</code> library, I still can't work out an elegant way of <code>SELECT</code>ing on more than one column.</p>

<pre><code>&gt;&gt;&gt; d = pd.DataFrame({'x':[1, 2, 3, 4, 5], 'y':[4, 5, 6, 7, 8]})
&gt;&gt;&gt; d
   x  y
0  1  4
1  2  5
2  3  6
3  4  7
4  5  8
&gt;&gt;&gt; d[d['x']&gt;2] # This works fine
   x  y
2  3  6
3  4  7
4  5  8
&gt;&gt;&gt; d[d['x']&gt;2 &amp; d['y']&gt;7] # I had expected this to work, but it doesn't
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>

<p>I have found (what I think is) a rather inelegant way of doing it, like this</p>

<pre><code>&gt;&gt;&gt; d[d['x']&gt;2][d['y']&gt;7]
</code></pre>

<p>But it's not pretty, and it scores fairly low for readability (I think).</p>

<p>Is there a better, more Python-tastic way?</p>
";2013-06-21T11:01:02.780;0;;2013-06-20T14:21:32.137;4.0;17216153;2017-06-15T20:41:52.533;;;;;2071807.0;;1;12;<python><pandas><dataframe>;Python Pandas: Boolean indexing on multiple columns;19384.0
4484;4484;17242374.0;4.0;"<p>Do you know how to get the index column of an dataframe as an array? I have a list of accession numbers in the ""Accession"" column of a CSV file, which I imported into Pandas, and during the import, I set the index to the ""Accession"" column. Now, I need the ""Accession"" column to be a set of labels in a later step, but I don't know how to extract it standalone.</p>
";;0;;2013-06-21T17:25:25.663;18.0;17241004;2017-05-20T07:51:40.650;;;;;1274908.0;;1;105;<python><pandas>;Pandas - how to get the data frame index as an array;158874.0
4488;4488;17243346.0;4.0;"<p>I have a multi-index DataFrame created via a groupby operation.  I'm trying to do a compound sort using several levels of the index, but I can't seem to find a sort function that does what I need.</p>

<p>Initial dataset looks something like this (daily sales counts of various products):</p>

<pre><code>         Date Manufacturer Product Name Product Launch Date  Sales
0  2013-01-01        Apple         iPod          2001-10-23     12
1  2013-01-01        Apple         iPad          2010-04-03     13
2  2013-01-01      Samsung       Galaxy          2009-04-27     14
3  2013-01-01      Samsung   Galaxy Tab          2010-09-02     15
4  2013-01-02        Apple         iPod          2001-10-23     22
5  2013-01-02        Apple         iPad          2010-04-03     17
6  2013-01-02      Samsung       Galaxy          2009-04-27     10
7  2013-01-02      Samsung   Galaxy Tab          2010-09-02      7
</code></pre>

<p>I use groupby to get a sum over the date range:</p>

<pre><code>&gt; grouped = df.groupby(['Manufacturer', 'Product Name', 'Product Launch Date']).sum()
                                               Sales
Manufacturer Product Name Product Launch Date       
Apple        iPad         2010-04-03              30
             iPod         2001-10-23              34
Samsung      Galaxy       2009-04-27              24
             Galaxy Tab   2010-09-02              22
</code></pre>

<p>So far so good!</p>

<p>Now the last thing I want to do is sort each manufacturer's products by launch date, but keep them grouped hierarchically under Manufacturer - here's all I am trying to do:</p>

<pre><code>                                               Sales
Manufacturer Product Name Product Launch Date       
Apple        iPod         2001-10-23              34
             iPad         2010-04-03              30
Samsung      Galaxy       2009-04-27              24
             Galaxy Tab   2010-09-02              22
</code></pre>

<p>When I try sortlevel() I lose the nice per-company hierarchy I had before:</p>

<pre><code>&gt; grouped.sortlevel('Product Launch Date')
                                               Sales
Manufacturer Product Name Product Launch Date       
Apple        iPod         2001-10-23              34
Samsung      Galaxy       2009-04-27              24
Apple        iPad         2010-04-03              30
Samsung      Galaxy Tab   2010-09-02              22
</code></pre>

<p>sort() and sort_index() just fail:</p>

<pre><code>grouped.sort(['Manufacturer','Product Launch Date'])
KeyError: u'no item named Manufacturer'

grouped.sort_index(by=['Manufacturer','Product Launch Date'])
KeyError: u'no item named Manufacturer'
</code></pre>

<p>Seems like a simple operation, but I can't quite figure it out.</p>

<p>I'm not tied to using a MultiIndex for this, but since that's what groupby() returns, that's what I've been working with.</p>

<p>BTW the code to produce the initial DataFrame is:</p>

<pre><code>data = {
  'Date': ['2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-02', '2013-01-02', '2013-01-02', '2013-01-02'],
  'Manufacturer' : ['Apple', 'Apple', 'Samsung', 'Samsung', 'Apple', 'Apple', 'Samsung', 'Samsung',],
  'Product Name' : ['iPod', 'iPad', 'Galaxy', 'Galaxy Tab', 'iPod', 'iPad', 'Galaxy', 'Galaxy Tab'], 
  'Product Launch Date' : ['2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02','2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02'],
  'Sales' : [12, 13, 14, 15, 22, 17, 10, 7]
}
df = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])
</code></pre>
";;1;;2013-06-21T19:32:03.730;9.0;17242970;2015-10-09T20:46:03.137;2013-06-21T19:38:11.840;;20588.0;;20588.0;;1;12;<python><sorting><pandas><multi-index>;Multi-Index Sorting in Pandas;10275.0
4500;4500;17244095.0;2.0;"<p>I have a pandas dataframe:</p>

<pre><code>import pandas as pnd
d = pnd.Timestamp('2013-01-01 16:00')
dates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)

df = pnd.DataFrame(index=dates, columns=['a'])
df['a'] = 6

print(df)
                     a
2013-01-01 16:00:00  6
2013-01-02 16:00:00  6
2013-01-03 16:00:00  6
2013-01-04 16:00:00  6
2013-01-07 16:00:00  6
2013-01-08 16:00:00  6
2013-01-09 16:00:00  6
2013-01-10 16:00:00  6
2013-01-11 16:00:00  6
</code></pre>

<p>I am interested in find the label location of one of the labels, say,</p>

<pre><code>ds = pnd.Timestamp('2013-01-02 16:00')
</code></pre>

<p>Looking at the index values, I know that is integer location of this label 1. How can get pandas to tell what the integer value of this label is?</p>
";;2;;2013-06-21T20:46:40.977;4.0;17244049;2016-12-09T23:09:35.400;2016-12-09T23:09:35.400;;1240268.0;;1257953.0;;1;11;<python><pandas>;Finding label location in a DataFrame Index;14603.0
4526;4526;17287046.0;1.0;"<p>I am trying to classify my data in percentile buckets based on their values. My data looks like, </p>

<pre><code>a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])
a.data = np.random.randn(10)
print a
print '\nthese are ranked as shown'
print a.rank()

       data
a -0.310188
b -0.191582
c  0.860467
d -0.458017
e  0.858653
f -1.640166
g -1.969908
h  0.649781
i  0.218000
j  1.887577

these are ranked as shown
   data
a     4
b     5
c     9
d     3
e     8
f     2
g     1
h     7
i     6
j    10
</code></pre>

<p>To rank this data, I am using the rank function. However, I am interested in the creating a bucket of the top 20%. In the example shown above, this would be a list containing labels ['c', 'j']</p>

<pre><code>desired result : ['c','j']
</code></pre>

<p>How do I get the desired result</p>
";;0;;2013-06-24T23:17:22.450;5.0;17286672;2013-06-24T23:55:58.377;;;;;1257953.0;;1;17;<python><pandas>;Creating percentile buckets in pandas;8866.0
4538;4538;17298454.0;1.0;"<p>I have the following dataframe:</p>

<pre><code>Year    Country          medal    no of medals
1896    Afghanistan      Gold        5
1896    Afghanistan      Silver      4
1896    Afghanistan      Bronze      3
1896    Algeria          Gold        1
1896    Algeria          Silver      2
1896    Algeria          Bronze      3
</code></pre>

<p>I want it this way.</p>

<pre><code>Year    Country      Gold   Silver   Bronze
1896    Afghanistan    5      4         3
1896    Algeria        1      2         3
</code></pre>

<p>Stack/Unstack dont seem to work.</p>
";;0;;2013-06-25T13:12:10.197;11.0;17298313;2013-06-25T13:18:36.270;;;;;1948860.0;;1;19;<python><pandas>;Python Pandas: Convert Rows as Column headers;10395.0
4556;4556;17315875.0;4.0;"<p>I have a large dataframe with 423244 lines. I want to split this in to 4. I tried the following code which gave an error? <code>ValueError: array split does not result in an equal division</code></p>

<pre><code>for item in np.split(df, 4):
    print item
</code></pre>

<p>How to split this dataframe in to 4 groups?</p>
";;0;;2013-06-26T09:01:24.063;3.0;17315737;2017-07-12T10:06:30.077;;;;;461436.0;;1;15;<python><pandas>;Split a large pandas dataframe;20454.0
4558;4558;17347945.0;3.0;"<p>I have a vanilla pandas dataframe with an index. I need to check if the index is sorted. Preferably without sorting it again.</p>

<p>e.g. I can test an index to see if it is unique by index.is_unique() is there a similar way for testing sorted?</p>
";;4;;2013-06-26T09:07:27.710;2.0;17315881;2014-12-03T16:56:00.183;;;;;110478.0;;1;17;<python><pandas>;How can I check if a Pandas dataframe's index is sorted;3592.0
4566;4566;17322585.0;1.0;"<p>I want to get the count of dataframe rows based on conditional selection. I tried the following code.</p>

<pre><code>print df[(df.IP == head.idxmax()) &amp; (df.Method == 'HEAD') &amp; (df.Referrer == '""-""')].count()
</code></pre>

<p>output:</p>

<pre><code>IP          57
Time        57
Method      57
Resource    57
Status      57
Bytes       57
Referrer    57
Agent       57
dtype: int64
</code></pre>

<p>The output shows the count for each an every column in the dataframe. Instead I need to get a single count where all of the above conditions satisfied? How to do this? If you need more explanation about my dataframe please let me know.</p>
";;0;;2013-06-26T13:56:02.770;2.0;17322109;2013-06-26T14:14:12.050;;;;;461436.0;;1;11;<python><pandas>;get dataframe row count based on conditions;16817.0
4574;4574;;4.0;"<p>I am being asked to generate some Excel reports. I am currently using pandas quite heavily for my data, so naturally I would like to use the pandas.ExcelWriter method to generate these reports.  However the fixed column widths are a problem.   </p>

<p>The code I have so far is simple enough.  Say I have a dataframe called 'df':</p>

<pre><code>writer = pd.ExcelWriter(excel_file_path)
df.to_excel(writer, sheet_name=""Summary"")
</code></pre>

<p>I was looking over the pandas code, and I don't really see any options to set column widths.  Is there a trick out there in the universe to make it such that the columns auto-adjust to the data? Or is there something I can do after the fact to the xlsx file to adjust the column widths? </p>

<p>(I am using the OpenPyXL library, and generating .xlsx files - if that makes any difference.)</p>

<p>Thank you.</p>
";;4;;2013-06-26T17:44:08.363;7.0;17326973;2016-11-10T19:23:47.110;2013-06-27T13:48:12.123;;152018.0;;152018.0;;1;21;<python><excel><pandas><openpyxl>;Is there a way to auto-adjust Excel column widths with pandas.ExcelWriter?;11512.0
4635;4635;17383140.0;4.0;"<p>I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?</p>

<p>EDIT:
The answers below do not seem to hold in the case of numpy that, given an array with both integers and True/False values, returns <code>dtype=object</code> on such array. In order to proceed with further calculations in numpy, I had to set explicitly <code>np_values    = np.array(df.values, dtype = np.float64)</code>.</p>
";;1;;2013-06-29T17:53:19.490;3.0;17383094;2016-06-05T21:54:35.933;2013-06-30T18:05:27.977;;2107632.0;;2107632.0;;1;30;<python><numpy><pandas>;python pandas/numpy True/False to 1/0 mapping;27133.0
4682;4682;17426500.0;2.0;"<p>What is the most efficient way to organise the following pandas Dataframe:</p>

<p>data =</p>

<pre><code>Position    Letter
1           a
2           b
3           c
4           d
5           e
</code></pre>

<p>into a dictionary like <code>alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']</code>?</p>
";;0;;2013-07-02T12:58:45.367;6.0;17426292;2016-09-01T15:56:11.733;;;;;1083734.0;;1;19;<python><dictionary><pandas><dataframe>;What is the most efficient way to create a dictionary of two pandas Dataframe columns?;12430.0
4701;4701;17439693.0;2.0;"<p>I have a DataFrame with an index called <code>city_id</code> of cities in the format <code>[city],[state]</code> (e.g., <code>new york,ny</code> containing integer counts in the columns. The problem is that I have multiple rows for the same city, and I want to collapse the rows sharing a <code>city_id</code> by adding their column values. I looked at <code>groupby()</code> but it wasn't immediately obvious how to apply it to this problem.</p>

<p>Edit:</p>

<p>An example: I'd like to change this:</p>

<pre><code>city_id    val1 val2 val3
houston,tx    1    2    0
houston,tx    0    0    1
houston,tx    2    1    1
</code></pre>

<p>into this:</p>

<pre><code>city_id    val1 val2 val3
houston,tx    3    3    2
</code></pre>

<p>if there are ~10-20k rows.</p>
";2013-07-04T13:54:20.563;1;;2013-07-03T02:51:45.803;9.0;17438906;2013-07-03T18:33:13.883;2013-07-03T18:33:13.883;;19679.0;;2337204.0;;1;30;<python><pandas>;Combining rows in pandas;26878.0
4744;4744;17468012.0;4.0;"<p>Today I was positively surprised by the fact that while reading data from a data file (for example) pandas is able to recognize types of values:</p>

<pre><code>df = pandas.read_csv('test.dat', delimiter=r""\s+"", names=['col1','col2','col3'])
</code></pre>

<p>For example it can be checked in this way:</p>

<pre><code>for i, r in df.iterrows():
    print type(r['col1']), type(r['col2']), type(r['col3'])
</code></pre>

<p>In particular integer, floats and strings were recognized correctly. However, I have a column that has dates in the following format: <code>2013-6-4</code>. These dates were recognized as strings (not as python date-objects). Is there a way to ""learn"" pandas to recognized dates?</p>
";;0;;2013-07-04T08:08:39.807;29.0;17465045;2017-04-10T02:46:30.413;;;;;245549.0;;1;40;<python><date><types><dataframe><pandas>;Can pandas automatically recognize dates?;49798.0
4757;4757;17478495.0;4.0;"<p>what is the quickest/simplest way to drop nan and inf/-inf values from a pandas DataFrame without resetting <code>mode.use_inf_as_null</code>? I'd like to be able to use the <code>subset</code> and <code>how</code> arguments of <code>dropna</code>, except with <code>inf</code> values considered missing, like:</p>

<pre><code>df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)
</code></pre>

<p>is this possible? Is there a way to tell <code>dropna</code> to include <code>inf</code> in its definition of missing values?</p>
";;0;;2013-07-04T20:55:20.863;13.0;17477979;2017-08-17T23:10:32.467;;;;;248237.0;;1;60;<python><numpy><scipy><pandas>;dropping infinite values from dataframes in pandas?;45786.0
4815;4815;17531025.0;4.0;"<p>I want to know if it is possible to use the pandas <code>to_csv()</code> function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data. </p>
";;1;;2013-07-08T15:33:46.973;22.0;17530542;2017-06-17T00:26:37.633;2017-03-20T11:50:56.293;;3923281.0;;2355050.0;;1;55;<csv><pandas>;How to add pandas data to an existing csv file?;48770.0
4822;4822;17534682.0;4.0;"<p>I am reading two columns of a csv file using pandas <code>readcsv()</code> and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be <code>None</code> but instead <code>nan</code> is assigned. Surely <code>None</code> is more descriptive of an empty cell as it has a null value, whereas <code>nan</code> just says that the value read is not a number.</p>

<p>Is my understanding correct, what IS the difference between <code>None</code> and <code>nan</code>? Why is <code>nan</code> assigned instead of <code>None</code>?</p>

<p>Also, my dictionary check for any empty cells has been using <code>numpy.isnan()</code>:</p>

<pre><code>for k, v in my_dict.iteritems():
    if np.isnan(v):
</code></pre>

<p>But this gives me an error saying that I cannot use this check for <code>v</code>. I guess it is because an integer or float variable, not a string is meant to be used. If this is true, how can I check <code>v</code> for an ""empty cell""/<code>nan</code> case?</p>
";;3;;2013-07-08T19:06:17.690;16.0;17534106;2014-10-26T17:50:06.163;2014-10-02T01:09:33.327;;12892.0;;1083734.0;;1;43;<python><numpy><pandas><nan>;What is the difference between NaN and None?;23404.0
4850;4850;;5.0;"<p>I am trying to do something fairly simple, reading a large csv file into a pandas dataframe.</p>

<pre><code>data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)
</code></pre>

<p>The code either fails with a <code>MemoryError</code>, or just never finishes.</p>

<p>Mem usage in the task manager stopped at 506 Mb and after 5 minutes of no change and no CPU activity in the process I stopped it.</p>

<p>I am using pandas version 0.11.0.</p>

<p>I am aware that there used to be a memory problem with the file parser, but according to <strong><a href=""http://wesmckinney.com/blog/?p=543"">http://wesmckinney.com/blog/?p=543</a></strong> this should have been fixed.</p>

<p>The file I am trying to read is 366 Mb, the code above works if I cut the file down to something short (25 Mb).</p>

<p>It has also happened that I get a pop up telling me that it can't write to address 0x1e0baf93... </p>

<p>Stacktrace:</p>

<pre><code>Traceback (most recent call last):
  File ""F:\QA ALM\Python\new WIM data\new WIM data\new_WIM_data.py"", line 25, in
 &lt;module&gt;
    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2
)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 401, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 216, in _read
    return parser.read()
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 643, in read
    df = DataFrame(col_dict, columns=columns, index=index)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 394, in __init__
    mgr = self._init_dict(data, index, columns, dtype=dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 525, in _init_dict
    dtype=dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 5338, in _arrays_to_mgr
    return create_block_manager_from_arrays(arrays, arr_names, axes)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1820, in create_block_manager_from_arrays
    blocks = form_blocks(arrays, names, axes)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1872, in form_blocks
    float_blocks = _multi_blockify(float_items, items)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1930, in _multi_blockify
    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1962, in _stack_arrays
    stacked = np.empty(shape, dtype=dtype)
MemoryError
Press any key to continue . . .
</code></pre>

<hr>

<p>A bit of background - I am trying to convince people that Python can do the same as R. For this I am trying to replicate an R script that does</p>

<pre><code>data &lt;- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)
</code></pre>

<p>R not only manages to read the above file just fine, it even reads several of these files in a for loop (and then does some stuff with the data). If Python does have a problem with files of that size I might be fighting a loosing battle...</p>
";;17;;2013-07-09T19:57:49.667;9.0;17557074;2017-02-16T08:58:04.550;2015-07-21T14:34:03.637;;236195.0;;2565842.0;;1;61;<python><windows><pandas>;Memory error when using pandas read_csv;17699.0
4876;4876;;2.0;"<p>First, I create a DataFrame</p>

<pre><code>In [61]: import pandas as pd
In [62]: df = pd.DataFrame([[1], [2], [3]])
</code></pre>

<p>Then, I deeply copy it by <code>copy</code></p>

<p>In [63]: df2 = df.copy(deep=True)</p>

<p>Now the <code>DataFrame</code> are different.</p>

<pre><code>In [64]: id(df), id(df2)
Out[64]: (4385185040, 4385183312)
</code></pre>

<p>However, the <code>index</code> are still the same.</p>

<pre><code>In [65]: id(df.index), id(df2.index)
Out[65]: (4385175264, 4385175264)
</code></pre>

<p>Same thing happen in columns, is there any way that I can easily deeply copy it not only values but also index and columns?</p>
";;1;;2013-07-11T10:31:56.360;5.0;17591104;2014-08-08T14:52:55.733;;;;;1426056.0;;1;16;<python><pandas>;In pandas, can I deeply copy a DataFrame including its index and column?;14921.0
4898;4898;;6.0;"<p>I have the following data frame:</p>

<pre><code>df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])
</code></pre>

<p>Or, in human readable form:</p>

<pre><code>   c1   c2
0   3   10
1   2   30
2   1   20
3   2   15
4   2  100
</code></pre>

<p>The following sorting-command works as expected:</p>

<pre><code>df.sort(['c1','c2'], ascending=False)
</code></pre>

<p>Output:</p>

<pre><code>   c1   c2
0   3   10
4   2  100
1   2   30
3   2   15
2   1   20
</code></pre>

<p>But the following command:</p>

<pre><code>df.sort(['c1','c2'], ascending=[False,True])
</code></pre>

<p>results in</p>

<pre><code>   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>

<p>and this is not what I expect. I expect to have the values in the first column ordered from largest to smallest, and if there are identical values in the first column, order by the ascending values from the second column.</p>

<p>Does anybody know why it does not work as expected?</p>

<p><strong>ADDED</strong></p>

<p>This is copy-paste:</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>
";;1;;2013-07-12T15:54:28.233;11.0;17618981;2017-08-16T23:15:46.740;2013-07-12T18:15:35.183;;168868.0;;245549.0;;1;35;<python><sorting><dataframe><pandas>;How to sort pandas data frame using values from several columns?;82354.0
4913;4913;39104306.0;5.0;"<p>Given a sparse matrix listing, what's the best way to calculate the cosine similarity between each of the columns (or rows) in the matrix? I would rather not iterate n-choose-two times.</p>

<p>Say the input matrix is:</p>

<pre><code>A= 
[0 1 0 0 1
 0 0 1 1 1
 1 1 0 1 0]
</code></pre>

<p>The sparse representation is:</p>

<pre><code>A = 
0, 1
0, 4
1, 2
1, 3
1, 4
2, 0
2, 1
2, 3
</code></pre>

<p>In Python, it's straightforward to work with the matrix-input format:</p>

<pre><code>import numpy as np
from sklearn.metrics import pairwise_distances
from scipy.spatial.distance import cosine

A = np.array(
[[0, 1, 0, 0, 1],
[0, 0, 1, 1, 1],
[1, 1, 0, 1, 0]])

dist_out = 1-pairwise_distances(A, metric=""cosine"")
dist_out
</code></pre>

<p>Gives:</p>

<pre><code>array([[ 1.        ,  0.40824829,  0.40824829],
       [ 0.40824829,  1.        ,  0.33333333],
       [ 0.40824829,  0.33333333,  1.        ]])
</code></pre>

<p>That's fine for a full-matrix input, but I really want to start with the sparse representation (due to the size and sparsity of my matrix). Any ideas about how this could best be accomplished? Thanks in advance.</p>
";;3;;2013-07-13T05:18:07.833;19.0;17627219;2017-05-15T14:38:28.897;2015-11-03T16:02:03.180;;74291.0;;601059.0;;1;29;<python><numpy><pandas><similarity><cosine-similarity>;What's the fastest way in Python to calculate cosine similarity given sparse matrix data?;33178.0
4921;4921;17645475.0;1.0;"<p>Is there a way to generate time range in pandas similar to date_range?
something like:</p>

<pre><code>pandas.time_range(""11:00"", ""21:30"", freq=""30min"")
</code></pre>
";;0;;2013-07-14T21:45:51.793;3.0;17644100;2013-07-15T07:58:26.103;2013-07-15T07:58:26.103;;1252759.0;;1588032.0;;1;11;<python><time><pandas><range>;Create hourly/minutely time range using pandas;10401.0
4947;4947;17666287.0;3.0;"<p>I have the following python pandas data frame:</p>

<pre><code>df = pd.DataFrame( {
   'A': [1,1,1,1,2,2,2,3,3,4,4,4],
   'B': [5,5,6,7,5,6,6,7,7,6,7,7],
   'C': [1,1,1,1,1,1,1,1,1,1,1,1]
    } );

df
    A  B  C
0   1  5  1
1   1  5  1
2   1  6  1
3   1  7  1
4   2  5  1
5   2  6  1
6   2  6  1
7   3  7  1
8   3  7  1
9   4  6  1
10  4  7  1
11  4  7  1
</code></pre>

<p>I would like to have another column storing a value of a sum over C values for fixed (both) A and B. That is, something like:</p>

<pre><code>    A  B  C  D
0   1  5  1  2
1   1  5  1  2
2   1  6  1  1
3   1  7  1  1
4   2  5  1  1
5   2  6  1  2
6   2  6  1  2
7   3  7  1  2
8   3  7  1  2
9   4  6  1  1
10  4  7  1  2
11  4  7  1  2
</code></pre>

<p>I have tried with pandas <code>groupby</code> and it kind of works:</p>

<pre><code>res = {}
for a, group_by_A in df.groupby('A'):
    group_by_B = group_by_A.groupby('B', as_index = False)
    res[a] = group_by_B['C'].sum()
</code></pre>

<p>but I don't know how to 'get' the results from <code>res</code> into <code>df</code> in the orderly fashion. Would be very happy with any advice on this. Thank you. </p>
";;0;;2013-07-16T00:24:25.113;4.0;17666075;2015-12-30T14:54:02.467;;;;;2107632.0;;1;14;<python><group-by><pandas>;python pandas groupby() result;9112.0
4965;4965;17679980.0;5.0;"<p>I have a pandas dataframe in the following format:</p>

<pre><code>df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T
df.columns = ['col1','col2','col3','col4','col5']
</code></pre>

<p>df:</p>

<pre><code>   col1 col2 col3     col4 col5
0   1.1    A  1.1    x/y/z    1
1   1.1    A  1.7      x/y    3
2   1.1    A  2.5  x/y/z/n    3
3   2.6    B  2.6      x/u    2
4   2.5    B  3.3        x    4
5   3.4    B  3.8    x/u/v    2
6   2.6    B    4    x/y/z    5
7   2.6    A  4.2        x    3
8   3.4    B  4.3  x/u/v/b    6
9   3.4    C  4.5        -    3
10  2.6    B  4.6      x/y    5
11  1.1    D  4.7    x/y/z    1
12  1.1    D  4.7        x    1
13  3.3    D  4.8  x/u/v/w    1
</code></pre>

<p>Now I want to group this by two columns like following:</p>

<pre><code>df.groupby(['col5','col2']).reset_index()
</code></pre>

<p>OutPut:</p>

<pre><code>             index col1 col2 col3     col4 col5
col5 col2                                      
1    A    0      0  1.1    A  1.1    x/y/z    1
     D    0     11  1.1    D  4.7    x/y/z    1
          1     12  1.1    D  4.7        x    1
          2     13  3.3    D  4.8  x/u/v/w    1
2    B    0      3  2.6    B  2.6      x/u    2
          1      5  3.4    B  3.8    x/u/v    2
3    A    0      1  1.1    A  1.7      x/y    3
          1      2  1.1    A  2.5  x/y/z/n    3
          2      7  2.6    A  4.2        x    3
     C    0      9  3.4    C  4.5        -    3
4    B    0      4  2.5    B  3.3        x    4
5    B    0      6  2.6    B    4    x/y/z    5
          1     10  2.6    B  4.6      x/y    5
6    B    0      8  3.4    B  4.3  x/u/v/b    6
</code></pre>

<p>I want to get the count by each row like following.
Expected Output:</p>

<pre><code>col5 col2 count
1    A      1
     D      3
2    B      2
etc...
</code></pre>

<p>How to get my expected output? And I want to find largest count for each 'col2' value?</p>
";;1;;2013-07-16T14:19:44.310;12.0;17679089;2017-06-06T12:20:15.123;;;;;461436.0;;1;41;<python><pandas><dataframe>;Pandas DataFrame Groupby two columns and get counts;65459.0
4971;4971;17682726.0;3.0;"<p>I'm wondering if there is a simpler, memory efficient way to select a subset of rows and columns from a pandas DataFrame.</p>

<p>For instance, given this dataframe:</p>

<pre>
df = DataFrame(np.random.rand(4,5), columns = list('abcde'))
print df

          a         b         c         d         e
0  0.945686  0.000710  0.909158  0.892892  0.326670
1  0.919359  0.667057  0.462478  0.008204  0.473096
2  0.976163  0.621712  0.208423  0.980471  0.048334
3  0.459039  0.788318  0.309892  0.100539  0.753992
</pre>

<p>I want only those rows in which the value for column 'c' is greater than 0.5, but I only need columns 'b' and 'e' for those rows.</p>

<p>This is the method that I've come up with - perhaps there is a better ""pandas"" way?</p>

<pre>
locs = [df.columns.get_loc(_) for _ in ['a', 'd']]
print df[df.c > 0.5][locs]

          a         d
0  0.945686  0.892892
</pre>

<p>My final goal is to convert the result to a numpy array to pass into an sklearn regression algorithm, so I will use the code above like this:</p>

<pre>
training_set = array(df[df.c > 0.5][locs])
</pre>

<p>... and that peeves me since I end up with a huge array copy in memory. Perhaps there's a better way for that too?</p>
";;0;;2013-07-16T16:56:52.513;21.0;17682613;2015-07-21T16:59:23.477;2013-08-13T11:49:08.463;;832621.0;;159252.0;;1;41;<python><arrays><numpy><pandas><scikit-learn>;How to convert a pandas DataFrame subset of columns AND rows into a numpy array?;99966.0
4983;4983;17690795.0;2.0;"<pre><code>import pandas as pd
date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')

a = pd.Series(range(4),index = (range(4)))

for idx, date in enumerate(date_stngs):
    a[idx]= pd.to_datetime(date)
</code></pre>

<p>This code bit produces error: </p>

<blockquote>
  <p>TypeError:"" 'int' object is not iterable""</p>
</blockquote>

<p>Can anyone tell me how to get this series of date time strings into a DataFrame as <code>DateTime</code> objects?</p>
";;0;;2013-07-17T03:46:44.860;5.0;17690738;2016-08-09T14:01:24.770;2016-08-09T14:01:24.770;;5907870.0;;313558.0;;1;41;<python><datetime><pandas>;In Pandas how do I convert a string of date strings to datetime objects and put them in a DataFrame?;50026.0
4986;4986;17692156.0;2.0;"<p>I have a Pandas DataFrame like following:</p>

<pre><code>               A              B              C
0   192.168.2.85   192.168.2.85  124.43.113.22
1  192.248.8.183  192.248.8.183   192.168.2.85
2  192.168.2.161            NaN  192.248.8.183
3   66.249.74.52            NaN  192.168.2.161
4            NaN            NaN   66.249.74.52
</code></pre>

<p>I want to get the count of a certain values across columns. So my expected output is something like:</p>

<pre><code>IP          Count
192.168.2.85 3 #Since this value is there in all coulmns
192.248.8.183 3
192.168.2.161 2
66.249.74.52 2
124.43.113.22 1
</code></pre>

<p>I know how to this across rows, but doing this for columns is bit strange?Help me to solve this? Thanks.</p>
";;0;;2013-07-17T04:58:55.913;7.0;17691447;2015-07-06T19:40:00.067;;;;;461436.0;;1;22;<python><pandas><dataframe>;Get count of values across columns-Pandas DataFrame;21387.0
4995;4995;17702833.0;3.0;"<p>I have a DataFrame named <code>df</code> as</p>

<pre><code>  Order Number       Status
1         1668  Undelivered
2        19771  Undelivered
3    100032108  Undelivered
4         2229    Delivered
5        00056  Undelivered
</code></pre>

<p>I would like to convert the <code>Status</code> column to boolean (<code>True</code> when Status is Delivered and <code>False</code> when Status is Undelivered)
but if Status is neither 'Undelivered' neither 'Delivered' it should be considered as <code>NotANumber</code> or something like that.</p>

<p>I would like to use a dict</p>

<pre><code>d = {
  'Delivered': True,
  'Undelivered': False
}
</code></pre>

<p>so I could easily add other string which could be either considered as <code>True</code> or <code>False</code>.</p>
";;0;;2013-07-17T14:18:00.890;2.0;17702272;2017-03-19T07:35:20.040;2016-11-17T06:51:59.967;;202229.0;;2051311.0;;1;11;<python><pandas><boolean><type-conversion><series>;Convert Pandas series containing string to boolean;8843.0
5013;5013;;2.0;"<p>I am more familiar with R but I wanted to see if there was a way to do this in pandas. I want to create a count of unique values from one of my dataframe columns and then add a new column with those counts to my original data frame. I've tried a couple different things. I created a pandas series and then calculated counts with the value_counts method. I tried to merge these values back to my original dataframe, but I the keys that I want to merge on are in the Index(ix/loc). Any suggestions or solutions would be appreciated</p>

<pre><code>Color Value
Red   100
Red   150
Blue  50
</code></pre>

<p>and I wanted to return something like</p>

<pre><code>Color Value Counts
Red   100   2
Red   150   2 
Blue  50    1
</code></pre>
";;1;;2013-07-17T20:08:43.323;12.0;17709270;2016-07-05T14:38:08.647;2013-07-17T20:18:43.573;;1649780.0;;2592989.0;;1;20;<python><merge><pandas>;I want to create a column of value_counts in my pandas dataframe;29277.0
5016;5016;;10.0;"<p>I just installed pandas and statsmodels package on my python 2.7
When I tried  ""import pandas as pd"", this error message comes out.
Can anyone help? Thanks!!!</p>

<pre><code>numpy.dtype has the wrong size, try recompiling
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\formula\__init__.py"",
line 4, in &lt;module&gt;
    from formulatools import handle_formula_data
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\formula\formulatools.p
y"", line 1, in &lt;module&gt;
    import statsmodels.tools.data as data_util
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\tools\__init__.py"", li
ne 1, in &lt;module&gt;
    from tools import add_constant, categorical
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\tools\tools.py"", line
14, in &lt;module&gt;
    from pandas import DataFrame
  File ""C:\analytics\ext\python27\lib\site-packages\pandas\__init__.py"", line 6, in &lt;module&gt;
    from . import hashtable, tslib, lib
  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\tslib.c:49133)
ValueError: numpy.dtype has the wrong size, try recompiling
</code></pre>
";;7;;2013-07-17T20:30:55.710;12.0;17709641;2016-09-17T10:35:07.847;2013-07-17T20:34:23.830;;416467.0;;2583769.0;;1;79;<python><numpy><install><pandas><statsmodels>;ValueError: numpy.dtype has the wrong size, try recompiling;64914.0
5020;5020;17712440.0;1.0;"<p>I have a dataframe in Pandas, I would like to sort its columns (i.e. get a new dataframe, or a view) according to the mean value of its columns (or e.g. by their std value). The documentation talks about <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#sorting-by-index-and-value"" rel=""noreferrer"">sorting by label or value</a>, but I could not find anything on custom sorting methods.</p>

<p>How can I do this? </p>
";;0;;2013-07-17T23:44:15.973;2.0;17712163;2016-12-09T19:11:37.160;2013-07-17T23:50:43.373;;283296.0;;283296.0;;1;11;<python><pandas>;Pandas: Sorting columns by their mean value;2276.0
5025;5025;17729985.0;2.0;"<p>this is a rather similar question to <a href=""https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe"" title=""Set value for particular cell in pandas DataFrame"">this question</a> but with one key difference: I'm selecting the data I want to change not by its index but by some criteria.</p>

<p>If the criteria I apply return a single row, I'd expect to be able to set the value of a certain column in that row in an easy way, but my first attempt doesn't work:</p>

<pre><code>&gt;&gt;&gt; d = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], 
...                   'flavour':['strawberry','strawberry','banana','banana',
...                   'strawberry','strawberry','banana','banana'],
...                   'day':['sat','sun','sat','sun','sat','sun','sat','sun'],
...                   'sales':[10,12,22,23,11,13,23,24]})

&gt;&gt;&gt; d
   day     flavour  sales  year
0  sat  strawberry     10  2008
1  sun  strawberry     12  2008
2  sat      banana     22  2008
3  sun      banana     23  2008
4  sat  strawberry     11  2009
5  sun  strawberry     13  2009
6  sat      banana     23  2009
7  sun      banana     24  2009

&gt;&gt;&gt; d[d.sales==24]
   day flavour  sales  year
7  sun  banana     24  2009

&gt;&gt;&gt; d[d.sales==24].sales = 100
&gt;&gt;&gt; d
   day     flavour  sales  year
0  sat  strawberry     10  2008
1  sun  strawberry     12  2008
2  sat      banana     22  2008
3  sun      banana     23  2008
4  sat  strawberry     11  2009
5  sun  strawberry     13  2009
6  sat      banana     23  2009
7  sun      banana     24  2009
</code></pre>

<p>So rather than setting 2009 Sunday's Banana sales to 100, nothing happens! What's the nicest way to do this? Ideally the solution should use the row number, as you normally don't know that in advance!</p>

<p>Many thanks in advance,
Rob</p>
";;0;;2013-07-18T17:08:17.710;11.0;17729853;2015-07-29T07:04:00.973;2017-05-23T11:47:20.273;;-1.0;;2071807.0;;1;20;<python><pandas>;Set value for a selected cell in pandas DataFrame;29305.0
5044;5044;;2.0;"<p>I have a user defined number which I want to compare to a certain column of a dataframe. </p>

<p>I would like to return the rows of a dataframe which contain (in a certain column of df, say, df.num) the 5 closest numbers to the given number x. </p>

<p>Any suggestions for the best way to do this without loops would be greatly appreciated. </p>
";;0;;2013-07-20T02:18:41.027;5.0;17758023;2015-12-03T20:27:05.043;2013-07-20T02:25:11.693;;1252759.0;;1374969.0;;1;12;<python><python-2.7><pandas>;return rows in a dataframe closest to a user-defined number;7922.0
5047;5047;17779230.0;3.0;"<p>First off, I'm a novice... I'm a newbie to Python, pandas, and Linux.  </p>

<p>I'm getting some errors when trying to populate a DataFrame (sql.read_frame() gives an exception when trying to read from my MySQL DB, but I am able to execute and fetch a query / stored proc).  I noticed that pandas is at version 0.7.0, and running ""sudo apt-get install python-pandas"" just says that it's up to date (no errors): ""... python-pandas is already the newest version. 0 upgraded...""</p>

<p>Based on some other posts I found on the web, I think my DataFrame problem may be due to the older version of pandas (something about a pandas bug involving tuples of tuples?).  Why won't pandas update to a more current version? </p>

<p>Setup:</p>

<pre><code>Ubuntu: 12.04.2 LTS Desktop (virtual workstation on VMWare)
sudo apt-get update, sudo apt-get upgrade, and sudo apt-get dist-upgrade all current
Python: 2.7.3 (default, April 10 2013, 06:20:15) /n [GCC 4.6.3] on Linux2
$ ""which python"" only show a single instance: /usr/bin/python
pandas.__version__ = 0.7.0
numpy.__version__ = 1.6.1
</code></pre>

<p>I tried installing Anaconda previously, but that turned into a big nightmare, with conflicting versions of Python.  I finally rolled back to previous VM snapshot and started over, installing all of the MySQL, pandas, and iPython using apt-get on the individual packages.  </p>

<p>I'm not having any other problems on this workstation... apt-get seems to be working fine in general, and all other apps (MySQL Workbench, Kettle / spoon, etc.) are all working properly and up to date.  </p>

<p>Any ideas why Python pandas won't upgrade to 0.11.0?  Thank you.</p>
";;2;;2013-07-20T05:48:46.533;4.0;17759128;2016-11-22T18:50:53.570;;;;;1778335.0;;1;15;<python><pandas>;Python pandas stuck at version 0.7.0;12366.0
5064;5064;17777078.0;3.0;"<p>I come from a sql background and I use the following data processing step frequently:</p>

<ol>
<li>Partition the table of data by one or more fields</li>
<li>For each partition, add a rownumber to each of its rows that ranks the row by one or more other fields, where the analyst specifies ascending or descending</li>
</ol>

<p>EX:  </p>

<pre><code>df = pd.DataFrame({'key1' : ['a','a','a','b','a'],
           'data1' : [1,2,2,3,3],
           'data2' : [1,10,2,3,30]})
df
     data1        data2     key1    
0    1            1         a           
1    2            10        a        
2    2            2         a       
3    3            3         b       
4    3            30        a        
</code></pre>

<p>I'm looking for how to do the PANDAS equivalent to this sql window function:</p>

<pre><code>RN = ROW_NUMBER() OVER (PARTITION BY Key1, Key2 ORDER BY Data1 ASC, Data2 DESC)


    data1        data2     key1    RN
0    1            1         a       1    
1    2            10        a       2 
2    2            2         a       3
3    3            3         b       1
4    3            30        a       4
</code></pre>

<p>I've tried the following which I've gotten to work where there are no 'partitions':</p>

<pre><code>def row_number(frame,orderby_columns, orderby_direction,name):
    frame.sort_index(by = orderby_columns, ascending = orderby_direction, inplace = True)
    frame[name] = list(xrange(len(frame.index)))
</code></pre>

<p>I tried to extend this idea to work with partitions (groups in pandas) but the following didn't work:</p>

<pre><code>df1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()

def nf(x):
    x['rn'] = list(xrange(len(x.index)))

df1['rn1'] = df1.groupby('key1').apply(nf)
</code></pre>

<p>But I just got a lot of NaNs when I do this.</p>

<p>Ideally, there'd be a succinct way to replicate the window function capability of sql (i've figured out the window based aggregates...that's a one liner in pandas)...can someone share with me the most idiomatic way to number rows like this in PANDAS?</p>
";;1;;2013-07-21T19:16:35.007;3.0;17775935;2016-09-06T22:16:36.823;2016-09-06T22:16:36.823;;5741205.0;;2241563.0;;1;11;<python><pandas><dataframe>;SQL-like window functions in PANDAS: Row Numbering in Python Pandas Dataframe;8442.0
5067;5067;17778786.0;3.0;"<p>How do you find the top correlations in a correlation matrix with Pandas? There are many answers on how to do this with R (<a href=""https://stackoverflow.com/questions/7074246/show-correlations-as-an-ordered-list-not-as-a-large-matrix"">Show correlations as an ordered list, not as a large matrix</a> or <a href=""https://stackoverflow.com/questions/11268822/efficient-way-to-get-highly-correlated-pairs-from-large-data-set-in-python-or-r"">Efficient way to get highly correlated pairs from large data set in Python or R</a>), but I am wondering how to do it with pandas? In my case the matrix is 4460x4460, so can't do it visually.</p>
";;0;;2013-07-22T00:31:30.230;2.0;17778394;2017-03-28T15:30:17.780;2017-05-23T12:18:06.583;;-1.0;;107156.0;;1;14;<python><pandas>;List Highest Correlation Pairs from a Large Correlation Matrix in Pandas?;8078.0
5095;5095;17813222.0;2.0;"<p>I have a pandas data frame and would like to plot values from one column versus the values from another column. Fortunately, there is <code>plot</code> method associated with the data-frames that seems to do what I need:</p>

<pre><code>df.plot(x='col_name_1', y='col_name_2')
</code></pre>

<p>Unfortunately, it looks like among the plot styles (listed <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.plot.html"" rel=""noreferrer"">here</a> after the <code>kind</code> parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem.</p>
";;0;;2013-07-23T14:23:42.153;8.0;17812978;2017-06-28T19:15:14.133;;;;;245549.0;;1;30;<python><matplotlib><plot><pandas><dataframe>;How to plot two columns of a pandas data frame using points?;74379.0
5105;5105;17819427.0;3.0;"<p>I noticed Pandas now has <a href=""http://pandas.pydata.org/pandas-docs/dev/sparse.html"">support for Sparse Matrices and Arrays</a>.  Currently, I create <code>DataFrame()</code>s like this:</p>

<pre><code>return DataFrame(matrix.toarray(), columns=features, index=observations)
</code></pre>

<p>Is there a way to create a <code>SparseDataFrame()</code> with a <code>scipy.sparse.csc_matrix()</code> or <code>csr_matrix()</code>? Converting to dense format kills RAM badly. Thanks!</p>
";;1;;2013-07-23T18:58:05.310;18.0;17818783;2017-06-07T21:43:06.423;;;;;145279.0;;1;25;<python><numpy><scipy><pandas><sparse-matrix>;Populate a Pandas SparseDataFrame from a SciPy Sparse Matrix;9661.0
5137;5137;;7.0;"<p>The Python library <a href=""/questions/tagged/pandas"" class=""post-tag"" title=""show questions tagged &#39;pandas&#39;"" rel=""tag"">pandas</a> can read Excel spreadsheets and convert them to a <code>pandas.DataFrame</code> with <code>pandas.read_excel(file)</code> command. Under the hood, it uses <a href=""https://pypi.python.org/pypi/xlrd"" rel=""noreferrer"">xlrd</a> library which <a href=""https://github.com/python-excel/xlrd/blob/fcfdb721abe650c0b25d8a874dc7314e9eb8dc59/xlrd/__init__.py#L146"" rel=""noreferrer"">does not support</a> ods files.</p>

<p>Is there an equivalent of <code>pandas.read_excel</code> for ods files? If not, how can I do the same for an Open Document Formatted spreadsheet (ods file)? ODF is used by LibreOffice and OpenOffice.</p>
";;0;;2013-07-24T13:09:38.073;4.0;17834995;2017-08-01T19:51:42.430;2016-12-02T17:39:32.347;;2285236.0;;1725303.0;;1;19;<python><pandas><libreoffice><dataframe><opendocument>;How to convert OpenDocument spreadsheets to a pandas DataFrame?;6003.0
5153;5153;17840195.0;6.0;"<p>This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.</p>

<pre><code>a = 2
b = 3
</code></pre>

<p>I want to construct a DataFrame from this:</p>

<pre><code>df2 = pd.DataFrame({'A':a,'B':b})
</code></pre>

<p>This generates an error:  </p>

<blockquote>
  <p>ValueError: If using all scalar values, you must pass an index</p>
</blockquote>

<p>I tried this also:</p>

<pre><code>df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()
</code></pre>

<p>This gives the same error message.</p>
";;0;;2013-07-24T16:40:24.047;8.0;17839973;2017-07-22T13:53:46.923;2016-05-24T00:34:37.787;;2623899.0;;461436.0;;1;64;<python><pandas><dataframe>;construct pandas DataFrame from values in variables;53479.0
5159;5159;17841294.0;4.0;"<p>I have a dataframe like this:</p>

<pre><code>   A         B       C
0  1  0.749065    This
1  2  0.301084      is
2  3  0.463468       a
3  4  0.643961  random
4  1  0.866521  string
5  2  0.120737       !
</code></pre>

<p>Calling </p>

<pre><code>In [10]: print df.groupby(""A"")[""B""].sum()
</code></pre>

<p>will return </p>

<pre><code>A
1    1.615586
2    0.421821
3    0.463468
4    0.643961
</code></pre>

<p>Now I would like to do ""the same"" for column ""C"". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e. </p>

<pre><code>A
1    {This, string}
2    {is, !}
3    {a}
4    {random}
</code></pre>

<p>I have been trying to find ways to do this. </p>

<p>Series.unique() (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html</a>) doesn't work, although</p>

<pre><code>df.groupby(""A"")[""B""]
</code></pre>

<p>is a</p>

<pre><code>pandas.core.groupby.SeriesGroupBy object
</code></pre>

<p>so I was hoping any Series method would work. Any ideas?</p>

<p>Thanks,
Anne</p>
";;0;;2013-07-24T17:43:24.300;16.0;17841149;2016-10-03T13:11:42.593;;;;;2565842.0;;1;37;<python><pandas>;Pandas groupby: How to get a union of strings;24290.0
5175;5175;17877159.0;4.0;"<p>I would like to compare two histograms by having the Y axis show the percentage of each column from the overall dataset size instead of an absolute value. Is that possible? I am using Pandas and matplotlib.
Thanks</p>
";;3;;2013-07-26T06:04:59.417;7.0;17874063;2017-07-26T05:47:35.240;2017-07-26T05:47:35.240;;6829195.0;;1575294.0;;1;22;<python><pandas><matplotlib>;Is there a parameter in matplotlib/pandas to have the Y axis of a histogram as percentage?;15779.0
5229;5229;18103894.0;3.0;"<p>Code example:</p>

<pre><code>In [171]: A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])

In [172]: B = np.array([111, 222, 222, 333, 333, 777])

In [173]: C = randint(10, 99, 6)

In [174]: df = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])

In [175]: df.set_index(['A', 'B'], inplace=True)

In [176]: df
Out[176]: 
          C
A   B      
1.1 111  20
    222  31
3.3 222  24
    333  65
5.5 333  22
6.6 777  74 
</code></pre>

<p>Now, I want to retrieve A values:<br>
<strong>Q1</strong>: in range [3.3, 6.6] - expected return value: [3.3, 5.5, 6.6] or [3.3, 3.3, 5.5, 6.6] in case last inclusive, and [3.3, 5.5] or [3.3, 3.3, 5.5] if not.<br>
<strong>Q2</strong>: in range [2.0, 4.0] - expected return value: [3.3] or [3.3, 3.3]  </p>

<p>Same for any other <em>MultiIndex</em> dimension, for example B values:<br>
<strong>Q3</strong>: in range [111, 500] with repetitions, as number of data rows in range - expected return value: [111, 222, 222, 333, 333]    </p>

<p>More formal:</p>

<p>Let us assume T is a table with columns A, B and C. The table includes <em>n</em> rows. Table cells are numbers, for example A double, B and C integers. Let's create a <em>DataFrame</em> of table T, let us name it DF. Let's set columns A and B indexes of DF (without duplication, i.e. no separate columns A and B as indexes, and separate as data), i.e. A and B in this case <em>MultiIndex</em>.  </p>

<p>Questions:  </p>

<ol>
<li>How to write a query on the index, for example, to query the index A (or B), say in the labels interval [120.0, 540.0]? Labels 120.0 and 540.0 exist. I must clarify that I am interested only in the list of indices as a response to the query!</li>
<li>How to the same, but in case of the labels 120.0 and 540.0 do not exist, but there are labels by value lower than 120, higher than 120 and less than 540, or higher than 540?</li>
<li>In case the answer for Q1 and Q2 was unique index values, now the same, but with repetitions, as number of data rows in index range.</li>
</ol>

<p>I know the answers to the above questions in the case of columns which are not indexes, but in the indexes case, after a long research in the web and experimentation with the functionality of <em>pandas</em>, I did not succeed. The only method (without additional programming) I see now is to have a duplicate of A and B as data columns in addition to index.</p>
";;3;;2013-07-29T09:56:50.183;12.0;17921010;2017-05-19T13:51:18.563;2017-05-19T13:51:18.563;;4720935.0;;1248167.0;;1;19;<python><pandas><indexing><slice><multi-index>;How to query MultiIndex index columns values in pandas;36303.0
5232;5232;17926411.0;2.0;"<p>I have a pandas data frame and group it by two columns (for example <code>col1</code> and <code>col2</code>). For fixed values of <code>col1</code> and <code>col2</code> (i.e. for a group) I can have several different values in the <code>col3</code>. I would like to count the number of distinct values from the third columns.</p>

<p>For example, If I have this as my input:</p>

<pre><code>1  1  1
1  1  1
1  1  2
1  2  3
1  2  3
1  2  3
2  1  1
2  1  2
2  1  3
2  2  3
2  2  3
2  2  3
</code></pre>

<p>I would like to have this table (data frame) as the output:</p>

<pre><code>1  1  2
1  2  1
2  1  3
2  2  1
</code></pre>
";;1;;2013-07-29T14:10:21.410;9.0;17926273;2013-07-29T14:42:08.197;2013-07-29T14:42:08.197;;245549.0;;245549.0;;1;19;<python><group-by><pandas>;How to count distinct values in a column of a pandas group by object?;37568.0
5239;5239;17942117.0;4.0;"<p>I am trying to read in a csv file with <code>numpy.genfromtxt</code> but some of the fields are strings which contain commas.  The strings are in quotes, but numpy is not recognizing the quotes as defining a single string.  For example, with the data in 't.csv':</p>

<pre><code>2012, ""Louisville KY"", 3.5
2011, ""Lexington, KY"", 4.0
</code></pre>

<p>the code</p>

<pre><code>np.genfromtxt('t.csv', delimiter=',')
</code></pre>

<p>produces the error:</p>

<blockquote>
  <p>ValueError: Some errors were detected !
      Line #2 (got 4 columns instead of 3)</p>
</blockquote>

<p>The data structure I am looking for is:</p>

<pre><code>array([['2012', 'Louisville KY', '3.5'],
       ['2011', 'Lexington, KY', '4.0']], 
      dtype='|S13')
</code></pre>

<p>Looking over the documentation, I don't see any options to deal with this.  Is there a way do to it with numpy, or do I just need to read in the data with the <code>csv</code> module and then convert it to a numpy array?</p>
";;4;;2013-07-29T20:11:02.253;7.0;17933282;2017-01-30T20:45:15.450;2013-07-30T15:04:04.770;;653364.0;;1829066.0;;1;20;<python><numpy><pandas><genfromtxt>;Using numpy.genfromtxt to read a csv file with strings containing commas;21881.0
5254;5254;;3.0;"<p>I'm just starting to work with pandas.
I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first columns within the dataframe, to do that I have to convert <code>int</code> column to <code>str</code>. 
I've tried to do that like that:</p>

<pre><code>mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])
</code></pre>

<p>or like that </p>

<pre><code>mtrx['X.3'] = mtrx['X.3'].astype(str)
</code></pre>

<p>but in both cases it's not working and I'm getting an error saying ""cannot concatenate 'str' and 'int' objects"". Concat for two <code>str</code> columns is working perfectly fine.</p>

<p>Any help would be greatly appreciated! Thanks!</p>
";;1;;2013-07-30T14:53:03.407;10.0;17950374;2017-05-16T17:49:56.810;;;;;2361675.0;;1;38;<string><int><pandas>;Converting a column within pandas dataframe from int to string;71902.0
5269;5269;17958424.0;1.0;"<p>This works (using Pandas 12 dev)</p>

<pre><code>table2=table[table['SUBDIVISION'] =='INVERNESS']
</code></pre>

<p>Then I realized I needed to select the field using ""starts with"" Since I was missing a bunch.
So per the Pandas doc as near as I could follow I tried </p>

<pre><code>criteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))
table2 = table[criteria]
</code></pre>

<p>And got AttributeError: 'float' object has no attribute 'startswith'</p>

<p>So I tried an alternate syntax with the same result</p>

<pre><code>table[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]
</code></pre>

<p>Reference <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing</a>
Section 4: List comprehensions and map method of Series can also be used to produce more complex criteria:</p>

<p>What am I missing?</p>
";;1;;2013-07-30T21:37:24.260;3.0;17957890;2016-08-19T02:20:37.413;;;;;137783.0;;1;18;<python><numpy><pandas>;pandas select from Dataframe using startswith;13690.0
5274;5274;17961468.0;1.0;"<p>Say I have a dataframe</p>

<pre><code>import pandas as pd
import numpy as np
foo = pd.DataFrame(np.random.random((10,5)))
</code></pre>

<p>and I create another dataframe from a subset of my data:</p>

<pre><code>bar = foo.iloc[3:5,1:4]
</code></pre>

<p>does <code>bar</code> hold a copy of those elements from <code>foo</code>? Is there any way to create a <code>view</code> of that data instead? If so, what would happen if I try to modify data in this view? Does Pandas provide any sort of <a href=""http://en.wikipedia.org/wiki/Copy_on_write"">copy-on-write</a> mechanism?</p>
";;1;;2013-07-31T02:16:42.917;3.0;17960511;2015-12-01T13:29:00.803;;;;;283296.0;;1;13;<python><pandas>;Pandas: Subindexing dataframes: Copies vs views;5342.0
5283;5283;17973255.0;2.0;"<p>If I have a frame like this</p>

<pre><code>frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})
</code></pre>

<p>and I want to check if any of those rows contain a certain word I just have to do this.</p>

<pre><code>frame['b'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")
</code></pre>

<p><code>frame['b']</code> outputs:</p>

<pre><code>True
False
True
</code></pre>

<p>If I decide to make a list </p>

<pre><code>mylist =['dog', 'cat', 'fish']
</code></pre>

<p>how would I check that the rows contain a certain word in the list?  </p>
";;0;;2013-07-31T14:18:28.373;7.0;17972938;2017-07-12T13:33:24.350;2016-03-11T02:39:21.747;;2822004.0;;2333196.0;;1;21;<python><python-2.7><pandas>;check if string in pandas dataframe column is in list;27027.0
5295;5295;17977609.0;2.0;"<p>The new version of Pandas uses <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.excel.read_excel.html#pandas.io.excel.read_excel"" rel=""noreferrer"">the following interface</a> to load Excel files:</p>

<pre><code>read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])
</code></pre>

<p>but what if I don't know the sheets that are available? </p>

<p>For example, I am working with excel files that the following sheets</p>

<blockquote>
  <p>Data 1, Data 2 ..., Data N, foo, bar</p>
</blockquote>

<p>but I don't know <code>N</code> a priori.</p>

<p>Is there any way to get the list of sheets from an excel document in Pandas?</p>
";;0;;2013-07-31T17:57:19.313;5.0;17977540;2017-08-10T01:59:40.563;2013-07-31T22:31:11.990;;283296.0;;283296.0;;1;32;<python><pandas>;Pandas: Looking up the list of sheets in an excel file;16531.0
5298;5298;17978188.0;4.0;"<p>I have a pandas dataframe with the following columns;</p>

<pre><code>Date              Time
01-06-2013      23:00:00
02-06-2013      01:00:00
02-06-2013      21:00:00
02-06-2013      22:00:00
02-06-2013      23:00:00
03-06-2013      01:00:00
03-06-2013      21:00:00
03-06-2013      22:00:00
03-06-2013      23:00:00
04-06-2013      01:00:00
</code></pre>

<p>How do I combine data['Date'] &amp; data['Time']  to get the following? Is there a way of doing it using <code>pd.to_datetime</code>?</p>

<pre><code>Date
01-06-2013 23:00:00
02-06-2013 01:00:00
02-06-2013 21:00:00
02-06-2013 22:00:00
02-06-2013 23:00:00
03-06-2013 01:00:00
03-06-2013 21:00:00
03-06-2013 22:00:00
03-06-2013 23:00:00
04-06-2013 01:00:00
</code></pre>
";;1;;2013-07-31T18:27:40.550;9.0;17978092;2017-07-05T08:47:05.980;;;;;1948860.0;;1;30;<python><pandas>;Combine Date and Time columns using python pandas;16717.0
5299;5299;17978414.0;4.0;"<p>Is it possible to only merge some columns? I have a DataFrame df1 with columns x, y, z, and df2 with columns x, a ,b, c, d, e, f, etc.</p>

<p>I want to merge the two DataFrames on x, but I only want to merge columns df2.a, df2.b - not the entire DataFrame. </p>

<p>The result would be a DataFrame with x, y, z, a, b.</p>

<p>I could merge then delete the unwanted columns, but it seems like there is a better method.</p>
";;1;;2013-07-31T18:30:28.557;6.0;17978133;2017-05-22T21:48:14.043;2016-12-22T01:08:09.120;;3100515.0;;1827947.0;;1;24;<python><merge><pandas>;Python Pandas merge only certain columns;15297.0
5320;5320;17995122.0;3.0;"<p>I am using .size() on a groupby result in order to count how many items are in each group.</p>

<p>I would like the result to be saved to a new column name without manually editing the column names array, how can it be done?</p>

<p>Thanks</p>

<p>This is what I have tried:</p>

<pre><code>grpd = df.groupby(['A','B'])
grpd['size'] = grpd.size()
grpd
</code></pre>

<p>and the error I got:</p>

<blockquote>
  <p>TypeError: 'DataFrameGroupBy' object does not support item assignment
  (on the second line)</p>
</blockquote>
";;0;;2013-08-01T13:11:07.157;4.0;17995024;2016-11-29T17:56:47.803;;;;;1575294.0;;1;17;<pandas>;How to assign a name to the a size() column?;4313.0
5350;5350;18013682.0;1.0;"<p>A python pandas dataframe with multi-columns, there are only two columns needed for dict. One as dict's keys and another as dict's values. How can I do that?</p>

<p>Dataframe:</p>

<pre><code>           area  count
co tp
DE Lake      10      7
Forest       20      5
FR Lake      30      2
Forest       40      3
</code></pre>

<p>Need to define area as key, count as value in dict. thank you in advance.</p>
";;0;;2013-08-02T08:46:33.197;8.0;18012505;2016-12-11T11:56:52.023;;;;;857130.0;;1;18;<python><dictionary><data-conversion><dataframe>;python pandas dataframe columns convert to dict key and value;19984.0
5367;5367;18023468.0;6.0;"<p>How do I get the index column name in python pandas?  Here's an example dataframe:</p>

<pre><code>             Column 1
Index Title          
Apples              1
Oranges             2
Puppies             3
Ducks               4  
</code></pre>

<p>What I'm trying to do is get/set the dataframe index title.  Here is what i tried:</p>

<pre><code>import pandas as pd
data = {'Column 1'     : [1., 2., 3., 4.],
        'Index Title'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}
df = pd.DataFrame(data)
df.index = df[""Index Title""]
del df[""Index Title""]
print df
</code></pre>

<p>Anyone know how to do this? </p>
";;0;;2013-08-02T17:30:53.160;21.0;18022845;2017-05-06T14:12:23.617;2017-05-06T14:12:23.617;;2901002.0;;1754273.0;;1;100;<python><pandas><dataframe><columnname>;Pandas index column title or name;114634.0
5385;5385;18129082.0;11.0;"<p>I'm trying to use pandas to manipulate a .csv file but I get this error:</p>

<pre><code>pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12
</code></pre>

<p>I have tried to read the pandas docs, but found nothing.</p>

<p>My code is simple:</p>

<pre><code>path = 'GOOG Key Ratios.csv'
#print(open(path).read())
data = pd.read_csv(path)
</code></pre>

<p>How can I resolve this? Should I use the <code>csv</code> module or another language ?</p>

<p>Thanks folks.</p>

<p>File is from <a href=""http://financials.morningstar.com/ratios/r.html?t=GOOG&amp;region=usa&amp;culture=en-US"" rel=""noreferrer"">Morningstar</a></p>
";;1;;2013-08-04T01:54:45.320;14.0;18039057;2017-07-07T09:32:01.953;2017-06-08T22:00:04.927;;2389827.0;;2649671.0;;1;84;<python><csv><pandas>;Python Pandas Error tokenizing data;84465.0
5409;5409;18061253.0;1.0;"<p>Can some please explain the difference between the asfreq and resample methods in pandas? When should one use what?</p>
";;0;;2013-08-05T14:23:48.207;5.0;18060619;2013-08-05T15:10:13.780;;;;;1910424.0;;1;17;<python><pandas>;Difference between asfreq and resample;4888.0
5412;5412;18062521.0;5.0;"<p>I have two Series <code>s1</code> and <code>s2</code> with the same (non-consecutive) indices. How do I combine <code>s1</code> and <code>s2</code> to being two columns in a DataFrame and keep one of the indices as a third column?</p>
";;0;;2013-08-05T15:37:39.250;34.0;18062135;2016-02-25T01:00:03.453;2015-12-29T20:10:51.720;;2901002.0;;213216.0;;1;113;<python><pandas><series><dataframe>;Combining two Series into a DataFrame in pandas;90070.0
5420;5420;18067431.0;2.0;"<p>I have multiple virtualenvs on a single machine, but all of them need numpy and pandas. I want to have seperated copies for each virtualenv, but creation of those virtualenvs takes quite some time. Is there some well defined way to precompile numpy and pandas on my machine just once and then to do something like:</p>

<pre><code>pip install my_precompiled_numpy 
</code></pre>
";;0;;2013-08-05T20:25:27.130;4.0;18067073;2014-09-22T11:37:11.747;;;;;110963.0;;1;13;<python><numpy><pandas><virtualenv><pip>;Speedup virtualenv creation with numpy and pandas;2062.0
5433;5433;18079695.0;5.0;"<p>I have two series s1 and s2 in pandas/python and want to compute the intersection i.e. where all of the values of the series are common.</p>

<p>How would I use the concat function to do this? I have been trying to work it out but have been unable to (I don't want to compute the intersection on the indices of s1 and S2, but on the values).</p>

<p>Thanks in advance.</p>
";;0;;2013-08-06T11:58:31.313;3.0;18079563;2016-10-17T01:35:21.157;;;;;213216.0;;1;22;<python><pandas><series>;Finding the intersection between two series in Pandas;23715.0
5451;5451;;6.0;"<p>I have been wondering... If I am reading, say, a 400MB csv file into a pandas dataframe (using read_csv or read_table), is there any way to guesstimate how much memory this will need? Just trying to get a better feel of data frames and  memory...</p>
";;4;;2013-08-06T20:18:55.503;13.0;18089667;2016-12-15T09:24:10.620;2014-12-03T03:24:24.193;;295307.0;;2565842.0;;1;42;<python><pandas>;How to estimate how much memory a Pandas' DataFrame will need?;17587.0
5464;5464;18101965.0;3.0;"<p>I'm creating tables using the pandas to_html function, and I'd like to be able to highlight the bottom row of the outputted table, which is of variable length. I don't have any real experience of html to speak of, and all I found online was this 
    
    
    </p>

<pre><code>&lt;table border=""1""&gt;
  &lt;tr style=""background-color:#FF0000""&gt;
    &lt;th&gt;Month&lt;/th&gt;
    &lt;th&gt;Savings&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;January&lt;/td&gt;
    &lt;td&gt;$100&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</code></pre>

<p>So I know that the final row must have <code>&lt;tr style=""""background-color:#FF0000""&gt;</code> (or whatever colour I want) rather than just <code>&lt;tr&gt;</code>, but what I don't really know how to do is get this to occur with the tables I'm making. I don't think I can do it with the to_html function itself, but how can I do it after the table has been created?</p>

<p>Any help is appreciated. </p>

<p>
</p>
";;0;;2013-08-07T07:04:24.430;9.0;18096748;2016-10-19T14:48:27.757;2013-08-08T07:08:50.507;;2545862.0;;2545862.0;;1;16;<python><html><pandas>;Pandas Dataframes to_html: Highlighting table rows;14996.0
5474;5474;18108357.0;3.0;"<p>I am having trouble querying a table of > 5 million records from my MS SQL Server database.  I want to be able to select all of the records, but my code seems to fail when selecting to much data into memory. </p>

<p>This works:</p>

<pre><code>import pandas.io.sql as psql
sql = ""SELECT TOP 1000000 * FROM MyTable"" 
data = psql.read_frame(sql, cnxn)
</code></pre>

<p>...but this does not work:</p>

<pre><code>sql = ""SELECT TOP 2000000 * FROM MyTable"" 
data = psql.read_frame(sql, cnxn)
</code></pre>

<p>It returns this error:</p>

<pre><code>File ""inference.pyx"", line 931, in pandas.lib.to_object_array_tuples
(pandas\lib.c:42733) Memory Error
</code></pre>

<p>I have read <a href=""https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas"">here</a> that a similar problem exists when creating a dataframe from a csv file, and that the work-around is to use the 'iterator' and 'chunksize' parameters like this:</p>

<pre><code>read_csv('exp4326.csv', iterator=True, chunksize=1000)
</code></pre>

<p>Is there a similar solution for querying from an SQL database?  If not, what is the preferred work-around?  Do I need to read in the records in chunks by some other method?  I read a bit of discussion <a href=""https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas"">here</a> for working with large datasets in pandas, but it seems like a lot of work to execute a SELECT * query.  Surely there is a simpler approach.</p>
";;4;;2013-08-07T15:50:01.913;9.0;18107953;2017-01-02T10:38:51.253;2017-05-23T12:34:40.763;;-1.0;;1700913.0;;1;15;<python><sql><pandas><bigdata>;How to create a large pandas dataframe from an sql query without running out of memory?;17209.0
5533;5533;18162021.0;5.0;"<p>I have a python dictionary of user-item ratings that looks something like this:</p>

<pre><code>sample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0}, 
'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0}, 
'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}
</code></pre>

<p>I was looking to convert it into a pandas data frame that would be structured like</p>

<pre><code>     col1   col2  col3
0   user1  item1   2.5
1   user1  item2   3.5
2   user1  item3   3.0
3   user1  item4   3.5
4   user1  item5   2.5
5   user1  item6   3.0
6   user2  item1   2.5
7   user2  item2   3.0
8   user2  item3   3.5
9   user2  item4   4.0
10  user3  item2   4.5
11  user3  item5   1.0
12  user3  item6   4.0
</code></pre>

<p>Any ideas would be much appreciated :)</p>
";;0;;2013-08-10T12:35:14.543;8.0;18161926;2013-08-10T13:39:57.863;;;;;1510812.0;;1;18;<python><pandas>;Pandas data frame from dictionary;26807.0
5543;5543;18172249.0;1.0;"<p>I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...</p>

<pre><code>   File ""C:\Importer\src\dfman\importer.py"", line 26, in import_chr
     data = pd.read_csv(filepath, names=fields)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 400, in parser_f
     return _read(filepath_or_buffer, kwds)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 205, in _read
     return parser.read()
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 608, in read
     ret = self._engine.read(nrows)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 1028, in read
     data = self._reader.read(nrows)
   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\parser.c:6745)
   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:6964)
   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\parser.c:7780)
   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:8793)
   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:9484)
   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:10642)
   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\parser.c:10853)
   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\parser.c:15657)
 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 6: invalid    continuation byte
</code></pre>

<p>The source/creation of these files all come from the same place. What's the best way to correct this to proceed with the import?</p>
";;0;;2013-08-11T12:06:25.160;27.0;18171739;2017-08-03T07:34:16.163;2016-01-11T00:25:08.977;;1494637.0;;867549.0;;1;82;<python><csv><pandas><unicode>;UnicodeDecodeError when reading CSV file in Pandas with Python;46551.0
5548;5548;18173074.0;4.0;"<p>I have the following DataFrame:</p>

<pre><code>             daysago  line_race rating        rw    wrating
 line_date                                                 
 2007-03-31       62         11     56  1.000000  56.000000
 2007-03-10       83         11     67  1.000000  67.000000
 2007-02-10      111          9     66  1.000000  66.000000
 2007-01-13      139         10     83  0.880678  73.096278
 2006-12-23      160         10     88  0.793033  69.786942
 2006-11-09      204          9     52  0.636655  33.106077
 2006-10-22      222          8     66  0.581946  38.408408
 2006-09-29      245          9     70  0.518825  36.317752
 2006-09-16      258         11     68  0.486226  33.063381
 2006-08-30      275          8     72  0.446667  32.160051
 2006-02-11      475          5     65  0.164591  10.698423
 2006-01-13      504          0     70  0.142409   9.968634
 2006-01-02      515          0     64  0.134800   8.627219
 2005-12-06      542          0     70  0.117803   8.246238
 2005-11-29      549          0     70  0.113758   7.963072
 2005-11-22      556          0     -1  0.109852  -0.109852
 2005-11-01      577          0     -1  0.098919  -0.098919
 2005-10-20      589          0     -1  0.093168  -0.093168
 2005-09-27      612          0     -1  0.083063  -0.083063
 2005-09-07      632          0     -1  0.075171  -0.075171
 2005-06-12      719          0     69  0.048690   3.359623
 2005-05-29      733          0     -1  0.045404  -0.045404
 2005-05-02      760          0     -1  0.039679  -0.039679
 2005-04-02      790          0     -1  0.034160  -0.034160
 2005-03-13      810          0     -1  0.030915  -0.030915
 2004-11-09      934          0     -1  0.016647  -0.016647
</code></pre>

<p>I need to remove the rows where <code>line_race</code> is equal to <code>0</code>. What's the most efficient way to do this?</p>
";;1;;2013-08-11T14:14:57.950;57.0;18172851;2017-03-06T12:50:53.673;2016-07-29T21:00:00.240;;4403872.0;;867549.0;;1;194;<python><pandas>;Deleting DataFrame row in Pandas based on column value;264894.0
5557;5557;18176957.0;2.0;"<p>I've got a data frame <strong>df1</strong> with multiple columns and rows. Simple example:</p>

<pre><code>    TIME T1  T2 
       1 10 100
       2 20 200
       3 30 300
</code></pre>

<p>I'd like to create an empty data frame <strong>df2</strong> and later on, add new columns with the calculation results.</p>

<p>For this moment my code looks like this:
</p>

<pre><code>     df1=pd.read_csv(""1.txt"",index_col=""TIME"")

     df2=df1.copy()[[]] #copy df1 and erase all columns
</code></pre>

<p>...adding two new columns:
</p>

<pre><code>     df2[""results1""],df2[""results2""]=df1[""T1""]*df[""T2""]*3,df1[""T2""]+100
</code></pre>

<p>Is there any better/safer/faster way to do this ? 
Is it possible to create an empty data frame df2 and only copy index from df1 ?</p>
";;0;;2013-08-11T21:35:34.747;2.0;18176933;2013-08-12T06:51:27.053;;;;;2669472.0;;1;13;<python><indexing><pandas>;Create an empty data frame with index from another data frame;13059.0
5561;5561;27579192.0;7.0;"<p>A simple pandas question: </p>

<p>Is there a <code>drop_duplicates()</code> functionality to drop every row involved in the duplication? </p>

<p>An equivalent question is the following: Does pandas have a set difference for dataframes? </p>

<p>For example:</p>

<pre><code>In [5]: df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})

In [6]: df2 = pd.DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})

In [7]: df1
Out[7]: 
   col1  col2
0     1     2
1     2     3
2     3     4

In [8]: df2
Out[8]: 
   col1  col2
0     4     6
1     2     3
2     5     5
</code></pre>

<p>so maybe something like df2.set_diff(df1) will produce this:</p>

<pre><code>   col1  col2
0     4     6
2     5     5
</code></pre>

<p>However, I don't want to rely on indexes because in my case, I have to deal with dataframes that have distinct indexes.</p>

<p>By the way, I initially thought about an extension of the current drop_duplicates() method, but now I realize that the second approach using properties of set theory would be far more useful in general. Both approaches solve my current problem, though.</p>

<p>Thanks!</p>
";;6;;2013-08-12T06:29:51.680;2.0;18180763;2017-08-10T18:10:20.990;2013-08-12T06:54:23.273;;460147.0;;460147.0;;1;18;<python><pandas>;set difference for pandas;13295.0
5584;5584;18196299.0;1.0;"<p>With this DataFrame, how can I conditionally set <code>rating</code> to 0 when <code>line_race</code> is equal to zero? </p>

<pre><code>    line_track  line_race  rating foreign
 25        MTH         10     84    False
 26        MTH          6     88    False
 27        TAM          5     87    False
 28         GP          2     86    False
 29         GP          7     59    False
 30        LCH          0    103     True
 31        LEO          0    125     True
 32        YOR          0    126     True
 33        ASC          0    124     True
</code></pre>

<p>In other words, what is the proper way on a DataFrame to say if ColumnA = x then ColumnB = y else ColumnB = ColumnB</p>
";;0;;2013-08-12T20:35:01.190;7.0;18196203;2013-08-12T20:40:21.523;;;;;867549.0;;1;22;<python><pandas>;How to conditionally update DataFrame column in Pandas;20823.0
5586;5586;;2.0;"<p>The pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"">documentation</a> includes a note:</p>

<blockquote>
  <p>Note Unlike list.append method, which appends to the original list and returns nothing, append here does not modify df1 and returns its copy with df2 appended.</p>
</blockquote>

<p>How can I append to an existing DataFrame without making a copy? Or in the terms of the note how can I modify df1 in place by appending df2 and return nothing?</p>
";;4;;2013-08-12T21:00:37.083;2.0;18196616;2014-04-30T17:27:12.017;;;;;148022.0;;1;17;<python><pandas>;Append rows to a pandas DataFrame without making a new copy;14082.0
5594;5594;18199337.0;2.0;"<p>I have the following DataFrame:</p>

<pre><code>   a  b  c
b
2  1  2  3
5  4  5  6
</code></pre>

<p>As you can see, column <code>b</code> is used as an index. I want to get the ordinal number of the row fulfilling <code>('b' == 5)</code>, which in this case would be <code>1</code>.</p>

<p>The column being tested can be either an index column (as with <code>b</code> in this case) or a regular column, e.g. I may want to find the index of the row fulfilling <code>('c' == 6)</code>.</p>
";;0;;2013-08-13T01:31:08.160;5.0;18199288;2016-02-01T19:35:48.007;;;;;916907.0;;1;14;<python><numpy><pandas>;Getting the integer index of a Pandas DataFrame row fulfilling a condition?;33227.0
5611;5611;18215499.0;3.0;"<p>I am using pandas/python and I have two date time series s1 and s2, that have been generated using the 'to_datetime' function on a field of the df containing dates/times.</p>

<p>When I subtract s1 from s2</p>

<blockquote>
  <p>s3 = s2 - s1</p>
</blockquote>

<p>I get a series, s3, of type </p>

<blockquote>
  <p>timedelta64[ns]</p>
</blockquote>

<pre><code>0    385 days, 04:10:36
1     57 days, 22:54:00
2    642 days, 21:15:23
3    615 days, 00:55:44
4    160 days, 22:13:35
5    196 days, 23:06:49
6     23 days, 22:57:17
7      2 days, 22:17:31
8    622 days, 01:29:25
9     79 days, 20:15:14
10    23 days, 22:46:51
11   268 days, 19:23:04
12                  NaT
13                  NaT
14   583 days, 03:40:39
</code></pre>

<p>How do I look at 1 element of the series:</p>

<blockquote>
  <p>s3[10]</p>
</blockquote>

<p>I get something like this:</p>

<blockquote>
  <p>numpy.timedelta64(2069211000000000,'ns')</p>
</blockquote>

<p>How do I extract days from s3 and maybe keep them as integers(not so interested in hours/mins etc.)?</p>

<p>Thanks in advance for any help.</p>
";;1;;2013-08-13T17:17:13.153;11.0;18215317;2016-12-19T18:30:46.110;2015-11-30T18:47:21.450;;505165.0;;213216.0;;1;36;<python><numpy><pandas>;extracting days from a numpy.timedelta64 value;32906.0
5623;5623;18233876.0;3.0;"<p>I have written a function to convert pandas datetime dates to month-end:</p>

<pre><code>import pandas
import numpy
import datetime
from pandas.tseries.offsets import Day, MonthEnd

def get_month_end(d):
    month_end = d - Day() + MonthEnd() 
    if month_end.month == d.month:
        return month_end # 31/March + MonthEnd() returns 30/April
    else:
        print ""Something went wrong while converting dates to EOM: "" + d + "" was converted to "" + month_end
        raise
</code></pre>

<p>This function seems to be quite slow, and I was wondering if there is any faster alternative? The reason I noticed it's slow is that I am running this on a dataframe column with 50'000 dates, and I can see that the code is much slower since introducing that function (before I was converting dates to end-of-month).</p>

<pre><code>df = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})
df[open_date] = df[open_date].apply(get_month_end)
</code></pre>

<p>I am not sure if that's relevant, but I am reading the dates in as follows:</p>

<pre><code>def read_as_date(x):
    return datetime.datetime.strptime(x, fmt)
</code></pre>
";;0;;2013-08-14T13:28:48.080;4.0;18233107;2017-07-25T10:42:54.193;;;;;2565842.0;;1;11;<python><pandas>;pandas: convert datetime to end-of-month;9648.0
5664;5664;18259236.0;2.0;"<p>I currently have a <code>DataFrame</code> laid out as:</p>

<pre><code>        Jan Feb Mar Apr ...
2001    1   12  12  19  
2002    9   ...
2003    ...
</code></pre>

<p>and I would like to ""unpivot"" the data to look like:</p>

<pre><code>Date    Value
Jan 2001    1
Feb 2001    1
Mar 2001    12
...
Jan 2002    9
</code></pre>

<p>What is the best way to accomplish this using Pandas/ numpy?</p>
";;0;;2013-08-15T18:18:20.900;7.0;18259067;2013-08-15T21:01:05.930;;;;;2638485.0;;1;16;<numpy><pandas>;Unpivot Pandas Data;5020.0
5704;5704;18283014.0;1.0;"<p>In many places in our Pandas-using code, we have some Python function <code>process(row)</code>. That function is used over <code>DataFrame.iterrows()</code>, taking each <code>row</code>, and doing some processing, and returning a value, which we ultimate collect into a new <code>Series</code>.</p>

<p>I realize this usage pattern circumvents most of the performance benefits of the numpy / Pandas stack.</p>

<ol>
<li>What would be the best way to make this usage pattern as efficient
as possible?</li>
<li>Can we possibly do it without rewriting most of our code?</li>
</ol>

<p>Another aspect of this question: can all such functions be converted to a numpy-efficient representation?  I've much to learn about the numpy / scipy / Pandas stack, but it seems that for truly arbitrary logic, you may sometimes need to just use a slow pure Python architecture like the one above. Is that the case?</p>
";;1;;2013-08-16T22:20:56.407;3.0;18282988;2013-08-17T19:10:28.587;;;;;916907.0;;1;11;<python><numpy><pandas>;Efficiently processing DataFrame rows with a Python function?;8669.0
5710;5710;18290143.0;5.0;"<p>How can I drop or disable the indices in a pandas Data Frame?</p>

<p>I am learning the pandas from the book ""python for data analysis"" and I already know I can use the dataframe.drop to drop one column or one row. But I did not find anything about disabling the all the indices in place.</p>
";;0;;2013-08-17T15:00:10.823;2.0;18290123;2017-07-26T14:02:27.277;;;;;2233683.0;;1;11;<python><pandas>;disable index pandas data frame;20065.0
5728;5728;18316830.0;4.0;"<p>Lets say I have a MultiIndex Series <code>s</code>:</p>

<pre><code>&gt;&gt;&gt; s
     values
a b
1 2  0.1 
3 6  0.3
4 4  0.7
</code></pre>

<p>and I want to apply a function which uses the index of the row:</p>

<pre><code>def f(x):
   # conditions or computations using the indexes
   if x.index[0] and ...: 
   other = sum(x.index) + ...
   return something
</code></pre>

<p>How can I do <code>s.apply(f)</code> for such a function? What is the recommended way to make this kind of operations? I expect to obtain a new Series with the values resulting from this function applied on each row and the same MultiIndex.</p>
";;1;;2013-08-19T14:22:07.393;8.0;18316211;2015-06-16T23:22:00.750;2013-08-19T14:56:46.493;;1330293.0;;1330293.0;;1;33;<python><pandas>;Access index in pandas.Series.apply;16141.0
5746;5746;18327852.0;6.0;"<p>I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)</p>

<p>I.e., I'd like something like:</p>

<pre><code>import pandas as pd
myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])
print myseries.find(7) # should output 3
</code></pre>

<p>Certainly, it is possible to define such a method with a loop:</p>

<pre><code>def find(s, el):
    for i in s.index:
        if s[i] == el: 
            return i
    return None

print find(myseries, 7)
</code></pre>

<p>but I assume there should be a better way. Is there?</p>
";;0;;2013-08-20T05:33:54.513;17.0;18327624;2017-03-25T05:15:59.523;;;;;1564449.0;;1;43;<python><pandas>;Find element's index in pandas Series;81639.0
5765;5765;18360223.0;2.0;"<p>I'm probably using poor search terms when trying to find this answer. Right now, before indexing a DataFrame, I'm getting a list of values in a column this way...</p>

<pre><code> list = list(df['column']) 
</code></pre>

<p>...then I'll <code>set_index</code> on the column. This seems like a wasted step. When trying the above on an index, I get a key error.</p>

<p>How can I grab the values in an index (both single and multi) and put them in a list or a list of tuples?</p>
";;4;;2013-08-21T13:36:05.940;5.0;18358938;2016-11-17T10:51:27.453;2016-11-17T10:47:03.243;;202229.0;;867549.0;;1;25;<python><list><pandas><indexing>;Get index values of Pandas DataFrame as list?;35190.0
5772;5772;31931208.0;3.0;"<p>I think I misunderstand the intention of read_csv. If I have a file 'j' like</p>

<pre><code># notes
a,b,c
# more notes
1,2,3
</code></pre>

<p>How can I pandas.read_csv this file, skipping any '#' commented lines? I see in the help 'comment' of lines is not supported but it indicates an empty line should be returned. I see an error</p>

<pre><code>df = pandas.read_csv('j', comment='#')
</code></pre>

<p>CParserError: Error tokenizing data. C error: Expected 1 fields in line 2, saw 3</p>

<p>I'm currently on </p>

<pre><code>In [15]: pandas.__version__
Out[15]: '0.12.0rc1'
</code></pre>

<p>On version'0.12.0-199-g4c8ad82':</p>

<pre><code>In [43]: df = pandas.read_csv('j', comment='#', header=None)
</code></pre>

<p>CParserError: Error tokenizing data. C error: Expected 1 fields in line 2, saw 3</p>
";;6;;2013-08-21T20:13:27.313;2.0;18366797;2017-07-15T00:14:47.193;2013-08-22T00:11:56.397;;287238.0;;287238.0;;1;14;<python><pandas>;pandas.read_csv: how to skip comment lines;8228.0
5796;5796;22677264.0;3.0;"<p>I am using the anaconda distribution of ipython/Qt console. I want to plot things inline so I type the following from the ipython console: </p>

<pre><code>%pylab inline
</code></pre>

<p>Next I type the tutorial at (<a href=""http://pandas.pydata.org/pandas-docs/dev/visualization.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/visualization.html</a>) into ipython...  </p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd 
ts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))
ts = ts.cumsum()
ts.plot()
</code></pre>

<p>... and this is all that i get back: </p>

<pre><code>&lt;matplotlib.axes.AxesSubplot at 0x109253410&gt;
</code></pre>

<p>But there is no plot. What could be wrong? Is there another command that I need to supply? The tutorial suggests that that is all that I need to type. </p>
";;9;;2013-08-22T19:05:19.417;2.0;18388870;2016-08-01T02:59:27.070;2013-08-23T00:49:03.900;;434217.0;;1600821.0;;1;18;<python><matplotlib><pandas><ipython><anaconda>;ipython pandas plot does not show;12938.0
5817;5817;18405221.0;1.0;"<p>As Python newbie I recently discovered that with Py 2.7 I can do something like:</p>

<pre><code>print '{:20,.2f}'.format(123456789)
</code></pre>

<p>which will give the resulting output:</p>

<pre><code>123,456,789.00
</code></pre>

<p>I'm now looking to have a similar outcome for a pandas df so my code was like:</p>

<pre><code>import pandas as pd
import random
data = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]
df = pd.DataFrame (data)
print '{:20,.2f}'.format(df)
</code></pre>

<p>In this case I have the error:</p>

<pre><code> Unknown format code 'f' for object of type 'str'
</code></pre>

<p>Any suggestions to perform something like <code>'{:20,.2f}'.format(df)</code> ?</p>

<p>As now my idea is to index the dataframe (it's a small one), then format each individual float within it, might be assign astype(str), and rebuild the DF ... but looks so looks ugly :-( and I'm not even sure it'll work ..</p>

<p>What do you think ? I'm stuck ... and would like to have a better format for my dataframes when these are converted to reportlabs grids.</p>
";;0;;2013-08-23T14:10:45.420;5.0;18404946;2017-05-12T11:27:40.817;2017-05-12T11:27:40.817;;396967.0;;2496487.0;;1;14;<python><format><pandas><dataframe>;Py Pandas .format(dataframe);12208.0
5840;5840;;6.0;"<p>I have a DataFrame with many missing values in columns which I wish to groupby:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})

In [4]: df.groupby('b').groups
Out[4]: {'4': [0], '6': [2]}
</code></pre>

<p>see that Pandas has dropped the rows with NaN target values. (I want to include these rows!)</p>

<p><em>Since I need many such operations (many cols have missing values), and use more complicated functions than just medians (typically random forests), I want to avoid writing too complicated pieces of code.</em></p>

<p>Any suggestions? Should I write a function for this or is there a simple solution?</p>
";;4;;2013-08-25T13:28:41.277;8.0;18429491;2017-05-23T19:07:02.837;2013-08-25T17:11:08.600;;1240268.0;;2715498.0;;1;40;<pandas><grouping><nan>;groupby columns with NaN (missing) values;20547.0
5848;5848;33795876.0;3.0;"<p>Say I have a column in a dataframe that has some numbers and some non-numbers</p>

<pre><code>&gt;&gt; df['foo']
0       0.0
1     103.8
2     751.1
3       0.0
4       0.0
5         -
6         -
7       0.0
8         -
9       0.0
Name: foo, Length: 9, dtype: object
</code></pre>

<p>How can I convert this column to <code>np.float</code>, and have everything else that is not float convert it to <code>NaN</code>?</p>

<p>When I try:</p>

<pre><code>&gt;&gt; df['foo'].astype(np.float)
</code></pre>

<p>or</p>

<pre><code>&gt;&gt; df['foo'].apply(np.float)
</code></pre>

<p>I get <code>ValueError: could not convert string to float: -</code></p>
";;0;;2013-08-25T22:04:50.713;5.0;18434208;2016-07-18T13:38:02.743;;;;;283296.0;;1;17;<python><pandas>;Pandas: Converting to numeric, creating NaNs when necessary;39278.0
5921;5921;18505101.0;2.0;"<p>Here is a simplified example of my df:</p>

<pre><code>ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])
ds
      A         B         C         D
1  1.099679  0.042043  0.083903  0.410128
2  0.268205  0.718933  1.459374  0.758887
3  0.680566  0.538655  0.038236  1.169403
</code></pre>

<p>I would like to sum the data in the columns row wise:</p>

<pre><code>ds['sum']=ds.sum(axis=1)
ds
      A         B         C         D       sum
1  0.095389  0.556978  1.646888  1.959295  4.258550
2  1.076190  2.668270  0.825116  1.477040  6.046616
3  0.245034  1.066285  0.967124  0.791606  3.070049
</code></pre>

<p>Now, here comes my question! I would like to create 4 new columns and calculate the percentage value from the total (sum) in every row. So first value in the first new column should be (0.095389/4.258550), first value in the second new column (0.556978/4.258550)...and so on...
Help please </p>
";;0;;2013-08-29T07:40:53.390;4.0;18504967;2013-08-29T08:03:58.960;;;;;2219369.0;;1;21;<python><pandas><calculated-columns>;pandas dataframe create new columns and fill with calculated values from same df;37638.0
5941;5941;18518561.0;2.0;"<p>I have data sampled at essentially random intervals. I would like to compute a weighted moving average using numpy (or other python package). I have a crude implementation of a moving average, but I am having trouble finding a good way to do a weighted moving average, so that the values towards the center of the bin are weighted more than values towards the edges.</p>

<p>Here I generate some sample data and then take a moving average. How can I most easily implement a weighted moving average? Thanks!</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

#first generate some datapoint for a randomly sampled noisy sinewave
x = np.random.random(1000)*10
noise = np.random.normal(scale=0.3,size=len(x))
y = np.sin(x) + noise

#plot the data
plt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')
plt.xlabel('Time')
plt.ylabel('Intensity')

#define a moving average function
def moving_average(x,y,step_size=.1,bin_size=1):
    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size
    bin_avg = np.zeros(len(bin_centers))

    for index in range(0,len(bin_centers)):
        bin_center = bin_centers[index]
        items_in_bin = y[(x&gt;(bin_center-bin_size*0.5) ) &amp; (x&lt;(bin_center+bin_size*0.5))]
        bin_avg[index] = np.mean(items_in_bin)

    return bin_centers,bin_avg

#plot the moving average
bins, average = moving_average(x,y)
plt.plot(bins, average,label='moving average')

plt.show()
</code></pre>

<p>The output:
<img src=""https://i.stack.imgur.com/ca0kN.png"" alt=""Data and moving average""></p>

<p>Using the advice from crs17 to use ""weights="" in the np.average function, I came up weighted average function, which uses a Gaussian function to weight the data:</p>

<pre><code>def weighted_moving_average(x,y,step_size=0.05,width=1):
    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size
    bin_avg = np.zeros(len(bin_centers))

    #We're going to weight with a Gaussian function
    def gaussian(x,amp=1,mean=0,sigma=1):
        return amp*np.exp(-(x-mean)**2/(2*sigma**2))

    for index in range(0,len(bin_centers)):
        bin_center = bin_centers[index]
        weights = gaussian(x,mean=bin_center,sigma=width)
        bin_avg[index] = np.average(y,weights=weights)

    return (bin_centers,bin_avg)
</code></pre>

<p>Results look good:
<img src=""https://i.stack.imgur.com/SV7wj.png"" alt=""Working weighted average using numpy""></p>
";;2;;2013-08-29T17:48:14.383;5.0;18517722;2013-08-29T21:09:41.977;2013-08-29T21:09:41.977;;1078391.0;;1078391.0;;1;13;<python><numpy><pandas><scipy>;Weighted moving average in python;11277.0
5964;5964;;3.0;"<p>How can I print a pandas dataframe as a nice text-based table, like the following?</p>

<pre><code>+------------+---------+-------------+
| column_one | col_two |   column_3  |
+------------+---------+-------------+
|          0 |  0.0001 | ABCD        |
|          1 |  1e-005 | ABCD        |
|          2 |  1e-006 | long string |
|          3 |  1e-007 | ABCD        |
+------------+---------+-------------+
</code></pre>

<p>Update: I found a solution to this, posted as an answer below.</p>
";;0;;2013-08-30T08:40:28.667;7.0;18528533;2017-08-24T20:13:57.880;2017-08-24T20:13:57.880;;4909087.0;;374047.0;;1;19;<python><pandas><dataframe><printing>;Pretty Printing a pandas dataframe;21075.0
5997;5997;18554949.0;2.0;"<p>Let's say I have a log of user activity and I want to generate a report of total duration and the number of unique users per day.</p>

<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],
    'user_id': ['0001', '0001', '0002', '0002', '0002'],
    'duration': [30, 15, 20, 15, 30]})
</code></pre>

<p>Aggregating duration is pretty straightforward:</p>

<pre><code>group = df.groupby('date')
agg = group.aggregate({'duration': np.sum})
agg
            duration
date
2013-04-01        65
2013-04-02        45
</code></pre>

<p>What I'd like to do is sum the duration and count distincts at the same time, but I can't seem to find an equivalent for count_distinct:</p>

<pre><code>agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})
</code></pre>

<p>This works, but surely there's a better way, no?</p>

<pre><code>group = df.groupby('date')
agg = group.aggregate({'duration': np.sum})
agg['uv'] = df.groupby('date').user_id.nunique()
agg
            duration  uv
date
2013-04-01        65   2
2013-04-02        45   1
</code></pre>

<p>I'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function, but I don't have a lot of exposure to the various libraries at my disposal. Also, it seems that the groupby object already knows this information, so wouldn't I just be duplicating effort?</p>
";;0;;2013-09-01T03:25:36.923;13.0;18554920;2017-07-11T21:27:50.523;;;;;2464065.0;;1;39;<python><pandas>;Pandas aggregate count distinct;33368.0
6017;6017;18580496.0;2.0;"<p>I have a pandas <code>DataFrame</code> called <code>data</code> with a column called <code>ms</code>.  I want to eliminate all the rows where <code>data.ms</code> is above the 95% percentile.  For now, I'm doing this:</p>

<pre><code>limit = data.ms.describe(90)['95%']
valid_data = data[data['ms'] &lt; limit]
</code></pre>

<p>which works, but I want to generalize that to any percentile.  What's the best way to do that?</p>
";;0;;2013-09-02T20:37:48.347;2.0;18580461;2016-09-01T16:02:54.840;2013-09-02T20:54:14.060;;564538.0;;506824.0;;1;12;<python><pandas><filtering><percentile>;Eliminating all data over a given percentile;9134.0
6035;6035;18594595.0;2.0;"<p>What is the most idiomatic way to normalize each row of a pandas DataFrame? Normalizing the columns is easy, so one (very ugly!) option is:</p>

<pre><code>(df.T / df.T.sum()).T
</code></pre>

<p>Pandas broadcasting rules prevent <code>df / df.sum(axis=1)</code> from doing this</p>
";;0;;2013-09-03T14:09:24.770;7.0;18594469;2017-08-07T00:27:07.210;2017-08-07T00:27:07.210;;395857.0;;1332492.0;;1;27;<python><pandas><normalization><dataframe>;Normalizing a pandas DataFrame by row;11082.0
6056;6056;18611535.0;3.0;"<p>I regularly perform pandas operations on data frames in excess of 15 million or so rows and I'd love to have access to a progress indicator for particular operations.</p>

<p>Does a text based progress indicator for pandas split-apply-combine operations exist?</p>

<p>For example, in something like:</p>

<pre><code>df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)
</code></pre>

<p>where <code>feature_rollup</code> is a somewhat involved function that take many DF columns and creates new user columns through various methods.  These operations can take a while for large data frames so I'd like to know if it is possible to have text based output in an iPython notebook that updates me on the progress.</p>

<p>So far, I've tried canonical loop progress indicators for Python but they don't interact with pandas in any meaningful way.</p>

<p>I'm hoping there's something I've overlooked in the pandas library/documentation that allows one to know the progress of a split-apply-combine.  A simple implementation would maybe look at the total number of data frame subsets upon which the <code>apply</code> function is working and report progress as the completed fraction of those subsets.</p>

<p>Is this perhaps something that needs to be added to the library?</p>
";;4;;2013-09-03T23:55:02.253;10.0;18603270;2015-12-20T20:36:20.840;;;;;1968405.0;;1;20;<python><pandas><ipython>;Progress indicator during pandas operations (python);6648.0
6070;6070;18624069.0;2.0;"<p>I created a <code>Series</code> from a <code>DataFrame</code>, when I resampled some data with a count
like so: where <code>H2</code> is a <code>DataFrame</code>:</p>

<pre><code>    H3=H2[['SOLD_PRICE']]
    H5=H3.resample('Q',how='count')
    H6=pd.rolling_mean(H5,4)
</code></pre>

<p>This yielded a series that looks like this:</p>

<pre><code>1999-03-31  SOLD_PRICE     NaN
1999-06-30  SOLD_PRICE     NaN
1999-09-30  SOLD_PRICE     NaN
1999-12-31  SOLD_PRICE    3.00
2000-03-31  SOLD_PRICE    3.00
</code></pre>

<p>with an index that looks like:</p>

<pre><code>MultiIndex
[(1999-03-31 00:00:00, u'SOLD_PRICE'), (1999-06-30 00:00:00, u'SOLD_PRICE'), (1999-09-30 00:00:00, u'SOLD_PRICE'), (1999-12-31 00:00:00, u'SOLD_PRICE'),.....
</code></pre>

<p>I don't want the second column as an index. Ideally I'd have a <code>DataFrame</code> with column 1 as ""Date"" and column 2 as ""Sales"" (dropping the second level of the index). I don't quite see how to reconfigure the index.</p>
";;2;;2013-09-04T21:23:25.173;8.0;18624039;2013-09-04T21:46:08.830;2013-09-04T21:38:21.033;;564538.0;;137783.0;;1;14;<python><pandas>;Pandas reset index on series to remove multiindex;24921.0
6091;6091;18661440.0;5.0;"<p>I'm trying to do some data work in Python pandas and having trouble writing out my results.
I read my data in as a CSV file and been exporting each script as it's own CSV file which works fine. Lately though I've tried exporting everything in 1 Excel file with worksheets and a few of the sheets give me an error </p>

<p>""'utf8' codec can't decode byte 0xe9 in position 1: invalid continuation byte""</p>

<p>I have no idea how to even start finding any characters that could be causing problems exporting to Excel. Not sure why it exports to CSV just fine though :(</p>

<p>relevant lines</p>

<pre><code>from pandas import ExcelWriter
data = pd.read_csv(input)
writer = ExcelWriter(output) #output is just the filename
fundraisers.to_excel(writer, ""fundraisers"")
locations.to_excel(writer, ""locations"") #error
locations.to_csv(outputcsv) #works
writer.save()
</code></pre>

<p>printing head of offending dataframe</p>

<pre><code>Event ID    Constituent ID  Email Address   First Name  \   Last Name
f       1       A       A       1
F       4       L       R       C
M       1       1       A       D
F       4       A       A       G
M       2       0       R       G
M       3       O       O       H
M       2       T       E       H
M       2       A       A       H
M       2       M       M       K
F       3       J       E       K
Location ID raised  raised con  raised email
a   0   0   0
a   8   0   0
o   0   0   0
o   0   0   0
o   0   0   0
t   5   0   0
o   1   0   0
o   6   a   0
o   6   0   0
d   0   0   0
</code></pre>

<p>looking at the excel sheet I do actually get a partial print out. Anything in the first name column and beyond are blank, but event, constituent and email all print. </p>

<p>edit: Trying to read the csv in as utf8 fails, but reading it in as latin1 works. Is there a way to specify the to_excel encoding? Or decode and encode my dataframe to utf8? </p>
";;6;;2013-09-05T20:31:06.083;5.0;18645401;2014-11-17T18:52:11.747;2013-09-06T13:38:47.487;;1477617.0;;1477617.0;;1;14;<python><excel><utf-8><pandas>;Python pandas to_excel 'utf8' codec can't decode byte;28678.0
6095;6095;18646275.0;2.0;"<p>I have a Pandas data frame object of shape (X,Y) that looks like this:</p>

<pre><code>[[1, 2, 3],
[4, 5, 6],
[7, 8, 9]]
</code></pre>

<p>and a numpy sparse matrix (CSC) of shape (X,Z) that looks something like this</p>

<pre><code>[[0, 1, 0],
[0, 0, 1],
[1, 0, 0]]
</code></pre>

<p>How can I add the content from the matrix to the data frame in a new named column such that the data frame will end up like this:</p>

<pre><code>[[1, 2, 3, [0, 1, 0]],
[4, 5, 6, [0, 0, 1]],
[7, 8, 9, [1, 0, 0]]]
</code></pre>

<p>Notice the data frame now has shape (X, Y+1) and rows from the matrix are elements in the data frame.</p>
";;5;;2013-09-05T21:15:01.987;1.0;18646076;2013-09-05T22:13:48.380;;;;;64167.0;;1;20;<python><numpy><pandas>;Add numpy array as column to Pandas data frame;32167.0
6114;6114;18666142.0;1.0;"<p><strong>TL;DR If loaded fields in a Pandas DataFrame contain JSON documents themselves, how can they be worked with in a Pandas like fashion?</strong></p>

<p>Currently I'm directly dumping json/dictionary results from a Twitter library (<a href=""https://github.com/ryanmcgrath/twython"" rel=""noreferrer"">twython</a>) into a Mongo collection (called users here).</p>

<pre><code>from twython import Twython
from pymongo import MongoClient

tw = Twython(...&lt;auth&gt;...)

# Using mongo as object storage 
client = MongoClient()
db = client.twitter
user_coll = db.users

user_batch = ... # collection of user ids
user_dict_batch = tw.lookup_user(user_id=user_batch)

for user_dict in user_dict_batch:
    if(user_coll.find_one({""id"":user_dict['id']}) == None):
        user_coll.insert(user_dict)
</code></pre>

<p>After populating this database I read the documents into Pandas:</p>

<pre><code># Pull straight from mongo to pandas
cursor = user_coll.find()
df = pandas.DataFrame(list(cursor))
</code></pre>

<p>Which works like magic:</p>

<p><img src=""https://i.stack.imgur.com/30lZs.png"" alt=""Pandas is magic""></p>

<p>I'd like to be able to mangle the 'status' field Pandas style (directly accessing attributes). Is there a way?</p>

<p><img src=""https://i.stack.imgur.com/jZ6G3.png"" alt=""status field""></p>

<p>EDIT: Something like df['status:text']. Status has fields like 'text', 'created_at'. One option could be flattening/normalizing this json field like <a href=""https://github.com/pydata/pandas/pull/4007"" rel=""noreferrer"">this pull request</a> Wes McKinney was working on.</p>
";;7;;2013-09-06T19:32:54.310;9.0;18665284;2013-11-05T04:44:22.207;2013-11-05T04:44:22.207;;700228.0;;700228.0;;1;12;<python><json><mongodb><twitter><pandas>;How do I access embedded json objects in a Pandas DataFrame?;4640.0
6126;6126;18674915.0;2.0;"<p>Can I insert a column at a specific column index in pandas? </p>

<pre><code>import pandas as pd
df = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})
df['n'] = 0
</code></pre>

<p>This will put column <code>n</code> as the last column of <code>df</code>, but isn't there a way to tell <code>df</code> to put <code>n</code> at the beginning?</p>
";;0;;2013-09-07T13:59:01.607;10.0;18674064;2017-01-27T08:53:59.197;;;;;2635863.0;;1;61;<python><indexing><pandas>;how do I insert a column at a specific column index in pandas?;47504.0
6130;6130;18677517.0;1.0;"<p>I have the table below in a Pandas <code>DataFrame</code>:</p>

<pre><code>    q_string    q_visits    q_date
0   nucleus         1790        2012-10-02 00:00:00
1   neuron          364         2012-10-02 00:00:00
2   current         280         2012-10-02 00:00:00
3   molecular       259         2012-10-02 00:00:00
4   stem            201         2012-10-02 00:00:00
</code></pre>

<p>The table contains query volume from a server log, by day. I would like to do 2 things:</p>

<ol>
<li>I would like to group queries by month summing the query volume of a query for the whole month e.g. if 'molecular' was present on the 2012-10-02 with volume 1000 and on the 2012-10-03 with volume 500, then it should have an entry in the new table of 1500 (volume) with date 2012-10-31 (end of the month end-point representing the month &ndash; all dates in the transformed table will be month ends representing the whole month to which they relate).</li>
<li>I want to add a 5th column which contains the <strong>month</strong>-normalized <code>q_visits</code>.  I.e., a term's monthly query volume divided by the total query volume for the month across all terms.</li>
</ol>

<p>What is the best way of doing this?</p>
";;0;;2013-09-07T19:50:29.667;0.0;18677271;2016-11-30T20:22:49.823;2016-11-30T20:22:49.823;;2662901.0;;213216.0;;1;11;<python><pandas>;Grouping daily data by month in python/pandas and then normalizing;19066.0
6142;6142;18689514.0;1.0;"<p>Is it possible to groupby a multi-index (2 levels) pandas dataframe  by one of the multi-index levels ?  </p>

<p>The only way I know of doing it is to reset_index on a multiindex and then set index again. I am sure there is a better way to do it, and I want to know how. </p>
";;0;;2013-09-08T22:57:55.550;1.0;18689474;2013-09-08T23:05:39.930;;;;;1594352.0;;1;13;<python><pandas>;group multi-index pandas dataframe;6147.0
6143;6143;18689712.0;2.0;"<p>My numpy arrays use <code>np.nan</code> to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.</p>

<p>Naively I used <code>numpy.isnan(val)</code>, which works well unless <code>val</code> isn't among the subset of types supported by <code>numpy.isnan()</code>. For example, missing data can occur in string fields, in which case I get:</p>

<pre><code>&gt;&gt;&gt; np.isnan('some_string')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: Not implemented for this type
</code></pre>

<p>Other than writing an expensive wrapper that catches the exception and returns <code>False</code>, is there a way to handle this elegantly and efficiently?</p>
";;2;;2013-09-08T23:05:11.623;10.0;18689512;2013-09-09T00:20:13.470;2013-09-08T23:11:07.083;;916907.0;;916907.0;;1;55;<python><numpy><pandas>;Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?;68431.0
6147;6147;18691949.0;6.0;"<p>I've got a pandas DataFrame filled mostly with real numbers, but there is a few <code>nan</code> values in it as well.</p>

<p>How can I replace the <code>nan</code>s with averages of columns where they are?</p>

<p>This question is very similar to this one: <a href=""https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns"">numpy array: replace nan values with average of columns</a>  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.</p>
";;0;;2013-09-08T23:54:05.380;12.0;18689823;2017-07-10T18:54:20.213;2017-05-23T11:55:10.630;;-1.0;;300886.0;;1;29;<python><pandas><nan>;pandas DataFrame: replace nan values with average of columns;36285.0
6157;6157;;4.0;"<p>I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance. </p>

<p>Dataframe:</p>

<pre><code>    id    value
0    0     10.2
1    1      5.7
2    2      7.4
</code></pre>
";;4;;2013-09-09T09:49:56.927;21.0;18695605;2016-10-03T17:41:02.203;;;;;857130.0;;1;42;<python><dictionary><pandas>;python pandas dataframe to dictionary;63301.0
6177;6177;18714509.0;3.0;"<p>I have a DataFrame loaded from a tsv file. I wanted to generate some exploratory plots. The problem is that the data set is large (~1 million rows), so it there are too many points on the plot to see a trend. Plus, it is taking a while to plot.</p>

<p>I wanted to sub-sample 10000 randomly distributed rows. Also, this should be reproducible, so the same sequence of random numbers is generated in each run.</p>

<p>Thanks for help.</p>

<p>This: <a href=""https://stackoverflow.com/questions/17260109/sample-two-pandas-dataframes-the-same-way/17260475#17260475"">Sample two pandas dataframes the same way</a> seems to be on the right track, but I cannot guarantee the subsample size. </p>
";;5;;2013-09-10T08:22:06.647;2.0;18713929;2016-06-09T20:22:27.043;2017-05-23T12:16:49.660;;-1.0;;298455.0;;1;11;<python><numpy><pandas><subsampling>;Subsample pandas dataframe;9619.0
6266;6266;18799713.0;3.0;"<p>I have 2 dataframes:</p>

<p>restaurant_ids_dataframe </p>

<pre><code>Data columns (total 13 columns):
business_id      4503  non-null values
categories       4503  non-null values
city             4503  non-null values
full_address     4503  non-null values
latitude         4503  non-null values
longitude        4503  non-null values
name             4503  non-null values
neighborhoods    4503  non-null values
open             4503  non-null values
review_count     4503  non-null values
stars            4503  non-null values
state            4503  non-null values
type             4503  non-null values
dtypes: bool(1), float64(3), int64(1), object(8)`
</code></pre>

<p>and </p>

<p>restaurant_review_frame</p>

<pre><code>Int64Index: 158430 entries, 0 to 229905
Data columns (total 8 columns):
business_id    158430  non-null values
date           158430  non-null values
review_id      158430  non-null values
stars          158430  non-null values
text           158430  non-null values
type           158430  non-null values
user_id        158430  non-null values
votes          158430  non-null values
dtypes: int64(1), object(7)
</code></pre>

<p>I would like to join these two DataFrames to make them into a single dataframe using the DataFrame.join() command in pandas.</p>

<p>I have tried the following line of code:</p>

<pre><code>#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'
restaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')
</code></pre>

<p>But when I try this I get the following error:</p>

<pre><code>Exception: columns overlap: Index([business_id, stars, type], dtype=object)
</code></pre>

<p>I am very new to pandas and have no clue what I am doing wrong as far as executing the join statement is concerned.</p>

<p>any help would be much appreciated.</p>
";;0;;2013-09-13T18:29:46.787;5.0;18792918;2015-09-03T20:33:53.623;;;;;1009091.0;;1;28;<python><pandas><left-join><dataframe>;Pandas Combining 2 Data Frames (join on a common column);63764.0
6292;6292;18835121.0;2.0;"<p>I have a multi-index data frame with columns  'A' and 'B'. </p>

<p>Is there is a way to select rows by filtering on one column of the multi-index without reseting the index to single column index. </p>

<p>For Example.</p>

<pre><code># has multi-index (A,B)
df
#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple

df.ix[df.A ==1]
</code></pre>
";;2;;2013-09-16T18:45:37.560;10.0;18835077;2017-08-21T16:46:53.860;;;;;1594352.0;;1;34;<python><pandas>;selecting from multi-index pandas;34779.0
6295;6295;18837389.0;8.0;"<p>I have a Python dictionary like the following:</p>

<pre><code>{u'2012-06-08': 388,
 u'2012-06-09': 388,
 u'2012-06-10': 388,
 u'2012-06-11': 389,
 u'2012-06-12': 389,
 u'2012-06-13': 389,
 u'2012-06-14': 389,
 u'2012-06-15': 389,
 u'2012-06-16': 389,
 u'2012-06-17': 389,
 u'2012-06-18': 390,
 u'2012-06-19': 390,
 u'2012-06-20': 390,
 u'2012-06-21': 390,
 u'2012-06-22': 390,
 u'2012-06-23': 390,
 u'2012-06-24': 390,
 u'2012-06-25': 391,
 u'2012-06-26': 391,
 u'2012-06-27': 391,
 u'2012-06-28': 391,
 u'2012-06-29': 391,
 u'2012-06-30': 391,
 u'2012-07-01': 391,
 u'2012-07-02': 392,
 u'2012-07-03': 392,
 u'2012-07-04': 392,
 u'2012-07-05': 392,
 u'2012-07-06': 392}
</code></pre>

<p>The keys are <a href=""http://en.wikipedia.org/wiki/Unicode"" rel=""noreferrer"">Unicode</a> dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)</p>

<pre><code>     Date         DateValue
0    2012-07-01    391
1    2012-07-02    392
2    2012-07-03    392
.    2012-07-04    392
.    ...           ...
.    ...           ...
</code></pre>

<p>Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.</p>

<p>I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.</p>

<p>So far I have tried converting the dict into a series object but this doesn't seem to maintain the relationship between the columns:</p>

<pre><code>s  = Series(my_dict,index=my_dict.keys())
</code></pre>
";;4;;2013-09-16T21:02:22.863;38.0;18837262;2017-08-23T16:36:57.107;2015-11-16T21:03:25.943;;63550.0;;1009091.0;;1;78;<python><pandas><dataframe>;Convert Python dict into a dataframe;151737.0
6298;6298;18837709.0;1.0;"<p>I'm loading a csv file, which has the following columns:
date, textA, textB, numberA, numberB</p>

<p>I want to group by the columns: date, textA and textB - but want to apply ""sum"" to numberA, but ""min"" to numberB. </p>

<pre><code>data = pd.read_table(""file.csv"", sep="","", thousands=',')
grouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)
</code></pre>

<p>...but I cannot see how to then apply two different aggregate functions, to two different columns?
I.e. <code>sum(numberA), min(numberB)</code></p>
";;0;;2013-09-16T21:30:59.703;2.0;18837659;2013-09-16T23:54:54.147;2013-09-16T21:44:39.600;;2158473.0;;2785406.0;;1;18;<pandas><aggregation>;Pandas - possible to aggregate two columns using two different aggregations?;11390.0
6310;6310;18852410.0;4.0;"<p>I have a dataset with potentially duplicate records of the identifier <code>appkey</code>. The duplicated records should ideally not exist and therefore I take them to be data collection mistakes. I need to drop all instances of an <code>appkey</code> which occurs more than once.</p>

<p>The <code>drop_duplicates</code> method is not useful in this case (or is it?) as it either selects the first or the last of the duplicates. Is there any obvious idiom to achieve this with pandas?</p>
";;0;;2013-09-17T13:29:02.153;2.0;18851216;2015-10-28T11:09:54.860;;;;;1533691.0;;1;11;<pandas><duplicates>;Pandas: Drop all records of duplicate indices;1815.0
6328;6328;18878267.0;3.0;"<p>How can I format IPython html display of pandas dataframes so that</p>

<ol>
<li>numbers are right justified</li>
<li>numbers have commas as thousands separator</li>
<li>large floats have no decimal places</li>
</ol>

<p>I understand that <code>numpy</code> has the facility of <code>set_printoptions</code> where I can do:</p>

<pre><code>int_frmt:lambda x : '{:,}'.format(x)
np.set_printoptions(formatter={'int_kind':int_frmt})
</code></pre>

<p>and similarly for other data types. </p>

<p>But IPython does not pick up these formatting options when displaying dataframes in html. I still need to have</p>

<pre><code>pd.set_option('display.notebook_repr_html', True)
</code></pre>

<p>but with 1, 2, 3 as in above.</p>

<p><strong>Edit:</strong> Below is my solution for 2 &amp; 3 ( not sure this is the best way ), but I still need to figure out how to make number columns right justified.</p>

<pre><code>from IPython.display import HTML
int_frmt = lambda x: '{:,}'.format(x)
float_frmt = lambda x: '{:,.0f}'.format(x) if x &gt; 1e3 else '{:,.2f}'.format(x)
frmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}
frmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}
HTML(df.to_html(formatters=frmt))
</code></pre>
";;1;;2013-09-18T15:12:15.097;14.0;18876022;2017-05-12T11:31:26.277;2016-12-06T10:33:26.330;;2543372.0;;625914.0;;1;25;<python><html><pandas><ipython>;How to format IPython html display of Pandas dataframe?;11860.0
6330;6330;18878413.0;2.0;"<p>I'm having some trouble sorting and then resetting my Index in Pandas:</p>

<pre><code>dfm = dfm.sort(['delt'],ascending=False)
dfm = dfm.reindex(index=range(1,len(dfm)))
</code></pre>

<p>The dataframe returns unsorted after I reindex.  My ultimate goal is to have a sorted dataframe with index numbers from 1 --> len(dfm) so if there's a better way to do that, I wouldn't mind,</p>

<p>Thanks!</p>
";;0;;2013-09-18T17:11:03.883;2.0;18878308;2017-03-06T17:07:17.980;;;;;2088886.0;;1;15;<python><sorting><pandas><reindex>;Pandas: Reindex Unsorts Dataframe;11800.0
6342;6342;;3.0;"<p>I'm trying to unzip a csv file and pass it into pandas so I can work on the file.<br>
The code I have tried so far is: </p>

<pre><code>import requests, zipfile, StringIO
r = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')
z = zipfile.ZipFile(StringIO.StringIO(r.content))
crime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))
</code></pre>

<p><em>After the last line, although python is able to get the file, I get a ""does not exist"" at the end of the error.</em></p>

<p>Can someone tell me what I'm doing incorrectly?</p>
";;0;;2013-09-19T02:05:15.120;8.0;18885175;2017-07-05T20:55:41.000;2013-09-19T20:50:15.630;;1240268.0;;2793667.0;;1;29;<python><zip><pandas>;Read a zipped file as a pandas DataFrame;18856.0
6344;6344;25208947.0;3.0;"<p>I am looking for for a pythonic way to handle the following problem.</p>

<p>The <code>pandas.get_dummies()</code> method is great to create dummies from a categorical column of a dataframe. For example, if the column has values in <code>['A', 'B']</code>, <code>get_dummies()</code> creates 2 dummy variables and assigns 0 or 1 accordingly.</p>

<p>Now, I need to handle this situation. A single column, let's call it 'label', has values like <code>['A', 'B', 'C', 'D', 'A*C', 'C*D']</code> . <code>get_dummies()</code> creates 6 dummies, but I only want 4 of them, so that a row could have multiple 1s. </p>

<p>Is there a way to handle this in a pythonic way? I could only think of some step-by-step algorithm to get it, but that would not include get_dummies(). 
Thanks</p>

<p>Edited, hope it is more clear!</p>
";;3;;2013-09-19T08:20:56.207;15.0;18889588;2016-09-13T00:38:52.143;2014-06-04T18:32:42.407;;121704.0;;2775630.0;;1;21;<python><pandas><dummy-data><categorical-data>;Create dummies from column with multiple values in pandas;14589.0
6371;6371;18916457.0;4.0;"<p>I've create a tuple generator that extract information from a file filtering only the records of interest and converting it to a tuple that generator returns.</p>

<p>I've try to create a DataFrame from:</p>

<pre><code>import pandas as pd
df = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)
</code></pre>

<p>but throws an error:</p>

<pre><code>... 
C:\Anaconda\envs\py33\lib\site-packages\pandas\core\frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)
   1046                 values.append(row)
   1047                 i += 1
-&gt; 1048                 if i &gt;= nrows:
   1049                     break
   1050 

TypeError: unorderable types: int() &gt;= NoneType()
</code></pre>

<p>I managed it to work consuming the generator in a list, but uses twice memory:</p>

<pre><code>df = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)
</code></pre>

<p>The files I want to load are big, and memory consumption matters. The last try my computer spends two hours trying to increment virtual memory :(</p>

<p><strong>The question:</strong> Anyone knows a method to create a DataFrame from a record generator directly, without previously convert it to a list?</p>

<p>Note: I'm using python 3.3 and pandas 0.12 with Anaconda on Windows.</p>

<p><strong>Update:</strong></p>

<p>It's not problem of reading the file, my tuple generator do it well, it scan a text compressed file of intermixed records line by line and convert only the wanted data to the correct types, then it yields fields in a generator of tuples form.
Some numbers, it scans 2111412 records on a 130MB gzip file, about 6.5GB uncompressed, in about a minute and with little memory used.</p>

<p>Pandas 0.12 does not allow generators, dev version allows it but put all the generator in a list and then convert to a frame. It's not efficient but it's something that have to deal internally pandas. Meanwhile I've must think about buy some more memory.</p>
";;4;;2013-09-20T11:42:51.307;5.0;18915941;2017-04-27T15:10:05.023;2013-09-21T15:00:25.277;;2125065.0;;2125065.0;;1;17;<python><pandas>;Create a pandas DataFrame from generator?;9974.0
6409;6409;18937309.0;6.0;"<p>I've a Pandas data frame, where one column contains text. I'd like to get a list of unique words appearing across the entire column (space being the only split).</p>

<pre><code>import pandas as pd

r1=['My nickname is ft.jgt','Someone is going to my place']

df=pd.DataFrame(r1,columns=['text'])
</code></pre>

<p>The output should look like this:</p>

<pre><code>['my','nickname','is','ft.jgt','someone','going','to','place']
</code></pre>

<p>It wouldn't hurt to get a count as well, but it is not required.</p>

<p>Thanks,</p>

<p>G</p>
";;0;;2013-09-21T19:56:21.650;3.0;18936957;2014-06-27T18:19:04.063;;;;;2423116.0;;1;14;<python><text><pandas>;Count distinct words from a Pandas Data Frame;11485.0
6419;6419;18942558.0;1.0;"<p>I have dataframe in Pandas for example:</p>

<pre><code>Col1 Col2
A     1 
B     2
C     3
</code></pre>

<p>Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be:</p>

<pre><code>Col1 Col2 Col3
A    1    1
B    2    0
C    3    0
</code></pre>

<p>Any idea on how to achieve this?</p>
";;1;;2013-09-22T09:50:44.967;12.0;18942506;2016-10-25T22:54:18.913;2016-10-25T22:54:18.913;;1079075.0;;2691630.0;;1;41;<python><pandas><dataframe>;Add new column in Pandas DataFrame Python;83369.0
6465;6465;18973430.0;3.0;"<p>Supposely, I have the bar chart as below:</p>

<p><img src=""https://i.stack.imgur.com/wy1lP.png"" alt=""BarPlot""></p>

<p>Any ideas on how to set different colors for each carrier? As for example, AK would be Red, GA would be Green, etc?</p>

<p>I am using Pandas and matplotlib in Python</p>

<pre><code>&gt;&gt;&gt; f=plt.figure()
&gt;&gt;&gt; ax=f.add_subplot(1,1,1)
&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])
&lt;Container object of 4 artists&gt;
&gt;&gt;&gt; ax.get_children()
[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, &lt;matplotlib.text.Text object at 0x7836450&gt;, &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, &lt;matplotlib.spines.Spine object at 0x6529950&gt;, &lt;matplotlib.spines.Spine object at 0x69aef50&gt;, &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]
&gt;&gt;&gt; ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.
</code></pre>

<p>For the suggestions above, how do exactly we could enumerate ax.get_children() and check if the object type is rectangle? So if the object is rectangle, we would assign different random color?</p>
";;1;;2013-09-24T05:13:17.217;10.0;18973404;2016-03-11T18:02:12.313;2014-07-31T08:44:22.730;;77764.0;;2691630.0;;1;34;<python><matplotlib><pandas><bar-chart>;Setting Different Bar color in matplotlib Python;46910.0
6482;6482;18992172.0;1.0;"<p>In ipython Notebook, first create a pandas Series object, then by calling the instance method .hist(), the browser displays the figure. </p>

<p>I am wondering how to save this figure to a file (I mean not by right click and save as, but the commands needed in the script).</p>
";;0;;2013-09-24T21:08:19.303;10.0;18992086;2013-09-24T21:28:43.127;2013-09-24T21:19:54.133;;564538.0;;2233683.0;;1;31;<python><pandas><histogram>;save a pandas.Series histogram plot to file;26802.0
6530;6530;19062640.0;1.0;"<p>I have a pandas dataframe and would like to drop all columns save the index and a column named 'bob'</p>

<p>How would I do this?</p>
";;0;;2013-09-28T02:36:22.727;1.0;19062612;2013-09-28T02:40:59.117;;;;;1849997.0;;1;14;<pandas>;Delete all but one column of pandas dataframe?;3170.0
6536;6536;19071572.0;4.0;"<p>I have a pandas dataframe with the following column names:</p>

<p>Result1, Test1, Result2, Test2, Result3, Test3, etc...</p>

<p>I want to drop all the columns whose name contains the word ""Test"". The numbers of such columns is not static but depends on a previous function.</p>

<p>How can I do that?</p>
";;0;;2013-09-28T20:10:42.167;;19071199;2017-05-30T22:20:33.127;2016-06-14T20:06:26.390;;1534017.0;;2827060.0;;1;15;<python><pandas><dataframe>;Pandas dataframe: drop columns whose name contains a specific string;10447.0
6546;6546;19078773.0;3.0;"<p>I'm having trouble with Pandas' groupby functionality. I've read <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html"" rel=""noreferrer"">the documentation</a>, but I can't see to figure out how to apply aggregate functions to multiple columns <em>and</em> have custom names for those columns.</p>

<p>This comes very close, but the data structure returned has nested column headings:</p>

<pre><code>data.groupby(""Country"").agg(
        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})
</code></pre>

<p>(ie. I want to take the mean and std of column2, but return those columns as ""mean"" and ""std"")</p>

<p>What am I missing?</p>
";;1;;2013-09-29T13:00:05.817;11.0;19078325;2017-07-09T13:30:36.087;;;;;249323.0;;1;22;<python><group-by><pandas><aggregate-functions>;Naming returned columns in Pandas aggregate function?;15533.0
6569;6569;19103754.0;2.0;"<p>I have a 719mb CSV file that looks like:</p>

<pre><code>from, to, dep, freq, arr, code, mode   (header row)
RGBOXFD,RGBPADTON,127,0,27,99999,2
RGBOXFD,RGBPADTON,127,0,33,99999,2
RGBOXFD,RGBRDLEY,127,0,1425,99999,2
RGBOXFD,RGBCHOLSEY,127,0,52,99999,2
RGBOXFD,RGBMDNHEAD,127,0,91,99999,2
RGBDIDCOTP,RGBPADTON,127,0,46,99999,2
RGBDIDCOTP,RGBPADTON,127,0,3,99999,2
RGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2
RGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2
RGBDIDCOTP,RGBPADTON,127,0,115,99999,2
and so on... 
</code></pre>

<p>I want to load in to a pandas DataFrame. Now I know there is a load from csv method:</p>

<pre><code> r = pd.DataFrame.from_csv('test_data2.csv')
</code></pre>

<p>But I specifically want to load it as a 'MultiIndex' DataFrame where from and to are the indexes:</p>

<p>So ending up with:</p>

<pre><code>                   dep, freq, arr, code, mode
RGBOXFD RGBPADTON  127     0   27  99999    2
        RGBRDLEY   127     0   33  99999    2
        RGBCHOLSEY 127     0 1425  99999    2
        RGBMDNHEAD 127     0 1525  99999    2
</code></pre>

<p>etc. I'm not sure how to do that?</p>
";;0;;2013-09-30T20:52:39.493;3.0;19103624;2013-09-30T22:14:18.177;;;;;411929.0;;1;14;<python><csv><numpy><pandas>;Load CSV to Pandas MultiIndex DataFrame;11781.0
6572;6572;19106012.0;3.0;"<p>dates seem to be a tricky thing in python, and I am having a lot of trouble simply stripping the date out of the pandas TimeStamp. I would like to get from <code>2013-09-29 02:34:44</code> to simply <code>09-29-2013</code></p>

<p>I have a dataframe with a column Created_date:</p>

<pre><code>Name: Created_Date, Length: 1162549, dtype: datetime64[ns]`
</code></pre>

<p>I have tried applying the <code>.date()</code> method on this Series, eg: <code>df.Created_Date.date()</code>, but I get the error <code>AttributeError: 'Series' object has no attribute 'date'</code></p>

<p>Can someone help me out?</p>
";;0;;2013-10-01T00:16:51.657;8.0;19105976;2017-08-09T03:15:55.033;;;;;1214985.0;;1;15;<python><date><pandas>;Get MM-DD-YYYY from pandas Timestamp;25815.0
6580;6580;19112890.0;2.0;"<p>I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   So</p>

<pre><code>table = Cell(""A1"").table
</code></pre>

<p>gives</p>

<pre><code>table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]

headers = table.pop(0) # gives the headers as list and leaves data
</code></pre>

<p>I am busy writing code to translate this, but my guess is that it is such a simple use that there must be method to do this.    Cant seem to find it in documentation.   Any pointers to the method that would simplify this?</p>
";;0;;2013-10-01T09:19:40.023;18.0;19112398;2017-05-11T14:21:29.533;;;;;2107677.0;;1;65;<python><pandas><datanitro>;Getting list of lists into pandas DataFrame;72639.0
6587;6587;;3.0;"<p>I have time-indexed data:</p>

<pre><code>df2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })
df2 = df2.set_index('day')
df2
               b
 day             
2012-01-01  0.22
2012-01-03  0.30
</code></pre>

<p>What is the best way to extend this data frame so that it has one row for every day in January 2012 (say), where all columns are set to <code>NaN</code> (here only <code>b</code>) where we don't have data?</p>

<p>So the desired result would be:</p>

<pre><code>               b
 day             
2012-01-01  0.22
2012-01-02   NaN
2012-01-03  0.30
2012-01-04   NaN
...
2012-01-31   NaN
</code></pre>

<p>Many thanks!</p>
";;0;;2013-10-01T14:36:23.273;3.0;19119039;2016-05-19T16:36:28.750;;;;;1939450.0;;1;13;<python><pandas>;pandas - Extend Index of a DataFrame setting all columns for new rows to NaN?;6379.0
6599;6599;30691921.0;4.0;"<p>I work with Series and DataFrames on the terminal a lot. The default <code>__repr__</code> for a Series returns a reducted sample, with some head and tail values, but the rest missing.</p>

<p>Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.</p>
";;4;;2013-10-01T19:46:07.170;45.0;19124601;2017-06-15T17:51:27.017;2013-10-01T19:53:47.350;;2827648.0;;916907.0;;1;138;<python><pandas><dataframe>;Is there a way to (pretty) print the entire Pandas Series / DataFrame?;94738.0
6600;6600;19125531.0;2.0;"<p>I am attempting a merge between two data frames.  Each data frame has two index levels (date, cusip).  In the columns, some columns match between the two (currency, adj date) for example.</p>

<p>What is the best way to merge these by index, but to not take two copies of currency and adj date.</p>

<p>Each data frame is 90 columns, so I am trying to avoid writing everything out by hand.</p>

<pre><code>df:                 currency  adj_date   data_col1 ...
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45
...

df2:                currency  adj_date   data_col2 ...
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45
...
</code></pre>

<p>If I do:</p>

<pre><code>dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')
</code></pre>

<p>I get </p>

<pre><code>dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03
</code></pre>

<p>Thank you!
    ...</p>
";;4;;2013-10-01T20:16:26.607;11.0;19125091;2017-07-13T19:29:46.013;;;;;1911092.0;;1;22;<python><pandas>;Pandas Merge - How to avoid duplicating columns;21524.0
6647;6647;;2.0;"<p>I have a dataframe df :</p>

<pre><code>   20060930  10.103       NaN     10.103   7.981
   20061231  15.915       NaN     15.915  12.686
   20070331   3.196       NaN      3.196   2.710
   20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>Then I want to select rows with certain sequence numbers which indicated in a list, suppose here is [1,3], then left:</p>

<pre><code>   20061231  15.915       NaN     15.915  12.686
   20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>How or what function can do that ?</p>
";;0;;2013-10-03T09:36:32.243;9.0;19155718;2017-08-08T11:54:29.667;;;;;2806761.0;;1;23;<python><pandas>;Select Pandas rows based on list index;38383.0
6658;6658;19170098.0;1.0;"<p>I'm wondering if there is a more efficient way to use the str.contains() function in Pandas, to search for two partial strings at once. I want to search a given column in a dataframe for data that contains either ""nt"" or ""nv"". Right now, my code looks like this:</p>

<pre><code>    df[df['Behavior'].str.contains(""nt"", na=False)]
    df[df['Behavior'].str.contains(""nv"", na=False)]
</code></pre>

<p>And then I append one result to another. What I'd like to do is use a single line of code to search for any data that includes ""nt"" OR ""nv"" OR ""nf."" I've played around with some ways that I thought should work, including just sticking a pipe between terms, but all of these result in errors. I've checked the documentation, but I don't see this as an option. I get errors like this: </p>

<pre><code>    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-113-1d11e906812c&gt; in &lt;module&gt;()
    3 
    4 
    ----&gt; 5 soctol = f_recs[f_recs['Behavior'].str.contains(""nt""|""nv"", na=False)]
    6 soctol

    TypeError: unsupported operand type(s) for |: 'str' and 'str'
</code></pre>

<p>Is there a fast way to do this? Thanks for any help, I am a beginner but am LOVING pandas for data wrangling.</p>
";;0;;2013-10-03T21:35:36.073;4.0;19169649;2017-06-15T20:50:07.757;;;;;2809024.0;;1;13;<python><pandas><contains>;Using str.contains() in pandas with dataframes;23580.0
6711;6711;;2.0;"<p>How do you plot a vertical line (vlines) in a Pandas series plot?  I am using Pandas to plot rolling means, etc and would like to mark important positions with a vertical line.  Is it possible to use vlines or something similar to accomplish this?  If so, could someone please provide an example?  In this case, the x axis is date-time.</p>
";;0;;2013-10-06T20:48:04.850;9.0;19213789;2016-11-23T20:09:01.090;;;;;759640.0;;1;44;<python><matplotlib><plot><pandas>;How do you plot a vertical line on a time series plot in Pandas?;39922.0
6718;6718;19214708.0;1.0;"<p>I've started using <code>pandas</code> to do some aggregation by date. My goal is to count all of the instances of a measurement that occur on a particular day, and to then represent this in <code>D3</code>. To illustrate my workflow, I have a queryset (from <code>Django</code>) that looks like this:</p>

<pre><code>queryset = [{'created':""05-16-13"", 'counter':1, 'id':13}, {'created':""05-16-13"", 'counter':1, 'id':34}, {'created':""05-17-13"", 'counter':1, 'id':12}, {'created':""05-16-13"", 'counter':1, 'id':7}, {'created':""05-18-13"", 'counter':1, 'id':6}]
</code></pre>

<p>I make a dataframe in <code>pandas</code> and aggregate the measure 'counter' by the day created:</p>

<pre><code>import pandas as pd
queryset_df = pd.DataFrame.from_records(queryset).set_index('id')
aggregated_df = queryset_df.groupby('created').sum()
</code></pre>

<p>This gives me a dataframe like this:</p>

<pre><code>          counter
created          
05-16-13        3
05-17-13        1
05-18-13        1
</code></pre>

<p>As I'm using <code>D3</code> I thought that a <code>JSON</code> object would be the most useful. Using the <code>Pandas</code> <code>to_json()</code> function I convert my dataframe like this:</p>

<pre><code>aggregated_df.to_json()
</code></pre>

<p>giving me the following <code>JSON</code> object</p>

<pre><code>{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}
</code></pre>

<p>This is not exactly what I want, as I would like to be able to access both the date, and the measurement. Is there a way that I can export the data such that I end up with something like this?</p>

<pre><code>data = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}
</code></pre>

<p>I thought that if I could structure this differently on the <code>Python</code> side, it would reduce the amount of data formatting I would need to do on the <code>JS</code> side as I planned to load the data doing something like this:</p>

<pre><code>  x.domain(d3.extent(data, function(d) { return d.date; }));
  y.domain(d3.extent(data, function(d) { return d.counter; }));
</code></pre>

<p>I'm very open to suggestions of better workflows overall as this is something I will need to do frequently but am unsure of the best way of handling the connection between <code>D3</code> and <code>pandas</code>. (I have looked at several packages that combine both <code>python</code> and <code>D3</code> directly, but that is not something that I am looking for as they seem to focus on static chart generation and not making an svg)</p>
";;0;;2013-10-06T22:12:55.597;6.0;19214588;2013-10-06T22:27:43.080;2013-10-06T22:19:10.480;;258755.0;;258755.0;;1;11;<javascript><python><json><d3.js><pandas>;How can I efficiently move from a Pandas dataframe to JSON;11979.0
6733;6733;19226617.0;3.0;"<p>I'm trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the data.</p>

<p>Let's say I want to iterate over all values in the column head 'ID.' If that ID matches a specific number, then I want to change two corresponding values FirstName and LastName.</p>

<p>In Stata it looks like this:</p>

<pre><code>replace FirstName = ""Matt"" if ID==103
replace LastName =  ""Jones"" if ID==103
</code></pre>

<p>So this replaces all values in FirstName that correspond with values of ID == 103 to Matt.  </p>

<p>In PANDAS, I'm trying something like this</p>

<pre><code>df = read_csv(""test.csv"")
for i in df['ID']:
    if i ==103:
          ...
</code></pre>

<p>Not sure where to go from here.  Any ideas?</p>
";;0;;2013-10-07T13:42:28.390;7.0;19226488;2017-07-10T20:43:18.393;;;;;372526.0;;1;15;<python><pandas>;Python PANDAS, change one value based on another value;25399.0
6742;6742;19231939.0;2.0;"<p>I have a data frame with unix times and prices in it. I want to convert the index column so that it shows in human readable dates. So for instance i have ""date"" as 1349633705 in the index column but I'd want it to show as 10/07/2012 (or at least 10/07/2012 18:15). For some context, here is the code I'm working with and what I've tried already:</p>

<pre><code>import json
import urllib2
from datetime import datetime
response = urllib2.urlopen('http://blockchain.info/charts/market-price?&amp;format=json')
data = json.load(response)   
df = DataFrame(data['values'])
df.columns = [""date"",""price""]
#convert dates 
df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))
df.index = df.date   
df
</code></pre>

<p>As you can see I'm using
<code>df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))</code> here which doesn't work since I'm working with integers, not strings. I think I need to use <code>datetime.date.fromtimestamp</code> but I'm not quite sure how to apply this to the whole of df.date. Thanks.</p>
";;0;;2013-10-07T18:21:02.947;8.0;19231871;2017-07-07T20:18:23.420;2013-10-08T19:57:23.883;;2714570.0;;2714570.0;;1;30;<python><pandas><unix-timestamp><dataframe>;Convert unix time to readable date in pandas DataFrame;24317.0
6751;6751;19237920.0;3.0;"<p>I am transitioning from R to Python. I just began using Pandas. I have an R code that subsets nicely:</p>

<pre><code>k1 &lt;- subset(data, Product = p.id &amp; Month &lt; mn &amp; Year == yr, select = c(Time, Product))
</code></pre>

<p>Now, I want to do similar stuff in Python. this is what I have got so far:</p>

<pre><code>import pandas as pd
data = pd.read_csv(""../data/monthly_prod_sales.csv"")


#first, index the dataset by Product. And, get all that matches a given 'p.id' and time.
 data.set_index('Product')
 k = data.ix[[p.id, 'Time']]

# then, index this subset with Time and do more subsetting..
</code></pre>

<p>I am beginning to feel that I am doing this the wrong way. perhaps, there is an elegant solution. Can anyone help? I need to extract month and year from the timestamp I have and do subsetting. Perhaps there is a one-liner that will accomplish all this:</p>

<pre><code>k1 &lt;- subset(data, Product = p.id &amp; Time &gt;= start_time &amp; Time &lt; end_time, select = c(Time, Product))
</code></pre>

<p>thanks.</p>
";;0;;2013-10-08T02:04:52.023;11.0;19237878;2017-07-05T17:40:09.253;;;;;1717931.0;;1;30;<pandas><subset>;subsetting a Python DataFrame;71327.0
6791;6791;19271841.0;1.0;"<p>After looking at this <a href=""https://stackoverflow.com/questions/19265942/pandas-create-a-new-column-filled-with-the-number-of-observations-in-another-co"">question</a> I did some messing about and found this:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})
df['num_totals'] = df.groupby('a').transform('count')

gives ValueError:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-157c6339ad93&gt; in &lt;module&gt;()
      3 #df = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4], 'b':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})
      4 df = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})
----&gt; 5 df['num_totals'] = df.groupby('a').transform('count')
      6 
      7 #df['num_totals']=df.groupby('a')[['a']].transform('count')

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\frame.pyc in __setitem__(self, key, value)
   2117         else:
   2118             # set column
-&gt; 2119             self._set_item(key, value)
   2120 
   2121     def _setitem_slice(self, key, value):

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\frame.pyc in _set_item(self, key, value)
   2164         """"""
   2165         value = self._sanitize_column(key, value)
-&gt; 2166         NDFrame._set_item(self, key, value)
   2167 
   2168     def insert(self, loc, column, value, allow_duplicates=False):

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\generic.pyc in _set_item(self, key, value)
    677 
    678     def _set_item(self, key, value):
--&gt; 679         self._data.set(key, value)
    680         self._clear_item_cache()
    681 

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\internals.pyc in set(self, item, value)
   1779         except KeyError:
   1780             # insert at end
-&gt; 1781             self.insert(len(self.items), item, value)
   1782 
   1783         self._known_consolidated = False

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\internals.pyc in insert(self, loc, item, value, allow_duplicates)
   1793 
   1794             # new block
-&gt; 1795             self._add_new_block(item, value, loc=loc)
   1796 
   1797         except:

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\internals.pyc in _add_new_block(self, item, value, loc)
   1909             loc = self.items.get_loc(item)
   1910         new_block = make_block(value, self.items[loc:loc + 1].copy(),
-&gt; 1911                                self.items, fastpath=True)
   1912         self.blocks.append(new_block)
   1913 

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\internals.pyc in make_block(values, items, ref_items, klass, fastpath, placement)
    964             klass = ObjectBlock
    965 
--&gt; 966     return klass(values, items, ref_items, ndim=values.ndim, fastpath=fastpath, placement=placement)
    967 
    968 # TODO: flexible with index=None and/or items=None

C:\WinPython-64bit-2.7.5.3\python-2.7.5.amd64\lib\site-packages\pandas\core\internals.pyc in __init__(self, values, items, ref_items, ndim, fastpath, placement)
     42         if len(items) != len(values):
     43             raise ValueError('Wrong number of items passed %d, indices imply %d'
---&gt; 44                              % (len(items), len(values)))
     45 
     46         self.set_ref_locs(placement)

ValueError: Wrong number of items passed 1, indices imply 0
</code></pre>

<p>But if I have 2 columns then it works fine:</p>

<pre><code>df = pd.DataFrame({'a':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4],'b':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})
df['num_totals'] = df.groupby('a').transform('count')
df



Out[40]:
    a  b  num_totals
0   1  1           4
1   1  1           4
2   1  1           4
3   1  1           4
4   2  2           2
5   2  2           2
6   3  3           3
7   3  3           3
8   3  3           3
9   4  4           7
10  4  4           7
11  4  4           7
12  4  4           7
13  4  4           7
14  4  4           7
15  4  4           7
</code></pre>

<p>or if I do this using a single column df:</p>

<pre><code>df['num_totals']=df.groupby('a')[['a']].transform('count')
</code></pre>

<p>There is a similar <a href=""https://stackoverflow.com/questions/13854476/pandas-transform-doesnt-work-sorting-groupby-output"">SO post</a> but it is unclear to me why a series should fail and a dataframe should work in the immediate above example, and why having 2 or more columns would work.</p>

<p>I am using Python 2.7 64-bit and Pandas 0.12</p>
";;1;;2013-10-09T08:42:22.437;1.0;19267029;2013-10-09T12:20:19.317;2017-05-23T11:46:40.410;;-1.0;;704848.0;;1;11;<python><pandas>;Why Pandas Transform fails if you only have a single column;7646.0
6851;6851;19324591.0;5.0;"<p>My data can have multiple events on a given date or NO events on a date. I take these events, get a count by date and plot them.  However, when I plot them, my two series don't always match.</p>

<pre><code>idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())
s = df.groupby(['simpleDate']).size()
</code></pre>

<p>In the above code <strong>idx</strong> becomes a range of say 30 dates. 09-01-2013 to 09-30-2013
However <strong>S</strong> may only have 25 or 26 days because no events happened for a given date. I then get an AssertionError as the sizes dont match.</p>

<p>What's the proper way to tackle this? Do I want to remove dates with no values from <strong>IDX</strong> or (which I'd rather do) is add to the series the missing date with a count of 0. I'd rather have a full graph of 30 days with 0 values. If this approach is right, any suggestions on how to get started? Do I need some sort of dynamic <code>reindex</code> function?</p>

<p>Here's a snippet of <strong>S</strong> ( <code>df.groupby(['simpleDate']).size()</code>  ), notice no entries for 04 and 05.</p>

<pre><code>09-02-2013     2
09-03-2013    10
09-06-2013     5
09-07-2013     1
</code></pre>
";;0;;2013-10-11T17:58:15.307;17.0;19324453;2017-08-10T18:24:28.377;2017-08-10T17:54:35.353;;3877338.0;;1756833.0;;1;32;<python><date><plot><pandas><dataframe>;Add missing dates to pandas dataframe;17676.0
6863;6863;34592295.0;3.0;"<p>Python Pandas provides two methods for sorting DataFrame :</p>

<ul>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""noreferrer"">sort_values</a> (or DEPRECATED <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html"" rel=""noreferrer"">sort</a>)</li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html"" rel=""noreferrer"">sort_index</a></li>
</ul>

<p>What are differences between these two methods ?</p>
";;3;;2013-10-12T08:27:36.020;0.0;19332171;2017-04-23T23:53:17.777;2017-04-23T23:53:17.777;;10335.0;;2051311.0;;1;16;<python><pandas>;Difference between sort_values and sort_index;6654.0
6883;6883;19351003.0;7.0;"<p>I have a dataframe where the first 3 columns are 'MONTH', 'DAY', 'YEAR'</p>

<p>In each column there is an integer.
Is there a Pythonic way to convert all three columns into datetimes while there are in the dataframe?</p>

<p>From:</p>

<pre><code>M    D    Y    Apples   Oranges
5    6  1990      12        3
5    7  1990      14        4
5    8  1990      15       34
5    9  1990      23       21
</code></pre>

<p>into:</p>

<pre><code>Datetimes    Apples   Oranges
1990-6-5        12        3
1990-7-5        14        4
1990-8-5        15       34
1990-9-5        23       21
</code></pre>
";;1;;2013-10-13T22:03:52.487;7.0;19350806;2016-11-01T13:51:41.467;;;;;1367204.0;;1;19;<datetime><python-2.7><pandas>;How to convert columns into one datetime column in pandas?;14607.0
6897;6897;19368360.0;3.0;"<p>If I have an empty dataframe as such:</p>

<pre><code>columns = ['Date', 'Name', 'Action','ID']
df = pd.DataFrame(columns=columns) 
</code></pre>

<p>is there a way to append a new row to this newly created dataframe? Currently I have to create a dictionary, populate it, then append the dictionary to the dataframe at the end. Is there a more direct way? </p>
";2016-12-28T19:25:44.273;0;;2013-10-14T17:09:43.707;16.0;19365513;2017-06-01T13:37:55.310;2016-03-06T15:34:12.700;;1534017.0;;2759385.0;;1;22;<python><pandas>;How to add an extra row to a pandas dataframe;80190.0
6907;6907;;8.0;"<p>I have a 20 x 4000 dataframe in python using pandas. Two of these columns are named Year and quarter. I'd like to create a variable called period that makes Year = 2000 and quarter= q2 into 2000q2</p>

<p>Can anyone help with that?</p>
";;1;;2013-10-15T09:42:52.987;43.0;19377969;2017-07-21T20:26:34.640;2015-11-25T10:44:32.943;;4542359.0;;2866103.0;;1;103;<python><pandas><dataframe>;Combine two columns of text in dataframe in pandas/python;112188.0
6922;6922;19385591.0;2.0;"<p>I have a data frame <code>df</code> and I use several columns from it to <code>groupby</code>:</p>

<pre><code>df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()
</code></pre>

<p>In the above way I almost get the table (data frame) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many number were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.</p>
";;0;;2013-10-15T15:00:12.537;38.0;19384532;2016-07-29T16:52:01.607;;;;;245549.0;;1;93;<python><group-by><pandas><distinct>;How to count number of rows in a group in pandas group by object?;144711.0
6926;6926;;6.0;"<p>I recently started using Python so I could interact with the Bloomberg API, and I'm having some trouble storing the data into a Pandas dataframe (or a panel). I can get the output in the command prompt just fine, so that's not an issue.</p>

<p>A very similar question was asked here:
<a href=""https://stackoverflow.com/questions/18520416/pandas-wrapper-for-bloomberg-api"">Pandas wrapper for Bloomberg api?</a></p>

<p>The referenced code in the accepted answer for that question is for the old API, however, and it doesn't work for the new open API. Apparently the user who asked the question was able to easily modify that code to work with the new API, but I'm used to having my hand held in R, and this is my first endeavor with Python.</p>

<p>Could some benevolent user show me how to get this data into Pandas? There is an example in the Python API (available here: <a href=""http://www.openbloomberg.com/open-api/"" rel=""nofollow noreferrer"">http://www.openbloomberg.com/open-api/</a>) called SimpleHistoryExample.py that I've been working with that I've included below. I believe I'll need to modify mostly around the 'while(True)' loop toward the end of the 'main()' function, but everything I've tried so far has had issues.</p>

<p>Thanks in advance, and I hope this can be of help to anyone using Pandas for finance.</p>

<pre><code># SimpleHistoryExample.py

import blpapi
from optparse import OptionParser


def parseCmdLine():
    parser = OptionParser(description=""Retrieve reference data."")
    parser.add_option(""-a"",
                      ""--ip"",
                      dest=""host"",
                      help=""server name or IP (default: %default)"",
                      metavar=""ipAddress"",
                      default=""localhost"")
    parser.add_option(""-p"",
                      dest=""port"",
                      type=""int"",
                      help=""server port (default: %default)"",
                      metavar=""tcpPort"",
                      default=8194)

    (options, args) = parser.parse_args()

    return options


def main():
    options = parseCmdLine()

    # Fill SessionOptions
    sessionOptions = blpapi.SessionOptions()
    sessionOptions.setServerHost(options.host)
    sessionOptions.setServerPort(options.port)

    print ""Connecting to %s:%s"" % (options.host, options.port)
    # Create a Session
    session = blpapi.Session(sessionOptions)

    # Start a Session
    if not session.start():
        print ""Failed to start session.""
        return

    try:
        # Open service to get historical data from
        if not session.openService(""//blp/refdata""):
            print ""Failed to open //blp/refdata""
            return

        # Obtain previously opened service
        refDataService = session.getService(""//blp/refdata"")

        # Create and fill the request for the historical data
        request = refDataService.createRequest(""HistoricalDataRequest"")
        request.getElement(""securities"").appendValue(""IBM US Equity"")
        request.getElement(""securities"").appendValue(""MSFT US Equity"")
        request.getElement(""fields"").appendValue(""PX_LAST"")
        request.getElement(""fields"").appendValue(""OPEN"")
        request.set(""periodicityAdjustment"", ""ACTUAL"")
        request.set(""periodicitySelection"", ""DAILY"")
        request.set(""startDate"", ""20061227"")
        request.set(""endDate"", ""20061231"")
        request.set(""maxDataPoints"", 100)

        print ""Sending Request:"", request
        # Send the request
        session.sendRequest(request)

        # Process received events
        while(True):
            # We provide timeout to give the chance for Ctrl+C handling:
            ev = session.nextEvent(500)
            for msg in ev:
                print msg

            if ev.eventType() == blpapi.Event.RESPONSE:
                # Response completly received, so we could exit
                break
    finally:
        # Stop the session
        session.stop()

if __name__ == ""__main__"":
    print ""SimpleHistoryExample""
    try:
        main()
    except KeyboardInterrupt:
        print ""Ctrl+C pressed. Stopping...""
</code></pre>
";;1;;2013-10-15T17:55:56.187;9.0;19387868;2017-08-10T10:41:30.767;2017-05-23T12:34:41.687;;-1.0;;2769482.0;;1;17;<python><pandas><finance><bloomberg><blpapi>;How do I store data from the Bloomberg API into a Pandas dataframe?;23455.0
6937;6937;19403897.0;1.0;"<p>Is there a way to structure Pandas groupby and qcut commands to return one column that has nested  tiles? Specifically, suppose I have 2 groups of data and I want qcut applied to each group and then return the output to one column. This would be similar to MS SQL Server's ntile() command that allows Partition by().</p>

<pre><code>     A    B  C
0  foo  0.1  1
1  foo  0.5  2
2  foo  1.0  3
3  bar  0.1  1
4  bar  0.5  2
5  bar  1.0  3
</code></pre>

<p>In the dataframe above I would like to apply the qcut function to B while partitioning on A to return C.</p>
";;0;;2013-10-16T12:15:41.357;6.0;19403133;2016-07-18T13:21:05.057;;;;;2886162.0;;1;11;<python><group-by><pandas>;Pandas groupby and qcut;3879.0
7010;7010;;4.0;"<p>We have some Java code we want to use with new code we plan to write in Python, hence our interest in using Jython. However we also want to use numpy and pandas libraries to do complex statistical analysis in this Python code. </p>

<p>Is it possible to call numpy and pandas from Jython?</p>
";;3;;2013-10-18T16:57:25.397;1.0;19455100;2017-04-07T16:37:37.783;;;;;2895685.0;;1;11;<python><numpy><pandas><jython>;Can I run numpy and pandas with Jython;6121.0
7025;7025;19464054.0;2.0;"<p>What's the most efficient way to drop only consecutive duplicates in pandas?</p>

<p>drop_duplicates gives this:</p>

<pre><code>In [3]: a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])

In [4]: a.drop_duplicates()
Out[4]: 
1    1
2    2
4    3
dtype: int64
</code></pre>

<p>But I want this:</p>

<pre><code>In [4]: a.something()
Out[4]: 
1    1
2    2
4    3
5    2
dtype: int64
</code></pre>
";;0;;2013-10-19T08:19:56.027;6.0;19463985;2016-11-01T12:01:09.170;;;;;939259.0;;1;17;<python><pandas>;Pandas: Drop consecutive duplicates;3912.0
7036;7036;19486140.0;2.0;"<p>Using python 2.7.5 and pandas 0.12.0, I'm trying to import fixed-width-font text files into a DataFrame with 'pd.io.parsers.read_fwf()'.  The values I'm importing are all numeric, but it's important that leading zeros be preserved, so I'd like to specify the dtype as string rather than int.</p>

<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_fwf.html#pandas.io.parsers.read_fwf"">documentation for this function</a>, the dtype attribute is supported in read_fwf, but when I try to use it:</p>

<p><code>data= pd.io.parsers.read_fwf(file, colspecs = ([79,81], [87,90]), header = None, dtype = {0: np.str, 1: np.str})</code></p>

<p>I get the error:</p>

<p><code>ValueError: dtype is not supported with python-fwf parser</code></p>

<p>I've tried as many variations as I can think of for setting 'dtype = something', but all of them return the same message.  </p>

<p>Any help would be much appreciated!    </p>
";;0;;2013-10-20T00:20:53.163;;19472566;2013-10-21T11:30:14.640;;;;;2899013.0;;1;13;<python><parsing><pandas>;python read_fwf error: 'dtype is not supported with python-fwf parser';2114.0
7051;7051;19483025.0;12.0;"<p>I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won't know how many columns there will be or what they will be called.</p>

<p>For example, if I'm given a DataFrame like this:</p>

<pre><code>&gt;&gt;&gt; my_dataframe
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7
5   4    8    3
6   8    2    8
7   9    9   10
8   6    6    4
9  10   10    7
</code></pre>

<p>I would want to get a list like this:</p>

<pre><code>&gt;&gt;&gt; header_list
[y, gdp, cap]
</code></pre>
";;1;;2013-10-20T21:18:37.957;68.0;19482970;2017-07-13T14:40:12.647;2015-12-20T07:23:27.313;;1240268.0;;2428549.0;;1;304;<python><pandas><dataframe>;Get list from pandas DataFrame column headers;438665.0
7103;7103;19523512.0;2.0;"<p>1). I have a following example dataset: </p>

<pre><code>&gt;&gt;&gt; df
    ID     Region  count
0  100       Asia      2
1  101     Europe      3
2  102         US      1
3  103     Africa      5
4  100     Russia      5
5  101  Australia      7
6  102         US      8
7  104       Asia     10
8  105     Europe     11
9  110     Africa     23
</code></pre>

<p>I wanted to group the observations of this dataset by ID an Region and summing the count for each group. So I used something like this:</p>

<pre><code>&gt;&gt;&gt; print(df.groupby(['ID','Region'],as_index=False).count.sum())

    ID     Region  count
0  100       Asia      2
1  100     Russia      5
2  101  Australia      7
3  101     Europe      3
4  102         US      9
5  103     Africa      5
6  104       Asia     10
7  105     Europe     11
8  110     Africa     23
</code></pre>

<p>On using as_index=False I am able to get a ""Sql-Like"" output. My problem is that I am unable to rename the aggregate variable count here. So in Sql if wanted to do the above thing I would do something like this:</p>

<pre><code>select ID, Region, sum(count) as Total_Numbers
from df
group by ID,Region
order by ID, Region
</code></pre>

<p>As we see, it's very easy for me to rename the aggregate variable 'count' to Total_Numbers in SQL. I wanted to do the same thing in Pandas but unable to find such option in groupby function. Can somebody help?</p>

<p>2). The second question and more of an observation is that is it possible to use directly the column names in Pandas dataframe function witout enclosing them inside quotes? I understand that the variable names are string, so has to be inside quotes, but I see if use outside dataframe function and as an attribute we don't require them to be inside quotes. Like df.ID.sum() etc. It's only when we use it in a DataFrame function like df.sort() or df.groupby we have to use it inside quotes. This is actually a bit of pain as in SQL or in SAS or other language we simply use the variable name without quoting them. Any suggestion on this? </p>

<p>Kindly suggest on the above two points(1st one main, 2nd more of an opinion). </p>

<p>Thanks</p>
";;0;;2013-10-22T16:23:05.630;4.0;19523277;2017-05-08T23:20:45.707;;;;;2769240.0;;1;18;<python><group-by><pandas><rename>;Renaming Column Names in Pandas Groupby function;24019.0
7121;7121;19537059.0;5.0;"<p>I've had success using the groupby function to sum or average a given variable by groups, but is there a way to aggregate into a list of values, rather than to get a single result? (And would this still be called aggregation?) </p>

<p>I am not entirely sure this is the approach I should be taking anyhow, so below is an example of the transformation I'd like to make, with toy data. </p>

<p>That is, if the data look something like this:</p>

<pre><code>    A    B    C  
    1    10   22
    1    12   20
    1    11   8
    1    10   10
    2    11   13
    2    12   10 
    3    14   0
</code></pre>

<p>What I am trying to end up with is something like the following. I am not totally sure whether this can be done through groupby aggregating into lists, and am rather lost as to where to go from here. </p>

<p>Hypothetical output:</p>

<pre><code>     A    B    C  New1  New2  New3  New4  New5  New6
    1    10   22  12    20    11    8     10    10
    2    11   13  12    10 
    3    14   0
</code></pre>

<p>Perhaps I should be pursuing pivots instead? The order by which the data are put into columns does not matter - all columns B through New6 in this example are equivalent. All suggestions/corrections are much appreciated.</p>
";;0;;2013-10-23T00:14:15.860;10.0;19530568;2017-02-13T22:44:15.067;;;;;2809024.0;;1;25;<python><pandas>;Can pandas groupby aggregate into a list, rather than sum, mean, etc?;16908.0
7152;7152;;3.0;"<p>I'm using pandas to generate a plot from a dataframe, which I would like to save to a file:</p>

<pre><code>dtf = pd.DataFrame.from_records(d,columns=h)
fig = plt.figure()
ax = dtf2.plot()
ax = fig.add_subplot(ax)
fig.savefig('~/Documents/output.png')
</code></pre>

<p>It seems like the last line, using matplotlib's savefig, should do the trick.  But that code produces the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""./testgraph.py"", line 76, in &lt;module&gt;
    ax = fig.add_subplot(ax)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot
    assert(a.get_figure() is self)
AssertionError
</code></pre>

<p>Alternatively, trying to call savefig directly on the plot also errors out:</p>

<pre><code>dtf2.plot().savefig('~/Documents/output.png')


  File ""./testgraph.py"", line 79, in &lt;module&gt;
    dtf2.plot().savefig('~/Documents/output.png')
AttributeError: 'AxesSubplot' object has no attribute 'savefig'
</code></pre>

<p>I think I need to somehow add the subplot returned by plot() to a figure in order to use savefig.  I also wonder if perhaps this has to do with the <a href=""https://stackoverflow.com/questions/11690597/there-is-a-class-matplotlib-axes-axessubplot-but-the-module-matplotlib-axes-has"">magic</a> behind the AxesSubPlot class.</p>

<p>EDIT: </p>

<p>the following works (raising no error), but leaves me with a blank page image....</p>

<pre><code>fig = plt.figure()
dtf2.plot()
fig.savefig('output.png')
</code></pre>
";;2;;2013-10-24T01:55:50.297;11.0;19555525;2017-06-11T06:31:08.073;2017-05-23T11:54:47.530;;-1.0;;1046775.0;;1;28;<python><matplotlib><pandas>;Saving plots (AxesSubPlot) generated from python pandas with matplotlib's savefig;25060.0
7199;7199;19603918.0;3.0;"<p>I need some guidance in working out how to plot a block of histograms from grouped data in a pandas dataframe. Here's an example to illustrate my question:</p>

<pre><code>from pandas import DataFrame
import numpy as np
x = ['A']*300 + ['B']*400 + ['C']*300
y = np.random.randn(1000)
df = DataFrame({'Letter':x, 'N':y})
grouped = df.groupby('Letter')
</code></pre>

<p>In my ignorance I tried this code command:</p>

<pre><code>df.groupby('Letter').hist()
</code></pre>

<p>which failed with the error message ""TypeError: cannot concatenate 'str' and 'float' objects""</p>

<p>Any help most appreciated.</p>
";;0;;2013-10-25T07:44:55.650;10.0;19584029;2015-12-10T20:50:05.890;;;;;2918893.0;;1;27;<python><pandas><histogram>;Plotting histograms from grouped data in a pandas DataFrame;47761.0
7201;7201;;3.0;"<p>I have a pandas data frame like this:  </p>

<pre><code>admit   gpa  gre  rank   
0  3.61  380     3  
1  3.67  660     3  
1  3.19  640     4  
0  2.93  520     4
</code></pre>

<p>Now I want to get a list of rows in pandas like:  </p>

<pre><code>[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   
</code></pre>

<p>How can I do it? please help me. 
Thanks a lot. </p>
";;0;;2013-10-25T08:49:36.703;7.0;19585280;2017-04-20T13:02:44.523;2013-10-25T08:54:32.167;;1744834.0;;2806761.0;;1;21;<python><pandas>;Convert a row in pandas into list;21327.0
7224;7224;19599661.0;2.0;"<p>How do we get a particular filtered row as series?</p>

<p>Example dataframe:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'date': [20130101, 20130101, 20130102], 'location': ['a', 'a', 'c']})
&gt;&gt;&gt; df
       date location
0  20130101        a
1  20130101        a
2  20130102        c
</code></pre>

<p>I need to select the row where <code>location</code> is <code>c</code> <strong>as a series</strong>.</p>

<p><strong>I tried:</strong></p>

<pre><code>row = df[df[""location""] == ""c""].head(1)  # gives a dataframe
row = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row
</code></pre>

<p>In either cases I can't the row as series.</p>
";;0;;2013-10-25T21:18:07.850;6.0;19599578;2013-10-25T22:13:45.517;;;;;232649.0;;1;18;<python><pandas>;Get particular row as series from pandas dataframe;9615.0
7234;7234;19609945.0;3.0;"<p>I have a data frame called <code>followers_df</code> as below:</p>

<pre><code> followers_df

             0
0         oasikhia 
0     LEANEnergyUS
0  _johannesngwako
0     jamesbreenre
0   CaitlinFecteau
0  mantequillaFACE
0         apowersb
0       ecoprinter
0        tsdesigns
0      GreenBizDoc
0        JimHarris
0    Jmarti11Julia
0         JAslat63
0            prAna
0    GrantLundberg 
0        Jitasa_Is
0     ChoosePAWind
0  cleanpowerperks
0          WoWEorg
0      Laura_Chuck
</code></pre>

<p>I want to change this data frame into something like this:</p>

<pre><code> followers_df

             0
0          oasikhia 
1      LEANEnergyUS
2   _johannesngwako
3      jamesbreenre
4    CaitlinFecteau
5   mantequillaFACE
6          apowersb
7        ecoprinter
8         tsdesigns
9       GreenBizDoc
10        JimHarris
11    Jmarti11Julia
12         JAslat63
13            prAna
14    GrantLundberg 
15        Jitasa_Is
16     ChoosePAWind
17  cleanpowerperks
18          WoWEorg
19      Laura_Chuck
</code></pre>

<p>how can I do this? I tried:</p>

<pre><code>     index = pandas.Index(range(20))
     followers_df = pandas.DataFrame(followers_df, index=index)
</code></pre>

<p>but it's giving me the following error:</p>

<pre><code>  ValueError: Shape of passed values is (1, 39), indices imply (1, 20)
</code></pre>

<p>thanks,</p>
";;1;;2013-10-26T17:22:43.533;6.0;19609631;2017-03-09T06:26:24.763;2013-10-26T17:36:30.107;;2871333.0;;2871333.0;;1;18;<python><pandas>;python: changing row index of pandas data frame;34376.0
7240;7240;19611857.0;2.0;"<p>I uploaded a file to Google spreadsheets (to make a publically accessible example IPython Notebook, with data) I was using the file in it's native form could be read into a Pandas Dataframe. So now I use the following code to read the spreadsheet, works fine but just comes in as string,, and I'm not having any luck trying to get it back into a dataframe (you can get the data)</p>

<pre><code>import requests
r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv')
data = r.content
</code></pre>

<p>The data ends up looking like: (1st row headers)</p>

<pre><code>',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\n
</code></pre>

<p>The native pandas code that brings in the disk resident file looks like:</p>

<pre><code>df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])
</code></pre>

<p>A ""clean"" solution would be helpful to many to provide an easy way to share datasets for Pandas use! I tried a bunch of alternative with no success and I'm pretty sure I'm missing something obvious again.</p>

<p>Just a Update note The new Google spreadsheet has a different URL pattern Just use this in place of the URL in the above example and or the below answer and you should be fine here is an example: </p>

<pre><code>https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&amp;id
</code></pre>

<p>see solution below from @Max Ghenis which just used pd.read_csv, no need for StringIO or requests...</p>
";;0;;2013-10-26T20:46:51.827;7.0;19611729;2017-06-19T13:36:15.403;2016-02-07T16:35:44.657;;137783.0;;137783.0;;1;23;<python><pandas><google-spreadsheet><google-apps>;Getting Google Spreadsheet CSV into A Pandas Dataframe;7482.0
7258;7258;30535957.0;3.0;"<p>Assume I have two dataframes of this format (call them <code>df1</code> and <code>df2</code>):</p>

<pre><code>+------------------------+------------------------+--------+
|        user_id         |      business_id       | rating |
+------------------------+------------------------+--------+
| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |
| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |
| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |
+------------------------+------------------------+--------+
</code></pre>

<p>I'm looking to get a dataframe of all the rows that have a common <code>user_id</code> in <code>df1</code> and <code>df2</code>. (ie. if a <code>user_id</code> is in both <code>df1</code> and <code>df2</code>, include the two rows in the output dataframe)</p>

<p>I can think of many ways to approach this, but they all strike me as clunky. For example, we could find all the unique <code>user_id</code>s in each dataframe, create a set of each, find their intersection, filter the two dataframes with the resulting set and concatenate the two filtered dataframes.</p>

<p>Maybe that's the best approach, but I know Pandas is clever. Is there a simpler way to do this? I've looked at <code>merge</code> but I don't think that's what I need.</p>
";;0;;2013-10-27T14:03:18.073;9.0;19618912;2016-10-14T21:21:52.977;2016-10-14T21:21:52.977;;249323.0;;249323.0;;1;21;<python><python-2.7><pandas><dataframe><intersect>;Finding common rows (intersection) in two Pandas dataframes;23593.0
7278;7278;19633103.0;2.0;"<p>I try to read the file into pandas.
The file has values separated by space, but with different number of spaces
I tried:</p>

<pre><code>pd.read_csv('file.csv', delimiter=' ')
</code></pre>

<p>but it doesn't work</p>
";;1;;2013-10-28T10:14:45.090;5.0;19632075;2016-12-14T14:38:18.300;;;;;2006977.0;;1;25;<python><pandas>;how to read file with space separated values;17660.0
7317;7317;19667189.0;1.0;"<p>I am getting an error and I'm not sure how to fix it. </p>

<p>The following seems to work:</p>

<pre><code>def random(row):
   return [1,2,3,4]

df = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))

df.apply(func = random, axis = 1)
</code></pre>

<p>and my output is:</p>

<pre><code>[1,2,3,4]
[1,2,3,4]
[1,2,3,4]
[1,2,3,4]
</code></pre>

<p>However, when I change one of the of the columns to a value such as 1 or None:</p>

<pre><code>def random(row):
   return [1,2,3,4]

df = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))
df['E'] = 1

df.apply(func = random, axis = 1)
</code></pre>

<p>I get the the error:</p>

<pre><code>ValueError: Shape of passed values is (5,), indices imply (5, 5)
</code></pre>

<p>I've been wrestling with this for a few days now and nothing seems to work. What is interesting is that when I change </p>

<pre><code>def random(row):
   return [1,2,3,4]
</code></pre>

<p>to </p>

<pre><code>def random(row):
   print [1,2,3,4]
</code></pre>

<p>everything seems to work normally. </p>

<p>This question is a clearer way of <a href=""https://stackoverflow.com/questions/19628159/pandas-incorrect-shape-of-indices-passed-not-sure-how-to-fix-it"">asking this question</a>, which I feel may have been confusing.</p>

<p>My goal is to compute a list for each row and then create a column out of that.  </p>

<p>EDIT: I originally start with a dataframe that hase one column. I add 4 columns in 4 difference apply steps, and then when I try to add another column I get this error. </p>
";;7;;2013-10-29T18:59:30.517;2.0;19666904;2013-11-21T11:25:59.013;2017-05-23T12:26:22.437;;-1.0;;1367204.0;;1;14;<python-2.7><pandas><ipython><dataframe>;Pandas Dataframe ValueError: Shape of passed values is (X, ), indices imply (X, Y);22595.0
7401;7401;19726078.0;3.0;"<p>I want to make all column headers in my pandas data frame lower case</p>

<p>for example, if I have:</p>

<pre><code>data =

  country country isocode  year     XRAT          tcgdp
0  Canada             CAN  2001  1.54876   924909.44207
1  Canada             CAN  2002  1.56932   957299.91586
2  Canada             CAN  2003  1.40105  1016902.00180
....
</code></pre>

<p>I would like to change XRAT to xrat by doing something like:</p>

<pre><code>data.headers.lowercase()
</code></pre>

<p>So that I get:</p>

<pre><code>  country country isocode  year     xrat          tcgdp
0  Canada             CAN  2001  1.54876   924909.44207
1  Canada             CAN  2002  1.56932   957299.91586
2  Canada             CAN  2003  1.40105  1016902.00180
3  Canada             CAN  2004  1.30102  1096000.35500
....
</code></pre>

<p>I will not know the names of each column header ahead of time.</p>

<p>Thanks!</p>
";;0;;2013-11-01T11:39:26.053;4.0;19726029;2016-08-13T10:41:42.127;;;;;2428549.0;;1;20;<python><pandas><dataframe>;python: make pandas dataframe column headers all lowercase;12664.0
7403;7403;39358752.0;4.0;"<p>It sounds somewhat weird? but I need to save the Pandas console output string to png pics. For example:</p>

<pre><code>&gt;&gt;&gt; df
                   sales  net_pft     ROE    ROIC
STK_ID RPT_Date                                  
600809 20120331  22.1401   4.9253  0.1651  0.6656
       20120630  38.1565   7.8684  0.2567  1.0385
       20120930  52.5098  12.4338  0.3587  1.2867
       20121231  64.7876  13.2731  0.3736  1.2205
       20130331  27.9517   7.5182  0.1745  0.3723
       20130630  40.6460   9.8572  0.2560  0.4290
       20130930  53.0501  11.8605  0.2927  0.4369 
</code></pre>

<p>Is there any way like <code>df.output_as_png(filename='df_data.png')</code> to generate a pic file which just display above content inside?</p>
";;4;;2013-11-01T12:22:13.153;8.0;19726663;2017-05-19T13:57:59.127;;;;;1072888.0;;1;27;<python><matplotlib><pandas>;How to save the Pandas dataframe/series data as a figure?;22214.0
7412;7412;19736406.0;3.0;"<p>Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.</p>

<p>How can I create a dataframe where each column holds a different entry?</p>

<p>When I try:</p>

<pre><code>pd.DataFrame(my_dict)
</code></pre>

<p>I get:</p>

<pre><code>ValueError: arrays must all be the same length
</code></pre>

<p>Any way to overcome this? I am happy to have Pandas use <code>NaN</code> to pad those columns for the shorter entries.</p>
";;0;;2013-11-01T21:59:09.703;6.0;19736080;2017-08-09T17:56:10.000;;;;;1732769.0;;1;23;<python><pandas>;Creating dataframe from a dictionary where entries have different lengths;17482.0
7439;7439;19758398.0;2.0;"<p>I've got a dataframe called <code>data</code>. How would I rename the only one column header? For example <code>gdp</code> to <code>log(gdp)</code>?</p>

<pre><code>data =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7
5   4    8    3
6   8    2    8
7   9    9   10
8   6    6    4
9  10   10    7
</code></pre>
";;2;;2013-11-03T21:28:48.587;22.0;19758364;2016-09-23T09:17:35.220;2015-12-29T20:32:11.657;;2901002.0;;2428549.0;;1;80;<python><pandas><dataframe>;python: rename single column header in pandas dataframe;50105.0
7465;7465;19782137.0;3.0;"<p>Say I import the following excel spreadsheet into a df</p>

<pre><code>Val1 Val2 Val3
1     2    3 
5     6    7 
9     1    2
</code></pre>

<p>How do I delete the column name row (in this case Val1, Val2, Val3) so that I can export a csv with no column names, just the data?</p>

<p>I have tried df.drop and df.ix[1:] and have not been successful with either.</p>
";;0;;2013-11-05T03:46:31.273;2.0;19781609;2016-08-05T21:04:08.363;;;;;2845269.0;;1;13;<python><pandas>;How do you remove the column name row from a pandas DataFrame?;19161.0
7481;7481;19809616.0;6.0;"<p>I have a very large dataframe (around 1 million rows) with data from an experiment (60 respondents).
I would like to split the dataframe into 60 dataframes (a dataframe for each participant). </p>

<p>In the dataframe (called = data) there is a variable called 'name' which is the unique code for each participant.</p>

<p>I have tried the following, but nothing happens (or the does not stop within an hour). What I intend to do is to split the dataframe (data) into smaller dataframes and append these to a list (datalist):</p>

<pre><code>import pandas as pd

def splitframe(data, name='name'):

    n = data[name][0]

    df = pd.DataFrame(columns=data.columns)

    datalist = []

    for i in range(len(data)):
        if data[name][i] == n:
            df = df.append(data.iloc[i])
        else:
            datalist.append(df)
            df = pd.DataFrame(columns=data.columns)
            n = data[name][i]
            df = df.append(data.iloc[i])

    return datalist
</code></pre>

<p>I do not get an error message, the script just seems to run forever!</p>

<p>Is there a smart way to do it?</p>
";;3;;2013-11-05T14:01:13.440;11.0;19790790;2017-07-30T13:18:37.803;;;;;2086357.0;;1;33;<python><split><pandas><dataframe>;Splitting dataframe into multiple dataframes;55592.0
7497;7497;19900276.0;3.0;"<p>I'm Looking for a generic way of turning a DataFrame to a nested dictionary</p>

<p>This is a sample data frame </p>

<pre><code>    name    v1  v2  v3
0   A       A1  A11 1
1   A       A2  A12 2
2   B       B1  B12 3
3   C       C1  C11 4
4   B       B2  B21 5
5   A       A2  A21 6
</code></pre>

<p>The number of columns may differ and so does the column names.</p>

<p>like this : </p>

<pre><code>{
'A' : { 
    'A1' : { 'A11' : 1 }
    'A2' : { 'A12' : 2 , 'A21' : 6 }} , 
'B' : { 
    'B1' : { 'B12' : 3 } } , 
'C' : { 
    'C1' : { 'C11' : 4}}
}
</code></pre>

<p>What is best way to achieve this ? </p>

<p>closest I got was with the <code>zip</code> function but haven't managed to make it work for more then one level (two columns).</p>
";;1;;2013-11-05T20:17:41.060;3.0;19798112;2016-06-07T06:53:29.237;2016-06-07T06:53:29.237;;1009479.0;;2000875.0;;1;15;<python><pandas>;Convert pandas DataFrame to a nested dict;5387.0
7498;7498;19798528.0;6.0;"<p>Can you tell me when to use these vectorization methods with basic examples? I see that <code>map</code> is a <code>Series</code> method whereas the rest are <code>DataFrame</code> methods. I got confused about <code>apply</code> and <code>applymap</code> methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!</p>

<p>Thanks!</p>
";;0;;2013-11-05T20:20:14.250;126.0;19798153;2016-08-11T15:20:32.563;2014-07-16T18:18:57.947;;1505259.0;;1505259.0;;1;189;<python><numpy><pandas><vectorization>;Difference between map, applymap and apply methods in Pandas;98032.0
7535;7535;19818942.0;3.0;"<p>A similar question is asked here:
<a href=""https://stackoverflow.com/questions/15705630/python-how-can-i-get-the-row-which-has-the-max-value-in-goups-making-groupby"">Python : Getting the Row which has the max value in groups using groupby</a></p>

<p>However, I just need one record per group even if there are more than one record with maximum value in that group. </p>

<p>In the example below, I need one record for ""s2"". For me it doesn't matter which one. </p>

<pre><code>&gt;&gt;&gt; df = DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'count':[3,2,5,10,10,6]})
&gt;&gt;&gt; df
   Mt Sp  Value  count
0  s1  a      1      3
1  s1  b      2      2
2  s2  c      3      5
3  s2  d      4     10
4  s2  e      5     10
5  s3  f      6      6
&gt;&gt;&gt; idx = df.groupby(['Mt'])['count'].transform(max) == df['count']
&gt;&gt;&gt; df[idx]
   Mt Sp  Value  count
0  s1  a      1      3
3  s2  d      4     10
4  s2  e      5     10
5  s3  f      6      6
&gt;&gt;&gt; 
</code></pre>
";;1;;2013-11-06T17:30:08.163;11.0;19818756;2016-02-17T21:50:00.593;2017-05-23T11:55:07.730;;-1.0;;1140126.0;;1;19;<python><pandas>;Extract row with maximum value in a group pandas dataframe;20452.0
7555;7555;19828967.0;3.0;"<p>How to check whether a pandas <code>DataFrame</code> is empty? In my case I want to print some message in terminal if the <code>DataFrame</code> is empty. </p>
";;1;;2013-11-07T05:45:35.707;17.0;19828822;2016-03-13T14:37:57.207;;;;;461436.0;;1;111;<python><pandas>;How to check whether a pandas DataFrame is empty?;70044.0
7589;7589;19851521.0;4.0;"<p>I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0</p>

<pre><code>In [2]: df = pd.read_csv(r'D:\Data\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )

In [3]: df.head()
Out[3]: 
                   1
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667

In [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)

In [5]: df.head()
Out[5]: 
                  SM
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667
</code></pre>
";;0;;2013-11-08T03:19:20.497;10.0;19851005;2016-10-06T10:29:03.307;2016-05-09T19:37:09.373;;1984745.0;;2459096.0;;1;43;<python><pandas><dataframe>;Rename Pandas DataFrame Index;76528.0
7599;7599;19861545.0;2.0;"<p>I've got a pandas dataframe called data and I want to remove all rows that contain a string in any column. For example, below we see the 'gdp' column has a string at index 3, and 'cap' at index 1.</p>

<pre><code>data =

    y  gdp  cap
0   1    2    5
1   2    3    ab
2   8    7    2
3   3    bc   7
4   6    7    7
5   4    8    3
...
</code></pre>

<p>I've been trying to use something like this script because I will not know what is contained in exp_list ahead of time. Unfortunately, ""data.var_name"" throws out this error: 'DataFrame' object has no attribute 'var_name'. I also don't know what the strings will be ahead of time so is there anyway to generalize that as well?</p>

<pre><code>exp_list = ['gdp', 'cap']

for var_name in exp_list:
    data = data[data.var_name != 'ab']
</code></pre>
";;0;;2013-11-08T13:36:55.930;4.0;19860389;2015-05-25T20:06:21.100;;;;;2428549.0;;1;14;<python><pandas><dataframe>;python: remove all rows in pandas dataframe that contain a string;6964.0
7619;7619;19867768.0;2.0;"<p>Suppose I have the following DataFrame:</p>

<pre><code>In [1]: df
Out[1]:
  apple banana cherry
0     0      3   good
1     1      4    bad
2     2      5   good
</code></pre>

<p>This works as expected:</p>

<pre><code>In [2]: df['apple'][df.cherry == 'bad'] = np.nan
In [3]: df
Out[3]:
  apple banana cherry
0     0      3   good
1   NaN      4    bad
2     2      5   good
</code></pre>

<p>But this doesn't:</p>

<pre><code>In [2]: df[['apple', 'banana']][df.cherry == 'bad'] = np.nan
In [3]: df
Out[3]:
  apple banana cherry
0     0      3   good
1     1      4    bad
2     2      5   good
</code></pre>

<p>Why?  How can I achieve the conversion of both the 'apple' and 'banana' values without having to write out two lines, as in</p>

<pre><code>In [2]: df['apple'][df.cherry == 'bad'] = np.nan
In [3]: df['banana'][df.cherry == 'bad'] = np.nan
</code></pre>
";;1;;2013-11-08T20:11:35.900;8.0;19867734;2013-11-08T20:14:38.900;;;;;2623899.0;;1;20;<python><pandas>;Changing certain values in multiple columns of a pandas DataFrame at once;18591.0
7646;7646;19895152.0;1.0;"<p>Currently there is a <code>median</code> method on the Pandas's <code>GroupBy</code> objects.</p>

<p>Is there is a way to calculate an arbitrary <code>percentile</code> (see: <a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html"" rel=""noreferrer"">http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html</a>) on the groupings? </p>

<p>Median would be the calcuation of percentile with <code>q=50</code>.</p>
";;0;;2013-11-10T20:43:40.700;5.0;19894939;2013-11-10T21:05:25.567;;;;;2638485.0;;1;11;<pandas>;Calculate Arbitrary Percentile on Pandas GroupBy;7394.0
7655;7655;;5.0;"<p>Is there a better way to determine whether a variable in Pandas/Numpy is numeric or not ?</p>

<p>At the moment I have a self defined dictionary with dtypes as keys and 'numeric'/'not' as values.</p>
";;2;;2013-11-11T06:32:11.383;3.0;19900202;2017-08-08T12:39:33.727;2016-11-23T20:34:32.060;;2285236.0;;2808117.0;;1;12;<python><pandas><numpy>;How to determine whether a column/variable is numeric or not in Pandas/Numpy?;7605.0
7671;7671;19913845.0;5.0;"<p>I have a dataframe along the lines of the below:</p>

<pre><code>    Type       Set
1    A          Z
2    B          Z           
3    B          X
4    C          Y
</code></pre>

<p>I want to add another column to the dataframe (or generate a series) of the same length as the dataframe (= equal number of records/rows) which sets a colour green if Set = 'Z' and 'red' if Set = otherwise.</p>

<p>What's the best way to do this?</p>
";;1;;2013-11-11T18:52:06.417;49.0;19913659;2017-08-16T16:49:28.483;2015-05-10T17:10:04.613;;3923281.0;;213216.0;;1;85;<python><pandas>;Pandas conditional creation of a series/dataframe column;70108.0
7676;7676;19976286.0;4.0;"<p>I want to create a new column in a <code>pandas</code> data frame by applying a function to two existing columns. Following this <a href=""https://stackoverflow.com/a/14603893/2327821"">answer</a> I've been able to create a new column when I only need one column as an argument:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})

def fx(x):
    return x * x

print(df)
df['newcolumn'] = df.A.apply(fx)
print(df)
</code></pre>

<p>However, I cannot figure out how to do the same thing when the function requires multiple arguments. For example, how do I create a new column by passing column A and column B to the function below?</p>

<pre><code>def fxy(x, y):
    return x * y
</code></pre>
";;0;;2013-11-11T20:07:30.877;28.0;19914937;2016-07-11T07:56:20.307;2017-05-23T10:31:15.870;;-1.0;;2327821.0;;1;52;<python><pandas>;Applying function with multiple arguments to create a new pandas column;45281.0
7682;7682;19918849.0;5.0;"<p>I've got a script updating 5-10 columns worth of data , but sometimes the start csv will be identical to the end csv so instead of writing an identical csvfile I want it to do nothing... </p>

<p>How can I compare two dataframes to check if they're the same or not?</p>

<pre><code>csvdata = pandas.read_csv('csvfile.csv')
csvdata_old = csvdata

# ... do stuff with csvdata dataframe

if csvdata_old != csvdata:
    csvdata.to_csv('csvfile.csv', index=False)
</code></pre>

<p>Any ideas?</p>
";;2;;2013-11-11T22:50:01.893;3.0;19917545;2017-04-15T00:25:14.360;2016-12-31T14:17:26.177;;3001761.0;;2487602.0;;1;14;<python><python-2.7><pandas>;Comparing two pandas dataframes for differences;29225.0
7695;7695;19928288.0;4.0;"<p>Another Pandas question!</p>

<p>I am writing some unit tests that test two data frames for equality, however, the test does not appear to look at the values of the data frame, only the structure:</p>

<pre><code>dates = pd.date_range('20130101', periods=6)

df1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))
df2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))

print df1
print df2
self.assertItemsEqual(df1, df2)
</code></pre>

<p>-->True</p>

<p>Do I need to convert the data frames to another data structure before asserting equality?</p>
";;0;;2013-11-12T11:45:48.087;1.0;19928284;2017-04-15T00:27:40.943;2016-08-11T16:43:25.253;;2419215.0;;57215.0;;1;23;<pandas>;Pandas dataframe values equality test;7575.0
7722;7722;19969224.0;1.0;"<p>I have a pandas df as below:</p>

<pre><code>&gt;&gt;&gt; df
                   sales  net_pft  sales_gr  net_pft_gr
STK_ID RPT_Date                                        
600809 20120331  22.1401   4.9253    0.1824     -0.0268
       20120630  38.1565   7.8684    0.3181      0.1947
       20120930  52.5098  12.4338    0.4735      0.7573
       20121231  64.7876  13.2731    0.4435      0.7005
       20130331  27.9517   7.5182    0.2625      0.5264
       20130630  40.6460   9.8572    0.0652      0.2528
       20130930  53.0501  11.8605    0.0103     -0.0461
</code></pre>

<p>Then <code>df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)</code> create bar chart.</p>

<p>And <code>df[['sales_gr','net_pft_gr']].plot(kind='line', use_index=True)</code> create line chart:</p>

<p>Now I want to put them together in a chart of two y-axes, using twinx(). </p>

<pre><code>import matplotlib.pyplot as plt
fig = plt.figure()
ax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)
ax2 = ax.twinx()
ax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)
</code></pre>

<p>The result is like this :
<img src=""https://i.stack.imgur.com/XgKhp.png"" alt=""enter image description here""></p>

<p>My issues are:</p>

<ul>
<li>How to shift the line to align with the bar at the same x-tickers ?</li>
<li>How to let the left and right y_axis tickers aligned at the same line?  </li>
</ul>
";;0;;2013-11-13T11:16:20.653;3.0;19952290;2013-11-14T03:59:07.403;2013-11-14T03:38:07.583;;1072888.0;;1072888.0;;1;11;<python><matplotlib><pandas>;How to align the bar and line in matplotlib two y-axes chart?;7826.0
7727;7727;19960116.0;4.0;"<p>How can I achieve the equivalents of SQL's <code>IN</code> and <code>NOT IN</code>?</p>

<p>I have a list with the required values.
Here's the scenario:</p>

<pre><code>df = pd.DataFrame({'countries':['US','UK','Germany','China']})
countries = ['UK','China']

# pseudo-code:
df[df['countries'] not in countries]
</code></pre>

<p>My current way of doing this is as follows:</p>

<pre><code>df = pd.DataFrame({'countries':['US','UK','Germany','China']})
countries = pd.DataFrame({'countries':['UK','China'], 'matched':True})

# IN
df.merge(countries,how='inner',on='countries')

# NOT IN
not_in = df.merge(countries,how='left',on='countries')
not_in = not_in[pd.isnull(not_in['matched'])]
</code></pre>

<p>But this seems like a horrible kludge. Can anyone improve on it?</p>
";;2;;2013-11-13T17:11:07.133;28.0;19960077;2017-07-19T12:19:40.260;2015-07-17T20:25:16.837;;202229.0;;2071807.0;;1;84;<python><pandas><dataframe><sql-function>;How to implement 'in' and 'not in' for Pandas dataframe;72686.0
7730;7730;19961557.0;2.0;"<p>I have a list of tuples like</p>

<pre><code>data = [
('r1', 'c1', avg11, stdev11),
('r1', 'c2', avg12, stdev12),
('r2', 'c1', avg21, stdev21),
('r2', 'c2', avg22, stdev22)
]
</code></pre>

<p>and I would like to put them into a pandas DataFrame with rows named by the first column and columns named by the 2nd column. It seems the way to take care of the row names is something like <code>pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data])</code> but how do I take care of the columns to get a 2x2 matrix (the output from the previous set is 3x4)? Is there a more intelligent way of taking care of row labels as well, instead of explicitly omitting them?</p>

<p><strong>EDIT</strong> It seems I will need 2 DataFrames - one for averages and one for standard deviations, is that correct? Or can I store a list of values in each ""cell""?</p>
";;3;;2013-11-13T18:24:29.217;12.0;19961490;2015-05-23T10:08:01.587;2015-05-23T10:08:01.587;;3923281.0;;399573.0;;1;43;<python><python-2.7><pandas>;Construct pandas DataFrame from list of tuples;96105.0
7741;7741;;5.0;"<p>This should be straightforward, but the closest thing I've found is this post:
<a href=""https://stackoverflow.com/questions/18265930/pandas-filling-missing-values-within-a-group"">pandas: Filling missing values within a group</a>, and I still can't solve my problem....</p>

<p>Suppose I have the following dataframe</p>

<pre><code>df = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})

  name  value
0    A      1
1    A    NaN
2    B    NaN
3    B      2
4    B      3
5    B      1
6    C      3
7    C    NaN
8    C      3
</code></pre>

<p>and I'd like to fill in ""NaN"" with mean value in each ""name"" group, i.e.</p>

<pre><code>      name  value
0    A      1
1    A      1
2    B      2
3    B      2
4    B      3
5    B      1
6    C      3
7    C      3
8    C      3
</code></pre>

<p>I'm not sure where to go after:</p>

<pre><code>grouped = df.groupby('name').mean()
</code></pre>

<p>Thanks a bunch.</p>
";;0;;2013-11-13T22:43:25.247;8.0;19966018;2017-07-28T12:32:53.460;2017-05-23T12:18:19.107;;-1.0;;1535386.0;;1;18;<python><pandas>;Pandas: filling missing values by mean in each group;6968.0
7762;7762;19982756.0;2.0;"<p>I have a dataframe which Im loading from a csv file and then setting the index to few of its columns (usually two or three) by the set_index method. The idea is to then access parts of the dataframe using several key combination, as such:</p>

<pre><code>df.set_index(['fileName','phrase'])
df.ix['somePath','somePhrase']
</code></pre>

<p>Appearntly, this type of selection with multiple keys is only possible if the Multi-Index of the dataframe is sorted to sufficient depth. In this case, since im supplying two keys, the .ix operation will not fail only if the dataframe multi index is sorted to depth of at least 2. </p>

<p>for some reason, when Im setting the index as shown, while to me it seems both layers are sorted, calling  <code>df.index.lexsort_depth</code> command returns 1, and i get the following error when trying to access with two keys: <code>MultiIndex lexsort depth 1, key was length 2</code></p>

<p>Any help?</p>
";;0;;2013-11-14T15:22:38.373;3.0;19981518;2015-08-11T17:17:21.907;2013-11-14T17:31:06.617;;1945306.0;;1945306.0;;1;15;<python><pandas>;Sorting Multi-Index to full depth (Pandas);6570.0
7775;7775;;4.0;"<p>I have a <code>pandas</code> data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"": [10,20,30,40,50], 
                   ""B"": [20, 30, 10, 40, 50], 
                   ""C"": [32, 234, 23, 23, 42523]})
</code></pre>

<p>Ideally, I would have something like <code>ols(A ~ B + C, data = df)</code> but when I look at the <a href=""http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"" rel=""noreferrer"">examples</a> from algorithm libraries like <code>scikit-learn</code> it appears to feed the data to the model with a list of rows instead of columns. This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place. What is the most pythonic way to run an OLS regression (or any machine learning algorithm more generally) on data in a pandas data frame? </p>
";;0;;2013-11-15T00:47:00.030;36.0;19991445;2017-06-30T18:04:07.577;2016-04-04T18:33:37.083;;2230844.0;;2327821.0;;1;60;<python><pandas><scikit-learn><regression><statsmodels>;Run an OLS regression with Pandas Data Frame;82103.0
7796;7796;20019426.0;5.0;"<h2>Question</h2>

<p>Is it possible to specify a float precision specifically for each column to be printed by the Python <code>pandas</code> package method <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_csv.html"" rel=""noreferrer"">pandas.DataFrame.to_csv</a>?</p>

<h2>Background</h2>

<p>If I have a <code>pandas</code> dataframe that is arranged like this:</p>

<pre><code>In [53]: df_data[:5]
Out[53]: 
    year  month  day       lats       lons  vals
0   2012      6   16  81.862745 -29.834254   0.0
1   2012      6   16  81.862745 -29.502762   0.1
2   2012      6   16  81.862745 -29.171271   0.0
3   2012      6   16  81.862745 -28.839779   0.2
4   2012      6   16  81.862745 -28.508287   0.0
</code></pre>

<p>There is the <code>float_format</code> option that can be used to specify a precision, but this applys that precision to all columns of the dataframe when printed.</p>

<p>When I use that like so:</p>

<pre><code>df_data.to_csv(outfile, index=False,
                   header=False, float_format='%11.6f')
</code></pre>

<p>I get the following, where <code>vals</code> is given an inaccurate precision:</p>

<pre><code>2012,6,16,  81.862745, -29.834254,   0.000000
2012,6,16,  81.862745, -29.502762,   0.100000
2012,6,16,  81.862745, -29.171270,   0.000000
2012,6,16,  81.862745, -28.839779,   0.200000
2012,6,16,  81.862745, -28.508287,   0.000000
</code></pre>
";;2;;2013-11-15T14:26:21.287;9.0;20003290;2017-02-18T13:14:13.613;2013-11-16T13:02:58.843;;943773.0;;943773.0;;1;22;<python><csv><numpy><floating-point><pandas>;Print different precision by column with pandas.DataFrame.to_csv()?;16557.0
7808;7808;20012628.0;2.0;"<p>I'm new to pandas, therefore perhaps I'm asking a very stupid question. Normally initialization of data frame in pandas would be column-wise, where I put in dict with key of column names and values of list-like object with same length.</p>

<p>But I would love to initialize row-wise without dynamically concat-ing rows. Say I have a list of namedtuple, is there a optimized operation that will give me a pandas data frame directly from it?</p>

<p>Many many thanks</p>
";;0;;2013-11-15T23:33:48.527;4.0;20012507;2013-11-16T05:35:04.143;;;;;2593536.0;;1;14;<python-2.7><pandas><dataframe>;Pandas: A clean way to initialize data frame with a list of namedtuple;5253.0
7830;7830;;4.0;"<p>Say I have a dataframe in Pandas like the following:</p>

<pre><code>&gt; my_dataframe

col1   col2
A      foo
B      bar
C      something
A      foo
A      bar
B      foo
</code></pre>

<p>where rows represent instances, and columns input features (not showing the target label, but this would be for a classification task), i.e. I trying to build <strong>X</strong> out of <code>my_dataframe</code>.</p>

<p>How can I vectorize this efficiently using e.g. <a href=""http://scikit-learn.org/stable/modules/feature_extraction.html#loading-features-from-dicts""><code>DictVectorizer</code></a> ?</p>

<p>Do I need to convert each and every entry in my DataFrame to a dictionary first? (that's the way it is done in the example in the link above). Is there a more efficient way to do this?</p>
";;0;;2013-11-16T22:12:45.323;7.0;20024584;2016-08-01T14:08:35.143;2013-11-16T22:48:26.400;;283296.0;;283296.0;;1;12;<python><pandas><scikit-learn>;Vectorizing a Pandas dataframe for Scikit-Learn;11256.0
7833;7833;;3.0;"<p>What is the best way to apply a function over the index of a Pandas <code>DataFrame</code>?
Currently I am using this verbose approach:</p>

<pre><code>pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})
</code></pre>

<p>where <code>Date</code> is the name of the index and <code>foo</code> is the name of the function that I am applying.</p>
";;5;;2013-11-16T23:40:04.587;4.0;20025325;2017-05-15T08:56:14.170;2016-09-22T13:11:55.553;;3730397.0;;2638485.0;;1;29;<python><pandas><indexing><dataframe>;Apply Function on DataFrame Index;15303.0
7834;7834;20027386.0;1.0;"<p>I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).
I already figured out how to kind-of do this and I am currently using:</p>

<pre><code>df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']
</code></pre>

<p>This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?</p>

<p>In case this is yet unclear, I would like to turn:</p>

<pre><code>    col 
1     a
2     0
</code></pre>

<p>into:</p>

<pre><code>       col 
1     stra
2     str0
</code></pre>
";;6;;2013-11-17T00:56:09.157;5.0;20025882;2013-11-17T05:00:19.437;2013-11-17T01:43:48.777;;1893275.0;;1893275.0;;1;20;<python><pandas><dataframe>;Append string to the start of each value in a said column of a pandas dataframe (elegantly);18507.0
7841;7841;20033232.0;2.0;"<pre><code>data = {'name' : ['bill', 'joe', 'steve'],
    'test1' : [85, 75, 85],
    'test2' : [35, 45, 83],
     'test3' : [51, 61, 45]}
frame = pd.DataFrame(data)
</code></pre>

<p>I would like to add a new column that shows the max value for each row.</p>

<p>desired output:</p>

<pre><code> name test1 test2 test3 HighScore
 bill  75    75    85    85
 joe   35    45    83    83 
 steve  51   61    45    61 
</code></pre>

<p><strong>Sometimes</strong>  </p>

<pre><code>frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])
</code></pre>

<p>works but most of the time gives this error:</p>

<p><strong>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</strong></p>

<p>Why does it only work sometimes?  Is there another way of doing it?</p>
";;0;;2013-11-17T16:22:56.853;12.0;20033111;2017-06-28T09:30:40.273;;;;;2333196.0;;1;37;<python><python-2.7><pandas><max>;Python Pandas max value of selected columns;43953.0
7845;7845;20043785.0;1.0;"<p>I'd like to insert a link (to a web page) inside a pandas table, so when it is displayed in ipython notebook, I could press the link.</p>

<p>I tried the following:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame(range(5), columns=['a'])

In [3]: df['b'] = df['a'].apply(lambda x: 'http://example.com/{0}'.format(x))

In [4]: df
Out[4]:
   a                     b
0  0  http://example.com/0
1  1  http://example.com/1
2  2  http://example.com/2
3  3  http://example.com/3
4  4  http://example.com/4
</code></pre>

<p>but the url is just displayed as text. </p>

<p>I also tried using ipython HTML object:</p>

<pre><code>In [5]: from IPython.display import HTML

In [6]: df['b'] = df['a'].apply(lambda x:HTML('http://example.com/{0}'.format(x)))

In [7]: df
Out[7]:
   a                                                 b
0  0  &lt;IPython.core.display.HTML object at 0x0481E530&gt;
1  1  &lt;IPython.core.display.HTML object at 0x0481E770&gt;
2  2  &lt;IPython.core.display.HTML object at 0x0481E7B0&gt;
3  3  &lt;IPython.core.display.HTML object at 0x0481E810&gt;
4  4  &lt;IPython.core.display.HTML object at 0x0481EA70&gt;
</code></pre>

<p>but it will only display the repr of the object.</p>

<p>Any other ideas?</p>

<p>EDIT:
alko got the right answer, just wanted to add that the cell width is limited by default, and long html code will be truncated, ie:</p>

<pre><code>&lt;a href=""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0""&gt;xxx&lt;/a&gt;
</code></pre>

<p>will become this:</p>

<pre><code>&lt;a href=""aaaaaaaaaaaaaaaaaaaaaa...
</code></pre>

<p>and won't be displayed correctly.
(even though the text xxx is short and can fit in the cell)</p>

<p>I've bypassed it by setting:</p>

<pre><code>pd.set_printoptions(max_colwidth=-1)
</code></pre>
";;0;;2013-11-17T20:07:36.420;2.0;20035518;2017-02-16T05:04:04.127;2017-02-16T05:04:04.127;;245024.0;;245024.0;;1;18;<python><pandas><ipython-notebook>;Insert a link inside a pandas table;5078.0
7848;7848;20038973.0;4.0;"<p>I'd like to know if there is a memory efficient way of reading multi record JSON file ( each line is a JSON dict) into a pandas dataframe. Below is a 2 line example with working solution, I need it for potentially very large number of records. Example use would be to process output from Hadoop Pig JSonStorage function.</p>

<pre><code>import json
import pandas as pd

test='''{""a"":1,""b"":2}
{""a"":3,""b"":4}'''
#df=pd.read_json(test,orient='records') doesn't work, expects []

l=[ json.loads(l) for l in test.splitlines()]
df=pd.DataFrame(l)
</code></pre>
";;0;;2013-11-17T23:10:03.640;6.0;20037430;2016-10-20T15:14:28.640;;;;;1651150.0;;1;12;<python><json><pandas>;pandas - reading multiple JSON records into dataframe;12000.0
7876;7876;;2.0;"<p>I used pyodbc with python before but now I have installed it on a new machine ( win 8 64 bit, Python 2.7 64 bit, PythonXY with Spyder). </p>

<p>Before I used to (at the bottom you can find more real examples):</p>

<pre><code>columns = [column[0] for column in cursor.description]
temp = cursor.fetchall()
data = pandas.DataFrame(temp,columns=columns)
</code></pre>

<p>and it would work fine. Now it seems like DataFrame is not able to convert from the data fetched from the cursor anymore. It returns:</p>

<p>Shape of passed values is (x,y), indices imply (w,z)</p>

<p>I kind of see where the issue is. Basically, imagine I fetch only one row. Then DataFrame would like to shape it (1,1), one element only. While I would like to have (1,X) where X is the length of the list. </p>

<p>I am not sure why the behavior changed. Maybe it is the Pandas version I have, or the pyodbc, but updating is problematic. I tried to update some modules but it screws up everything, any method I use (binaries--for the right machine/installation--pip install, easy-install,anything! etc.. which is very frustrating indeed. I would probably avoid Win 8 64 bit from now on for Python).</p>

<p>Real examples:</p>

<pre><code>sql = 'Select * form TABLE'
cursor.execute(sql)
columns = [column[0] for column in cursor.description]
data    = cursor.fetchall()
        con.close()
            results = DataFrame(data, columns=columns)
</code></pre>

<p>Returns:
<em>*</em> ValueError: Shape of passed values is (1, 1540), indices imply (51, 1540)</p>

<p>Notice that:</p>

<pre><code>ipdb&gt; type(data)
&lt;type 'list'&gt;
ipdb&gt; np.shape(data)
(1540, 51)
ipdb&gt; type(data[0])
&lt;type 'pyodbc.Row'&gt;
</code></pre>

<p>Now, for example, if we do:</p>

<pre><code>ipdb&gt; DataFrame([1,2,3],columns=['a','b','c'])
</code></pre>

<p><em>*</em> ValueError: Shape of passed values is (1, 3), indices imply (3, 3)</p>

<p>and if we do:</p>

<pre><code>ipdb&gt; DataFrame([[1,2,3]],columns=['a','b','c'])
</code></pre>

<p>a  b  c
0  1  2  3</p>

<p>However, even trying:</p>

<pre><code>ipdb&gt; DataFrame([data[0]], columns=columns)
*** ValueError: Shape of passed values is (1, 1), indices imply (51, 1)
</code></pre>

<p>or</p>

<pre><code>ipdb&gt; DataFrame(data[0], columns=columns)
*** PandasError: DataFrame constructor not properly called!
</code></pre>

<p>Please help :) Thanks!</p>
";;6;;2013-11-18T18:38:30.470;4.0;20055257;2014-02-05T22:12:28.523;2013-11-18T20:48:57.657;;1350191.0;;1350191.0;;1;11;<python><windows-8><pandas><64bit><pyodbc>;PYODBC to Pandas - DataFrame not working - Shape of passed values is (x,y), indices imply (w,z);10039.0
7903;7903;20067665.0;3.0;"<p>I have a pandas <code>DataFrame</code> like following.</p>

<pre><code>df = pd.DataFrame({'id' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],
                'value'  : [""first"",""second"",""second"",""first"",
                            ""second"",""first"",""third"",""fourth"",
                            ""fifth"",""second"",""fifth"",""first"",
                            ""first"",""second"",""third"",""fourth"",""fifth""]})
</code></pre>

<p>I want to group this by [""id"",""value""] and get the first row of each group.</p>

<pre><code>        id   value
0        1   first
1        1  second
2        1  second
3        2   first
4        2  second
5        3   first
6        3   third
7        3  fourth
8        3   fifth
9        4  second
10       4   fifth
11       5   first
12       6   first
13       6  second
14       6   third
15       7  fourth
16       7   fifth
</code></pre>

<p>Expected outcome</p>

<pre><code>    id   value
     1   first
     2   first
     3   first
     4  second
     5  first
     6  first
     7  fourth
</code></pre>

<p>I tried following which only gives the first row of the <code>DataFrame</code>. Any help regarding this is appreciated.</p>

<pre><code>In [25]: for index, row in df.iterrows():
   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])
</code></pre>
";;0;;2013-11-19T09:24:34.953;13.0;20067636;2016-10-28T18:39:39.443;;;;;461436.0;;1;38;<python><pandas><dataframe>;Pandas dataframe get first row of each group;35554.0
7907;7907;20069379.0;2.0;"<p>Suppose I have pandas DataFrame like this:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})
&gt;&gt;&gt; df
   id  value
0   1      1
1   1      2
2   1      3
3   2      1
4   2      2
5   2      3
6   2      4
7   3      1
8   4      1
</code></pre>

<p>I want to get new DataFrame with top 2 records for each id, like this:</p>

<pre><code>   id  value
0   1      1
1   1      2
3   2      1
4   2      2
7   3      1
8   4      1
</code></pre>

<p>I can do it with numbering records within group after group by:</p>

<pre><code>&gt;&gt;&gt; dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()
&gt;&gt;&gt; dfN
   id  level_1  index  value
0   1        0      0      1
1   1        1      1      2
2   1        2      2      3
3   2        0      3      1
4   2        1      4      2
5   2        2      5      3
6   2        3      6      4
7   3        0      7      1
8   4        0      8      1
&gt;&gt;&gt; dfN[dfN['level_1'] &lt;= 1][['id', 'value']]
   id  value
0   1      1
1   1      2
3   2      1
4   2      2
7   3      1
8   4      1
</code></pre>

<p>But is there more effective/elegant approach to do this? And also is there more elegant approach to number records within each group (like SQL window function <a href=""http://msdn.microsoft.com/en-us/library/ms186734.aspx"" rel=""noreferrer"">row_number()</a>).</p>

<p>Thanks in advance.</p>
";;1;;2013-11-19T10:28:36.650;16.0;20069009;2016-11-05T01:39:39.500;2016-11-05T01:39:39.500;;202229.0;;1744834.0;;1;54;<python><pandas><greatest-n-per-group><window-functions><top-n>;Pandas good approach to get top-n records within each group;41787.0
7911;7911;20076611.0;2.0;"<p>I have a large (about 12M rows) dataframe df with say:</p>

<pre><code>df.columns = ['word','documents','frequency']
</code></pre>

<p>So the following ran in a timely fashion:</p>

<pre><code>word_grouping = df[['word','frequency']].groupby('word')
MaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()
MaxFrequency_perWord.columns = ['word','MaxFrequency']
</code></pre>

<p>However, this is taking an unexpected long time to run:</p>

<pre><code>Occurrences_of_Words = word_grouping[['word']].count().reset_index()
</code></pre>

<p>What am I doing wrong here?  Is there a better way to count occurences in a large dataframe?</p>

<pre><code>df.word.describe()
</code></pre>

<p>ran pretty well, so I really did not expect this Occurrences_of_Words dataframe to take very long to build.</p>

<p>ps: If the answer is obvious and you feel the need to penalize me for asking this question, please include the answer as well.  thank you.</p>
";;0;;2013-11-19T15:58:33.823;11.0;20076195;2017-06-30T13:03:31.677;;;;;1311866.0;;1;27;<pandas>;what is the most efficient way of counting occurrences in pandas?;34623.0
7919;7919;20543819.0;2.0;"<p>I've been using pandas for research now for about two months to great effect. With large numbers of medium-sized trace event datasets, pandas + PyTables (the HDF5 interface) does a tremendous job of allowing me to process heterogenous data using all the Python tools I know and love.</p>

<p>Generally speaking, I use the Fixed (formerly ""Storer"") format in PyTables, as my workflow is write-once, read-many, and many of my datasets are sized such that I can load 50-100 of them into memory at a time with no serious disadvantages. (NB: I do much of my work on Opteron server-class machines with 128GB+ system memory.)</p>

<p>However, for large datasets (500MB and greater), I would like to be able to use the more scalable random-access and query abilities of the PyTables ""Tables"" format, so that I can perform my queries out-of-memory and then load the much smaller result set into memory for processing. The big hurdle here, however, is the write performance. Yes, as I said, my workflow is write-once, read-many, but the relative times are still unacceptable. </p>

<p>As an example, I recently ran a large Cholesky factorization that took 3 minutes, 8 seconds (188 seconds) on my 48 core machine. This generated a trace file of ~2.2 GB - the trace is generated in parallel with the program, so there is no additional ""trace creation time.""</p>

<p>The initial conversion of my binary trace file into the pandas/PyTables format takes a decent chunk of time, but largely because the binary format is deliberately out-of-order in order to reduce the performance impact of the trace generator itself. This is also irrelevant to the performance loss when moving from the Storer format to the Table format.</p>

<p>My tests were initially run with pandas 0.12, numpy 1.7.1, PyTables 2.4.0, and numexpr 0.20.1. My 48 core machine runs at 2.8GHz per core, and I am writing to an ext3 filesystem which is probably (but not certainly) on a SSD.</p>

<p>I can write the entire dataset to a Storer format HDF5 file (resulting filesize: 3.3GB) in 7.1 seconds. The same dataset, written to the Table format (resulting file size is also 3.3GB), takes 178.7 seconds to write. </p>

<p>The code is as follows:</p>

<pre><code>with Timer() as t:
    store = pd.HDFStore('test_storer.h5', 'w')
    store.put('events', events_dataset, table=False, append=False)
print('Fixed format write took ' + str(t.interval))
with Timer() as t:
    store = pd.HDFStore('test_table.h5', 'w')
    store.put('events', events_dataset, table=True, append=False)
print('Table format write took ' + str(t.interval))
</code></pre>

<p>and the output is simply</p>

<pre><code>Fixed format write took 7.1
Table format write took 178.7
</code></pre>

<p>My dataset has 28,880,943 rows, and the columns are basic datatypes:</p>

<pre><code>node_id           int64
thread_id         int64
handle_id         int64
type              int64
begin             int64
end               int64
duration          int64
flags             int64
unique_id         int64
id                int64
DSTL_LS_FULL    float64
L2_DMISS        float64
L3_MISS         float64
kernel_type     float64
dtype: object
</code></pre>

<p>...so I don't think there should be any data-specific issues with the write speed. </p>

<p>I've also tried adding BLOSC compression, to rule out any strange I/O issues that might affect one scenario or the other, but compression seems to decrease the performance of both equally.</p>

<p>Now, I realize that the pandas documentation says that the Storer format offers significantly faster writes, and slightly faster reads. (I do experience the faster reads, as a read of the Storer format seems to take around 2.5 seconds, while a read of the Table format takes around 10 seconds.) But it really seems excessive that the Table format write should take 25 times as long as the Storer format write.</p>

<p>Can any of the folks involved with PyTables or pandas explain the architectural (or otherwise) reasons why writing to the queryable format (which clearly requires very little extra data) should take an order of magnitude longer? And is there any hope for improving this in the future? I'd love to jump in to contributing to one project or the other, as my field is high performance computing and I see a significant use case for both projects in this domain.... but it would be helpful to get some clarification on the issues involved first, and/or some advice on how to speed things up from those who know how the system is built.</p>

<p>EDIT:</p>

<p>Running the former tests with %prun in IPython gives the following (somewhat reduced for readability) profile output for the Storer/Fixed format:</p>

<pre><code>%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)

3223 function calls (3222 primitive calls) in 7.385 seconds

Ordered by: internal time
List reduced from 208 to 20 due to restriction &lt;20&gt;

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}
    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}
    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}
   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}
</code></pre>

<p>and the following for the Tables format:</p>

<pre><code>   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)

   499082 function calls (499040 primitive calls) in 188.981 seconds

   Ordered by: internal time
   List reduced from 526 to 40 due to restriction &lt;40&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}
      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}
       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}
      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}
    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}
      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}
       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}
        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)
       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}
       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}
       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}
      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}
       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)
    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)
       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)
        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)
        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)
    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}
    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)
    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)
</code></pre>

<p>EDIT 2:</p>

<p>Running again with a pre-release copy of pandas 0.13 (pulled Nov 20 2013 at about 11:00 EST), write times for the Tables format improve significantly but still don't compare ""reasonably"" to the write speeds of the Storer/Fixed format.</p>

<pre><code>%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)

         499748 function calls (499720 primitive calls) in 117.187 seconds

   Ordered by: internal time
   List reduced from 539 to 20 due to restriction &lt;20&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}
       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}
      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}
    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}
      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}
       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}
       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)
        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)
       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}
       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}
       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}
      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}
      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}
       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)
    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)
       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)
        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)
    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}
        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)
        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)
</code></pre>

<p>I noticed while running these tests that there are long periods where writing seems to ""pause"" (the file on disk is not actively growing), and yet there is also low CPU usage during some of these periods.</p>

<p>I begin to suspect that some known ext3 limitations may interact badly with either pandas or PyTables. Ext3 and other non-extent-based filesystems sometimes struggle to unlink large files promptly, and similar system performance (low CPU usage, but long wait times) is apparent even during a simple 'rm' of a 1GB file, for instance. </p>

<p>To clarify, in each test case, I made sure to remove the existing file, if any, before starting the test, so as not to incur any ext3 file removal/overwrite penalty.</p>

<p>However, when re-running this test with index=None, performance improves drastically (~50s vs the ~120 when indexing). So it would seem that either this process continues to be CPU-bound (my system has relatively old AMD Opteron Istanbul CPUs running @ 2.8GHz, though it does also have 8 sockets with 6 core CPUs in each, all but one of which, of course, sit idle during the write), or that there is some conflict between the way PyTables or pandas attempts to manipulate/read/analyze the file when already partially or fully on the filesystem that causes pathologically bad I/O behavior when the indexing is occurring.</p>

<p>EDIT 3:</p>

<p>@Jeff's suggested tests on a smaller dataset (1.3 GB on disk), after upgrading PyTables from 2.4 to 3.0.0, have gotten me here:</p>

<pre><code>In [7]: %timeit f(df)
1 loops, best of 3: 3.7 s per loop

In [8]: %timeit f2(df) # where chunksize= 2 000 000
1 loops, best of 3: 13.8 s per loop

In [9]: %timeit f3(df) # where chunksize= 2 000 000
1 loops, best of 3: 43.4 s per loop
</code></pre>

<p>In fact, my performance seems to beat his in all scenarios except for when indexing is turned on (the default). However, indexing still seems to be a killer, and if the way I'm interpreting the output from <code>top</code> and <code>ls</code> as I run these tests is correct, there remain periods of time when there is neither significant processing nor any file writing happening (i.e., CPU usage for the Python process is near 0, and the filesize remains constant). I can only assume these are file reads. Why file reads would be causing slowdowns is hard for me to understand, as I can reliably load an entire 3+ GB file from this disk into memory in under 3 seconds. If they're not file reads, then what is the system 'waiting' on? (No one else is logged into the machine, and there is no other filesystem activity.)</p>

<p>At this point, with upgraded versions of the relevant python modules, the performance for my original dataset is down to the following figures. Of special interest are the system time, which I assume is at least an upper-bound on the time spent performing IO, and the Wall time, which seems to perhaps account for these mysterious periods of no write/no CPU activity.</p>

<pre><code>In [28]: %time f(profile.events)
CPU times: user 0 ns, sys: 7.16 s, total: 7.16 s
Wall time: 7.51 s

In [29]: %time f2(profile.events)
CPU times: user 18.7 s, sys: 14 s, total: 32.7 s
Wall time: 47.2 s

In [31]: %time f3(profile.events)
CPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s
Wall time: 2min 5s
</code></pre>

<p>Nevertheless, it would appears that indexing causes significant slowdown for my use case. Perhaps I should attempt limiting the fields indexed instead of simply performing the default case (which may very well be indexing on all of the fields in the DataFrame)? I am not sure how this is likely to affect query times, especially in the cases where a query selects based on a non-indexed field.</p>

<p>Per Jeff's request, a ptdump of the resulting file.</p>

<pre><code>ptdump -av test.h5
/ (RootGroup) ''
  /._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'GROUP',
    PYTABLES_FORMAT_VERSION := '2.1',
    TITLE := '',
    VERSION := '1.0']
/df (Group) ''
  /df._v_attrs (AttributeSet), 14 attributes:
   [CLASS := 'GROUP',
    TITLE := '',
    VERSION := '1.0',
    data_columns := [],
    encoding := None,
    index_cols := [(0, 'index')],
    info := {1: {'type': 'Index', 'names': [None]}, 'index': {}},
    levels := 1,
    nan_rep := 'nan',
    non_index_axes := 
    [(1, ['node_id', 'thread_id', 'handle_id', 'type', 'begin', 'end', 'duration', 'flags', 'unique_id', 'id', 'DSTL_LS_FULL', 'L2_DMISS', 'L3_MISS', 'kernel_type'])],
    pandas_type := 'frame_table',
    pandas_version := '0.10.1',
    table_type := 'appendable_frame',
    values_cols := ['values_block_0', 'values_block_1']]
/df/table (Table(28880943,)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),
  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}
  byteorder := 'little'
  chunkshape := (4369,)
  autoindex := True
  colindexes := {
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}
  /df/table._v_attrs (AttributeSet), 15 attributes:
   [CLASS := 'TABLE',
    FIELD_0_FILL := 0,
    FIELD_0_NAME := 'index',
    FIELD_1_FILL := 0,
    FIELD_1_NAME := 'values_block_0',
    FIELD_2_FILL := 0.0,
    FIELD_2_NAME := 'values_block_1',
    NROWS := 28880943,
    TITLE := '',
    VERSION := '2.7',
    index_kind := 'integer',
    values_block_0_dtype := 'int64',
    values_block_0_kind := ['node_id', 'thread_id', 'handle_id', 'type', 'begin', 'end', 'duration', 'flags', 'unique_id', 'id'],
    values_block_1_dtype := 'float64',
    values_block_1_kind := ['DSTL_LS_FULL', 'L2_DMISS', 'L3_MISS', 'kernel_type']]
</code></pre>

<p>and another %prun with the updated modules and the full dataset:</p>

<pre><code>%prun -l 25  %time f3(profile.events)
CPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s
Wall time: 1min 48s

        542678 function calls (542650 primitive calls) in 108.678 seconds

   Ordered by: internal time
   List reduced from 629 to 25 due to restriction &lt;25&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}
       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}
      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}
    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}
      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}
       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)
       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}
       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}
       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}
      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}
       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}
        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)
       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)
    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)
        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)
      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}
       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)
    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}
        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)
        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)
    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)
    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)
       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}
    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)
    87797    0.048    0.000    0.048    0.000 {isinstance}
</code></pre>
";;1;;2013-11-19T22:08:16.547;15.0;20083098;2013-12-12T12:38:25.390;2013-11-20T21:28:11.970;;1807456.0;;1807456.0;;1;29;<python><performance><pandas><hdf5><pytables>;Improve pandas (PyTables?) HDF5 table write performance;10552.0
7920;7920;20084895.0;2.0;"<p>I have a Pandas dataframe and I want to find all the unique values in that dataframe...irrespective of row/columns. If I have a 10 x 10 dataframe, and suppose they have 84 unique values, I need to find them - Not the count.</p>

<p>I can create a set and add the values of each rows by iterating over the rows of the dataframe. But, I feel that it may be inefficient (cannot justify that). Is there an efficient way to find it? Is there a predefined function?</p>
";;0;;2013-11-19T23:26:53.300;15.0;20084382;2017-03-10T09:19:15.737;2016-08-18T21:48:48.223;;5225453.0;;1717931.0;;1;48;<python><pandas><dataframe>;Find unique values in a Pandas dataframe, irrespective of row or column location;81292.0
7941;7941;20096827.0;3.0;"<p>I've got a pandas dataframe. I want to 'lag' one of my columns. Meaning, for example, shifting the entire column 'gdp' up by one, and then removing all the excess data at the bottom of the remaining rows so that all columns are of equal length again.</p>

<pre><code>df =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7

df_lag =
    y  gdp  cap
0   1    3    5
1   2    7    9
2   8    4    2
3   3    7    7
</code></pre>

<p>Anyway to do this?</p>
";;0;;2013-11-20T12:12:11.843;3.0;20095673;2016-05-16T11:59:34.213;;;;;2428549.0;;1;26;<python><pandas><dataframe>;python: shift column in pandas dataframe up by one;24501.0
7942;7942;20096494.0;1.0;"<p>I would like to load a csv file into a Pandas DataFrame. How do I for each column specify what type of data it contains?</p>

<p>I guess this is easily done by using the <code>dtype</code> argument?</p>

<p>Here is an example specifying <strong>numeric</strong> data.</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.read_csv(&lt;file-name&gt;, dtype={'A': np.int64, 'B': np.float64})
</code></pre>

<p>But how do I specify <strong>time</strong> data and <strong>categorical</strong> data such as factors or booleans? I have tried <code>np.bool_</code> and <code>pd.tslib.Timestamp</code> without luck.</p>
";;1;;2013-11-20T12:26:46.750;1.0;20095983;2013-11-20T13:49:53.303;;;;;1824828.0;;1;13;<python><csv><pandas><data-type-conversion>;Specify correct dtypes using pandas.read_csv;17163.0
7959;7959;20107825.0;5.0;"<p>I have the following code which imports a CSV file.  There are 3 columns and I want to set the first two of them to variables.  When I set the second column to the variable ""efficiency"" the index column is also tacked on.  How can I get rid of the index column?</p>

<pre><code>df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)
energy = df.index
efficiency = df.Efficiency
print efficiency
</code></pre>

<p>I tried using </p>

<pre><code>del df['index']
</code></pre>

<p>after I set </p>

<pre><code>energy = df.index
</code></pre>

<p>which I found in another post but that results in ""KeyError: 'index' ""</p>
";;0;;2013-11-20T21:38:13.393;7.0;20107570;2016-12-12T04:18:43.890;;;;;2137083.0;;1;27;<python><pandas>;Removing index column in pandas;70802.0
7968;7968;20159305.0;5.0;"<p>Having spent a decent amount of time watching both the <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged &#39;r&#39;"" rel=""tag"">r</a> and <a href=""/questions/tagged/pandas"" class=""post-tag"" title=""show questions tagged &#39;pandas&#39;"" rel=""tag"">pandas</a> tags on SO, the impression that I get is that <code>pandas</code> questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like <a href=""https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example"">this</a>, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.</p>

<p>How can we create good reproducible examples for <code>pandas</code> questions? Simple dataframes can be put together, e.g.:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], 
                   'income': [40000, 50000, 42000]})
</code></pre>

<p>But many example datasets need more complicated structure, e.g.:</p>

<ul>
<li><code>datetime</code> indices or data</li>
<li>Multiple categorical variables (is there an equivalent to R's <code>expand.grid()</code> function, which produces all possible combinations of some given variables?)</li>
<li>MultiIndex or Panel data</li>
</ul>

<p>For datasets that are hard to mock up using a few lines of code, is there an equivalent to R's <code>dput()</code> that allows you to generate copy-pasteable code to regenerate your datastructure?</p>
";;3;;2013-11-20T23:31:39.790;49.0;20109391;2017-08-24T10:32:27.833;2017-05-23T11:54:58.497;;-1.0;;1222578.0;;1;105;<python><pandas>;How to make good reproducible pandas examples;3594.0
7972;7972;25733562.0;1.0;"<p>I have a dataframe with 2 index levels:</p>

<pre><code>                         value
Trial    measurement
    1              0        13
                   1         3
                   2         4
    2              0       NaN
                   1        12
    3              0        34 
</code></pre>

<p>Which I want to turn into this:</p>

<pre><code>Trial    measurement       value

    1              0        13
    1              1         3
    1              2         4
    2              0       NaN
    2              1        12
    3              0        34 
</code></pre>

<p>How can I best do this?   </p>

<p>I need this because I want to aggregate the data <a href=""https://stackoverflow.com/a/20108446/1893275"">as instructed here</a>, but I can't select my columns like that if they are in use as indices.</p>
";;3;;2013-11-21T00:37:03.023;11.0;20110170;2014-10-21T21:02:50.783;2017-05-23T12:34:44.037;;-1.0;;1893275.0;;1;63;<python><pandas><dataframe><flatten><multi-index>;Turn Pandas Multi-Index into column;31159.0
7982;7982;20120225.0;2.0;"<p>Was trying to generate a pivot table with multiple ""values"" columns. I know I can use aggfunc to aggregate values the way I want to, but what if I don't want to sum or avg both columns but instead I want sum of one column while mean of the other one. So is it possible to do so using pandas?</p>

<pre><code>df = pd.DataFrame({
          'A' : ['one', 'one', 'two', 'three'] * 6,
          'B' : ['A', 'B', 'C'] * 8,
          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,
          'D' : np.random.randn(24),
          'E' : np.random.randn(24)
})
</code></pre>

<p>Now this will get a pivot table with sum:</p>

<pre><code>pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)
</code></pre>

<p>And this for mean:</p>

<pre><code>pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)
</code></pre>

<p>How can I get sum for <code>D</code> and mean for <code>E</code>?</p>

<p>Hope my question is clear enough.</p>
";;0;;2013-11-21T11:14:46.027;7.0;20119414;2016-01-07T22:28:06.120;2016-01-07T22:28:06.120;;384803.0;;2814765.0;;1;11;<python><python-2.7><pandas>;define aggfunc for each values column in pandas pivot table;9943.0
8020;8020;20154429.0;4.0;"<p>I have a csv file that has a few hundred rows and 26 columns, but the last few columns only have a value in a few rows and they are towards the middle or end of the file. When I try to read it in using read_csv() I get the following error. 
""ValueError: Expecting 23 columns, got 26 in row 64""</p>

<p>I can't see where to explicitly state the number of columns in the file, or how it determines how many columns it thinks the file should have. 
The dump is below</p>

<pre><code>In [3]:

infile =open(easygui.fileopenbox(),""r"")
pledge = read_csv(infile,parse_dates='true')


---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-b35e7a16b389&gt; in &lt;module&gt;()
      1 infile =open(easygui.fileopenbox(),""r"")
      2 
----&gt; 3 pledge = read_csv(infile,parse_dates='true')


C:\Python27\lib\site-packages\pandas-0.8.1-py2.7-win32.egg\pandas\io\parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)
    234         kwds['delimiter'] = sep
    235 
--&gt; 236     return _read(TextParser, filepath_or_buffer, kwds)
    237 
    238 @Appender(_read_table_doc)

C:\Python27\lib\site-packages\pandas-0.8.1-py2.7-win32.egg\pandas\io\parsers.pyc in _read(cls, filepath_or_buffer, kwds)
    189         return parser
    190 
--&gt; 191     return parser.get_chunk()
    192 
    193 @Appender(_read_csv_doc)

C:\Python27\lib\site-packages\pandas-0.8.1-py2.7-win32.egg\pandas\io\parsers.pyc in get_chunk(self, rows)
    779             msg = ('Expecting %d columns, got %d in row %d' %
    780                    (col_len, zip_len, row_num))
--&gt; 781             raise ValueError(msg)
    782 
    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))

ValueError: Expecting 23 columns, got 26 in row 64
</code></pre>
";;3;;2013-11-22T20:54:15.650;3.0;20154303;2017-01-19T07:17:55.773;2015-04-02T09:24:01.390;;202229.0;;442158.0;;1;12;<python><csv><pandas><ragged>;Pandas read_csv expects wrong number of columns, with ragged csv file;8426.0
8024;8024;;4.0;"<p>My question is the same as this previous one:</p>

<p><a href=""https://stackoverflow.com/questions/18921570/binning-with-zero-values-in-pandas"">Binning with zero values in pandas</a></p>

<p>however, I still want to include the 0 values in a fractile.  Is there a way to do this?  In other words, if I have 600 values, 50% of which are 0, and the rest are let's say between 1 and 100, how would I categorize all the 0 values in fractile 1, and then the rest of the non-zero values in fractile labels 2 to 10 (assuming I want 10 fractiles).  Could I convert the 0's to nan, qcut the remaining non nan data into 9 fractiles (1 to 9), then add 1 to each label (now 2 to 10) and label all the 0 values as fractile 1 manually?  Even this is tricky, because in my data set in addition to the 600 values, I also have another couple hundred which may already be nan before I would convert the 0s to nan.</p>

<p>Update 1/26/14:</p>

<p>I came up with the following interim solution.  The problem with this code though, is if the high frequency value is not on the edges of the distribution, then it inserts an extra bin in the middle of the existing set of bins and throws everything a little (or a lot) off.</p>

<pre><code>def fractile_cut(ser, num_fractiles):
    num_valid = ser.valid().shape[0]
    remain_fractiles = num_fractiles
    vcounts = ser.value_counts()
    high_freq = []
    i = 0
    while vcounts.iloc[i] &gt; num_valid/ float(remain_fractiles):
        curr_val = vcounts.index[i]
        high_freq.append(curr_val)
        remain_fractiles -= 1
        num_valid = num_valid - vcounts[i]
        i += 1
    curr_ser = ser.copy()
    curr_ser = curr_ser[~curr_ser.isin(high_freq)]
    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)
    qcut_bins = qcut[1]
    all_bins = list(qcut_bins)
    for val in high_freq:
        bisect.insort(all_bins, val)
    cut = pd.cut(ser, bins=all_bins)
    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)
    return ser_fractiles
</code></pre>
";;5;;2013-11-23T04:37:44.290;8.0;20158597;2017-08-25T08:22:10.760;2017-05-23T12:26:04.590;;-1.0;;1330060.0;;1;29;<python><pandas>;How to qcut with non unique bin edges?;8987.0
8036;8036;20167984.0;5.0;"<p>What is the quickest way to insert a pandas DataFrame into mongodb using <code>PyMongo</code>?</p>

<p><strong>Attempts</strong></p>

<pre><code>db.myCollection.insert(df.to_dict())
</code></pre>

<p>gave an error <code>InvalidDocument: documents must have only string keys, key was Timestamp('2013-11-23 13:31:00', tz=None)</code></p>

<pre><code>db.myCollection.insert(df.to_json())
</code></pre>

<p>gave an error <code>TypeError: 'str' object does not support item assignment</code></p>

<pre><code>db.myCollection.insert({id: df.to_json()})
</code></pre>

<p>gave an error <code>InvalidDocument: documents must have only string keys, key was &lt;built-in function id&gt;</code></p>

<p><strong>df</strong></p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 150 entries, 2013-11-23 13:31:26 to 2013-11-23 13:24:07
Data columns (total 3 columns):
amount    150  non-null values
price     150  non-null values
tid       150  non-null values
dtypes: float64(2), int64(1)
</code></pre>
";;2;;2013-11-23T20:01:32.790;10.0;20167194;2016-06-15T00:00:49.233;2013-11-23T20:37:50.120;;741099.0;;741099.0;;1;11;<python><mongodb><python-2.7><pandas><pymongo>;Insert a Pandas Dataframe into mongodb using PyMongo;13066.0
8038;8038;20168416.0;4.0;"<p>I need the index to start at 1 rather than 0 when writing a Pandas DataFrame to CSV. Here's an example:</p>

<pre><code>In [1]: import pandas as pd

In [2]: result = pd.DataFrame({'Count': [83, 19, 20]})

In [3]: result.to_csv('result.csv', index_label='Event_id')                               
</code></pre>

<p>Which produces the following output:</p>

<pre><code>In [4]: !cat result.csv
Event_id,Count
0,83
1,19
2,20
</code></pre>

<p>But my desired output is this:</p>

<pre><code>In [5]: !cat result2.csv
Event_id,Count
1,83
2,19
3,20
</code></pre>

<p>I realize that this could be done by adding a sequence of integers shifted by 1 as a column to my data frame, but I'm new to Pandas and wondering if a cleaner way exists.</p>

<p>Thanks.</p>
";;0;;2013-11-23T21:12:24.147;4.0;20167930;2017-08-25T14:06:53.890;;;;;2681019.0;;1;14;<python><csv><pandas>;Start index at 1 when writing Pandas DataFrame to CSV;8795.0
8051;8051;20181686.0;1.0;"<p>In a pandas DataFrame, is it possible to collapse columns which have identical values, and sum up the values in another column?</p>

<p><strong>Code</strong></p>

<pre><code>data = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}
df = pd.DataFrame(data)
df
</code></pre>

<p><strong>Output</strong></p>

<pre><code>     count       score       type
0    394.189306  9.397000    advanced
1    143.142267  9.397000    advanced
2    9.641728    9.397995    advanced
3    0.100000    9.397996    newbie
4    19.654137   9.399900    expert
</code></pre>

<p>In the example above, the first two rows have the same <code>score</code> and <code>type</code> , so these rows should be merged together and their scores added up.</p>

<p><strong>Desired Output</strong></p>

<pre><code>     count       score       type
0    537.331573  9.397000    advanced
1    9.641728    9.397995    advanced
2    0.100000    9.397996    newbie
3    19.654137   9.399900    expert
</code></pre>
";;0;;2013-11-24T21:48:18.760;3.0;20181456;2013-11-24T22:12:09.130;2013-11-24T22:12:09.130;;487339.0;;741099.0;;1;14;<python><python-2.7><pandas>;Sum up column values in Pandas DataFrame;28899.0
8077;8077;20200594.0;5.0;"<p>Given a dataframe, I want to get the duplicated indexes, which do not have duplicate values in the columns, and see which values are different.</p>

<p>Specifically, I have this dataframe:</p>

<pre><code>import pandas as pd
wget https://www.dropbox.com/s/vmimze2g4lt4ud3/alt_exon_repeatmasker_intersect.bed
alt_exon_repeatmasker = pd.read_table('alt_exon_repeatmasker_intersect.bed', header=None, index_col=3)

In [74]: alt_exon_repeatmasker.index.is_unique
Out[74]: False
</code></pre>

<p>And some of the indexes have duplicate values in the 9th column (the type of DNA repetitive element in this location), and I want to know what are the different types of repetitive elements for individual locations (each index = a genome location).</p>

<p>I'm guessing this will require some kind of <code>groupby</code> and hopefully some <code>groupby</code> ninja can help me out.</p>

<p>To simplify even further, if we only have the index and the repeat type,</p>

<pre><code>genome_location1    MIR3
genome_location1    AluJb
genome_location2    Tigger1
genome_location3    AT_rich
</code></pre>

<p>So the output I'd like to see all duplicate indexes and their repeat types, as such:</p>

<pre><code>genome_location1    MIR3
genome_location1    AluJb
</code></pre>

<p>EDIT: added toy example</p>
";;1;;2013-11-25T17:15:14.817;;20199129;2017-02-15T20:19:07.723;2013-11-25T17:26:36.657;;1628971.0;;1628971.0;;1;15;<python><indexing><pandas>;Pandas: Get duplicated indexes;9123.0
8089;8089;;3.0;"<p>I have two DataFrames in pandas, trying to merge them. But pandas keeps changing the order. I've tried setting indexes, resetting them, no matter what I do, I can't get the returned output to have the rows in the same order. Is there a trick?
  Note we start out with the loans order 'a,b,c' but after the merge, it's ""a,c,b"".</p>

<pre><code>import pandas
loans = [  'a',  'b', 'c' ]
states = [  'OR',  'CA', 'OR' ]
x = pandas.DataFrame({ 'loan' : loans, 'state' : states })
y = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})
z = x.merge(y, how='left', on='state')
</code></pre>

<p>But now the order is no longer the original 'a,b,c'. Any ideas? I'm using pandas version 11.</p>
";;1;;2013-11-26T00:59:09.100;3.0;20206615;2016-04-29T20:49:10.470;2013-11-26T01:09:32.357;;2543623.0;;2543623.0;;1;17;<python><pandas>;How can a pandas merge preserve order?;6052.0
8106;8106;20221655.0;5.0;"<p>I use pandas to write to excel file in the following fashion:</p>

<pre><code>import pandas

writer = pandas.ExcelWriter('Masterfile.xlsx') 

data_filtered.to_excel(writer, ""Main"", cols=['Diff1', 'Diff2'])

writer.save()
</code></pre>

<p>Masterfile.xlsx already consists of number of different tabs.</p>

<p>Pandas correctly writes to ""Main"" sheet, unfortunately it also deletes all other tabs.</p>
";;5;;2013-11-26T14:05:22.957;20.0;20219254;2017-08-08T18:30:17.240;2016-08-02T21:28:05.270;;5741205.0;;2282367.0;;1;46;<python><excel><python-2.7><pandas>;How to write to an existing excel file without overwriting data (using pandas)?;29344.0
8113;8113;20228113.0;5.0;"<p>I have two dataframes.  Examples:</p>

<pre><code>df1:
Date       Fruit  Num  Color 
2013-11-24 Banana 22.1 Yellow
2013-11-24 Orange  8.6 Orange
2013-11-24 Apple   7.6 Green
2013-11-24 Celery 10.2 Green

df2:
Date       Fruit  Num  Color 
2013-11-24 Banana 22.1 Yellow
2013-11-24 Orange  8.6 Orange
2013-11-24 Apple   7.6 Green
2013-11-24 Celery 10.2 Green
2013-11-25 Apple  22.1 Red
2013-11-25 Orange  8.6 Orange
</code></pre>

<p>Each dataframe has the Date as an index. Both dataframes have the same structure. </p>

<p>What i want to do, is compare these two dataframes and find which rows are in df2 that aren't in df1. I want to compare the date (index) and the first column (Banana, APple, etc) to see if they exist in df2 vs df1.</p>

<p>I have tried the following:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/17095101/outputting-difference-in-two-pandas-dataframes-side-by-side-highlighting-the-d"">Outputting difference in two Pandas dataframes side by side - highlighting the difference</a></li>
<li><a href=""https://stackoverflow.com/questions/19917545/comparing-two-pandas-dataframes-for-differences"">Comparing two pandas dataframes for differences</a></li>
</ul>

<p>For the first approach I get this error: <strong><em>""Exception: Can only compare identically-labeled DataFrame objects""</em></strong>.  I have tried removing the Date as index but get the same error.</p>

<p>On the <a href=""https://stackoverflow.com/questions/19917545/comparing-two-pandas-dataframes-for-differences"">third approach</a>, I get the assert to return False but cannot figure out how to actually see the different rows.</p>

<p>Any pointers would be welcome</p>
";;3;;2013-11-26T18:31:15.550;17.0;20225110;2017-08-25T10:16:42.730;2017-05-23T12:26:09.273;;-1.0;;2887031.0;;1;18;<python><pandas><dataframe>;Comparing two dataframes and getting the differences;29288.0
8124;8124;20230859.0;4.0;"<p>Is there a way to select all but one column in a pandas DataFrame object? I've seen ways to delete a column, but I don't want to do that.</p>
";;0;;2013-11-26T23:42:37.230;3.0;20230326;2017-07-28T08:01:28.753;2017-05-04T21:58:52.957;;2285236.0;;1802143.0;;1;29;<python><pandas><dataframe>;Retrieve DataFrame of all but one specified column;17402.0
8129;8129;20233649.0;1.0;"<p>I have a pandas DataFrame from 6:36 AM to 5:31 PM.  I want to remove all observations where the time is less than 8:00:00 AM.  Here is my attempt:</p>

<pre><code>df = df[df.index &lt; '2013-10-16 08:00:00']
</code></pre>

<p>This does nothing, please help.</p>
";;4;;2013-11-27T02:59:30.487;1.0;20233071;2013-11-27T04:01:40.383;;;;;2113095.0;;1;21;<python><pandas>;Filter Pandas DataFrame by time index;15944.0
8135;8135;;1.0;"<p>Is there a way to remove a NaN values from a panda series? I have a series that may or may not have some NaN values in it, and I'd like to return a copy of the series with all the NaNs removed.</p>
";;0;;2013-11-27T06:26:28.037;3.0;20235401;2017-03-18T07:26:59.520;2013-11-27T06:31:42.040;;1744834.0;;1802143.0;;1;34;<python><pandas><series>;Remove NaN from pandas series;43902.0
8150;8150;20250996.0;3.0;"<p>I have a dictionary which looks like this: <code>di = {1: ""A"", 2: ""B""}</code></p>

<p>I would like to apply it to the ""col1"" column of a dataframe similar to:</p>

<pre><code>     col1   col2
0       w      a
1       1      2
2       2    NaN
</code></pre>

<p>to get:</p>

<pre><code>     col1   col2
0       w      a
1       A      2
2       B    NaN
</code></pre>

<p>How can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/ </p>
";;0;;2013-11-27T18:56:58.520;29.0;20250771;2017-07-11T14:17:56.317;2013-12-01T04:58:41.040;;1893275.0;;1893275.0;;1;69;<python><dictionary><pandas><remap>;Remap values in pandas column with a dict;48507.0
8195;8195;;3.0;"<p>I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?</p>
";;1;;2013-11-30T06:27:11.147;9.0;20297317;2016-02-16T10:48:10.050;2013-12-05T06:16:11.283;;1744834.0;;1802143.0;;1;37;<python><pandas><dataframe>;python dataframe pandas drop column using int;28907.0
8196;8196;20297639.0;3.0;"<p>How do you programmatically retrieve the number of columns in a pandas dataframe? I was hoping for something like:</p>

<pre><code>df.num_columns
</code></pre>
";;1;;2013-11-30T06:28:21.307;15.0;20297332;2015-12-20T01:03:42.173;2015-01-26T12:52:03.663;;1585017.0;;1802143.0;;1;52;<python><pandas><dataframe>;Python pandas dataframe: retrieve number of columns;66459.0
8231;8231;20333894.0;1.0;"<p>I have the following structure to my dataFrame:</p>

<pre><code>Index: 1008 entries, Trial1.0 to Trial3.84
Data columns (total 5 columns):
CHUNK_NAME                    1008  non-null values
LAMBDA                        1008  non-null values
BETA                          1008  non-null values
HIT_RATE                      1008  non-null values
AVERAGE_RECIPROCAL_HITRATE    1008  non-null values

chunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']
lam_beta=[(lambda1,beta1),(lambda1,beta2),(lambda1,beta3),...(lambda1,beta_n),(lambda2,beta1),(lambda2,beta2)...(lambda2,beta_n),........]

my_df.ix[my_df.CHUNK_NAME==chunks[0]&amp;my_df.LAMBDA==lam_beta[0][0]]
</code></pre>

<p>I want to get the rows of the Dataframe for a particular chunk lets say chunks[0] and particular lambda value. So in this case the output should be all rows in the dataframe having CHUNK_NAME='300_321' and LAMBDA=lambda1. There would be n rows one for each beta value that would be returned. But instead I get the follwoing error. Any help in solving this problem would be appreciated.</p>

<pre><code>TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]
</code></pre>
";;0;;2013-12-02T16:47:03.267;2.0;20333435;2017-07-22T20:56:42.650;2017-07-22T20:56:42.650;;2285236.0;;1009091.0;;1;11;<python><pandas><typeerror><dataframe>;pandas comparison raises TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool];7916.0
8250;8250;20341058.0;2.0;"<p>I have a dictionary object of the form:</p>

<pre><code>my_dict = {id1: val1, id2: val2, id3: val3, ...}
</code></pre>

<p>I want to create this into a dataframe where I want to name the 2 columns 'business_id' and 'business_code'.<br>
I tried:</p>

<pre><code>business_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])
</code></pre>

<p>but it says <code>from_dict</code> doesn't take in a columns argument.</p>

<pre><code>TypeError: from_dict() got an unexpected keyword argument 'columns' 
</code></pre>
";;0;;2013-12-03T00:56:01.160;1.0;20340844;2015-08-28T22:54:30.407;2013-12-03T01:21:30.967;;1240268.0;;1009091.0;;1;13;<python><pandas><typeerror><dataframe>;pandas create named columns in dataframe from dict;15536.0
8287;8287;20375692.0;2.0;"<p>I have two dataframes with the following column names:</p>

<pre><code>frame_1:
event_id, date, time, county_ID

frame_2:
countyid, state
</code></pre>

<p>I would like to get a dataframe with the following columns by joining (left) on <code>county_ID = countyid</code>:</p>

<pre><code>joined_dataframe
event_id, date, time, county, state
</code></pre>

<p>I cannot figure out how to do it if the columns on which I want to join are not the index. What's the easiest way? Thanks!</p>
";;0;;2013-12-04T12:35:48.447;4.0;20375561;2013-12-04T12:47:31.203;2013-12-04T12:38:23.187;;1646390.0;;2827060.0;;1;28;<python><pandas><dataframe>;Joining pandas dataframes by column names;16792.0
8294;8294;20384317.0;3.0;"<p>In Pandas, when I select a label that only has one entry in the index I get back a Series, but when I select an entry that has more then one entry I get back a data frame.</p>

<p>Why is that?  Is there a way to ensure I always get back a data frame?</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])

In [3]: type(df.loc[3])
Out[3]: pandas.core.frame.DataFrame

In [4]: type(df.loc[1])
Out[4]: pandas.core.series.Series
</code></pre>
";;0;;2013-12-04T19:01:40.187;7.0;20383647;2013-12-05T01:46:28.317;;;;;2752242.0;;1;26;<python><pandas>;Pandas selecting by label sometimes return series, sometimes returns dataframe;7456.0
8336;8336;20410720.0;6.0;"<p>Example</p>

<pre><code>s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])
print s 
1    5
2    4
3    3
4    2
5    1
</code></pre>

<p>Is there an efficient way to create a series. e.g. containing in each row the lagged values (in this example up to lag 2)</p>

<pre><code>3    [3, 4, 5]
4    [2, 3, 4]
5    [1, 2, 3]
</code></pre>

<p>This corresponds to <em>s=pd.Series([[3,4,5],[2,3,4],[1,2,3]], index=[3,4,5])</em></p>

<p>How can this be done in an efficient way for dataframes with a lot of timeseries which are very long?</p>

<p>Thanks</p>

<p><strong>Edited after seeing the answers</strong></p>

<p>ok, at the end I implemented this function:</p>

<pre><code>def buildLaggedFeatures(s,lag=2,dropna=True):
'''
Builds a new DataFrame to facilitate regressing over all possible lagged features
'''
if type(s) is pd.DataFrame:
    new_dict={}
    for col_name in s:
        new_dict[col_name]=s[col_name]
        # create lagged Series
        for l in range(1,lag+1):
            new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)
    res=pd.DataFrame(new_dict,index=s.index)

elif type(s) is pd.Series:
    the_range=range(lag+1)
    res=pd.concat([s.shift(i) for i in the_range],axis=1)
    res.columns=['lag_%d' %i for i in the_range]
else:
    print 'Only works for DataFrame or Series'
    return None
if dropna:
    return res.dropna()
else:
    return res 
</code></pre>

<p>it produces the wished outputs and manages the naming of columns in the resulting DataFrame.</p>

<p>For a Series as input:</p>

<pre><code>s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])
res=buildLaggedFeatures(s,lag=2,dropna=False)
   lag_0  lag_1  lag_2
1      5    NaN    NaN
2      4      5    NaN
3      3      4      5
4      2      3      4
5      1      2      3
</code></pre>

<p>and for a DataFrame as input:</p>

<pre><code>s2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])
res2=buildLaggedFeatures(s2,lag=2,dropna=True)

   a  a_lag1  a_lag2   b  b_lag1  b_lag2
3  3       4       5  30      40      50
4  2       3       4  20      30      40
5  1       2       3  10      20      30
</code></pre>
";;2;;2013-12-05T20:44:59.533;6.0;20410312;2017-03-26T20:40:08.680;2013-12-06T14:01:18.840;;1232506.0;;1232506.0;;1;18;<pandas>;How to create a lagged data structure using pandas dataframe;12939.0
8401;8401;20444256.0;1.0;"<p>Here is my code:</p>

<pre><code>import pandas as pd

data = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})

for i in reversed(data):
    print(data['Odd'], data['Even'])
</code></pre>

<p>When I run this code, i get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python33\lib\site-packages\pandas\core\generic.py"", line 665, in _get_item_cache
    return cache[item]
KeyError: 5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\*****\Documents\******\********\****.py"", line 5, in &lt;module&gt;
    for i in reversed(data):
  File ""C:\Python33\lib\site-packages\pandas\core\frame.py"", line 2003, in __getitem__
    return self._get_item_cache(key)
  File ""C:\Python33\lib\site-packages\pandas\core\generic.py"", line 667, in _get_item_cache
    values = self._data.get(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1656, in get
    _, block = self._find_block(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1936, in _find_block
    self._check_have(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1943, in _check_have
    raise KeyError('no item named %s' % com.pprint_thing(item))
KeyError: 'no item named 5'
</code></pre>

<p><strong>Why am I getting this error?<br>
How can I fix that?<br>
What is the right way to reverse <code>pandas.DataFrame</code>?</strong></p>
";;5;;2013-12-07T17:07:58.993;8.0;20444087;2014-10-21T23:06:55.173;2013-12-07T17:17:56.397;;2598876.0;;2598876.0;;1;28;<python><pandas><reverse>;Right way to reverse pandas.DataFrame?;31880.0
8403;8403;20455090.0;1.0;"<p>I've just compiled and installed pandas from source (cloned github repo, <code>&gt;&gt;&gt; setup.py install</code>).</p>

<p>It happened that the default behavior of module <code>pickle</code> for object serialization/deserialization changed being likely partially overridden by pandas internal modules.</p>

<p>I have quite some data classes serialized via ""standard"" <code>pickle</code> which apparently I cannot deserialize anymore; in particular, when I try to deserialize a class file (surely working), I get this error</p>

<pre><code>In [1]: import pickle

In [2]: pickle.load(open('pickle_L1cor_s1.pic','rb'))
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-2-88719f8f9506&gt; in &lt;module&gt;()
----&gt; 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))

/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)
   1376
   1377 def load(file):
-&gt; 1378     return Unpickler(file).load()
   1379
   1380 def loads(str):

/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)
    856             while 1:
    857                 key = read(1)
--&gt; 858                 dispatch[key](self)
    859         except _Stop, stopinst:
    860             return stopinst.value

/home/acorbe/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas-0.12.0_1090_g46008ec-py2.7-linux-x86_64.egg/pandas/compat/pickle_compat.pyc in load_reduce(self)
     28
     29         # try to reencode the arguments
---&gt; 30         if self.encoding is not None:
     31             args = tuple([ arg.encode(self.encoding) if isinstance(arg, string_types)     else arg for arg in args ])
     32             try:

AttributeError: Unpickler instance has no attribute 'encoding'
</code></pre>

<p>I have quite a large code relying on this which broke down. Is there any quick workaround? How can I obtain again default pickle behavior?</p>

<p>any help appreciated</p>

<hr>

<p><strong>EDIT:</strong></p>

<p>I realized that what I am willing to unpickle is a list of dicts which include a couple of <code>DataFrames</code> each. That's where pandas comes into play.</p>

<p>I applied the patch by @Jeff github.com/pydata/pandas/pull/5661.
Another error (maybe related to <a href=""https://github.com/pydata/pandas/issues/4744"">this</a>) shows up.</p>

<pre><code>In [4]: pickle.load(open('pickle_L1cor_s1.pic','rb'))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-88719f8f9506&gt; in &lt;module&gt;()
----&gt; 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))

/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)
   1376
   1377 def load(file):
-&gt; 1378     return Unpickler(file).load()
   1379
   1380 def loads(str):

/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)
    856             while 1:
    857                 key = read(1)
--&gt; 858                 dispatch[key](self)
    859         except _Stop, stopinst:
    860             return stopinst.value

/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in             load_reduce(self)
   1131         args = stack.pop()
   1132         func = stack[-1]
-&gt; 1133         value = func(*args)
   1134         stack[-1] = value
   1135     dispatch[REDUCE] = load_reduce

TypeError: _reconstruct: First argument must be a sub-type of ndarray
</code></pre>

<p>Pandas version of encoded data is (from Canopy package manager)</p>

<pre><code>Size: 7.32 MB
Version: 0.12.0
Build: 2
Dependencies:
 numpy 1.7.1
 python_dateutil
 pytz 2011n

  md5: 7dd4385bed058e6ac15b0841b312ae35
</code></pre>

<p>I am not sure I can provide minimal example of the files I am trying to unpickle. 
They are quite large (O(100MB)) and they have some non trivial dependencies. </p>
";;9;;2013-12-07T17:55:05.670;3.0;20444593;2013-12-08T15:25:44.920;2013-12-08T13:03:00.817;;1714661.0;;1714661.0;;1;12;<python><pandas><pickle>;Pandas compiled from source: default pickle behavior changed;2728.0
8416;8416;20459839.0;1.0;"<p>I am creating a matrix from a Pandas dataframe as follows:</p>

<pre><code>dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)
</code></pre>

<p>And then into a sparse matrix with:</p>

<pre><code>sparse_matrix = scipy.sparse.csr_matrix(dense_matrix)
</code></pre>

<p>Is there any way to go from a df straight to a sparse matrix?</p>

<p>Thanks in advance.</p>
";;0;;2013-12-08T21:45:49.780;7.0;20459536;2016-04-22T03:04:20.667;2015-07-17T14:43:48.623;;2039736.0;;213216.0;;1;14;<python><numpy><pandas><scipy>;Convert Pandas dataframe to Sparse Numpy Matrix directly;11512.0
8421;8421;20461206.0;2.0;"<p>This seems rather obvious, but I can't seem to figure out how do I convert an index of data frame to a column?</p>

<p>For example:</p>

<pre><code>df=
           gi  ptt_loc
 0  384444683      593  
 1  384444684      594 
 2  384444686      596  
</code></pre>

<p>To,</p>

<pre><code>df=
    index1       gi    ptt_loc
 0  0     384444683      593  
 1  1     384444684      594 
 2  2     384444686      596  
</code></pre>
";;0;;2013-12-09T00:34:16.403;43.0;20461165;2016-03-06T12:21:48.660;2015-11-07T10:19:36.177;;4370109.0;;1090629.0;;1;132;<python><pandas>;How to convert pandas index in a dataframe to a column?;106030.0
8453;8453;20481080.0;4.0;"<p>I need to add 1 day to each date I want to get the begining date of the following month eg 2014-01-2014 for the 1st item in the dataframe.
Tried: </p>

<pre><code>montdist['date'] + pd.DateOffset(1)
</code></pre>

<p>Which gives me: </p>

<pre><code>TypeError: cannot use a non-absolute DateOffset in datetime/timedelta operations [&lt;DateOffset&gt;]
</code></pre>

<p>Have a Dataframe:</p>

<pre><code>    Units   mondist                date
1    6491  0.057785 2013-12-31 00:00:00
2    7377  0.065672 2014-01-31 00:00:00
3    9990  0.088934 2014-02-28 00:00:00
4   10362  0.092245 2014-03-31 00:00:00
5   11271  0.100337 2014-04-30 00:00:00
6   11637  0.103596 2014-05-31 00:00:00
7   10199  0.090794 2014-06-30 00:00:00
8   10486  0.093349 2014-07-31 00:00:00
9    9282  0.082631 2014-08-31 00:00:00
10   8632  0.076844 2014-09-30 00:00:00
11   8204  0.073034 2013-10-31 00:00:00
12   8400  0.074779 2013-11-30 00:00:00
</code></pre>
";;0;;2013-12-09T21:12:20.250;3.0;20480897;2017-05-15T21:37:25.303;;;;;137783.0;;1;13;<python><pandas>;Pandas add one day to column;11860.0
8463;8463;20491748.0;2.0;"<p>I have a data frame from which I remove some rows. As a result, I get a data frame in which index is something like that: <code>[1,5,6,10,11]</code> and I would like to reset it to <code>[0,1,2,3,4]</code>. How can I do it?</p>

<p><strong>ADDED</strong></p>

<p>The following seems to work:</p>

<pre><code>df = df.reset_index()
del df['index']
</code></pre>

<p>The following does not work:</p>

<pre><code>df = df.reindex()
</code></pre>
";;0;;2013-12-10T09:12:55.510;33.0;20490274;2017-08-15T11:40:58.493;2013-12-10T09:26:19.633;;245549.0;;245549.0;;1;113;<python><indexing><pandas><dataframe>;How to reset index in a pandas data frame?;101981.0
8519;8519;20523271.0;1.0;"<p>I need to create MatplotLib heatmap (pcolormesh) using Pandas DataFrame TimeSeries column (df_all.ts) as my X-axis.</p>

<p>How to convert Pandas TimeSeries column to something which can be used as X-axis in  np.meshgrid(x, y) function to create heatmap? The workaround is to create Matplotlib drange using same parameters as in pandas column, but is there a simple way?</p>

<pre><code>x = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')
xt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))
y = arange(ylen)
X,Y = np.meshgrid(xt, y)
</code></pre>
";;3;;2013-12-11T13:19:21.250;6.0;20520246;2015-11-26T14:17:44.933;;;;;2134595.0;;1;11;<python><datetime><numpy><matplotlib><pandas>;Create heatmap using pandas TimeSeries;5264.0
8576;8576;20557179.0;1.0;"<p>When performing <code>pip install pandas</code> on a Digital Ocean 512MB droplet, I get the error <code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 41: ordinal not in range(128)
</code>.</p>

<p>Any ideas what may have caused it? I'm running Ubuntu 12.04 64bit.</p>

<p><a href=""https://gist.github.com/anonymous/7936900"">[Full Error]</a></p>
";;5;;2013-12-12T22:40:00.193;3.0;20555761;2013-12-13T00:44:49.447;;;;;3023615.0;;1;17;<python><ubuntu><pip><python-2.x><digital-ocean>;`pip install pandas` gives UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 41: ordinal not in range(128);8619.0
8603;8603;20574460.0;3.0;"<p>I know you how to do this in <a href=""https://stackoverflow.com/questions/10622730/constructing-a-co-occurrence-matrix-from-dummycoded-observations-in-r"">R</a>. But, is there any functions in pandas that transforms a dataframe to an nxn co-occurrence matrix containing the counts of two aspects co-occurring. </p>

<p>For example an matrix df:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],
                    'Snack' : ['1', '0', '1', '1', '0', '0'],
                    'Trans' : ['1', '1', '1', '0', '0', '1'],
                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')

print df

&gt;&gt;&gt; 
    Dop Snack Trans
TFD                
AA    1     1     1
SL    0     0     1
BB    1     1     1
D0    0     1     0
Dk    1     0     0
FF    1     0     1

[6 rows x 3 columns]
</code></pre>

<p>would yield:</p>

<pre><code>    Dop Snack Trans

Dop   0     2     3
Snack 2     0     2
Trans 3     2     0
</code></pre>

<p>Since the matrix is mirrored on the diagonal I guess there would be a way to optimize code.</p>
";;0;;2013-12-13T19:15:32.550;7.0;20574257;2016-11-10T16:01:05.657;2017-05-23T12:02:19.143;;-1.0;;3084006.0;;1;24;<python><pandas><statistics>;Constructing a co-occurrence matrix in python pandas;8487.0
8638;8638;20603020.0;3.0;"<p>This is probably easy, but I have the following data:</p>

<p>In data frame 1:</p>

<pre><code>index dat1
0     9
1     5
</code></pre>

<p>In data frame 2:</p>

<pre><code>index dat2
0     7
1     6
</code></pre>

<p>I want a data frame with the following form:</p>

<pre><code>index dat1  dat2
0     9     7
1     5     6
</code></pre>

<p>I've tried using the <code>append</code> method, but I get a cross join (i.e. cartesian product).</p>

<p>What's the right way to do this?</p>
";;4;;2013-12-16T03:23:41.003;3.0;20602947;2016-09-07T10:00:07.277;;;;;963250.0;;1;11;<python><pandas>;Append column to pandas dataframe;34026.0
8652;8652;20612691.0;3.0;"<p>I am having trouble with some of pandas functionalities. How do I check what is my installation version?</p>
";;0;;2013-12-16T13:55:12.397;15.0;20612645;2017-08-06T11:36:08.143;2017-01-06T02:34:34.367;;4627108.0;;1945306.0;;1;103;<python><pandas>;How to find the installed pandas version;58878.0
8676;8676;;2.0;"<p>I am working with an Oracle database with millions of rows and 100+ columns. I am attempting to store this data in an HDF5 file using pytables with certain columns indexed. I will be reading subsets of these data in a pandas DataFrame and performing computations.</p>

<p>I have attempted the following:</p>

<p>Download the the table, using a utility into a csv file, read the csv file chunk by chunk using pandas and append to HDF5 table using <code>pandas.HDFStore</code>. I created a dtype definition and provided the maximum string sizes.</p>

<p>However, now when I am trying to download data directly from Oracle DB and post it to HDF5 file via <code>pandas.HDFStore</code>, I run into some problems.</p>

<p>pandas.io.sql.read_frame does not support chunked reading. I don't have enough RAM to be able to download the entire data to memory first.</p>

<p>If I try to use <code>cursor.fecthmany()</code> with a fixed number of records, the read operation takes ages at the DB table is not indexed and I have to read records falling under a date range. I am using <code>DataFrame(cursor.fetchmany(), columns = ['a','b','c'], dtype=my_dtype)</code> 
however, the created DataFrame always infers the dtype rather than enforce the dtype I have provided (unlike read_csv which adheres to the dtype I provide). Hence, when I append this DataFrame to an already existing <code>HDFDatastore</code>, there is a type mismatch for e.g. a float64 will maybe interpreted as int64 in one chunk.</p>

<p>Appreciate if you guys could offer your thoughts and point me in the right direction.</p>
";;4;;2013-12-16T18:50:38.423;3.0;20618523;2015-03-24T05:29:24.827;2014-10-16T07:34:42.123;;504547.0;;1872506.0;;1;11;<python><pandas><hdf5><pytables>;Reading a large table with millions of rows from Oracle and writing to HDF5;2339.0
8679;8679;32750237.0;3.0;"<p>I'm looking for a way to replicate the <a href=""http://www.stata.com/help.cgi?encode"" rel=""noreferrer"">encode</a> behaviour in Stata, which will convert a categorical string column into a number column.</p>

<pre><code>x = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})
x = x.set_index('cat')
</code></pre>

<p>Which results in:</p>

<pre><code>     val
cat     
A     10
A     20
B     30
</code></pre>

<p>I'd like to convert the cat column from strings to integers, mapping each unique string to an (arbitrary) integer 1-to-1. It would result in:</p>

<pre><code>     val
cat     
1     10
1     20
2     30
</code></pre>

<p>Or, just as good:</p>

<pre><code>  cat  val
0   1   10
1   1   20
2   2   30
</code></pre>

<p>Any suggestions?</p>

<p>Many thanks as always,
Rob</p>
";;7;;2013-12-16T20:03:40.837;;20619851;2015-09-23T22:33:12.653;2013-12-17T16:05:49.783;;2071807.0;;2071807.0;;1;14;<python><pandas><stata>;pandas equivalent of Stata's encode;869.0
8684;8684;20627316.0;5.0;"<h2>Background</h2>

<p>I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:</p>

<pre><code>E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
</code></pre>

<p>I want to know what exactly it means?  Do I need to change something?</p>

<p>How should I suspend the warning if I insist to use <code>quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE</code>?</p>

<h2>The function that gives errors</h2>

<pre><code>def _decode_stock_quote(list_of_150_stk_str):
    """"""decode the webpage and return dataframe""""""

    from cStringIO import StringIO

    str_of_all = """".join(list_of_150_stk_str)

    quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}
    quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)
    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
    quote_df['TClose'] = quote_df['TPrice']
    quote_df['RT']     = 100 * (quote_df['TPrice']/quote_df['TPCLOSE'] - 1)
    quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
    quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
    quote_df['STK_ID'] = quote_df['STK'].str.slice(13,19)
    quote_df['STK_Name'] = quote_df['STK'].str.slice(21,30)#.decode('gb2312')
    quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])

    return quote_df
</code></pre>

<h2>More error messages</h2>

<pre><code>E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
E:\FinReporter\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
E:\FinReporter\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])
</code></pre>
";;1;;2013-12-17T03:48:02.570;52.0;20625582;2017-07-27T22:25:13.497;2016-10-24T09:06:22.687;;3730397.0;;1072888.0;;1;170;<python><parsing><pandas><dataframe>;How to deal with SettingWithCopyWarning in Pandas?;145172.0
8685;8685;21210728.0;1.0;"<p>I have a DataFrame with a column of timedeltas (actually upon inspection the dtype is <code>timedelta64[ns]</code> or <code>&lt;m8[ns]</code>), and I'd like to do a split-combine-apply, but the timedelta column is being dropped:</p>

<pre><code>import pandas as pd

import numpy as np

pd.__version__
Out[3]: '0.13.0rc1'

np.__version__
Out[4]: '1.8.0'

data = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])

data['td'] *= 10000000

data['td'] = pd.Series(data['td'], dtype='&lt;m8[ns]')

data
Out[8]: 
         f1        f2              td
0  0.990140  0.948313 00:00:00.003066
1  0.277125  0.993549 00:00:00.001443
2  0.016427  0.581129 00:00:00.009257
3  0.048662  0.512215 00:00:00.000702
4  0.846301  0.179160 00:00:00.000396
5  0.568323  0.419887 00:00:00.000266
6  0.328182  0.919897 00:00:00.006138
7  0.292882  0.213219 00:00:00.008876
8  0.623332  0.003409 00:00:00.000322
9  0.650436  0.844180 00:00:00.006873

[10 rows x 3 columns]

data.groupby(data.index &lt; 5).mean()
Out[9]: 
             f1        f2
False  0.492631  0.480118
True   0.435731  0.642873

[2 rows x 2 columns]
</code></pre>

<p>Or, forcing pandas to try the operation on the <code>'td'</code> column:</p>

<pre><code>data.groupby(data.index &lt; 5)['td'].mean()
---------------------------------------------------------------------------
DataError                                 Traceback (most recent call last)
&lt;ipython-input-12-88cc94e534b7&gt; in &lt;module&gt;()
----&gt; 1 data.groupby(data.index &lt; 5)['td'].mean()

/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in mean(self)
    417         """"""
    418         try:
--&gt; 419             return self._cython_agg_general('mean')
    420         except GroupByError:
    421             raise

/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)
    669 
    670         if len(output) == 0:
--&gt; 671             raise DataError('No numeric types to aggregate')
    672 
    673         return self._wrap_aggregated_output(output, names)

DataError: No numeric types to aggregate
</code></pre>

<p>However, taking the mean of the column works fine, so numeric operations should be possible:</p>

<pre><code>data['td'].mean()
Out[11]: 
0   00:00:00.003734
dtype: timedelta64[ns]
</code></pre>

<p>Obviously it's easy enough to coerce to float before doing the groupby, but I figured I might as well try to understand what I'm running into.</p>

<p>Edit: See <a href=""https://github.com/pydata/pandas/issues/5724"" rel=""nofollow"">https://github.com/pydata/pandas/issues/5724</a></p>
";;7;;2013-12-17T04:34:08.153;;20625982;2014-01-18T22:31:17.253;2014-01-17T18:25:38.147;;2909116.0;;2909116.0;;1;12;<python><pandas>;split-apply-combine on pandas timedelta column;1173.0
8687;8687;;3.0;"<p>I converted a pandas df to r using the the below:</p>

<pre><code>import pandas as pd
import pandas.rpy.common as com
import rpy2.robjects as ro
from rpy2.robjects.packages import importr
rdf = com.convert_to_r_dataframe(df)
</code></pre>

<p>How do I convert rdf back to a pandas df?</p>

<pre><code>df = f(rdf) ?
</code></pre>
";;1;;2013-12-17T09:14:22.760;4.0;20630121;2016-06-22T18:08:24.690;2013-12-17T22:44:04.790;;1203556.0;;1203556.0;;1;17;<python><r><pandas>;Pandas - how to convert r dataframe back to pandas?;4507.0
8701;8701;20637559.0;3.0;"<p>I'm trying to import a .csv file using <code>pandas.read_csv()</code>, however I don't want to import the 2nd row of the data file (the row with index = 1 for 0-indexing).</p>

<p>I can't see how not to import it because the arguments used with the command seem ambiguous:</p>

<p>From the pandas website: </p>

<p>""skiprows : list-like or integer</p>

<p>Row numbers to skip (0-indexed) or number of rows to skip (int) at the start of the file.""</p>

<p>If I put <code>skiprows=1</code> in the arguments, how does it know whether to skip the first row or skip the row with index 1?</p>

<p>Cheers.</p>
";;2;;2013-12-17T14:59:34.690;7.0;20637439;2016-06-22T16:01:39.930;;;;;3087409.0;;1;29;<python><csv><pandas>;Skip rows during csv import pandas;35315.0
8705;8705;20638258.0;3.0;"<p>I have a list of dictionaries like this:</p>

<pre><code>[{'points': 50, 'time': '5:00', 'year': 2010}, 
{'points': 25, 'time': '6:00', 'month': ""february""}, 
{'points':90, 'time': '9:00', 'month': 'january'}, 
{'points_h1':20, 'month': 'june'}]
</code></pre>

<p>and I want to turn this into a pandas <code>DataFrame</code> like this:</p>

<pre><code>      month  points  points_h1  time  year
0       NaN      50        NaN  5:00  2010
1  february      25        NaN  6:00   NaN
2   january      90        NaN  9:00   NaN
3      june     NaN         20   NaN   NaN
</code></pre>

<p><em>Note: Order of the columns does not matter.</em></p>

<p>Ultimately, the goal is to write this to a text file and this seems like the best solution I could find.  How can I turn the list of dictionaries into a panda DataFrame as shown above?</p>
";;0;;2013-12-17T15:24:51.033;29.0;20638006;2017-07-07T06:03:08.810;2015-12-20T07:18:17.143;;1240268.0;;1560517.0;;1;221;<python><dictionary><pandas><dataframe>;Convert list of dictionaries to Dataframe;57095.0
8724;8724;20644575.0;2.0;"<p>Very stupid question,but how can I square each element of a column/series of a dataFrame in pandas (and create another column)?</p>
";;0;;2013-12-17T20:59:05.103;0.0;20644536;2017-01-26T05:16:14.310;;;;;2634197.0;;1;13;<python><pandas>;Square of each element of a column in pandas;8956.0
8738;8738;20648462.0;4.0;"<p>Say I have a dataframe with 3 columns: Date, Ticker, Value (no index, at least to start with).  I have many dates and many tickers, but each <code>(ticker, date)</code> tuple is unique.  (But obviously the same date will show up in many rows since it will be there for multiple tickers, and the same ticker will show up in multiple rows since it will be there for many dates.)</p>

<p>Initially, my rows in a specific order, but not sorted by any of the columns.</p>

<p>I would like to compute first differences (daily changes) of each ticker (ordered by date) and put these in a new column in my dataframe.  Given this context, I <strong>cannot</strong> simply do</p>

<pre><code>df['diffs'] = df['value'].diff()
</code></pre>

<p>because adjacent rows do not come from the same ticker.  Sorting like this:</p>

<pre><code>df = df.sort(['ticker', 'date'])
df['diffs'] = df['value'].diff()
</code></pre>

<p><strong>doesn't</strong> solve the problem because there will be ""borders"".  I.e. after that sort, the last value for one ticker will be above the first value for the next ticker.  And computing differences then would take a difference between two tickers.  I don't want this.  I want the earliest date for each ticker to wind up with an <code>NaN</code> in its diff column.</p>

<p>This seems like an obvious time to use <code>groupby</code> but for whatever reason, I can't seem to get it to work properly.  To be clear, I would like to perform the following process:</p>

<ol>
<li>Group rows based on their <code>ticker</code></li>
<li>Within each group, sort rows by their <code>date</code></li>
<li>Within each sorted group, compute differences of the <code>value</code> column</li>
<li>Put these differences into the original dataframe in a new <code>diffs</code> column (ideally leaving the original dataframe order in tact.)</li>
</ol>

<p>I have to imagine this is a one-liner.  But what am I missing?</p>

<hr>

<p>Edit at 9:00pm 2013-12-17</p>

<p>Ok...some progress.  I can do the following to get a new dataframe:</p>

<pre><code>result = df.set_index(['ticker', 'date'])\
    .groupby(level='ticker')\
    .transform(lambda x: x.sort_index().diff())\
    .reset_index()
</code></pre>

<p>But if I understand the mechanics of groupby, my rows will now be sorted first by <code>ticker</code> and then by <code>date</code>.  Is that correct?  If so, would I need to do a merge to append the differences column (currently in <code>result['current']</code> to the original dataframe <code>df</code>?</p>
";;0;;2013-12-18T01:57:14.940;7.0;20648346;2015-02-10T00:46:00.320;2015-02-10T00:46:00.320;;283296.0;;2501018.0;;1;17;<python><pandas>;Computing diffs within groups of a dataframe;9122.0
8753;8753;20657592.0;1.0;"<p>I have a problem making histograms from pandas series objects and I can't understand why it does not work. The code has worked fine before but now it does not.</p>

<p>Here is a bit of my code (specifically, a pandas series object I'm trying to make a histogram of):</p>

<pre><code>type(dfj2_MARKET1['VSPD2_perc'])
</code></pre>

<p>which outputs the result: 
    <code>pandas.core.series.Series</code></p>

<p>Here's my plotting code:</p>

<pre><code>fig, axes = plt.subplots(1, 7, figsize=(30,4))
axes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')
axes[0].grid(True)
axes[0].set_title(MARKET1 + '  5-40 km / h')
</code></pre>

<p>Error message:</p>

<pre><code>    AttributeError                            Traceback (most recent call last)
    &lt;ipython-input-75-3810c361db30&gt; in &lt;module&gt;()
      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))
      2 
    ----&gt; 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')
      4 axes[1].grid(True)
      5 axes[1].set_xlabel('Time spent [%]')

    C:\Python27\lib\site-packages\matplotlib\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)
   8322             # this will automatically overwrite bins,
   8323             # so that each histogram uses the same bins
-&gt; 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)
   8325             m = m.astype(float) # causes problems later if it's an int
   8326             if mlast is None:

    C:\Python27\lib\site-packages\numpy\lib\function_base.pyc in histogram(a, bins, range,     normed, weights, density)
    158         if (mn &gt; mx):
    159             raise AttributeError(
--&gt; 160                 'max must be larger than min in range parameter.')
    161 
    162     if not iterable(bins):

AttributeError: max must be larger than min in range parameter.
</code></pre>
";;3;;2013-12-18T11:17:05.200;2.0;20656663;2017-04-19T11:03:36.647;2013-12-19T19:07:16.497;;2942522.0;;2219369.0;;1;36;<python><matplotlib><pandas><histogram>;Matplotlib/Pandas error using histogram;24955.0
8765;8765;20670901.0;2.0;"<p>This is a string I'm getting out of <code>pandas.DataFrame.to_json()</code>, putting it into redis, getting it out of redis elsewhere, and trying to read it via <code>pandas.read_json()</code>:</p>

<pre><code>DFJ {""args"":{""0"":""[]"",""1"":""[]"",""2"":""[]"",""3"":""[]"",""4"":""[]"",""5"":""[]"",""6"":""[]"",""7"":""[]""},""date"":{""0"":1385944439000000000,""1"":1385944439000000000,""2"":1385944440000000000,""3"":1385944440000000000,""4"":1385944440000000000,""5"":1385944440000000000,""6"":1385944440000000000,""7"":1385944440000000000},""host"":{""0"":""yy38.segm1.org"",""1"":""kyy1.segm1.org"",""2"":""yy10.segm1.org"",""3"":""yy24.segm1.org"",""4"":""yy24.segm1.org"",""5"":""yy34.segm1.org"",""6"":""yy15.segm1.org"",""7"":""yy15.segm1.org""},""kwargs"":{""0"":""{}"",""1"":""{}"",""2"":""{}"",""3"":""{}"",""4"":""{}"",""5"":""{}"",""6"":""{}"",""7"":""{}""},""operation"":{""0"":""x_gbinf"",""1"":""x_initobj"",""2"":""x_gobjParams"",""3"":""gtfull"",""4"":""x_gbinf"",""5"":""gxyzinf"",""6"":""deletemfg"",""7"":""gxyzinf""},""thingy"":{""0"":""a13yy38"",""1"":""a19kyy1"",""2"":""a14yy10"",""3"":""a14yy24"",""4"":""a14yy24"",""5"":""a12yy34"",""6"":""a15yy15"",""7"":""a15yy15""},""status"":{""0"":-101,""1"":1,""2"":-101,""3"":-101,""4"":-101,""5"":-101,""6"":1,""7"":-101},""time"":{""0"":0.000801,""1"":0.003244,""2"":0.002247,""3"":0.002787,""4"":0.001067,""5"":0.002652,""6"":0.004371,""7"":0.000602}}
</code></pre>

<p>It seems like it does not have any unicode in it. Yet on trying to <code>.read_json()</code> it I get:</p>

<pre><code>Traceback (most recent call last):
  File ""./sqlprofile.py"", line 160, in &lt;module&gt;
    maybe_save_dataframes(rconn, configd, results)
  File ""./sqlprofile.py"", line 140, in maybe_save_dataframes
    h5store.append(out_queue, df)
  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 658, in append
    self._write_to_group(key, value, table=True, append=True, **kwargs)
  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 923, in _write_to_group
    s.write(obj = value, append=append, complib=complib, **kwargs)
  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2985, in write
    **kwargs)
  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2717, in create_axes
    raise e
TypeError: [unicode] is not implemented as a table column
&gt; /home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py(2717)create_axes()
-&gt; raise e
(Pdb) locals()
</code></pre>

<p>This is what I'm getting in <code>locals()</code> - it seems that <code>append_axis</code> (column names?) values are unicode. Why?</p>

<pre><code>{'append_axis': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'existing_table': None, 'blocks': [FloatBlock: [time], 1 x 8, dtype float64, ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, IntBlock: [status], 1 x 8, dtype int64, DatetimeBlock: [date], 1 x 8, dtype datetime64[ns]], 'axis': 1, 'self': frame_table  (typ-&gt;appendable,nrows-&gt;None,ncols-&gt;1,indexers-&gt;[index]), 'axes': [0], 'kwargs': {}, 'klass': &lt;class 'pandas.io.pytables.DataCol'&gt;, 'block_obj':   args                date            host kwargs              operation      thingy  status      time
0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801
1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244
2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247
3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787
4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067
5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652
6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371
7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'axis_labels': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'nan_rep': 'nan', 'data_columns': [], 'obj':   args                date            host kwargs              operation      thingy  status      time
0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801
1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244
2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247
3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787
4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067
5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652
6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371
7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'validate': True, 'a': (1, [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time']), 'index_axes_map': {0: name-&gt;index,cname-&gt;index,axis-&gt;0,pos-&gt;0,kind-&gt;integer}, 'b': ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, 'e': TypeError('[unicode] is not implemented as a table column',), 'name': None, 'existing_col': None, 'j': 2, 'i': 1, 'min_itemsize': None, 'col': name-&gt;values_block_1,cname-&gt;values_block_1,dtype-&gt;None,shape-&gt;None}
</code></pre>

<p>How can I fix that? Is this a bug in Pandas / pytables?</p>

<p>Environment:</p>

<p>Python 2.7</p>

<p>pandas==0.12.0</p>

<p>tables==3.0.0</p>
";;1;;2013-12-18T23:07:00.240;9.0;20670370;2016-08-01T19:17:02.493;;;;;2022518.0;;1;11;<python><unicode><pandas>;Pandas and unicode;18168.0
8767;8767;20671047.0;2.0;"<p>I've got a dataframe, and I'm trying to append a column of sequential differences to it.  I have found a method that I like a lot (and generalizes well for my use case).  But I noticed one weird thing along the way.  Can you help me make sense of it?</p>

<p>Here is some data that has the right structure (code modeled on an answer <a href=""https://stackoverflow.com/a/20649094/2501018"">here</a>):</p>

<pre><code>import pandas as pd
import numpy as np
import random
from itertools import product

random.seed(1)       # so you can play along at home
np.random.seed(2)    # ditto

# make a list of dates for a few periods
dates = pd.date_range(start='2013-10-01', periods=4).to_native_types()
# make a list of tickers
tickers = ['ticker_%d' % i for i in range(3)]
# make a list of all the possible (date, ticker) tuples
pairs = list(product(dates, tickers))
# put them in a random order
random.shuffle(pairs)
# exclude a few possible pairs
pairs = pairs[:-3]
# make some data for all of our selected (date, ticker) tuples
values = np.random.rand(len(pairs))

mydates, mytickers = zip(*pairs)
data = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})
</code></pre>

<p>Ok, great.  This gives me a frame like so:</p>

<pre><code>     date        ticker      value
0    2013-10-03  ticker_2    0.435995
1    2013-10-04  ticker_2    0.025926
2    2013-10-02  ticker_1    0.549662
3    2013-10-01  ticker_0    0.435322
4    2013-10-02  ticker_2    0.420368
5    2013-10-03  ticker_0    0.330335
6    2013-10-04  ticker_1    0.204649
7    2013-10-02  ticker_0    0.619271
8    2013-10-01  ticker_2    0.299655
</code></pre>

<p>My goal is to add a new column to this dataframe that will contain sequential changes.  The data needs to be in order to do this, but the ordering and the differencing needs to be done ""ticker-wise"" so that gaps in another ticker don't cause NA's for a given ticker. I want to do this without perturbing the dataframe in any other way (i.e. I do not want the resulting DataFrame to be reordered based on what was necessary to do the differencing).  The following code works:</p>

<pre><code>data1 = data.copy() #let's leave the original data alone for later experiments
data1.sort(['ticker', 'date'], inplace=True)
data1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())
data1.sort_index(inplace=True)
data1
</code></pre>

<p>and returns:</p>

<pre><code>     date        ticker      value       diffs
0    2013-10-03  ticker_2    0.435995    0.015627
1    2013-10-04  ticker_2    0.025926   -0.410069
2    2013-10-02  ticker_1    0.549662    NaN
3    2013-10-01  ticker_0    0.435322    NaN
4    2013-10-02  ticker_2    0.420368    0.120713
5    2013-10-03  ticker_0    0.330335   -0.288936
6    2013-10-04  ticker_1    0.204649   -0.345014
7    2013-10-02  ticker_0    0.619271    0.183949
8    2013-10-01  ticker_2    0.299655    NaN
</code></pre>

<p>So far, so good.  If I replace the middle line above with the more concise code shown here, everything still works:</p>

<pre><code>data2 = data.copy()
data2.sort(['ticker', 'date'], inplace=True)
data2['diffs'] = data2.groupby('ticker')['value'].diff()
data2.sort_index(inplace=True)
data2
</code></pre>

<p>A quick check shows that, in fact, <code>data1</code> is equal to <code>data2</code>.  However, if I do this:</p>

<pre><code>data3 = data.copy()
data3.sort(['ticker', 'date'], inplace=True)
data3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)
data3.sort_index(inplace=True)
data3
</code></pre>

<p>I get a strange result:</p>

<pre><code>     date        ticker     value       diffs
0    2013-10-03  ticker_2    0.435995    0
1    2013-10-04  ticker_2    0.025926   NaN
2    2013-10-02  ticker_1    0.549662   NaN
3    2013-10-01  ticker_0    0.435322   NaN
4    2013-10-02  ticker_2    0.420368   NaN
5    2013-10-03  ticker_0    0.330335    0
6    2013-10-04  ticker_1    0.204649   NaN
7    2013-10-02  ticker_0    0.619271   NaN
8    2013-10-01  ticker_2    0.299655    0
</code></pre>

<p>What's going on here?  When you call the <code>.diff</code> method on a Pandas object, is it not just calling <code>np.diff</code>?  I know there's a <code>diff</code> method on the <code>DataFrame</code> class, but I couldn't figure out how to pass that to <code>transform</code> without the <code>lambda</code> function syntax I used to make <code>data1</code> work.  Am I missing something?  Why is the <code>diffs</code> column in <code>data3</code> screwy?  How can I have call the Pandas <code>diff</code> method within <code>transform</code> without needing to write a <code>lambda</code> to do it?</p>
";;0;;2013-12-18T23:34:19.160;6.0;20670726;2013-12-19T00:08:54.960;2017-05-23T12:17:32.580;;-1.0;;2501018.0;;1;18;<python><pandas>;Computing diffs in Pandas after using groupby leads to unexpected result;5454.0
8827;8827;;3.0;"<p>Here is what I am doing:</p>

<pre><code>$ python
Python 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
&gt;&gt;&gt; import statsmodels.api as sm
&gt;&gt;&gt; statsmodels.__version__
'0.5.0'
&gt;&gt;&gt; import numpy 
&gt;&gt;&gt; y = numpy.array([1,2,3,4,5,6,7,8,9])
&gt;&gt;&gt; X = numpy.array([1,1,2,2,3,3,4,4,5])
&gt;&gt;&gt; res_ols = sm.OLS(y, X).fit()
&gt;&gt;&gt; res_ols.params
array([ 1.82352941])
</code></pre>

<p>I had expected an array with two elements?!?
The intercept and the slope coefficient?</p>
";;2;;2013-12-20T10:27:10.853;9.0;20701484;2016-09-19T12:05:17.430;;;;;2312926.0;;1;15;<python><pandas><linear-regression><statsmodels>;Why do I get only one parameter from a statsmodels OLS fit;8197.0
8876;8876;20739897.0;1.0;"<p>I have a datetime index in pandas</p>

<pre><code>index = np.array(['2013-11-11T12:36:00.078757888-0800',
                  '2013-11-11T12:36:03.692692992-0800',
                  '2013-11-11T12:36:07.085489920-0800',
                  '2013-11-11T12:36:08.957488128-0800'], dtype='datetime64[ns]')
</code></pre>

<p>I want to calculate the time difference in seconds. The way I came up with is:</p>

<pre><code>diff(index).astype('float64')/1e9
</code></pre>

<p>is there a better/cleaner way?</p>
";;0;;2013-12-23T06:49:39.943;5.0;20738357;2013-12-23T08:46:22.330;;;;;2585497.0;;1;12;<python><numpy><pandas>;getting seconds from numpy timedelta64;4626.0
8891;8891;20763459.0;2.0;"<p>I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:</p>

<pre><code>data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])
</code></pre>

<p>I'd like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values</p>

<p>I can specify the index as follows:</p>

<pre><code>df = pd.DataFrame(data,index=data[:,0]),
</code></pre>

<p>however I am unsure how to best assign column headers.</p>
";;1;;2013-12-24T15:09:07.377;14.0;20763012;2016-03-06T12:28:40.973;2013-12-24T15:52:12.337;;625914.0;;3132783.0;;1;62;<numpy><pandas>;Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?;128433.0
8933;8933;;5.0;"<p>I have a DataFrame with numerical values. What is the simplest way of appending a row (with a given index value) that represents the sum of each column?</p>
";;0;;2013-12-27T17:03:04.423;4.0;20804673;2016-12-08T19:53:46.337;2015-04-09T16:43:29.553;;2359271.0;;3132783.0;;1;16;<python><pandas>;Appending column totals to a Pandas DataFrame;14494.0
8963;8963;20830034.0;2.0;"<p>I have a DataFrame with a column containing labels for each row (in addition to some relevant data for each row).  I have a dictionary with keys equal to the possible labels and values equal to 2-tuples of information related to that label.  I'd like to tack two new columns onto my frame, one for each part of the 2-tuple corresponding to the label for each row.</p>

<p>Here is the setup:</p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed(1)
n = 10

labels = list('abcdef')
colors = ['red', 'green', 'blue']
sizes = ['small', 'medium', 'large']

labeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}

df = pd.DataFrame({'label': np.random.choice(labels, n), 
                   'somedata': np.random.randn(n)})
</code></pre>

<p>I can get what I want by running:</p>

<pre><code>df['color'], df['size'] = zip(*df['label'].map(labeldict))
print df

  label  somedata  color    size
0     b  0.196643    red  medium
1     c -1.545214  green   small
2     a -0.088104  green   small
3     c  0.852239  green   small
4     b  0.677234    red  medium
5     c -0.106878  green   small
6     a  0.725274  green   small
7     d  0.934889    red  medium
8     a  1.118297  green   small
9     c  0.055613  green   small
</code></pre>

<p>But how can I do this if I don't want to manually type out the two columns on the left side of the assignment?  I.e. how can I create multiple new columns on the fly.  For example, if I had 10-tuples in <code>labeldict</code> instead of 2-tuples, this would be a real pain as currently written.  Here are a couple things that don't work:</p>

<pre><code># set up attrlist for later use
attrlist = ['color', 'size']

# non-working idea 1)
df[attrlist] = zip(*df['label'].map(labeldict))

# non-working idea 2)
df.loc[:, attrlist] = zip(*df['label'].map(labeldict))
</code></pre>

<p>This does work, but seems like a hack:</p>

<pre><code>for a in attrlist:
    df[a] = 0
df[attrlist] = zip(*df['label'].map(labeldict))
</code></pre>

<p>Better solutions?</p>
";;2;;2013-12-29T20:30:24.547;3.0;20829748;2013-12-29T21:02:11.430;2013-12-29T20:56:50.720;;1265154.0;;2501018.0;;1;11;<python><pandas>;Pandas: Assigning multiple *new* columns simultaneously;6380.0
8968;8968;20842283.0;1.0;"<p>For the Series object (let's call it s), pandas offers three types of addressing.</p>

<p>s.iloc[] -- for integer position addressing;</p>

<p>s.loc[] -- for index label addressing; and</p>

<p>s.ix[] -- for a hybrid of integer position and label addressing.</p>

<p>The pandas object also performs ix addressing directly. </p>

<pre><code># play data ...
import string
idx = [i for i in string.uppercase] # A, B, C .. Z
t = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25

# examples ...
t[0]              # --&gt; 0
t['A']            # --&gt; 0
t[['A','M']]      # --&gt; [0, 12]
t['A':'D']        # --&gt; [0, 1, 2, 3]
t.iloc[25]        # --&gt; 25
t.loc['Z']        # --&gt; 25
t.loc[['A','Z']]  # --&gt; [0, 25]
t.ix['A':'C']     # --&gt; [0, 1, 2]
t.ix[0:2]         # --&gt; [0, 1]
</code></pre>

<p>So to my question: is there a point to the .ix method of indexing? Am I missing something important here?</p>
";;0;;2013-12-30T11:15:58.267;12.0;20838395;2014-05-08T22:43:02.987;;;;;1577110.0;;1;18;<python><pandas>;What is the point of .ix indexing for pandas Series;17681.0
8973;8973;;3.0;"<p>I am trying to save a csv to a folder after making some edits to the file. </p>

<p>Every time I use <code>pd.to_csv('C:/Path of file.csv')</code> the csv file has a separate column of indexes. I want to avoid printing the index to csv.</p>

<p>I tried: </p>

<pre><code>pd.read_csv('C:/Path to file to edit.csv', index_col = False)
</code></pre>

<p>And to save the file...</p>

<pre><code>pd.to_csv('C:/Path to save edited file.csv', index_col = False)
</code></pre>

<p>However, I still got the unwanted index column. How can I avoid this when I save my files?</p>
";;2;;2013-12-30T18:24:25.763;11.0;20845213;2017-08-19T11:06:09.190;2016-11-19T14:52:23.387;;445131.0;;3137488.0;;1;100;<python><csv><indexing><pandas>;How to avoid Python/Pandas creating an index in a saved csv?;47670.0
8989;8989;22920808.0;4.0;"<p>I am receiving the following error when importing <code>pandas</code> in a <code>Python</code> program</p>

<pre><code>monas-mbp:book mona$ sudo pip install python-dateutil
Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python
Cleaning up...
monas-mbp:book mona$ python t1.py
No module named dateutil.parser
Traceback (most recent call last):
  File ""t1.py"", line 4, in &lt;module&gt;
    import pandas as pd
  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in &lt;module&gt;
    from . import hashtable, tslib, lib
  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)
ImportError: No module named dateutil.parser
</code></pre>

<p>Also here's the program:</p>

<pre><code>import codecs 
from math import sqrt
import numpy as np
import pandas as pd

users = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,
                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,
                      ""Slightly Stoopid"": 1.5,
                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},

         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,
                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,
                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},

         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,
                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,
                  ""Slightly Stoopid"": 1.0},

         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,
                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,
                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,
                 ""Vampire Weekend"": 2.0},

         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,
                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,
                    ""Vampire Weekend"": 1.0},

         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,
                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,
                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,
                     ""Vampire Weekend"": 4.0},

         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,
                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,
                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},

         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,
                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,
                      ""The Strokes"": 3.0}
        }



class recommender:

    def __init__(self, data, k=1, metric='pearson', n=5):
        """""" initialize recommender
        currently, if data is dictionary the recommender is initialized
        to it.
        For all other data types of data, no initialization occurs
        k is the k value for k nearest neighbor
        metric is which distance formula to use
        n is the maximum number of recommendations to make""""""
        self.k = k
        self.n = n
        self.username2id = {}
        self.userid2name = {}
        self.productid2name = {}
        # for some reason I want to save the name of the metric
        self.metric = metric
        if self.metric == 'pearson':
            self.fn = self.pearson
        #
        # if data is dictionary set recommender data to it
        #
        if type(data).__name__ == 'dict':
            self.data = data

    def convertProductID2name(self, id):
        """"""Given product id number return product name""""""
        if id in self.productid2name:
            return self.productid2name[id]
        else:
            return id


    def userRatings(self, id, n):
        """"""Return n top ratings for user with id""""""
        print (""Ratings for "" + self.userid2name[id])
        ratings = self.data[id]
        print(len(ratings))
        ratings = list(ratings.items())
        ratings = [(self.convertProductID2name(k), v)
                   for (k, v) in ratings]
        # finally sort and return
        ratings.sort(key=lambda artistTuple: artistTuple[1],
                     reverse = True)
        ratings = ratings[:n]
        for rating in ratings:
            print(""%s\t%i"" % (rating[0], rating[1]))




    def loadBookDB(self, path=''):
        """"""loads the BX book dataset. Path is where the BX files are
        located""""""
        self.data = {}
        i = 0
        #
        # First load book ratings into self.data
        #
        f = codecs.open(path + ""BX-Book-Ratings.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #separate line into fields
            fields = line.split(';')
            user = fields[0].strip('""')
            book = fields[1].strip('""')
            rating = int(fields[2].strip().strip('""'))
            if user in self.data:
                currentRatings = self.data[user]
            else:
                currentRatings = {}
            currentRatings[book] = rating
            self.data[user] = currentRatings
        f.close()
        #
        # Now load books into self.productid2name
        # Books contains isbn, title, and author among other fields
        #
        f = codecs.open(path + ""BX-Books.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #separate line into fields
            fields = line.split(';')
            isbn = fields[0].strip('""')
            title = fields[1].strip('""')
            author = fields[2].strip().strip('""')
            title = title + ' by ' + author
            self.productid2name[isbn] = title
        f.close()
        #
        #  Now load user info into both self.userid2name and
        #  self.username2id
        #
        f = codecs.open(path + ""BX-Users.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #print(line)
            #separate line into fields
            fields = line.split(';')
            userid = fields[0].strip('""')
            location = fields[1].strip('""')
            if len(fields) &gt; 3:
                age = fields[2].strip().strip('""')
            else:
                age = 'NULL'
            if age != 'NULL':
                value = location + '  (age: ' + age + ')'
            else:
                value = location
            self.userid2name[userid] = value
            self.username2id[location] = userid
        f.close()
        print(i)


    def pearson(self, rating1, rating2):
        sum_xy = 0
        sum_x = 0
        sum_y = 0
        sum_x2 = 0
        sum_y2 = 0
        n = 0
        for key in rating1:
            if key in rating2:
                n += 1
                x = rating1[key]
                y = rating2[key]
                sum_xy += x * y
                sum_x += x
                sum_y += y
                sum_x2 += pow(x, 2)
                sum_y2 += pow(y, 2)
        if n == 0:
            return 0
        # now compute denominator
        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)
                       * sqrt(sum_y2 - pow(sum_y, 2) / n))
        if denominator == 0:
            return 0
        else:
            return (sum_xy - (sum_x * sum_y) / n) / denominator


    def computeNearestNeighbor(self, username):
        """"""creates a sorted list of users based on their distance to
        username""""""
        distances = []
        for instance in self.data:
            if instance != username:
                distance = self.fn(self.data[username],
                                   self.data[instance])
                distances.append((instance, distance))
        # sort based on distance -- closest first
        distances.sort(key=lambda artistTuple: artistTuple[1],
                       reverse=True)
        return distances

    def recommend(self, user):
       """"""Give list of recommendations""""""
       recommendations = {}
       # first get list of users  ordered by nearness
       nearest = self.computeNearestNeighbor(user)
       #
       # now get the ratings for the user
       #
       userRatings = self.data[user]
       #
       # determine the total distance
       totalDistance = 0.0
       for i in range(self.k):
          totalDistance += nearest[i][1]
       # now iterate through the k nearest neighbors
       # accumulating their ratings
       for i in range(self.k):
          # compute slice of pie 
          weight = nearest[i][1] / totalDistance
          # get the name of the person
          name = nearest[i][0]
          # get the ratings for this person
          neighborRatings = self.data[name]
          # get the name of the person
          # now find bands neighbor rated that user didn't
          for artist in neighborRatings:
             if not artist in userRatings:
                if artist not in recommendations:
                   recommendations[artist] = (neighborRatings[artist]
                                              * weight)
                else:
                   recommendations[artist] = (recommendations[artist]
                                              + neighborRatings[artist]
                                              * weight)
       # now make list from dictionary
       recommendations = list(recommendations.items())
       recommendations = [(self.convertProductID2name(k), v)
                          for (k, v) in recommendations]
       # finally sort and return
       recommendations.sort(key=lambda artistTuple: artistTuple[1],
                            reverse = True)
       # Return the first n items
       return recommendations[:self.n]

r = recommender(users)
# The author implementation
r.loadBookDB('/Users/mona/Downloads/BX-Dump/')

ratings = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")
books = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Books.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")
users = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Users.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")



pivot_rating = ratings.pivot(index='User-ID', columns='ISBN', values='Book-Rating')
</code></pre>
";;6;;2013-12-31T07:18:17.143;4.0;20853474;2016-06-02T18:15:19.690;2013-12-31T10:38:01.177;;1060350.0;;2414957.0;;1;59;<python><pandas><pip>;ImportError: No module named dateutil.parser;72499.0
9005;9005;20868446.0;3.0;"<p>I was looking for an elegant way to change a specified column name in a <code>DataFrame</code>.</p>

<p>play data ...</p>

<pre><code>import pandas as pd
d = {
         'one': [1, 2, 3, 4, 5],
         'two': [9, 8, 7, 6, 5],
         'three': ['a', 'b', 'c', 'd', 'e']
    }
df = pd.DataFrame(d)
</code></pre>

<p>The most elegant solution I have found so far ...</p>

<pre><code>names = df.columns.tolist()
names[names.index('two')] = 'new_name'
df.columns = names
</code></pre>

<p>I was hoping for a simple one-liner ... this attempt failed ...</p>

<pre><code>df.columns[df.columns.tolist().index('one')] = 'another_name'
</code></pre>

<p>Any hints gratefully received.</p>
";;0;;2014-01-01T11:59:54.937;40.0;20868394;2016-03-10T13:50:56.763;2014-01-01T12:05:08.580;;1577110.0;;1577110.0;;1;99;<python><pandas>;Changing a specific column name in pandas DataFrame;115075.0
9008;9008;20869270.0;3.0;"<p>I have a pandas dataframe and passing <code>df[list_of_columns]</code> as X and <code>df[[single_column]]</code> as <code>Y</code> to a Random Forest regressor.</p>

<p>What does the following warnning mean and what should be done to resolve it?</p>

<pre><code>DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   probas = cfr.fit(trainset_X, trainset_Y).predict(testset_X)
</code></pre>
";;3;;2014-01-01T12:30:33.710;;20868664;2016-08-05T18:01:06.933;2016-08-02T15:03:51.173;;122763.0;;2808117.0;;1;13;<pandas><scikit-learn>;Should a pandas dataframe column be converted in some way before passing it to a scikit learn regressor?;8083.0
9030;9030;;2.0;"<p>I have a dataframe with a 3-level deep multi-index on the columns.  I would like to compute subtotals across rows (<code>sum(axis=1)</code>) where I sum across one of the levels while preserving the others.  I think I know how to do this using the <code>level</code> keyword argument of <code>pd.DataFrame.sum</code>.  However, I'm having trouble thinking of how to incorporate the result of this sum back into the original table.</p>

<p>Setup:</p>

<pre><code>import numpy as np
import pandas as pd
from itertools import product

np.random.seed(0)

colors = ['red', 'green']
shapes = ['square', 'circle']
obsnum = range(5)

rows = list(product(colors, shapes, obsnum))
idx = pd.MultiIndex.from_tuples(rows)
idx.names = ['color', 'shape', 'obsnum']

df = pd.DataFrame({'attr1': np.random.randn(len(rows)), 
                   'attr2': 100 * np.random.randn(len(rows))},
                  index=idx)

df.columns.names = ['attribute']

df = df.unstack(['color', 'shape'])
</code></pre>

<p>Gives a nice frame like so:</p>

<p><img src=""https://i.stack.imgur.com/xQD2e.png"" alt=""Original frame""></p>

<p>Say I wanted to reduce the <code>shape</code> level.  I could run:</p>

<pre><code>tots = df.sum(axis=1, level=['attribute', 'color'])
</code></pre>

<p>to get my totals like so:</p>

<p><img src=""https://i.stack.imgur.com/0Dfvt.png"" alt=""totals""></p>

<p>Once I have this, I'd like to tack it on to the original frame.  I think I can do this in a somewhat cumbersome way:</p>

<pre><code>tots = df.sum(axis=1, level=['attribute', 'color'])
newcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))
tots.columns = newcols
bigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/eN0cv.png"" alt=""aggregated""></p>

<p><strong>Is there a more natural way to do this?</strong></p>
";;0;;2014-01-02T18:04:48.473;7.0;20888954;2016-12-18T19:09:28.173;2016-12-18T17:30:16.203;;6207849.0;;2501018.0;;1;11;<python><pandas>;Add subtotal columns in pandas with multi-index;2813.0
9056;9056;21232849.0;6.0;"<p>I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:</p>

<pre><code>import glob
import pandas as pd

# get data file names
path =r'C:\DRO\DCL_rawdata_files'
filenames = glob.glob(path + ""/*.csv"")

dfs = []
for filename in filenames:
    dfs.append(pd.read_csv(filename))

# Concatenate all data into one DataFrame
big_frame = pd.concat(dfs, ignore_index=True)
</code></pre>

<p>I guess I need some help within the for loop???</p>
";;14;;2014-01-03T15:00:46.510;64.0;20906474;2017-08-02T13:52:50.893;2015-12-20T07:25:28.017;;1240268.0;;2219369.0;;1;104;<python><csv><pandas><concatenation>;Import multiple csv files into pandas and concatenate into one DataFrame;80289.0
9091;9091;20937592.0;3.0;"<p>I would like to display a pandas dataframe with a given format using <code>print()</code> and the IPython <code>display()</code>. For example:</p>

<pre><code>df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
print df

         cost
foo   123.4567
bar   234.5678
baz   345.6789
quux  456.7890
</code></pre>

<p>I would like to somehow coerce this into printing</p>

<pre><code>         cost
foo   $123.46
bar   $234.57
baz   $345.68
quux  $456.79
</code></pre>

<p>without having to modify the data itself or create a copy, just change the way it is displayed.</p>

<p>How can I do this?</p>
";;2;;2014-01-05T18:39:43.930;22.0;20937538;2017-07-11T14:17:31.787;2016-12-09T20:06:37.243;;202229.0;;44330.0;;1;60;<python><python-2.7><pandas><ipython><dataframe>;How to display pandas DataFrame of floats using a format string for columns?;56902.0
9094;9094;21009442.0;5.0;"<h1>Problem description</h1>

<p>In writing a Monte Carlo particle simulator (brownian motion and photon emission) in python/numpy. I need to save the simulation output (>>10GB) to a file and process the data in a second step. Compatibility with both Windows and Linux is important.</p>

<p>The number of particles (<code>n_particles</code>) is 10-100. The number of time-steps (<code>time_size</code>) is ~10^9.</p>

<p>The simulation has 3 steps (the code below is for an all-in-RAM version):</p>

<ol>
<li><p>Simulate (and store) an <strong><code>emission</code></strong> rate array (contains many almost-0 elements): </p>

<ul>
<li>shape (<code>n_particles</code> x <code>time_size</code>), float32, size <strong>80GB</strong></li>
</ul></li>
<li><p>Compute <strong><code>counts</code></strong> array, (random values from a Poisson process with previously computed rates):</p>

<ul>
<li><p>shape (<code>n_particles</code> x <code>time_size</code>), uint8, size <strong>20GB</strong></p>

<pre><code>counts = np.random.poisson(lam=emission).astype(np.uint8)
</code></pre></li>
</ul></li>
<li><p>Find timestamps (or index) of counts. Counts are almost always 0, so the timestamp arrays  will fit in RAM.</p>

<pre><code># Loop across the particles
timestamps = [np.nonzero(c) for c in counts]
</code></pre></li>
</ol>

<p>I do step 1 once, then repeat step 2-3 many (~100) times. In the future I may need to pre-process <strong><code>emission</code></strong> (apply <code>cumsum</code> or other functions) before computing <strong><code>counts</code></strong>.</p>

<h1>Question</h1>

<p>I have a working in-memory implementation and I'm trying to understand what is the best approach to implement an out-of-core version that can scale to (much) longer simulations.</p>

<h3>What I would like it exist</h3>

<p>I need to save arrays to a file, and I would like to use a single file for a simulation. I also need a ""simple"" way to store and recall a dictionary of simulation parameter (scalars).</p>

<p>Ideally I would like a file-backed numpy array that I can preallocate and fill in chunks. Then, I would like the numpy array methods (<code>max</code>, <code>cumsum</code>, ...) to work transparently, requiring only a <code>chunksize</code> keyword to specify how much of the array to load at each iteration.</p>

<p>Even better, I would like a <strong>Numexpr</strong> that operates not between cache and RAM but between RAM and hard drive.</p>

<h3>What are the practical options</h3>

<p>As a first option
I started experimenting with pyTables, but I'm not happy with its complexity and abstractions (so different from numpy). Moreover my current solution (read below) is UGLY and not very efficient.</p>

<p>So my options for which I seek an answer are</p>

<ol>
<li><p>implement a numpy array with required functionality (how?)</p></li>
<li><p>use pytable in a smarter way (different data-structures/methods)</p></li>
<li><p>use another library: h5py, blaze, pandas... (I haven't tried any of them so far).</p></li>
</ol>

<h1>Tentative solution (pyTables)</h1>

<p>I save the simulation parameters in <code>'/parameters'</code> group: each parameter is converted to a numpy array scalar. Verbose solution but it works.</p>

<p>I save <code>emission</code> as an Extensible array (<code>EArray</code>), because I generate the data in chunks and I need to append each new chunk (I know the final size though). Saving <code>counts</code> is more problematic. If a save it like a pytable array it's difficult to perform queries like ""counts >= 2"". Therefore I saved counts as multiple tables (one per particle) [UGLY] and I query with <code>.get_where_list('counts &gt;= 2')</code>. I'm not sure this is space-efficient, and
generating all these tables instead of using a single array, clobbers significantly the HDF5 file. Moreover, strangely enough, creating those tables require creating a custom dtype (even for standard numpy dtypes):</p>

<pre><code>    dt = np.dtype([('counts', 'u1')])        
    for ip in xrange(n_particles):
        name = ""particle_%d"" % ip
        data_file.create_table(
                    group, name, description=dt, chunkshape=chunksize,
                    expectedrows=time_size,
                    title='Binned timetrace of emitted ph (bin = t_step)'
                        ' - particle_%d' % particle)
</code></pre>

<p>Each particle-counts ""table"" has a different name (<code>name = ""particle_%d"" % ip</code>) and that I need to put them in a python list for easy iteration.</p>

<p><strong>EDIT</strong>: The result of this question is a Brownian Motion simulator called <a href=""http://tritemio.github.io/PyBroMo/"" rel=""nofollow"">PyBroMo</a>.</p>
";;5;;2014-01-05T23:55:16.343;8.0;20940805;2017-08-12T17:48:52.447;2015-10-02T21:39:04.923;;2304916.0;;2304916.0;;1;17;<numpy><pandas><pytables><h5py><blaze>;Python particles simulator: out-of-core processing;1262.0
9126;9126;20965090.0;1.0;"<p>I have a <code>DataFrame</code> like this:</p>

<p><code>df</code>:</p>

<pre><code> fruit    val1 val2
0 orange    15    3
1 apple     10   13
2 mango     5    5 
</code></pre>

<p>How do I get Pandas to give me a cumulative sum and percentage column on only <code>val1</code>?</p>

<p>Desired output:</p>

<p><code>df_with_cumsum</code>:</p>

<pre><code> fruit    val1 val2   cum_sum    cum_perc
0 orange    15    3    15          50.00
1 apple     10   13    25          83.33
2 mango     5    5     30          100.00
</code></pre>

<p>I tried <code>df.cumsum()</code>, but it's giving me this error:</p>

<blockquote>
  <p>TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''</p>
</blockquote>
";;0;;2014-01-07T06:11:04.630;5.0;20965046;2015-11-25T23:27:31.120;2015-11-25T23:27:31.120;;4370109.0;;2358206.0;;1;23;<python><pandas><dataframe><cumulative-sum>;Cumulative sum and percentage on column?;27193.0
9130;9130;20970328.0;1.0;"<p>in a pandas dataframe how can I apply a sort of excel left('state',2) to only take the first two letters. Ideally I want to learn how to use left,right and mid in a dataframe too. So need an equivalent and not a ""trick"" for this specific example.</p>

<pre><code>data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],
'year': [2000, 2001, 2002, 2001, 2002],
'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
df = pd.DataFrame(data)

print df

     pop       state  year
 0  1.5    Auckland  2000
 1  1.7       Otago  2001
 2  3.6  Wellington  2002
 3  2.4     Dunedin  2001
 4  2.9    Hamilton  2002
</code></pre>

<p>I want to get this:</p>

<pre><code>    pop       state     year  StateInitial
 0  1.5       Auckland    2000     Au
 1  1.7       Otago       2001     Ot
 2  3.6       Wellington  2002     We
 3  2.4       Dunedin     2001     Du
 4  2.9       Hamilton    2002     Ha
</code></pre>
";;0;;2014-01-07T11:20:20.217;5.0;20970279;2014-01-07T11:27:50.217;;;;;2386086.0;;1;24;<python><pandas>;how to do a left,right and mid of a string in a pandas dataframe;23529.0
9180;9180;20995428.0;2.0;"<p>Are there single functions in pandas to perform the equivalents of <a href=""http://office.microsoft.com/en-us/excel-help/sumifs-function-HA010047504.aspx"" rel=""noreferrer"">SUMIF</a>, which sums over a specific condition and <a href=""http://office.microsoft.com/en-us/excel-help/countifs-function-HA010047494.aspx"" rel=""noreferrer"">COUNTIF</a>, which counts values of specific conditions from Excel?</p>

<p>I know that there are many multiple step functions that can be used for</p>

<p>for example for <code>sumif</code> I can use <code>(df.map(lambda x: condition), or df.size())</code> then use <code>.sum()</code></p>

<p>and for <code>countif</code> I can use <code>(groupby functions</code> and look for my answer or use a filter and the <code>.count())</code> </p>

<p>Is there simple one step process to do these functions where you enter the condition and the data frame and you get the sum or counted results?</p>
";;0;;2014-01-08T12:06:24.573;12.0;20995196;2016-08-07T00:29:35.467;2014-01-08T12:10:07.593;;1470009.0;;3084006.0;;1;23;<python><pandas><sum>;Python Pandas counting and summing specific conditions;38942.0
9226;9226;21020411.0;1.0;"<p>Why does Pandas tell me that I have objects, although every item in the selected column is a string  even after explicit conversion.</p>

<p>This is my DataFrame:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 56992 entries, 0 to 56991
Data columns (total 7 columns):
id            56992  non-null values
attr1         56992  non-null values
attr2         56992  non-null values
attr3         56992  non-null values
attr4         56992  non-null values
attr5         56992  non-null values
attr6         56992  non-null values
dtypes: int64(2), object(5)
</code></pre>

<p>Five of them are <code>dtype object</code>. I explicitly convert those objects to strings:</p>

<pre><code>for c in df.columns:
    if df[c].dtype == object:
        print ""convert "", df[c].name, "" to string""
        df[c] = df[c].astype(str)
</code></pre>

<p>Then, <code>df[""attr2""]</code> still has <code>dtype object</code>, although <code>type(df[""attr2""].ix[0]</code> reveals <code>str</code>, which is correct.</p>

<p>Pandas distinguishes between <code>int64</code> and <code>float64</code> and <code>object</code>. What is the logic behind it when there is no <code>dtype str</code>? Why is a <code>str</code> covered by <code>object</code>?</p>
";;0;;2014-01-09T11:16:23.333;13.0;21018654;2016-04-24T17:19:22.817;2016-04-24T17:19:22.817;;2265734.0;;2265734.0;;1;51;<python><types><pandas>;Strings in a DataFrame, but dtype is object;21985.0
9237;9237;21023125.0;3.0;"<p>I know how to do element by element multiplication between two Pandas dataframes. However, things get more complicated when the dimensions of the two dataframes are not compatible. For instance below <code>df * df2</code> is straightforward, but <code>df * df3</code> is a problem:</p>

<pre><code>df = pd.DataFrame({'col1' : [1.0] * 5, 
                   'col2' : [2.0] * 5, 
                   'col3' : [3.0] * 5 }, index = range(1,6),)
df2 = pd.DataFrame({'col1' : [10.0] * 5, 
                    'col2' : [100.0] * 5, 
                    'col3' : [1000.0] * 5 }, index = range(1,6),)
df3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)

df.mul(df2, 1) # element by element multiplication no problems

df.mul(df3, 1) # df(row*col) is not equal to df3(row*col)
   col1  col2  col3
1   0.1   NaN   NaN
2   0.1   NaN   NaN
3   0.1   NaN   NaN
4   0.1   NaN   NaN
5   0.1   NaN   NaN
</code></pre>

<p>In the above situation, <strong>how can I multiply every column of df with df3.col1</strong>?</p>

<p><strong>My attempt:</strong> I tried to replicate <code>df3.col1</code>  <code>len(df.columns.values)</code> times to get a dataframe that is of the same dimension as <code>df</code>:</p>

<pre><code>df3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])
df3
        1    2    3    4    5
col1  0.1  0.1  0.1  0.1  0.1
col1  0.1  0.1  0.1  0.1  0.1
col1  0.1  0.1  0.1  0.1  0.1
</code></pre>

<p>But this creates a dataframe of dimensions 3 * 5, whereas I am after 5*3. I know I can take the transpose with <code>df3.T()</code> to get what I need but I think this is not that the fastest way.</p>
";;0;;2014-01-09T14:25:13.620;5.0;21022865;2016-05-06T06:05:00.170;2014-01-09T14:32:28.603;;988125.0;;988125.0;;1;11;<python><pandas><multiplication><dataframe>;Pandas: Elementwise multiplication of two dataframes;17543.0
9268;9268;21033789.0;3.0;"<p>I'm making a fairly simple histogram in with pandas using</p>

<p><code>results.val1.hist(bins=120)</code></p>

<p>which works fine, but I really want to have a log scale on the y axis, which I normally (probably incorrectly) do like this:</p>

<pre><code>fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
plt.plot(np.random.rand(100))
ax.set_yscale('log')
plt.show()
</code></pre>

<p>If I replace the <code>plt</code> command with the pandas command, so I have:</p>

<pre><code>fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
results.val1.hist(bins=120)
ax.set_yscale('log')
plt.show()
</code></pre>

<p>results in many copies of the same error:</p>

<pre><code>Jan  9 15:53:07 BLARG.local python[6917] &lt;Error&gt;: CGContextClosePath: no current point.
</code></pre>

<p>I do get a log scale histogram, but it only has the top lines of the bars, but no vertical bars or colors. Am doing something horribly wrong or is this just not supported by pandas?</p>

<p>EDIT:</p>

<p>From Paul H code I replaced</p>

<p>Added <code>bottom=0.1</code> to <code>hist</code> call fixes the problem, I guess there is some kind of divide by zero thing, or something.</p>

<p>Thanks</p>
";;0;;2014-01-09T23:58:04.527;7.0;21033720;2015-11-18T14:17:31.690;2014-01-10T00:38:37.277;;2939028.0;;2939028.0;;1;20;<python><pandas>;Python Pandas Histogram Log Scale;11275.0
9309;9309;21055176.0;2.0;"<p>I have something like this in my code:</p>

<p><code>df2 = df[df['A'].str.contains(""Hello|World"")]</code></p>

<p>However, I want all the rows that <em>don't</em> contain either of Hello or World.  How do I most efficiently reverse this?</p>
";;0;;2014-01-10T21:51:10.743;4.0;21055068;2014-01-10T22:11:03.440;;;;;2898989.0;;1;20;<python><string><python-2.7><csv><pandas>;Reversal of string.contains In python, pandas;3980.0
9325;9325;21059308.0;4.0;"<p>It's pretty easy to write a function that computes the maximum drawdown of a time series.  It takes a small bit of thinking to write it in <code>O(n)</code> time instead of <code>O(n^2)</code> time.  But it's not that bad.  This will work:</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def max_dd(ser):
    max2here = pd.expanding_max(ser)
    dd2here = ser - max2here
    return dd2here.min()
</code></pre>

<p>Let's set up a brief series to play with to try it out:</p>

<pre><code>np.random.seed(0)
n = 100
s = pd.Series(np.random.randn(n).cumsum())
s.plot()
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/1qgKb.png"" alt=""Time series""></p>

<p>As expected, <code>max_dd(s)</code> winds up showing something right around -17.6.  Good, great, grand.  Now say I'm interested in computing the rolling drawdown of this Series.  I.e. for each step, I want to compute the maximum drawdown from the preceding sub series of a specified length.  This is easy to do using <code>pd.rolling_apply</code>.  It works like so:</p>

<pre><code>rolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)
df = pd.concat([s, rolling_dd], axis=1)
df.columns = ['s', 'rol_dd_10']
df.plot()
</code></pre>

<p><img src=""https://i.stack.imgur.com/xkkzh.png"" alt=""rolling drawdown""></p>

<p>This works perfectly.  But it feels very slow.  Is there a particularly slick algorithm in pandas or another toolkit to do this fast?  I took a shot at writing something bespoke: it keeps track of all sorts of intermediate data (locations of observed maxima, locations of previously found drawdowns) to cut down on lots of redundant calculations.  It does save some time, but not a whole lot, and not nearly as much as should be possible.  </p>

<p>I think it's because of all the looping overhead in Python/Numpy/Pandas.  But I'm not currently fluent enough in Cython to really know how to begin attacking this from that angle.  I was hoping someone had tried this before.  Or, perhaps, that someone might want to have a look at my ""handmade"" code and be willing to help me convert it to Cython.</p>

<hr>

<p>Edit: 
For anyone who wants a review of all the functions mentioned here (and some others!) have a look at the iPython notebook at: <a href=""http://nbviewer.ipython.org/gist/8one6/8506455"" rel=""noreferrer"">http://nbviewer.ipython.org/gist/8one6/8506455</a></p>

<p>It shows how some of the approaches to this problem relate, checks that they give the same results, and shows their runtimes on data of various sizes.</p>

<p>If anyone is interested, the ""bespoke"" algorithm I alluded to in my post is <code>rolling_dd_custom</code>.  I think that could be a very fast solution if implemented in Cython.</p>
";;0;;2014-01-11T03:52:30.770;23.0;21058333;2015-06-13T12:25:36.093;2014-01-19T15:47:25.797;;2501018.0;;2501018.0;;1;15;<python><algorithm><numpy><pandas>;Compute *rolling* maximum drawdown of pandas Series;10064.0
9391;9391;21266043.0;4.0;"<p>What I am trying to do is extract elevation data from a google maps API along a path specified by latitude and longitude coordinates as follows:</p>

<pre><code>from urllib2 import Request, urlopen
import json

path1 = '42.974049,-81.205203|42.974298,-81.195755'
request=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&amp;sensor=false')
response = urlopen(request)
elevations = response.read()
</code></pre>

<p>This gives me a data that looks like this:</p>

<pre><code>elevations.splitlines()

['{',
 '   ""results"" : [',
 '      {',
 '         ""elevation"" : 243.3462677001953,',
 '         ""location"" : {',
 '            ""lat"" : 42.974049,',
 '            ""lng"" : -81.205203',
 '         },',
 '         ""resolution"" : 19.08790397644043',
 '      },',
 '      {',
 '         ""elevation"" : 244.1318664550781,',
 '         ""location"" : {',
 '            ""lat"" : 42.974298,',
 '            ""lng"" : -81.19575500000001',
 '         },',
 '         ""resolution"" : 19.08790397644043',
 '      }',
 '   ],',
 '   ""status"" : ""OK""',
 '}']
</code></pre>

<p>when putting into as DataFrame here is what I get:</p>

<p><img src=""https://i.stack.imgur.com/t9aSp.png"" alt=""enter image description here""></p>

<pre><code>pd.read_json(elevations)
</code></pre>

<p>and here is what I want:</p>

<p><img src=""https://i.stack.imgur.com/4Kdlg.png"" alt=""enter image description here""></p>

<p>I'm not sure if this is possible, but mainly what I am looking for is a way to be able to put the elevation, latitude and longitude data together in a pandas dataframe (doesn't have to have fancy mutiline headers).</p>

<p>If any one can help or give some advice on working with this data that would be great! If you can't tell I haven't worked much with json data before...</p>

<p>EDIT:</p>

<p>This method isn't all that attractive but seems to work:</p>

<pre><code>data = json.loads(elevations)
lat,lng,el = [],[],[]
for result in data['results']:
    lat.append(result[u'location'][u'lat'])
    lng.append(result[u'location'][u'lng'])
    el.append(result[u'elevation'])
df = pd.DataFrame([lat,lng,el]).T
</code></pre>

<p>ends up dataframe having columns latitude, longitude, elevation</p>

<p><img src=""https://i.stack.imgur.com/seht1.png"" alt=""enter image description here""></p>
";;0;;2014-01-14T01:32:07.830;19.0;21104592;2017-07-19T13:17:18.570;2014-01-14T05:16:50.630;;2593236.0;;2593236.0;;1;44;<python><json><google-maps><pandas>;JSON to pandas DataFrame;64504.0
9434;9434;21140339.0;1.0;"<p>How can one modify the format for the output from a groupby operation in pandas that produces scientific notation for very large numbers. I know how to do string formatting in pythong but I'm at a loss when it comes to applying it here. </p>

<pre><code>df1.groupby('dept')['data1'].sum()

dept
value1       1.192433e+08
value2       1.293066e+08
value3       1.077142e+08
</code></pre>

<p>This suppresses the scientific notation if I convert to string but now I'm just wondering how to string format and add decimals. </p>

<pre><code>sum_sales_dept.astype(str)
</code></pre>
";;3;;2014-01-15T12:14:36.607;11.0;21137150;2016-11-16T14:48:55.253;2016-11-16T14:48:55.253;;202229.0;;2329714.0;;1;36;<python><pandas><floating-point><scientific-notation><numeric-format>;Format / Suppress Scientific Notation from Python Pandas Aggregation Results;25062.0
9478;9478;21165116.0;1.0;"<p>I currently have a dataframe consisting of columns with 1's and 0's as values, I would like to iterate through the columns and delete the ones that are made up of only 0's. Here's what I have tried so far:</p>

<pre><code>ones = []
zeros = []
for year in years:
    for i in range(0,599):
        if year[str(i)].values.any() == 1:
            ones.append(i)
        if year[str(i)].values.all() == 0:
            zeros.append(i)
    for j in ones:
        if j in zeros:
            zeros.remove(j)
    for q in zeros:
        del year[str(q)]
</code></pre>

<p>In which years is a list of dataframes for the various years I am analyzing, ones consists of columns with a one in them and zeros is a list of columns containing all zeros. Is there a better way to delete a column based on a condition? For some reason I have to check whether the ones columns are in the zeros list as well and remove them from the zeros list to obtain a list of all the zero columns. </p>
";;0;;2014-01-16T14:43:41.777;7.0;21164910;2017-06-12T01:04:06.340;2017-06-12T01:04:06.340;;3604745.0;;2587593.0;;1;14;<python><pandas>;Delete Column in Pandas if it is All Zeros;11895.0
9545;9545;21197863.0;5.0;"<p>I want to set the <code>dtype</code>s of multiple columns in <code>pd.Dataframe</code> (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for <code>pd.read_csv</code>)</p>

<pre><code>import pandas as pd
print pd.DataFrame([['a','1'],['b','2']],
                   dtype={'x':'object','y':'int'},
                   columns=['x','y'])
</code></pre>

<p>I get</p>

<pre><code>ValueError: entry not a 2- or 3- tuple
</code></pre>

<p>The only way I can set them is by looping through each column variable and recasting with <code>astype</code>. </p>

<pre><code>dtypes = {'x':'object','y':'int'}
mydata = pd.DataFrame([['a','1'],['b','2']],
                      columns=['x','y'])
for c in mydata.columns:
    mydata[c] = mydata[c].astype(dtypes[c])
print mydata['y'].dtype   #=&gt; int64
</code></pre>

<p>Is there a better way?</p>
";;4;;2014-01-17T23:16:27.693;18.0;21197774;2017-04-08T01:26:14.400;;;;;143476.0;;1;48;<python><pandas>;Assign pandas dataframe column dtypes;50855.0
9547;9547;21204417.0;3.0;"<p>I have two dataframes, both of which contain an irregularly spaced, millisecond resolution timestamp column. My goal here is to match up the rows so that for each matched row, 1) the first time stamp is always smaller or equal to the second timestamp, and 2) the matched timestamps are the closest for all pairs of timestamps satisfying 1). </p>

<p>Is there any way to do this with pandas.merge?</p>
";;0;;2014-01-18T08:00:49.657;9.0;21201618;2016-10-19T22:19:40.360;;;;;1919717.0;;1;15;<python><pandas>;pandas.merge: match the nearest time stamp >= the series of timestamps;6959.0
9583;9583;21244212.0;5.0;"<p>In pandas, given a DataFrame D:</p>

<pre><code>+-----+--------+--------+--------+   
|     |    1   |    2   |    3   |
+-----+--------+--------+--------+
|  0  | apple  | banana | banana |
|  1  | orange | orange | orange |
|  2  | banana | apple  | orange |
|  3  | NaN    | NaN    | NaN    |
|  4  | apple  | apple  | apple  |
+-----+--------+--------+--------+
</code></pre>

<p>How do I return rows that have the same contents across all of its columns when there are three columns or more such that it returns this:</p>

<pre><code>+-----+--------+--------+--------+   
|     |    1   |    2   |    3   |
+-----+--------+--------+--------+
|  1  | orange | orange | orange |
|  4  | apple  | apple  | apple  |
+-----+--------+--------+--------+
</code></pre>

<p>Note that it skips rows when all values are NaN.</p>

<p>If this were only two columns, I usually do <code>D[D[1]==D[2]]</code> but I don't know how to generalize this for more than 2 column DataFrames. </p>
";;0;;2014-01-20T10:26:59.750;;21231478;2015-09-07T15:02:58.440;2014-01-21T11:51:45.890;;434165.0;;434165.0;;1;15;<python><pandas><dataframe>;Get rows that have the same value across its columns in pandas;7328.0
9585;9585;21242140.0;2.0;"<p>I have 2 DataFrames df1 and df2 with the same column names ['a','b','c'] and indexed by dates.
The date index can have similar values.
I would like to create a DataFrame df3 with only the data from columns ['c'] renamed respectively 'df1' and 'df2' and with the correct date index. My problem is that I cannot get how to merge the index properly.</p>

<pre><code>df1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )
df2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )
df1
                 a        b            c
2014-01-02   0.580550    0.480814    1.135899
2014-01-03  -1.961033    0.546013    1.093204
2014-01-04   2.063441   -0.627297    2.035373
2014-01-05   0.319570    0.058588    0.350060
2014-01-06   1.318068   -0.802209   -0.939962

df2
                 a        b            c
2014-01-01   0.772482    0.899337    0.808630
2014-01-02   0.518431   -1.582113    0.323425
2014-01-03   0.112109    1.056705   -1.355067
2014-01-04   0.767257   -2.311014    0.340701
2014-01-05   0.794281   -1.954858    0.200922
2014-01-06   0.156088    0.718658   -1.030077
2014-01-07   1.621059    0.106656   -0.472080
2014-01-08  -2.061138   -2.023157    0.257151
</code></pre>

<p>The df3 DataFrame should have the following form : </p>

<pre><code>df3
                 df1        df2
2014-01-01   NaN        0.808630
2014-01-02   1.135899   0.323425
2014-01-03   1.093204   -1.355067
2014-01-04   2.035373   0.340701
2014-01-05   0.350060   0.200922
2014-01-06   -0.939962  -1.030077
2014-01-07   NaN        -0.472080
2014-01-08   NaN        0.257151
</code></pre>

<p>But with NaN in the df1 column as the date index of df2 is wider. (In this example, I would get NaN for the ollowing dates : 2014-01-01, 2014-01-07 and 2014-01-08)</p>

<p>Thanks for your help.</p>
";;0;;2014-01-20T10:43:44.183;3.0;21231834;2016-09-14T17:40:06.307;2015-03-19T02:57:24.890;;249341.0;;3153467.0;;1;20;<python><pandas><dataframe>;Creating a pandas DataFrame from columns of other DataFrames with similar indexes;41549.0
9609;9609;21247312.0;4.0;"<p>Given a dataframe with different categorical variables, how do I return a cross-tabulation with percentages instead of frequencies?</p>

<pre><code>df = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,
                   'B' : ['A', 'B', 'C'] * 8,
                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,
                   'D' : np.random.randn(24),
                   'E' : np.random.randn(24)})


pd.crosstab(df.A,df.B)


B       A    B    C
A               
one     4    4    4
three   2    2    2
two     2    2    2
</code></pre>

<p>Using the margins option in crosstab to compute row and column totals gets us close enough to think that it should be possible using an aggfunc or groupby, but my meager brain can't think it through.</p>

<pre><code>B       A     B    C
A               
one     .33  .33  .33
three   .33  .33  .33
two     .33  .33  .33
</code></pre>
";;3;;2014-01-21T00:57:56.083;10.0;21247203;2016-11-14T16:35:18.487;;;;;1574687.0;;1;19;<python><pandas><crosstab>;How to make a pandas crosstab with percentages?;15719.0
9611;9611;21248050.0;1.0;"<p>I have the following dataframe:</p>

<pre><code>mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})
</code></pre>

<p>I want to create a dataframe that makes summary statistics about the <code>val</code> column of items with the same <code>class</code> id. For this I use <code>groupby</code> as follows:</p>

<pre><code>mydf.groupby(""class"").val.sum()
</code></pre>

<p>that's the correct behavior, but I'd like to retain the <code>cat</code> column information in the resulting df. can that be done? do I have to <code>merge/join</code> that info in later? I tried:</p>

<pre><code>mydf.groupby([""cat"", ""class""]).val.sum()
</code></pre>

<p>but this uses hierarchical indexing. I'd like to have a plain dataframe back that just has the <code>cat</code> value for each group, where the group by is <code>class</code>. The output should be a dataframe (not series) with the values of cat and class, where the <code>val</code> entries are summed over each entry that has the same <code>class</code>:</p>

<pre><code>cat     class    val
first   A         7
second  B         3
third   C        10
</code></pre>

<p>is this possible?</p>
";;2;;2014-01-21T02:25:11.780;3.0;21247992;2014-01-21T03:01:14.697;2014-01-21T02:32:19.080;;248237.0;;248237.0;;1;11;<python><pandas><dataframe>;grouping pandas dataframe by two columns (or more)?;10280.0
9615;9615;21275962.0;1.0;"<p>I'm trying to configure my IPython output in my OS X terminal, but it would seem that none of the changes I'm trying to set are taking effect. I'm trying to configure the display settings such that wider outputs like a big <code>DataFrame</code> will output without any truncation or as the summary info.</p>

<p>After importing pandas into my script, I have a few options set where I tried a whole bunch, but any one (or all, for that matter) does not seem to take effect. I'm running the script from IPython using <code>%run</code>. Am I doing something wrong here?</p>

<pre><code>import pandas as pd

pd.set_option('display.expand_max_repr', False)
pd.set_option('display.max_columns', 30)
pd.set_option('display.width', None)
pd.set_option('display.line_width', 200)
</code></pre>

<p>I've looked at <a href=""https://stackoverflow.com/questions/11707586/python-pandas-widen-output-display"">some threads</a> on Stack and the <a href=""http://pandas.pydata.org/pandas-docs/stable/faq.html"" rel=""nofollow noreferrer"">pandas FAQ</a> to no avail, even when using these under the display namespace (or without), as I've attempted here. </p>

<p>I understand that there are some ways around this, such as calling <code>to_string()</code> or <code>describe()</code> methods on your output, but these are very manual, and don't always work as intended in some cases, like one where I have calling <code>to_string()</code> on a <code>groupby</code> object yields:</p>

<pre><code>    id       type
106125       puzzle       gameplay_id  sitting_id  user_id           ...
106253       frames       gameplay_id  sitting_id  user_id           ...
106260       trivia       gameplay_id  sitting_id  user_id           ...
</code></pre>

<p>My terminal window size is more than sufficient to accommodate the width, and calling <code>pd.util.terminal.get_terminal_size()</code> is correctly finding the window size tuple, so it would seem that auto detecting the size isn't working either. Any insight would be appreciated!</p>
";;10;;2014-01-21T04:30:50.993;7.0;21249206;2014-05-01T13:31:11.217;2017-05-23T11:46:43.360;;-1.0;;2392486.0;;1;19;<python><terminal><pandas><ipython>;How to configure display output in IPython pandas;12295.0
9645;9645;;3.0;"<p>I have a dataframe like this:</p>

<pre><code>In[1]: df
Out[1]:
      A      B       C            D
1   blue    red    square        NaN
2  orange  yellow  circle        NaN
3  black   grey    circle        NaN
</code></pre>

<p>and I want to update column D when it meets 3 conditions. Ex:</p>

<pre><code>df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'
</code></pre>

<p>It works for the first two conditions, but it doesn't work for the third, thus:</p>

<pre><code>df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'
</code></pre>

<p>has exactly the same result:</p>

<pre><code>In[1]: df
Out[1]:
      A      B       C            D
1   blue    red    square        succeed
2  orange  yellow  circle        NaN
3  black   grey    circle        NaN
</code></pre>
";;1;;2014-01-21T15:57:12.100;1.0;21263020;2017-08-12T14:38:17.373;;;;;3219943.0;;1;12;<python><pandas>;pandas : update value if condition in 3 columns are met;16650.0
9663;9663;;3.0;"<p>I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:</p>

<pre><code>headers = ['col1', 'col2', 'col3', 'col4']
dtypes = ['datetime', 'datetime', 'str', 'float']
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>When run gives a error:</p>

<pre><code>TypeError: data type ""datetime"" not understood
</code></pre>

<p>Converting columns after the fact, via pandas.to_datetime() isn't an option I can't know which columns will be datetime objects.  That information can change and comes from whatever informs my dtypes list.</p>

<p>Alternatively, I've tried to load the csv file with numpy.genfromtxt, set the dtypes in that function, and then convert to a pandas.dataframe but it garbles the data.  Any help is greatly appreciated!</p>
";;0;;2014-01-21T21:24:17.163;6.0;21269399;2016-09-26T09:10:47.623;2016-09-26T09:10:47.623;;3730397.0;;3221055.0;;1;24;<python><csv><datetime><pandas><dataframe>;datetime dtypes in pandas read_csv;28062.0
9674;9674;21272615.0;3.0;"<p>I was wondering if there is an elegant and shorthand way in Pandas DataFrames to select columns by data type (dtype). i.e. Select only int64 columns from a DataFrame.</p>

<p>To elaborate, something along the lines of</p>

<pre><code>df.select_columns(dtype=float64)
</code></pre>

<p>Thanks in advance for the help</p>
";;0;;2014-01-21T23:59:08.333;8.0;21271581;2015-08-13T13:17:44.127;;;;;2539672.0;;1;12;<python><pandas><scipy>;Selecting Pandas Columns by dtype;5181.0
9675;9675;;5.0;"<p>I would like to sort my data by a given column, specifically p-values. However, the issue is that I am not able to load my entire data into memory. Thus, the following doesn't work or rather works for only small datasets. </p>

<pre><code>data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)
</code></pre>

<p>Is there a quick way to sort my data by a given column that only takes chunks into account and doesn't require loading entire datasets in memory?</p>
";;8;;2014-01-22T00:11:33.627;4.0;21271727;2016-02-05T16:01:37.797;;;;;1867185.0;;1;12;<python><pandas>;Sorting in pandas for large datasets;2877.0
9702;9702;21285575.0;2.0;"<p>So, I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for <code>'spike'</code> in column names like <code>'spike-2'</code>, <code>'hey spike'</code>, <code>'spiked-in'</code> (the <code>'spike'</code> part is always continuous). </p>

<p>I want the column name to be returned as a string or a variable, so I access the column later with <code>df['name']</code> or <code>df[name]</code> as normal. I've tried to find ways to do this, to no avail. Any tips?</p>
";;0;;2014-01-22T14:17:43.450;12.0;21285380;2017-04-20T10:43:26.533;2014-01-22T14:28:44.833;;2997574.0;;2795726.0;;1;31;<string><python-3.x><pandas><find>;Pandas: find column whose name contains a specific string;28697.0
9709;9709;21290084.0;3.0;"<p>I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely <code>id</code>, I want to specify the column type as <code>int</code>. The problem is the <code>id</code> series has missing/empty values.</p>

<p>When I try to cast the <code>id</code> column to integer while reading the .csv, I get:</p>

<pre><code>df= pd.read_csv(""data.csv"", dtype={'id': int}) 
error: Integer column has NA values
</code></pre>

<p>Alternatively, I tried to convert the column type after reading as below, but this time I get:</p>

<pre><code>df= pd.read_csv(""data.csv"") 
df[['id']] = df[['id']].astype(int)
error: Cannot convert NA to integer
</code></pre>

<p>How can I tackle this?</p>
";;7;;2014-01-22T15:51:28.083;13.0;21287624;2017-05-26T16:15:51.753;2017-03-25T23:00:16.123;;4212158.0;;988125.0;;1;46;<python><pandas><na>;Convert Pandas column containing NaNs to dtype `int`;42262.0
9712;9712;21291622.0;5.0;"<p>I've working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers, or, without comma. Is there a way to convert them to integers or not diplay the comma?</p>
";;2;;2014-01-22T18:42:15.927;16.0;21291259;2017-06-20T11:04:33.617;2014-01-22T18:47:44.953;;257944.0;;2961929.0;;1;58;<python><pandas>;Convert floats to ints in Pandas?;84803.0
9727;9727;21295630.0;2.0;"<p>Is there a faster way to find the length of the longest string in a Pandas DataFrame than what's shown in the example below?</p>

<pre><code>import numpy as np
import pandas as pd

x = ['ab', 'bcd', 'dfe', 'efghik']
x = np.repeat(x, 1e7)
df = pd.DataFrame(x, columns=['col1'])

print df.col1.map(lambda x: len(x)).max()
# result --&gt; 6
</code></pre>

<p>It takes about 10 seconds to run <code>df.col1.map(lambda x: len(x)).max()</code> when timing it with IPython's <code>%timit</code>.</p>
";;1;;2014-01-22T22:26:49.777;4.0;21295334;2016-10-18T11:49:15.463;2014-01-22T22:53:01.773;;195294.0;;195294.0;;1;16;<python><pandas>;Find the length of the longest string in a Pandas DataFrame column;10209.0
9757;9757;21317700.0;1.0;"<p>I have a utility function for creating a Pandas MultiIndex when I have two or more iterables and I want an index key for each unique pairing of the values in those iterables. It looks like this</p>

<pre><code>import pandas as pd
import itertools

def product_index(values, names=None):
    """"""Make a MultiIndex from the combinatorial product of the values.""""""
    iterable = itertools.product(*values)
    idx = pd.MultiIndex.from_tuples(list(iterable), names=names)
    return idx
</code></pre>

<p>And could be used like:</p>

<pre><code>a = range(3)
b = list(""ab"")
product_index([a, b])
</code></pre>

<p>To create</p>

<pre><code>MultiIndex(levels=[[0, 1, 2], [u'a', u'b']],
           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])
</code></pre>

<p>This works perfectly fine, but it seems like a common usecase and I am surprised I had to implement it myself. So, my question is, what have I missed/misunderstood in the Pandas library itself that offers this functionality?</p>

<p><strong>Edit to add:</strong> This function has been <a href=""https://github.com/pydata/pandas/pull/6055"">added to Pandas</a> as <code>MultiIndex.from_product</code> for the 0.13.1 release.</p>
";;1;;2014-01-23T18:37:54.830;5.0;21316628;2014-01-27T20:03:13.453;2014-01-27T20:03:13.453;;1533576.0;;1533576.0;;1;12;<python><pandas>;Make a Pandas MultiIndex from a product of iterables?;1871.0
9763;9763;21317570.0;1.0;"<p>I'd like to concatenate two dataframes A, B to a new one without duplicate rows (if rows in B already exist in A, don't add):</p>

<p>Dataframe A:   Dataframe B:</p>

<pre><code>   I    II    I    II
0  1    2     5    6
1  3    1     3    1
</code></pre>

<p>New Dataframe:</p>

<pre><code>     I    II
  0  1    2
  1  3    1
  2  5    6
</code></pre>

<p>How can I do this?</p>
";;5;;2014-01-23T19:16:45.653;5.0;21317384;2014-01-23T19:31:49.710;2014-01-23T19:31:49.710;;2961929.0;;2961929.0;;1;11;<python><pandas>;Pandas/Python: How to concatenate two dataframes without duplicates?;12187.0
9770;9770;;1.0;"<p>I have a .csv file that looks like this:</p>

<pre><code>Male, Male, Male, Female, Female
R, R, L, R, R
.86, .67, .88, .78, .81
</code></pre>

<p>I want to read that into a df, so that I have:</p>

<pre><code>    Male        Female
    R       L   R
0   .86 .67 .88 .78 .81
</code></pre>

<p>I did:</p>

<pre><code>df = pd.read_csv('file.csv', header=[0,1])
</code></pre>

<p>But <code>headers</code> does not cut it. Which results in</p>

<pre><code>Empty DataFrame
Columns: [(Male, R), (Male, R), (Male, L), (Female, R), (Female, R)]
Index: []
</code></pre>

<p>Yet, the docs on headers says:</p>

<pre><code>(...)Can be a list of integers that specify row
locations for a multi-index on the columns E.g. [0,1,3]
</code></pre>

<p>What am I doing wrong? How can I possibly make it work? </p>
";;0;;2014-01-23T20:41:09.420;1.0;21318865;2014-01-24T04:04:49.687;2014-01-23T20:55:58.400;;1958583.0;;1958583.0;;1;12;<csv><pandas><multi-index>;Read multi-index on the columns from csv file;5225.0
9774;9774;21320011.0;2.0;"<p>I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with <code>if x in df['id']</code>. I thought this was working, except when I fed it a value that I knew was not in the column <code>43 in df['id']</code> it still returned <code>True</code>. When I subset to a data frame only containing entries matching the missing id <code>df[df['id'] == 43]</code> there are, obviously, no entries in it. How to I determine if a column in a Pandas data frame contains a particular value and why doesn't my current method work? (FYI, I have the same problem when I use the implementation in this <a href=""https://stackoverflow.com/a/19630449/2327821"">answer</a> to a similar question).</p>
";;0;;2014-01-23T21:41:11.000;1.0;21319929;2016-12-30T13:19:41.727;2017-05-23T12:34:08.677;;-1.0;;2327821.0;;1;26;<python><pandas>;How to determine whether a Pandas Column contains a particular value;42502.0
9839;9839;21361994.0;4.0;"<p>Environment: Python 2.7, matplotlib 1.3, IPython notebook 1.1, linux, chrome. The code is in one single input cell, using <code>--pylab=inline</code></p>

<p>I want to use IPython notebook and pandas to consume a stream and dynamically update a plot every 5 seconds. </p>

<p>When I just use print statement to print the data in text format, it works perfectly fine: the output cell just keeps printing data and adding new rows. But when I try to plot the data (and then update it in a loop), the plot never show up in the output cell. But if I remove the loop, just plot it once. It works fine.</p>

<p>Then I did some simple test:</p>

<pre><code>i = pd.date_range('2013-1-1',periods=100,freq='s')
while True:
    plot(pd.Series(data=np.random.randn(100), index=i))
    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one
    time.sleep(5)
</code></pre>

<p>The output will not show anything until I manually interrupt the process (ctrl+m+i). And after I interrupt it, the plot shows correctly as multiple overlapped lines. But what I really want is a plot that shows up and gets updated every 5 seconds (or whenever the <code>plot()</code> function gets called, just like what print statement outputs I mentioned above, which works well). Only showing the final chart after the cell is completely done is NOT what i want.</p>

<p>I even tried to explicitly add draw() function after each <code>plot()</code>, etc. None of them works. Wonder how to dynamically update a plot by a for/while loop within one cell in IPython notebook.</p>
";;0;;2014-01-26T06:10:38.663;24.0;21360361;2015-11-15T13:59:52.660;2015-08-14T02:52:40.910;;1832942.0;;3236895.0;;1;43;<python><matplotlib><pandas><ipython><ipython-notebook>;how to dynamically update a plot in a loop in ipython notebook (within one cell);30112.0
9877;9877;;1.0;"<p>My first SO question:
I am confused about this behavior of apply method of groupby in pandas (0.12.0-4), it appears to apply the function TWICE to the first row of a data frame. For example:</p>

<pre><code>&gt;&gt;&gt; from pandas import Series, DataFrame
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})
&gt;&gt;&gt; print(df)
   class  count  
0     A      1  
1     B      0    
2     C      2
</code></pre>

<p>I first check that the groupby function works ok, and it seems to be fine:</p>

<pre><code>&gt;&gt;&gt; for group in df.groupby('class', group_keys = True):
&gt;&gt;&gt;     print(group)
('A',   class  count
0     A      1)
('B',   class  count
1     B      0)
('C',   class  count
2     C      2)
</code></pre>

<p>Then I try to do something similar using apply on the groupby object and I get the first row output twice:</p>

<pre><code>&gt;&gt;&gt; def checkit(group):
&gt;&gt;&gt;     print(group)
&gt;&gt;&gt; df.groupby('class', group_keys = True).apply(checkit)
  class  count
0     A      1
  class  count
0     A      1
  class  count
1     B      0
  class  count
2     C      2
</code></pre>

<p>Any help would be appreciated! Thanks.</p>

<p>Edit: @Jeff provides the answer below. I am dense and did not understand it immediately, so here is a simple example to show that despite the double printout of the first group in the example above, the apply method operates only once on the first group and does not mutate the original data frame:</p>

<pre><code>&gt;&gt;&gt; def addone(group):
&gt;&gt;&gt;     group['count'] += 1
&gt;&gt;&gt;     return group

&gt;&gt;&gt; df.groupby('class', group_keys = True).apply(addone)
&gt;&gt;&gt; print(df)

      class  count
0     A      1
1     B      0
2     C      2
</code></pre>

<p>But by assigning the return of the method to a new object, we see that it works as expected:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>df2 = df.groupby('class', group_keys = True).apply(addone)
      print(df2)</p>
    </blockquote>
  </blockquote>
</blockquote>

<pre><code>      class  count
0     A      2
1     B      1
2     C      3
</code></pre>
";;9;;2014-01-27T19:37:56.617;6.0;21390035;2016-06-17T12:48:30.443;2016-06-17T12:48:30.443;;428862.0;;3240714.0;;1;20;<python-2.7><pandas><group-by>;Python pandas groupby object apply method duplicates first group;3453.0
9913;9913;21415990.0;1.0;"<p>I'm working with boolean index in Pandas.
The question is why the statement:</p>

<pre><code>a[(a['some_column']==some_number) &amp; (a['some_other_column']==some_other_number)]
</code></pre>

<p>works fine whereas</p>

<pre><code>a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]
</code></pre>

<p>exists with error?</p>

<p>Example:</p>

<pre><code>a=pd.DataFrame({'x':[1,1],'y':[10,20]})

In: a[(a['x']==1)&amp;(a['y']==10)]
Out:    x   y
     0  1  10

In: a[(a['x']==1) and (a['y']==10)]
Out: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()
</code></pre>
";;3;;2014-01-28T20:04:04.020;16.0;21415661;2017-07-18T19:00:02.870;2014-01-28T20:16:44.703;;2988577.0;;2988577.0;;1;32;<python><pandas><boolean-logic>;Logic operator for boolean indexing in Pandas;38894.0
9956;9956;21441621.0;2.0;"<p>Is there an easy method in pandas to invoke <code>groupby</code> on a range of values increments? For instance given the example below can I bin and group column <code>B</code> with a <code>0.155</code> increment so that for example, the first couple of groups in column <code>B</code> are divided into ranges between <code>0, -0.155, 0.155 - 0.31 ...</code></p>

<pre><code>import numpy as np
import pandas as pd
df=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})

     A         B
0  0.383493  0.250785
1  0.572949  0.139555
2  0.652391  0.401983
3  0.214145  0.696935
4  0.848551  0.516692
</code></pre>

<p>Alternatively I could first categorize the data by those increments into a new column and subsequently use <code>groupby</code> to determine any relevant statistics that may be applicable in column <code>A</code>?</p>
";;0;;2014-01-29T19:55:27.747;10.0;21441259;2016-10-21T16:58:46.013;2016-10-21T16:58:46.013;;5014218.0;;1041814.0;;1;33;<python><group-by><pandas>;Pandas Groupby Range of Values;13283.0
9964;9964;;1.0;"<p><code>pandas</code> has support for multi-level column names:</p>

<pre><code>&gt;&gt;&gt;  x = pd.DataFrame({'instance':['first','first','first'],'foo':['a','b','c'],'bar':rand(3)})
&gt;&gt;&gt; x = x.set_index(['instance','foo']).transpose()
&gt;&gt;&gt; x.columns
MultiIndex
[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]
&gt;&gt;&gt; x
instance     first                    
foo              a         b         c
bar       0.102885  0.937838  0.907467
</code></pre>

<p>This feature is very useful since it allows multiple versions of the same dataframe to be appended 'horizontally' with the 1st level of the column names (in my example <code>instance</code>) distinguishing the instances.</p>

<p>Imagine I already have a dataframe like this:</p>

<pre><code>                 a         b         c
bar       0.102885  0.937838  0.907467
</code></pre>

<p>Is there a nice way to add another level to the column names, similar to this for row index:</p>

<pre><code>x['instance'] = 'first'
x.set_level('instance',append=True)
</code></pre>
";;4;;2014-01-29T22:24:06.743;5.0;21443963;2014-10-13T03:26:18.730;;;;;2071807.0;;1;12;<python><pandas>;Pandas: Multilevel column names;8434.0
9990;9990;21463854.0;1.0;"<p>I have been reading this <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-view-versus-copy"" rel=""nofollow"">link</a> on ""Returning a view versus a copy"". I do not really get how the <strong>chained assignment</strong> concept in Pandas works and how the usage of <code>.ix()</code>, <code>.iloc()</code>, or <code>.loc()</code> affects it.</p>

<p>I get the <code>SettingWithCopyWarning</code> warnings for the following lines of codes, where <code>data</code> is a Panda dataframe and <code>amount</code> is a column (Series) name in that dataframe:</p>

<pre><code>data['amount'] = data['amount'].astype(float)

data[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)

data[""amount""].fillna(mean_avg, inplace=True)
</code></pre>

<p>Looking at this code, is it obvious that I am doing something suboptimal? If so, can you let me know the replacement code lines?</p>

<p>I am aware of the below warning and like to think that the warnings in my case are false positives: </p>

<blockquote>
  <p>The chained assignment warnings / exceptions are aiming to inform the
  user of a possibly invalid assignment. There may be false positives;
  situations where a chained assignment is inadvertantly reported.</p>
</blockquote>

<p><strong>EDIT :</strong> the code leading to the first copy warning error.</p>

<pre><code>data['amount'] = data.apply(lambda row: function1(row,date,qty), axis=1) 
data['amount'] = data['amount'].astype(float)

def function1(row,date,qty):
    try:
        if(row['currency'] == 'A'):
            result = row[qty]
        else:
            rate = lookup[lookup['Date']==row[date]][row['currency'] ]
            result = float(rate) * float(row[qty])
        return result
    except ValueError: # generic exception clause
        print ""The current row causes an exception:""
</code></pre>
";;0;;2014-01-30T17:37:52.117;7.0;21463589;2016-08-09T14:58:09.547;2016-08-09T14:58:09.547;;196844.0;;988125.0;;1;13;<python><pandas><copy>;Pandas: Chained assignments;5402.0
10033;10033;21487560.0;5.0;"<p>Suppose I have the following code that plots something very simple using pandas:</p>

<pre><code>import pandas as pd
values = [[1,2], [2,5]]
df2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')
</code></pre>

<p><img src=""https://i.stack.imgur.com/LIkH3.png"" alt=""Output""></p>

<p>How do I easily set x and y-labels while preserving my ability to use specific colormaps? I noticed that the plot() wrapper for pandas dataframes doesn't take any parameters specific for that.</p>
";;0;;2014-01-31T18:23:09.697;7.0;21487329;2017-05-01T03:56:42.807;;;;;369710.0;;1;62;<python><matplotlib><pandas>;Add x and y labels to a pandas plot;62710.0
10037;10037;;1.0;"<p>Consider this timeseries, the cumulative number of edits in a Wikipedia category. </p>

<pre><code>In [555]:
cum_edits.head()
Out[555]:
2001-08-31 23:37:28    1
2001-09-01 05:09:28    2
2001-09-18 10:01:17    3
2001-10-27 06:52:45    4
2001-10-27 07:01:45    5
Name: edits, dtype: int64
In [565]:
cum_edits.tail()
Out[565]:
2014-01-29 16:05:15    53254
2014-01-29 16:07:09    53255
2014-01-29 16:11:43    53256
2014-01-29 18:09:44    53257
2014-01-29 18:12:09    53258
Name: edits, dtype: int64
</code></pre>

<p>I have am to graph this like so:</p>

<pre><code>In [567]:

cum_edits.plot()

Out[567]:

&lt;matplotlib.axes.AxesSubplot at 0x1359c810&gt;
</code></pre>

<p><img src=""https://i.stack.imgur.com/Zg91t.png"" alt=""cummulative edits""></p>

<p>I would like to plot also vertical lines, after every <code>total_edits/n ; e.g. n=10</code> edits. I calculate these easily.</p>

<pre><code>In [568]:

dates

Out[568]:

[Timestamp('2006-06-04 04:46:22', tz=None),
 Timestamp('2007-01-28 23:53:02', tz=None),
 Timestamp('2007-09-16 10:52:02', tz=None),
 Timestamp('2008-04-28 21:20:40', tz=None),
 Timestamp('2009-04-12 22:07:13', tz=None),
 Timestamp('2010-04-09 18:45:37', tz=None),
 Timestamp('2011-03-28 23:38:12', tz=None),
 Timestamp('2012-05-24 13:44:35', tz=None),
 Timestamp('2013-03-05 17:57:29', tz=None),
 Timestamp('2014-01-29 16:05:15', tz=None)]
</code></pre>

<p>Normally one can use <code>axvline()</code> although I encounter two problems. Even if I call <code>plt.axvline(x=0.5, color='r')</code> just to produce an arbitrary line, I do not see it on top of the pandas plot. I am using IPython with <code>%pylab inline</code> by the way. And secondly, I do not now how to translate the dates into x position that are being used in <code>cum_edits.plot()</code> since the translation is invisible to me. Should I go about producing these vertical lines?</p>
";;3;;2014-01-31T19:05:54.443;5.0;21488085;2014-01-31T20:37:21.727;2014-01-31T19:37:36.890;;2997574.0;;1704279.0;;1;11;<python><matplotlib><pandas>;Pandas graphing a timeseries, with vertical lines at selected dates;6872.0
10046;10046;21500413.0;1.0;"<p>I'm trying to create a single Pandas DataFrame object from a deeply nested JSON string. </p>

<p>The JSON schema is:</p>

<pre><code>{""intervals"": [
{
pivots: ""Jane Smith"",
""series"": [
    {
        ""interval_id"": 0,
        ""p_value"": 1
       },
     {
         ""interval_id"": 1,
         ""p_value"": 1.1162791357932633e-8
     },
   {
        ""interval_id"": 2,
        ""p_value"": 0.0000028675012051504467
     }
    ],
   },
  {

""pivots"": ""Bob Smith"",
  ""series"": [
       {
            ""interval_id"": 0,
            ""p_value"": 1
           },
         {
             ""interval_id"": 1,
            ""p_value"": 1.1162791357932633e-8
         },
       {
            ""interval_id"": 2,
            ""p_value"": 0.0000028675012051504467
         }
       ]
     }
    ]
 }
</code></pre>

<p><strong>Desired Outcome</strong> I need to flatten this to produce a table:</p>

<pre><code>Actor Interval_id Interval_id Interval_id ... 
Jane Smith      1         1.1162        0.00000 ... 
Bob Smith       1         1.1162        0.00000 ... 
</code></pre>

<p>The first column is the <code>Pivots</code> values, and the remaining columns are the values of the keys <code>interval_id</code> and <code>p_value</code> stored in the list <code>series</code>.</p>

<p>So far i've got</p>

<pre><code>import requests as r
import pandas as pd
actor_data = r.get(""url/to/data"").json['data']['intervals']
df = pd.DataFrame(actor_data)
</code></pre>

<p><code>actor_data</code> is a list where the length is equal to the number of individuals ie <code>pivots.values()</code>. The df object simply returns</p>

<pre><code>&lt;bound method DataFrame.describe of  pivots             Series
0           Jane Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...
1           Bob Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...
.
.
.
</code></pre>

<p>How can I iterate through that <code>series</code> list to get to the dict values and create N distinct columns? Should I try to create a DataFrame for the <code>series</code> list, reshape it,and then do a column bind with the actor names? </p>

<p>UPDATE:</p>

<pre><code>pvalue_list = [i['p_value'] for i in json_data['series']]
</code></pre>

<p>this gives me a list of lists. Now I need to figure out how to add each list as a row in a DataFrame. </p>

<pre><code>value_list = []
for i in pvalue_list:
    pvs = [j['p_value'] for j in i]
    value_list = value_list.append(pvs)
return value_list
</code></pre>

<p>This returns a NoneType</p>

<p><strong>Solution</strong></p>

<pre><code>def get_hypthesis_data():
    raw_data = r.get(""/url/to/data"").json()['data']
    actor_dict = {}
    for actor_series in raw_data['intervals']:
        actor = actor_series['pivots']
        p_values = []
        for interval in actor_series['series']:
            p_values.append(interval['p_value'])
        actor_dict[actor] = p_values
    return pd.DataFrame(actor_dict).T
</code></pre>

<p>This returns the correct DataFrame. I transposed it so the individuals were rows and not columns. </p>
";;1;;2014-02-01T04:21:07.550;5.0;21494030;2014-02-03T19:51:27.720;2014-02-03T19:51:27.720;;530763.0;;530763.0;;1;14;<python><json><pandas>;Create a Pandas DataFrame from deeply nested JSON;11613.0
10107;10107;21546823.0;3.0;"<p>I am loading a txt file containig a mix of float and string data. I want to store them in an array where I can access each element. Now I am just doing </p>

<pre><code>import pandas as pd

data = pd.read_csv('output_list.txt', header = None)
print data
</code></pre>

<p>This is the structure of the input file: <code>1 0 2000.0 70.2836942112 1347.28369421 /file_address.txt</code>. </p>

<p>Now the data are imported as a unique column. How can I divide it, so to store different elements separately (so I can call <code>data[i,j]</code>)? And how can I define a header?</p>
";;0;;2014-02-04T07:48:58.177;6.0;21546739;2017-08-13T06:03:43.630;;;;;2828086.0;;1;15;<python><io><pandas>;Load data from txt with pandas;57146.0
10139;10139;27889133.0;2.0;"<p>I've written a bunch of code on the assumption that I was going to use Numpy arrays. Turns out the data I am getting is loaded through Pandas. I remember now that I loaded it in Pandas because I was having some problems loading it in Numpy. I believe the data was just too large.</p>

<p>Therefore I was wondering, is there a difference in computational ability when using Numpy vs Pandas?</p>

<p>If Pandas is more efficient then I would rather rewrite all my code for Pandas but if there is no more efficiency then I'll just use a numpy array...</p>
";;3;;2014-02-05T03:04:55.447;2.0;21567842;2015-01-11T16:04:56.743;2014-05-08T20:07:59.000;;832621.0;;1455043.0;;1;18;<python><numpy><pandas>;Is there a difference in computation for Numpy vs Pandas?;13667.0
10193;10193;21607530.0;2.0;"<p>I am parsing data from an Excel file that has extra white space in some of the column headings.</p>

<p>When I check the columns of the resulting dataframe, like so:</p>

<p><code>df.columns</code></p>

<p>The result looks like this:</p>

<p><code>Index(['Year', 'Month ', 'Value'])</code></p>

<p>Consequently, I can't run </p>

<p><code>df[""Month""]</code></p>

<p>Because it will tell me the column is not found, as I asked for ""Month"", not ""Month "".</p>

<p>My question, then, is how can I strip out the unwanted white space from the column headings?</p>
";;0;;2014-02-06T15:26:25.683;13.0;21606987;2016-05-13T08:12:28.397;2014-03-01T02:14:05.590;;73371.0;;73371.0;;1;26;<pandas>;How can I strip the whitespace from Pandas DataFrame headers?;14508.0
10196;10196;;2.0;"<p>I'm probably doing something very stupid, but I'm stumped.</p>

<p>I have a dataframe and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:</p>

<pre><code>df[df.my_channel &gt; 20000].my_channel = 0
</code></pre>

<p>If I copy the channel into a new data frame it's simple:</p>

<pre><code>df2 = df.my_channel 

df2[df2 &gt; 20000] = 0
</code></pre>

<p>this does exactly what I want, but seems not to work with the channel as part of the original dataframe.</p>

<p>Thanks is advance.</p>

<p>Ben</p>
";;1;;2014-02-06T16:16:54.490;7.0;21608228;2017-07-14T16:47:41.887;;;;;3215592.0;;1;26;<python><replace><pandas><conditional>;Conditional Replace Pandas;30186.0
10275;10275;21655256.0;4.0;"<p>I am trying to make a simple scatter plot in pyplot using a Pandas DataFrame object, but want an efficient way of plotting two variables but have the symbols dictated by a third column (key). I have tried various ways using df.groupby, but not successfully. A sample df script is below. This colours the markers according to 'key1', but Id like to see a legend with 'key1' categories. Am I close? Thanks.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))
df['key1'] = (4,4,4,6,6,6,8,8,8,8)
fig1 = plt.figure(1)
ax1 = fig1.add_subplot(111)
ax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)
plt.show()
</code></pre>
";;0;;2014-02-09T02:51:57.073;25.0;21654635;2017-07-04T06:43:19.070;;;;;2989613.0;;1;42;<python><matplotlib><pandas>;Scatter plots in Pandas/Pyplot: How to plot by category;44949.0
10319;10319;;3.0;"<p>I'm trying to run code provided by yhat <a href=""http://blog.yhathq.com/posts/random-forests-in-python.html"" rel=""noreferrer"">in their article about random forests in Python</a>, but I keep getting following error message:</p>

<pre><code>File ""test_iris_with_rf.py"", line 11, in &lt;module&gt;
    df['species'] = pd.Factor(iris.target, iris.target_names)
AttributeError: 'module' object has no attribute 'Factor'
</code></pre>

<p>Code:</p>

<pre><code>from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np

iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
print df
print iris.target_names
df['is_train'] = np.random.uniform(0, 1, len(df)) &lt;= .75

df['species'] = pd.Factor(iris.target, iris.target_names)

df.head()
</code></pre>
";;0;;2014-02-10T22:42:27.767;7.0;21689423;2017-01-26T16:49:11.713;2015-08-12T11:37:53.683;;1349048.0;;2672986.0;;1;16;<python><python-2.7><pandas>;pandas attribute error : no attribute 'Factor' found;6525.0
10354;10354;21720133.0;2.0;"<p>I have a dataframe, df, that has some columns of type float64, while the others are of object. Due to the mixed nature, I cannot use </p>

<pre><code>df.fillna('unknown') #getting error ""ValueError: could not convert string to float:""
</code></pre>

<p>as the error happened with the columns whose type is float64 (what a misleading error message!)</p>

<p>so I'd wish that I could do something like </p>

<pre><code>for col in df.columns[&lt;dtype == object&gt;]:
    df[col] = df[col].fillna(""unknown"")
</code></pre>

<p>So my question is if there is any such filter expression that I can use with df.columns?</p>

<p>I guess alternatively, less elegantly, I could do:  </p>

<pre><code> for col in df.columns:
        if (df[col].dtype == dtype('O')): # for object type
            df[col] = df[col].fillna('') 
            # still puzzled, only empty string works as replacement, 'unknown' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" 
</code></pre>

<p>I also would like to know why in the above code replacing '' with 'unknown' the code would work for certain cells but failed with a cell with the error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" </p>

<p>Thanks a lot!</p>

<p>Yu</p>
";;0;;2014-02-12T06:09:00.717;4.0;21720022;2015-06-09T18:34:13.417;;;;;126164.0;;1;17;<python><pandas><dataframe><data-cleansing>;Find all columns of dataframe in Pandas whose type is float, or a particular type?;12649.0
10376;10376;21734254.0;2.0;"<p>I'm quite new to Python and Pandas so this might be an obvious question.  </p>

<p>I have a dataframe with ages listed in it.  I want to create a new field with an age banding.  I can use the lambda statement to capture a single if / else statement but I want to use multiple if's e.g. <code>if age &lt; 18 then 'under 18' elif age &lt; 40 then 'under 40' else '&gt;40'</code>.</p>

<p>I don't think I can do this using lambda but am not sure how to do it in a different way.  I have this code so far:</p>

<pre><code>import pandas as pd
import numpy as n

d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }

df = pd.DataFrame(d)

df['Age_Group'] =  df['Age'].map(lambda x: '&lt;18' if x &lt; 19 else '&gt;18')

print(df)
</code></pre>
";;0;;2014-02-12T16:35:48.190;18.0;21733893;2016-03-17T11:57:14.783;2014-02-12T16:53:27.643;;832136.0;;3302483.0;;1;22;<python><lambda>;Pandas dataframe add a field based on multiple if statements;32357.0
10384;10384;;6.0;"<p>I am trying to set a variable to equal today's date.</p>

<p>I looked this up and found a related article:</p>

<p><a href=""https://stackoverflow.com/questions/5023788/set-today-date-as-default-value-in-the-model"">Set today date as default value in the model</a></p>

<p>However, this didn't particularly answer my question.</p>

<p>I used the suggested:</p>

<pre><code>dt.date.today
</code></pre>

<p>But after </p>

<pre><code>import datetime as dt     
date = dt.date.today
print date
 &lt;built-in method today of type object at 0x000000001E2658B0&gt;

 Df['Date'] = date
</code></pre>

<p>I didn't get what I actually wanted which as a clean date format of today's date...in Month/Day/Year.</p>

<p>How can I create a variable of today's day in order for me to input that variable in a DataFrame?</p>
";;3;;2014-02-12T20:10:39.777;3.0;21738566;2017-05-31T14:59:30.413;2017-05-23T12:18:17.367;;-1.0;;3137488.0;;1;17;<python><date><datetime><formatting><pandas>;"How to set a variable to be ""Today's"" date in Python/Pandas";42573.0
10400;10400;21755752.0;3.0;"<p>I have a dataframe, something like:</p>

<pre><code>     foo  bar  qux
0    a    1    3.14
1    b    3    2.72
2    c    2    1.62
3    d    9    1.41
4    e    3    0.58
</code></pre>

<p>and I would like to add a 'total' row to the end of the dataframe:</p>

<pre><code>     foo  bar  qux
0    a    1    3.14
1    b    3    2.72
2    c    2    1.62
3    d    9    1.41
4    e    3    0.58
5    tot  15   9.47
</code></pre>

<p>I've tried to use the <code>sum</code> command but I end up with a Series, which although I can convert back to a Dataframe, doesn't maintain the data types:</p>

<pre><code>tot_row = pd.DataFrame(df.sum()).T
tot_row['foo'] = 'tot'
tot_row.dtypes:
     foo    object
     bar    object
     qux    object
</code></pre>

<p>I would like to maintain the data types from the original data frame as I need to apply other operations to the total row, something like:</p>

<pre><code>baz = 2*tot_row['qux'] + 3*tot_row['bar']
</code></pre>
";;1;;2014-02-13T11:06:20.507;3.0;21752399;2017-07-11T05:56:21.437;;;;;2970186.0;;1;18;<python><pandas>;Pandas dataframe total row;14345.0
10436;10436;21768034.0;1.0;"<p>I have the following pandas dataframe:</p>

<pre><code>In [8]:  dfalph.head()

Out[8]:         token    year    uses  books
         386    xanthos  1830    3     3
         387    xanthos  1840    1     1
         388    xanthos  1840    2     2
         389    xanthos  1868    2     2
         390    xanthos  1875    1     1
</code></pre>

<p>I aggregate the rows with duplicate token and years like so:</p>

<pre><code>In [63]:  dfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])
          dfalph.columns = dfalph.columns.droplevel(1)
          dfalph.head()

Out[63]:                 uses  books
          token    year     
          xanthos  1830  3     3
                   1840  3     3
                   1867  2     2
                   1868  2     2
                   1875  1     1
</code></pre>

<p>Instead of having the 'token' and 'year' fields in the index, I would like to return them to columns and have an integer index. </p>
";;0;;2014-02-13T23:32:14.150;4.0;21767900;2014-02-13T23:42:13.130;;;;;2774479.0;;1;13;<python><pandas>;How to move pandas data from index to column after multiple groupby;8218.0
10447;10447;21772078.0;4.0;"<p>I have a large dataframe in pandas that apart from the column used as index is supposed to have only numeric values:</p>

<pre><code>df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})
df = df.set_index(""item"")
</code></pre>

<p>How can I find the row of the dataframe <code>df</code> that has a non-numeric value in it? In this example it's the fourth row in the dataframe, which has the string <code>""bad""</code> in the <code>a</code> column. How can this row be found programmatically? thanks.</p>
";;0;;2014-02-14T04:54:02.580;10.0;21771133;2017-08-05T17:09:41.470;2017-08-05T17:09:41.470;;4909087.0;;248237.0;;1;18;<python><pandas><dataframe>;Finding non-numeric rows in dataframe in pandas?;26480.0
10466;10466;21787325.0;1.0;"<p>I am new to using DataFrame and I would like to know how to perform a SQL equivalent of left outer join on multiple columns on a series of tables</p>

<p>Example:</p>

<pre><code>df1: 
Year    Week    Colour    Val1 
2014       A       Red      50
2014       B       Red      60
2014       B     Black      70
2014       C       Red      10
2014       D     Green      20

df2:
Year    Week    Colour    Val2
2014       A     Black      30
2014       B     Black     100
2014       C     Green      50
2014       C       Red      20
2014       D       Red      40

df3:
Year    Week    Colour    Val3
2013       B       Red      60
2013       C     Black      80
2013       B     Black      10
2013       D     Green      20
2013       D       Red      50
</code></pre>

<p>Essentially I want to do something like this SQL code (Notice that df3 is not joined on Year):</p>

<pre><code>SELECT df1.*, df2.Val2, df3.Val3
FROM df1
  LEFT OUTER JOIN df2
    ON df1.Year = df2.Year
    AND df1.Week = df2.Week
    AND df1.Colour = df2.Colour
  LEFT OUTER JOIN df3
    ON df1.Week = df3.Week
    AND df1.Colour = df3.Colour
</code></pre>

<p>The result should look like:</p>

<pre><code>Year    Week    Colour    Val1    Val2    Val3
2014       A       Red      50    Null    Null
2014       B       Red      60    Null      60
2014       B     Black      70     100    Null
2014       C       Red      10      20    Null
2014       D     Green      20    Null    Null
</code></pre>

<p>I have tried using merge and join but can't figure out how to do it on multiple tables and when there are multiple joints involved. Could someone help me on this please?</p>

<p>Thanks</p>
";;0;;2014-02-14T18:07:39.330;13.0;21786490;2017-08-26T13:25:03.640;2017-08-26T13:25:03.640;;1773367.0;;3311225.0;;1;30;<python><sql><merge><pandas>;Pandas left outer join multiple dataframes on multiple columns;45960.0
10486;10486;21800319.0;2.0;"<p>Given a DataFrame with a column ""BoolCol"", we want to find the indexes of the DataFrame in which the values for ""BoolCol"" == True</p>

<p>I currently have the iterating way to do it, which works perfectly:</p>

<pre><code>for i in range(100,3000):
    if df.iloc[i]['BoolCol']== True:
         print i,df.iloc[i]['BoolCol']
</code></pre>

<p>But this is not the correct panda's way to do it.
After some research, I am currently using this code:</p>

<pre><code>df[df['BoolCol'] == True].index.tolist()
</code></pre>

<p>This one gives me a list of indexes, but they dont match, when I check them by doing:</p>

<pre><code>df.iloc[i]['BoolCol']
</code></pre>

<p>The result is actually False!!</p>

<p>Which would be the correct Pandas way to do this?</p>
";;0;;2014-02-15T16:18:11.270;32.0;21800169;2017-04-14T19:05:13.930;;;;;2138906.0;;1;66;<python><indexing><pandas>;Python Pandas: Get index of rows which column matches certain value;149908.0
10529;10529;21869063.0;3.0;"<p>The <code>View</code> is a very useful function to allow me to see cross-section of large data frames in R. </p>

<p>Is there any equivalent of R's <code>View</code> function for Python's pandas <code>DataFrame</code>? </p>

<p>I use <code>RStudio</code> for R and <code>PyCharm</code> for Python. </p>
";;6;;2014-02-17T16:46:24.880;3.0;21834676;2017-04-22T17:31:54.140;2014-11-11T18:48:01.910;user212218;;;2526657.0;;1;12;<python><r><pandas><pycharm>;equivalent of R's View for Python's pandas;3070.0
10573;10573;;3.0;"<p>I've seen some reports PyCharm is slow but I'm having an issue that seems that's too slow even compared to normal operation.</p>

<p>I have a big set of data in a pandas dataframe (read from a 440 MB csv file).</p>

<p>When I'm using the ipython console inside PyCharm, every time I try to handle that data, let's say, I write <code>my_data.</code> it just hangs there for about 30 seconds.</p>

<p>I don't really understand what is going on, but it seems PyCharm is going trough all the data to find some smart auto completion (which is a really dumb thing to do).</p>

<p>Any way to deactivate this behavior?</p>
";;7;;2014-02-19T00:14:09.850;5.0;21868369;2016-11-02T08:00:40.420;2014-02-19T00:31:13.243;;865662.0;;865662.0;;1;15;<pandas><pycharm><dataframe>;PyCharm hanging for a long time in iPython console with big data;1146.0
10602;10602;21902162.0;2.0;"<p>I have a simple 2 column csv file called st1.csv:</p>

<pre><code>GRID    St1  
1457    614  
1458    657  
1459    679  
1460    732  
1461    754  
1462    811  
1463    748  
</code></pre>

<p>However, when I try to read the csv file, the first column is not loaded:</p>

<pre><code>a = pandas.DataFrame.from_csv('st1.csv')  
a.columns
</code></pre>

<p><strong>outputs:</strong></p>

<pre><code> Index([u'ST1'], dtype=object)
</code></pre>

<p>Why is the first column not being read?</p>
";;2;;2014-02-20T08:26:31.797;3.0;21902080;2017-05-10T15:06:09.123;2014-02-20T08:32:15.570;;1798187.0;;308827.0;;1;17;<python><csv><pandas>;python pandas not reading first column from csv file;12247.0
10649;10649;21940107.0;1.0;"<p>I would like to have the 3 columns of a numpy array</p>

<pre><code>px[:,:,0]
px[:,:,1]
px[:,:,0]
</code></pre>

<p>into a pandas Dataframe.</p>

<p>Should I use?</p>

<pre><code>df = pd.DataFrame(px, columns=['R', 'G', 'B'])
</code></pre>

<p>Thank you</p>

<p>Hugo</p>
";;7;;2014-02-21T15:49:07.600;4.0;21938932;2014-02-21T16:41:56.587;;;;;678321.0;;1;13;<python><numpy><pandas>;How do I convert a numpy array into a pandas dataframe?;41911.0
10693;10693;21961491.0;2.0;"<p>I have a pandas DataFrame that looks like this <code>training.head()</code></p>

<p><img src=""https://i.stack.imgur.com/G5PWJ.png"" alt=""enter image description here""></p>

<p>The DataFrame has been sorted by date. I'd like to make a scatterplot where the date of the campaign is on the x axis and the rate of success is on the y axis. I was able to get a line graph by using <code>training.plot(x='date',y='rate')</code>. However, when I changed that to <code>training.plot(kind='scatter',x='date',y='rate')</code> I get an error: KeyError: u'no item named date'</p>

<p>Why does my index column go away when I try to make a scatterplot? Also, I bet I need to do something with that date field so that it doesn't get treated like a simple string, don't I?</p>

<p>Extra credit, what would I do if I wanted each of the account numbers to plot with a different color?</p>
";;0;;2014-02-22T22:18:19.053;3.0;21961360;2015-04-20T08:23:05.357;;;;;813100.0;;1;11;<python><matplotlib><pandas>;matplotlib plot datetime in pandas DataFrame;16168.0
10727;10727;21989204.0;3.0;"<p>I have two Pandas DataFrames that I'm hoping to plot in single figure. I'm using IPython notebook.</p>

<p>I would like the legend to show the label for both of the DataFrames, but so far I've been able to get only the latter one to show. Also any suggestions as to how to go about writing the code in a more sensible way would be appreciated. I'm new to all this and don't really understand object oriented plotting.</p>

<pre><code>%pylab inline
import pandas as pd

#creating data

prng = pd.period_range('1/1/2011', '1/1/2012', freq='M')
var=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])
shares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])

#plotting

ax=var.total.plot(label='Variance')
ax=shares.average.plot(secondary_y=True,label='Average Age')
ax.left_ax.set_ylabel('Variance of log wages')
ax.right_ax.set_ylabel('Average age')
plt.legend(loc='upper center')
plt.title('Wage Variance and Mean Age')
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/ODceD.png"" alt=""Legend is missing one of the labels""></p>
";;0;;2014-02-24T12:44:55.353;4.0;21988196;2017-02-23T08:39:06.650;;;;;2998998.0;;1;14;<python><matplotlib><plot><pandas>;Legend only shows one label when plotting with pandas;11326.0
10767;10767;22006514.0;2.0;"<p>I have the following DataFrame from a SQL query:</p>

<pre><code>(Pdb) pp total_rows
     ColumnID  RespondentCount
0          -1                2
1  3030096843                1
2  3030096845                1
</code></pre>

<p>and I want to pivot it like this:</p>

<pre><code>total_data = total_rows.pivot_table(cols=['ColumnID'])

(Pdb) pp total_data
ColumnID         -1            3030096843   3030096845
RespondentCount            2            1            1

[1 rows x 3 columns]


total_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]

{3030096843: 1, 3030096845: 1, -1: 2}
</code></pre>

<p>but I want to make sure the 303 columns are casted as strings instead of integers so that I get this:</p>

<pre><code>{'3030096843': 1, '3030096845': 1, -1: 2}
</code></pre>
";;0;;2014-02-25T06:03:30.580;9.0;22005911;2017-08-23T21:32:24.417;2015-09-20T11:14:37.250;;1505120.0;;17176.0;;1;42;<python><numpy><pandas>;Convert Columns to String in Pandas;74333.0
10790;10790;22019831.0;1.0;"<p>I'm writing a script to reduce a large .xlsx file with headers into a csv, and then write a new csv file with only the required columns based on header name.</p>

<pre><code>import pandas
import csv

df = pandas.read_csv('C:\\Python27\\Work\\spoofing.csv')

time = df[""InviteTime (Oracle)""]
orignum = df[""Orig Number""]
origip = df[""Orig IP Address""]
destnum = df[""Dest Number""]

df.to_csv('output.csv', header=[time,orignum,origip,destnum])
</code></pre>

<p>The error I'm getting is with that last bit of code, and it says</p>

<pre><code>ValueError: Writing 102 cols but got 4 aliases
</code></pre>

<p>I'm sure i'm overlooking something stupid, but I've read over the to_csv documentation on the pandas website and I'm still at a loss. I know I'm using the to_csv parameters incorrectly but I can't seem to get my head around the documentation I guess.</p>

<p>Any help is appreciated, thanks!</p>
";;0;;2014-02-25T16:07:59.363;5.0;22019763;2015-04-25T13:05:24.960;;;;;2914183.0;;1;14;<python><csv><pandas>;Pandas Writing Dataframe Columns to csv;24908.0
10815;10815;22033364.0;3.0;"<p>I'd like to drop all values from a table if the rows = <code>nan</code> or <code>0</code>.</p>

<p>I know there's a way to do this using pandas i.e <code>pandas.dropna(how = 'all')</code> but I'd like a numpy method to remove rows with all <code>nan</code> or <code>0</code>.</p>

<p>Is there an efficient implementation of this?</p>
";;1;;2014-02-26T05:32:51.923;4.0;22032668;2014-02-26T17:30:38.960;;;;;2771315.0;;1;11;<python><numpy><pandas>;Numpy: Drop rows with all nan or 0 values;9273.0
10867;10867;22070926.0;1.0;"<p>This is my first attempt at plotting with python and I'm having problems creating a legend.</p>

<p>These are my imports:</p>

<pre><code>import matplotlib.pyplot as plt
import pandas
</code></pre>

<p>I load my data like this:</p>

<pre><code>data = pandas.read_csv( 'data/output/limits.dat', sep=r""\s+"", encoding = 'utf-8' )
</code></pre>

<p>and plot it like this:</p>

<pre><code>axdata = data.plot( label = '$|U|^{2}$' , x = 'mass', y = 'U2',
                    style = '-s', markeredgecolor = 'none' )
</code></pre>

<p>Apparently axdata is now an  <code>AxesSubplot</code>.</p>

<p>Now I want to create a legend as described <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend"">here</a> like this:</p>

<pre><code>plt.legend( (line1), ('label1') )
</code></pre>

<p>but I don't know how to extract a <code>line</code> object from an <code>AxesSubplot</code></p>

<p><code>plt.legend()</code> on its own works, but I only want some of my lines to feature in the legend. Is this the right approach? Is there another command I can use here?</p>

<p><strong>EDIT</strong>:</p>

<p>For example, if I try:</p>

<pre><code>plt.legend( [axdata], ['U2'])
</code></pre>

<p>I get the error:</p>

<pre><code>~/.virtualenvs/science/lib/python3.3/site-packages/matplotlib/legend.py:613:
UserWarning: Legend does not support Axes(0.125,0.1;0.775x0.8)
Use proxy artist instead.

http://matplotlib.sourceforge.net/users/legend_guide.html#using-proxy-artist

(str(orig_handle),))
</code></pre>

<p>I haven't worked out what a proxy artist is yet but I think it is a tool for when you are using a non-default graphical object, which I thought probably was not the case here because I am trying to produce a normal matlibplot plot. The words 'non-default' and 'normal' are mine - I'm not sure what they mean yet.</p>

<p><strong>ANOTHER EDIT:</strong> (because I misread the comment )</p>

<p><code>plt.legend()</code> on it's own doesn't output anything to the console but the resulting plot now has a legend auto-generated from the plotted data.</p>
";;3;;2014-02-27T13:38:49.367;4.0;22070263;2014-02-27T14:06:13.413;2014-02-27T13:56:25.550;;1527126.0;;1527126.0;;1;11;<python><matplotlib><pandas>;Create a legend with pandas and matplotlib.pyplot;13734.0
10887;10887;22082596.0;1.0;"<p>Is there a way to look back to a previous row, and calculate a new variable?  so as long as the previous row is the same case what is the (previous change) - (current change), and attribute it to the previous 'ChangeEvent' in new columns?</p>

<p>here is my DataFrame</p>

<pre><code>&gt;&gt;&gt; df
  ChangeEvent StartEvent  case              change      open  
0    Homeless   Homeless     1 2014-03-08 00:00:00 2014-02-08  
1       other   Homeless     1 2014-04-08 00:00:00 2014-02-08     
2    Homeless   Homeless     1 2014-05-08 00:00:00 2014-02-08      
3        Jail   Homeless     1 2014-06-08 00:00:00 2014-02-08     
4        Jail       Jail     2 2014-06-08 00:00:00 2014-02-08   
</code></pre>

<p>to add columns</p>

<pre><code>Jail  Homeless case
 0    6        1
 0    30       1
 0    0        1
</code></pre>

<p>... and so on</p>

<p>here is the df build</p>

<pre><code>import pandas as pd
import datetime as DT
d = {'case' : pd.Series([1,1,1,1,2]),
'open' : pd.Series([DT.datetime(2014, 3, 2), DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2)]),
'change' : pd.Series([DT.datetime(2014, 3, 8), DT.datetime(2014, 4, 8),DT.datetime(2014, 5, 8),DT.datetime(2014, 6, 8),DT.datetime(2014, 6, 8)]),
'StartEvent' : pd.Series(['Homeless','Homeless','Homeless','Homeless','Jail']),
'ChangeEvent' : pd.Series(['Homeless','irrelivant','Homeless','Jail','Jail']),
'close' : pd.Series([DT.datetime(2015, 3, 2), DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2)])}
df=pd.DataFrame(d)
</code></pre>
";;0;;2014-02-27T22:23:27.193;2.0;22081878;2014-02-27T23:36:29.370;2014-02-27T23:36:29.370;;2026611.0;;2026611.0;;1;13;<python><pandas>;get previous row's value and calculate new column pandas python;21522.0
10894;10894;22084742.0;5.0;"<p>Pandas is really great, but I am really surprised by how inefficient it is to retrieve values from a Pandas.DataFrame.  In the following toy example, even the DataFrame.iloc method is more than 100 times slower than a dictionary.  </p>

<p>The question: Is the lesson here just that dictionaries are the better way to look up values?  Yes, I get that that is precisely what they were made for.  But I just wonder if there is something I am missing about DataFrame lookup performance.</p>

<p>I realize this question is more ""musing"" than ""asking"" but I will accept an answer that provides insight or perspective on this.  Thanks.</p>

<pre><code>import timeit

setup = '''
import numpy, pandas
df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))
dictionary = df.to_dict()
'''

f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']

for func in f:
    print func
    print min(timeit.Timer(func, setup).repeat(3, 100000))
</code></pre>

<blockquote>
  <p>value = dictionary[5][5]</p>
  
  <p>0.130625009537</p>
  
  <p>value = df.loc[5, 5]</p>
  
  <p>19.4681699276</p>
  
  <p>value = df.iloc[5, 5]</p>
  
  <p>17.2575249672</p>
</blockquote>
";;0;;2014-02-28T01:24:19.190;13.0;22084338;2017-07-13T18:55:33.460;;;;;310441.0;;1;28;<python><dictionary><pandas>;Pandas DataFrame performance;13142.0
10901;10901;22086347.0;2.0;"<p>To filter a dataframe (df) by a single column, if we consider data with male and females we might:</p>

<pre><code>males = df[df[Gender]=='Male']
</code></pre>

<p>Question 1 - But what if the data spanned multiple years and i wanted to only see males for 2014?</p>

<p>In other languages I might do something like: </p>

<pre><code>if A = ""Male"" and if B = ""2014"" then 
</code></pre>

<p>(except I want to do this and get a subset of the original dataframe in a new dataframe object)</p>

<p>Question 2. How do I do this in a loop, and create a dataframe object for each unique sets of year and gender (i.e. a df for: 2013-Male, 2013-Female, 2014-Male, and 2014-Female</p>

<pre><code>for y in year:

for g in gender:

df = .....
</code></pre>
";;1;;2014-02-28T04:21:02.250;2.0;22086116;2016-10-02T18:37:23.883;;;;;3098818.0;;1;31;<python><filter><pandas>;how do you filter pandas dataframes by multiple columns;29798.0
10907;10907;22089870.0;1.0;"<p>I'm trying to print out a dataframe into Excel. Here I am using to_excel() functions. However, I found that the 1st column in Excel is the ""index"", </p>

<pre><code>0   6/6/2021 0:00   8/6/2021 0:00
1   4/10/2024 0:00  6/10/2024 0:00
2   4/14/2024 0:00  6/14/2024 0:00
</code></pre>

<p>Is there any ways to get rid of the first column?</p>
";;3;;2014-02-28T07:52:06.980;;22089317;2014-02-28T08:26:51.970;;;;;1670180.0;;1;11;<python-2.7><pandas><export-to-excel>;Python to_excel without row names (index)?;5479.0
10986;10986;22127685.0;1.0;"<p>I cannot figure out how to do ""reverse melt"" using Pandas in python.
This is my starting data</p>

<pre><code>import pandas as pd

from StringIO import StringIO

origin = pd.read_table(StringIO('''label    type    value
x   a   1
x   b   2
x   c   3
y   a   4
y   b   5
y   c   6
z   a   7
z   b   8
z   c   9'''))

origin
Out[5]: 
  label type  value
0     x    a      1
1     x    b      2
2     x    c      3
3     y    a      4
4     y    b      5
5     y    c      6
6     z    a      7
7     z    b      8
8     z    c      9
</code></pre>

<p>This is what I would like to have:</p>

<pre><code>    label   a   b   c
        x   1   2   3
        y   4   5   6
        z   7   8   9
</code></pre>

<p>I'm sure there is an easy way to do this, but I don't know how.</p>
";;2;;2014-03-02T12:32:48.130;7.0;22127569;2015-10-10T02:22:58.427;;;;;17523.0;;1;24;<python><pandas>;Opposite of melt in python pandas;3241.0
10994;10994;22132649.0;4.0;"<p>I want to subtract dates in 'A' from dates in 'B' and add a new column with the difference.</p>

<pre><code>df
          A        B
one 2014-01-01  2014-02-28 
two 2014-02-03  2014-03-01
</code></pre>

<p>I've tried the following, but get an error when I try to include this in a for loop...</p>

<pre><code>import datetime
date1=df['A'][0]
date2=df['B'][0]
mdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()
rdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()
delta =  (mdate1 - rdate1).days
print delta
</code></pre>

<p>What should I do?</p>
";;0;;2014-03-02T19:47:23.813;9.0;22132525;2017-07-11T16:18:10.593;2014-03-02T20:00:41.950;;1240268.0;;3371523.0;;1;18;<pandas><date-difference>;Add column with number of days between dates in DataFrame pandas;23577.0
11002;11002;22137890.0;1.0;"<p>I have a DataFrame that contains numbers as strings with commas for the thousands marker. I need to convert them to floats.</p>

<pre><code>a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]
df=pandas.DataFrame(a)
</code></pre>

<p>I am guessing I need to use locale.atof. Indeed </p>

<pre><code>df[0].apply(locale.atof)
</code></pre>

<p>works as expected. I get a Series of floats.</p>

<p>But when I apply it to the DataFrame, I get an error.</p>

<pre><code>df.apply(locale.atof)
</code></pre>

<p>TypeError: (""cannot convert the series to "", u'occurred at index 0')</p>

<p>and</p>

<pre><code>df[0:1].apply(locale.atof)
</code></pre>

<p>gives the error</p>

<p>ValueError: ('invalid literal for float(): 1,200', u'occurred at index 0')</p>

<p>So, how do I convert this DataFrame of strings to a DataFrame of floats?</p>
";;0;;2014-03-03T02:37:01.723;5.0;22137723;2017-04-25T14:05:30.757;2015-10-21T23:43:56.477;;1240268.0;;1816807.0;;1;23;<python><pandas>;Convert number strings with commas in pandas DataFrame to float;15553.0
11029;11029;22149930.0;6.0;"<p>Here is my code to generate a dataframe:</p>

<pre><code>import pandas as pd
import numpy as np

dff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))
</code></pre>

<p>then I got the dataframe:</p>

<pre><code>+------------+---------+--------+
|            |  A      |  B     |
+------------+---------+---------
|      0     | 0.626386| 1.52325|
+------------+---------+--------+
</code></pre>

<p>When I type the commmand :</p>

<pre><code>dff.mean(axis=1)
</code></pre>

<p>I got :</p>

<pre><code>0    1.074821
dtype: float64
</code></pre>

<p>According to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be</p>

<pre><code>A    0.626386
B    1.523255
dtype: float64
</code></pre>

<p>So here is my question: what does axis in pandas mean?</p>
";;0;;2014-03-03T14:41:04.217;44.0;22149584;2017-07-14T12:10:23.420;2016-10-26T09:18:05.770;;4903453.0;;636467.0;;1;86;<python><pandas>;What does axis in pandas mean?;39852.0
11040;11040;;2.0;"<p>Subclassing pandas classes seems a common need but I could not find references on the subject. (It seems that pandas developers are still working on it: <a href=""https://github.com/pydata/pandas/issues/60"">https://github.com/pydata/pandas/issues/60</a>). </p>

<p>There are some SO threads on the subject, but I am hoping that someone here can provide a more systematic account on currently the best way to subclass pandas.DataFrame that satisfies two, I think, general requirements:</p>

<pre><code>import numpy as np
import pandas as pd

class MyDF(pd.DataFrame):
    # how to subclass pandas DataFrame?
    pass

mydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])
print type(mydf)  # &lt;class '__main__.MyDF'&gt;

# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,
# should produce instances of MyDF.
mydf_sub = mydf[['A','C']]
print type(mydf_sub)  # &lt;class 'pandas.core.frame.DataFrame'&gt;

# Requirement 2: Attributes attached to instances of MyDF, when calling standard 
# methods of DataFrame, should still attach to the output.
mydf.myattr = 1
mydf_cp1 = MyDF(mydf)
mydf_cp2 = mydf.copy()
print hasattr(mydf_cp1, 'myattr')  # False
print hasattr(mydf_cp2, 'myattr')  # False
</code></pre>

<p>And is there any significant differences for subclassing pandas.Series? Thank you.</p>
";;10;;2014-03-03T19:49:17.570;9.0;22155951;2016-02-25T06:22:31.910;2014-03-03T19:54:38.517;;1991639.0;;1991639.0;;1;16;<python><pandas><dataframe><subclassing>;How to subclass pandas DataFrame?;4155.0
11042;11042;25162895.0;1.0;"<p>Using Pandas, what are the reasons to use a Panel versus a MultiIndex DataFrame?</p>

<p>I have personally found significant difference between the two in the ease of accessing different dimensions/levels, but that may just be my being more familiar with the interface for one versus the other. I assume there are more substantive differences, however.</p>
";;0;;2014-03-03T20:04:29.797;2.0;22156258;2014-08-06T14:22:36.390;;;;;897578.0;;1;14;<python><pandas>;Pandas MultiIndex versus Panel;956.0
11084;11084;22181298.0;2.0;"<p>I am using Flask but this probably applies to a lot of similar frameworks.</p>

<p>I construct a pandas Dataframe, e.g.</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x)
</code></pre>

<p>The template analysis.html looks like</p>

<pre><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{data}}
{% endblock %}
</code></pre>

<p>This works but the output looks horrible. It doesn't use linebreaks etc.
I have played with <code>data.to_html()</code> and <code>data.to_string()</code>
What's the easiest way to display a frame?</p>
";;4;;2014-03-04T19:19:54.123;17.0;22180993;2017-05-25T09:01:15.057;2017-05-25T09:01:15.057;;2314737.0;;1695486.0;;1;22;<python><pandas><flask>;Pandas Dataframe display on a webpage;13560.0
11132;11132;22211821.0;2.0;"<p>When there is an DataFrame like the following:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])
</code></pre>

<p>How can I sort this dataframe by index with each combination of index and column value intact?</p>
";;0;;2014-03-05T23:35:32.313;;22211737;2017-01-16T23:54:47.077;2014-03-05T23:42:37.247;;1552748.0;;2888042.0;;1;27;<python><pandas>;Python, pandas: how to sort dataframe by index;32238.0
11150;11150;22221675.0;3.0;"<p>I have a pandas data frame like:</p>

<pre><code>A 1
A 2
B 5
B 5
B 4
C 6
</code></pre>

<p>I want to group by the first column and get second column as lists in rows:</p>

<pre><code>A [1,2]
B [5,5,4]
C [6]
</code></pre>

<p>Is it possible to do something like this using pandas groupby?</p>
";;3;;2014-03-06T08:31:09.103;17.0;22219004;2017-08-15T22:54:22.507;;;;;1296816.0;;1;42;<python><pandas>;grouping rows in list in pandas groupby;24488.0
11171;11171;;4.0;"<p>If I've got a multi-level column index:</p>

<pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
&gt;&gt;&gt; pd.DataFrame([[1,2], [3,4]], columns=cols)
</code></pre>

<pre>
    a
   ---+--
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4
</pre>

<p>How can I drop the ""a"" level of that index, so I end up with:</p>

<pre>
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4
</pre>
";;0;;2014-03-06T18:58:06.367;10.0;22233488;2017-07-25T00:24:33.843;;;;;71522.0;;1;67;<python><pandas>;Pandas: drop a level from a multi-level column index?;36471.0
11181;11181;22235393.0;1.0;"<p>I have a dataframe of the following form (for example)</p>

<pre><code>shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method
1,FALSE,0,0,MX,
2,FALSE,1,0,MX,
3,FALSE,0,0,MX,
4,FALSE,22,0,MX,
5,FALSE,0,0,MX,
6,FALSE,0,0,MX,
7,FALSE,5,0,MX,
8,FALSE,0,0,MX,
9,FALSE,4,0,MX,
10,FALSE,2,0,MX,
11,FALSE,0,0,MX,
12,FALSE,13,0,MX,
13,FALSE,0,0,CA,
14,FALSE,0,0,US,
</code></pre>

<p>How can I use Pandas to calculate summary statistics of each column (column data types are variable, some columns have no information </p>

<p>And then return the a dataframe of the form:</p>

<pre><code>columnname, max, min, median,

is_martian, NA, NA, FALSE
</code></pre>

<p>So on and so on</p>
";;1;;2014-03-06T20:28:56.997;9.0;22235245;2015-11-25T19:20:18.267;2015-11-25T19:20:18.267;;4370109.0;;2909052.0;;1;21;<python><csv><pandas><dataframe>;Calculate summary statistics of columns in dataframe;35787.0
11205;11205;22247593.0;2.0;"<p>The following code does not work. </p>

<pre><code>import pandas as pd
import numpy as np
df=pd.DataFrame(['ONE','Two', np.nan],columns=['x']) 
xLower = df[""x""].map(lambda x: x.lower())
</code></pre>

<p>How should I tweak it to get xLower = ['one','two',np.nan] ?
Efficiency is important since the real data frame is huge.</p>
";;0;;2014-03-07T08:34:49.877;1.0;22245171;2014-12-01T23:56:21.020;;;;;2537640.0;;1;16;<python><string><pandas><missing-data>;How to lowercase a python dataframe string column if it has missing values?;22959.0
11229;11229;22257615.0;4.0;"<p>In <em>R</em> I can quickly see a count of missing data using the <code>summary</code> command, but the equivalent <code>pandas</code> DataFrame method, <code>describe</code> does not report these values.</p>

<p>I gather I can do something like</p>

<pre><code>len(mydata.index) - mydata.count()
</code></pre>

<p>to compute the number of missing values for each column, but I wonder if there's a better idiom (or if my approach is even right).</p>
";;1;;2014-03-07T18:08:08.703;6.0;22257527;2017-04-26T15:52:31.070;2016-11-17T10:57:10.620;;202229.0;;656912.0;;1;12;<r><pandas><reporting><nan><missing-data>;How do I get a summary count of missing/NaN data by column in 'pandas'?;12292.0
11232;11232;22264337.0;3.0;"<p>I am trying to join two numpy arrays. In one I have a set of columns/features after running TF-IDF on a single column of text. In the other I have one column/feature which is an integer. So I read in a column of train and test data, run TF-IDF on this, and then I want to add another integer column because I think this will help my classifier learn more accurately how it should behave. </p>

<p>Unfortunately, I am getting the error in the title when I try and run <code>hstack</code> to add this single column to my other numpy array.</p>

<p>Here is my code : </p>

<pre><code>  #reading in test/train data for TF-IDF
  traindata = list(np.array(p.read_csv('FinalCSVFin.csv', delimiter="";""))[:,2])
  testdata = list(np.array(p.read_csv('FinalTestCSVFin.csv', delimiter="";""))[:,2])

  #reading in labels for training
  y = np.array(p.read_csv('FinalCSVFin.csv', delimiter="";""))[:,-2]

  #reading in single integer column to join
  AlexaTrainData = p.read_csv('FinalCSVFin.csv', delimiter="";"")[[""alexarank""]]
  AlexaTestData = p.read_csv('FinalTestCSVFin.csv', delimiter="";"")[[""alexarank""]]
  AllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData)

  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  
        analyzer='word',token_pattern=r'\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) #tf-idf object
  rd = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, 
                             C=1, fit_intercept=True, intercept_scaling=1.0, 
                             class_weight=None, random_state=None) #Classifier
  X_all = traindata + testdata #adding test and train data to put into tf-idf
  lentrain = len(traindata) #find length of train data
  tfv.fit(X_all) #fit tf-idf on all our text
  X_all = tfv.transform(X_all) #transform it
  X = X_all[:lentrain] #reduce to size of training set
  AllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain] #reduce to size of training set
  X_test = X_all[lentrain:] #reduce to size of training set

  #printing debug info, output below : 
  print ""X.shape =&gt; "" + str(X.shape)
  print ""AllAlexaAndGoogleInfo.shape =&gt; "" + str(AllAlexaAndGoogleInfo.shape)
  print ""X_all.shape =&gt; "" + str(X_all.shape)

  #line we get error on
  X = np.hstack((X, AllAlexaAndGoogleInfo))
</code></pre>

<p>Below is the output and error message :</p>

<pre><code>X.shape =&gt; (7395, 238377)
AllAlexaAndGoogleInfo.shape =&gt; (7395, 1)
X_all.shape =&gt; (10566, 238377)



---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-2b310887b5e4&gt; in &lt;module&gt;()
     31 print ""X_all.shape =&gt; "" + str(X_all.shape)
     32 #X = np.column_stack((X, AllAlexaAndGoogleInfo))
---&gt; 33 X = np.hstack((X, AllAlexaAndGoogleInfo))
     34 sc = preprocessing.StandardScaler().fit(X)
     35 X = sc.transform(X)

C:\Users\Simon\Anaconda\lib\site-packages\numpy\core\shape_base.pyc in hstack(tup)
    271     # As a special case, dimension 0 of 1-dimensional arrays is ""horizontal""
    272     if arrs[0].ndim == 1:
--&gt; 273         return _nx.concatenate(arrs, 0)
    274     else:
    275         return _nx.concatenate(arrs, 1)

ValueError: all the input arrays must have same number of dimensions
</code></pre>

<p>What is causing my problem here? How can I fix this? As far as I can see I should be able to join these columns? What have I misunderstood?</p>

<p>Thank you.</p>

<p>Edit : </p>

<p>Using the method in the answer below gets the following error : </p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-16-640ef6dd335d&gt; in &lt;module&gt;()
---&gt; 36 X = np.column_stack((X, AllAlexaAndGoogleInfo))
     37 sc = preprocessing.StandardScaler().fit(X)
     38 X = sc.transform(X)

C:\Users\Simon\Anaconda\lib\site-packages\numpy\lib\shape_base.pyc in column_stack(tup)
    294             arr = array(arr,copy=False,subok=True,ndmin=2).T
    295         arrays.append(arr)
--&gt; 296     return _nx.concatenate(arrays,1)
    297 
    298 def dstack(tup):

ValueError: all the input array dimensions except for the concatenation axis must match exactly
</code></pre>

<p>Interestingly, I tried to print the <code>dtype</code> of X and this worked fine : </p>

<pre><code>X.dtype =&gt; float64
</code></pre>

<p>However, trying to print the dtype of <code>AllAlexaAndGoogleInfo</code> like so : </p>

<pre><code>print ""AllAlexaAndGoogleInfo.dtype =&gt; "" + str(AllAlexaAndGoogleInfo.dtype) 
</code></pre>

<p>produces :</p>

<pre><code>'DataFrame' object has no attribute 'dtype'
</code></pre>
";;1;;2014-03-07T18:25:25.577;4.0;22257836;2014-03-08T02:46:12.243;2014-03-07T18:42:44.253;;963156.0;;963156.0;;1;15;<python><arrays><numpy><pandas><scikit-learn>;"Numpy hstack - ""ValueError: all the input arrays must have same number of dimensions"" - but they do";28086.0
11235;11235;;6.0;"<p>The CSV file that I want to read does not fit into main memory. How can I read a few (~10K) random lines of it and do some simple statistics on the selected data frame?</p>
";;3;;2014-03-07T19:00:09.273;5.0;22258491;2016-06-10T17:50:05.317;2015-11-25T17:36:05.393;;4370109.0;;2537640.0;;1;20;<python><pandas><random><io><import-from-csv>;Read a small random sample from a big CSV file into a Python data frame;9360.0
11270;11270;22276757.0;3.0;"<p>I'm starting to tear my hair out with this - so I hope someone can help. I have a pandas DataFrame that was created from an Excel spreadsheet using openpyxl. The resulting DataFrame looks like:</p>

<pre><code>print image_name_data
     id           image_name
0  1001  1001_mar2014_report
1  1002  1002_mar2014_report
2  1003  1003_mar2014_report

[3 rows x 2 columns]
</code></pre>

<p>with the following datatypes:</p>

<pre><code>print image_name_data.dtypes
id            float64
image_name     object
dtype: object
</code></pre>

<p>The issue is that the numbers in the id column are, in fact, identification numbers and I need to treat them as strings. I've tried converting the id column to strings using:</p>

<pre><code>image_name_data['id'] = image_name_data['id'].astype('str')
</code></pre>

<p>This seems a bit ugly but it does produce a variable of type 'object' rather than 'float64':</p>

<pre><code>print image_name_data.dyptes
id            object
image_name    object
dtype: object
</code></pre>

<p>However, the strings that are created have a decimal point, as shown:</p>

<pre><code>print image_name_data
       id           image_name
0  1001.0  1001_mar2014_report
1  1002.0  1002_mar2014_report
2  1003.0  1003_mar2014_report

[3 rows x 2 columns]
</code></pre>

<p>How can I convert a float64 column in a pandas DataFrame to a string with a given format (in this case, for example, '%10.0f')?</p>

<p>Thanks in advance for any help and advice.</p>
";;0;;2014-03-08T23:34:11.077;5.0;22276503;2016-11-05T14:59:45.037;;;;;1718097.0;;1;16;<python><string><floating-point><pandas><format>;How to I change data-type of pandas data frame to string with a defined format;76176.0
11379;11379;22341390.0;2.0;"<p>I have an excel document which looks like this..</p>

<pre><code>cluster load_date   budget  actual  fixed_price
A   1/1/2014    1000    4000    Y
A   2/1/2014    12000   10000   Y
A   3/1/2014    36000   2000    Y
B   4/1/2014    15000   10000   N
B   4/1/2014    12000   11500   N
B   4/1/2014    90000   11000   N
C   7/1/2014    22000   18000   N
C   8/1/2014    30000   28960   N
C   9/1/2014    53000   51200   N
</code></pre>

<p>I want to be able to return the contents of column 1 - cluster as a list, so I can run a for loop over it, and create an excel worksheet for every cluster.</p>

<p>Is it also possible, to return the contents of a whole row to a list? e.g.</p>

<pre><code>list = [], list[column1] or list[df.ix(row1)]
</code></pre>
";;7;;2014-03-12T03:12:33.783;14.0;22341271;2017-08-02T10:14:23.853;;;;;3098818.0;;1;60;<python><list><pandas>;get list from pandas dataframe column;117410.0
11463;11463;22391554.0;9.0;"<p>I have a dataset</p>

<pre><code>|category|
cat a
cat b
cat a
</code></pre>

<p>I'd like to be able to return something like (showing unique values and frequency)</p>

<pre><code>category | freq |
cat a       2
cat b       1
</code></pre>
";;7;;2014-03-13T21:34:41.323;11.0;22391433;2017-07-05T16:41:32.350;;;;;3098818.0;;1;51;<python><pandas>;count the frequency that a value occurs in a dataframe column;98963.0
11493;11493;22455322.0;1.0;"<p>I have a Pandas <code>DataFrame</code> indexed by date. There a number of columns but many columns are only populated for part of the time series. I'd like to find where the first and last values non-<code>NaN</code> values are located so that I can extracts the dates and see how long the time series is for a particular column.</p>

<p>Could somebody point me in the right direction as to how I could go about doing something like this? Thanks in advance.</p>
";;3;;2014-03-14T11:15:17.283;2.0;22403469;2016-08-10T16:12:06.190;;;;;2694260.0;;1;21;<python><datetime><pandas>;Locate first and last non NaN values in a Pandas DataFrame;9357.0
11570;11570;22475141.0;5.0;"<p>If I have a dataframe with the following columns: </p>

<pre><code>1. NAME                                     object
2. On_Time                                      object
3. On_Budget                                    object
4. %actual_hr                                  float64
5. Baseline Start Date                  datetime64[ns]
6. Forecast Start Date                  datetime64[ns] 
</code></pre>

<p>I would like to be able to say: here is a dataframe, give me a list of the columns which are of type Object or of type DateTime?</p>

<p>I have a function which converts numbers (Float64) to two decimal places, and I would like to use this list of dataframe columns, of a particular type, and run it through this function to convert them all to 2dp.</p>

<p>Maybe:</p>

<pre><code>For c in col_list: if c.dtype = ""Something""
list[]
List.append(c)?
</code></pre>
";;0;;2014-03-18T04:54:22.053;19.0;22470690;2017-05-08T17:01:39.980;;;;;3098818.0;;1;65;<python><pandas>;get list of pandas dataframe columns based on data type;133397.0
11584;11584;22484249.0;4.0;"<p>I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking <code>df.plot()</code>, I get separate plot images. what I really want is to have them all in the same plot as subplots, but I'm unfortunately failing to come up with a solution to how and would highly appreciate some help. </p>
";;0;;2014-03-18T15:18:11.250;15.0;22483588;2016-12-09T15:32:33.037;;;;;1672420.0;;1;38;<python><matplotlib><pandas>;How can I plot separate Pandas DataFrames as subplots?;34397.0
11588;11588;22485573.0;2.0;"<h2>Problem</h2>

<p>Given data in a Pandas DataFrame like the following:</p>

<pre><code>Name     Amount
---------------
Alice       100
Bob          50
Charlie     200
Alice        30
Charlie      10
</code></pre>

<p>I want to select all rows where the <code>Name</code> is one of several values in a collection <code>{Alice, Bob}</code></p>

<pre><code>Name     Amount
---------------
Alice       100
Bob          50
Alice        30
</code></pre>

<h2>Question</h2>

<p>What is an efficient way to do this in Pandas?</p>

<p>Options as I see them</p>

<ol>
<li>Loop through rows, handling the logic with Python</li>
<li><p>Select and merge many statements like the following </p>

<pre><code>merge(df[df.name = specific_name] for specific_name in names) # something like this
</code></pre></li>
<li><p>Perform some sort of join</p></li>
</ol>

<p>What are the performance trade-offs here?  When is one solution better than the others?  What solutions am I missing?</p>

<p>While the example above uses strings my actual job uses matches on 10-100 integers over millions of rows and so fast NumPy operations may be relevant. </p>
";;2;;2014-03-18T16:29:19.860;6.0;22485375;2015-05-01T21:40:15.480;2014-03-18T16:43:28.917;;616616.0;;616616.0;;1;19;<python><pandas>;Efficiently select rows that match one of several values in Pandas DataFrame;19621.0
11594;11594;;1.0;"<p>I am using Python multiprocessing, more precisely</p>

<pre><code>from multiprocessing import Pool
p = Pool(15)

args = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple
res = p.map_async(func, args) #func is some arbitrary function
p.close()
p.join()
</code></pre>

<p>This approach has a huge memory consumption; eating up pretty much all my RAM (at which point it gets extremely slow, hence making the multiprocessing pretty useless). I assume the problem is that <code>df</code> is a huge object (a large pandas dataframe) and it gets copied for each process. I have tried using <code>multiprocessing.Value</code> to share the dataframe without copying</p>

<pre><code>shared_df = multiprocessing.Value(pandas.DataFrame, df)
args = [(shared_df, config1), (shared_df, config2), ...] 
</code></pre>

<p>(as suggested in <a href=""https://stackoverflow.com/q/14124588/2565842"">Python multiprocessing shared memory</a>), but that gives me <code>TypeError: this type has no size</code> (same as <a href=""https://stackoverflow.com/q/3671666/2565842"">Sharing a complex object between Python processes?</a>, to which I unfortunately don't understand the answer). </p>

<p>I am using multiprocessing for the first time and maybe my understanding is not (yet) good enough. Is <code>multiprocessing.Value</code> actually even the right thing to use in this case? I have seen other suggestions (e.g. queue) but am by now a bit confused. What options are there to share memory, and which one would be best in this case?</p>
";;1;;2014-03-18T17:56:48.107;9.0;22487296;2014-03-18T18:24:27.630;2017-05-23T12:16:47.310;;-1.0;;2565842.0;;1;15;<python><pandas><multiprocessing>;multiprocessing in python - sharing large object (e.g. pandas dataframe) between multiple processes;6405.0
11715;11715;22543333.0;4.0;"<p>When I look at the <a href=""http://pandas.pydata.org/pandas-docs/dev/visualization.html"">plotting style in the Pandas documentation</a>, the plots look different from the default one. It seems to mimic the ggplot ""look and feel"".</p>

<p>Same thing with the <a href=""http://nbviewer.ipython.org/github/mwaskom/seaborn/blob/master/examples/plotting_distributions.ipynb"">seaborn's package</a>.</p>

<p>How can I load that style? (even if I am not using a notebook?)</p>
";;3;;2014-03-20T19:28:24.777;7.0;22543208;2017-03-28T20:15:11.487;2014-03-20T19:44:15.647;;1732769.0;;1732769.0;;1;21;<python><matplotlib><pandas><ipython>;ggplot styles in Python;12579.0
11731;11731;22546459.0;2.0;"<p>I have a pandas df and would like to accomplish something along these lines (in SQL terms):</p>

<pre><code>SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...
</code></pre>

<p>Now this works, for one column/value pair:</p>

<pre><code>foo = df.ix[df['column']==value]
</code></pre>

<p>However, I'm not sure how to expand that to multiple column/value pairs</p>
";;0;;2014-03-20T22:23:36.933;8.0;22546425;2017-02-24T14:12:36.583;;;;;2909052.0;;1;28;<python><pandas>;using pandas to select rows conditional on multiple equivalencies;35625.0
11738;11738;22553757.0;1.0;"<p>Without using <code>groupby</code> how would I filter out data without <code>NaN</code>?</p>

<p>Let say I have a matrix where customers will fill in 'N/A','n/a' or any of its variations and others leave it blank:</p>

<pre><code>import pandas as pd
import numpy as np


df = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],
                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],
                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})

nbs = df['name'].str.extract('^(N/A|NA|na|n/a)')
nms=df[(df['name'] != nbs) ]
</code></pre>

<p>output:</p>

<pre><code>&gt;&gt;&gt; nms
  movie    name  rating
0   thg    John       3
1   thg     NaN       4
3   mol  Graham     NaN
4   lob     NaN     NaN
5   lob     NaN     NaN
</code></pre>

<p>How would I filter out NaN values so I can get results to work with like this:</p>

<pre><code>  movie    name  rating
0   thg    John       3
3   mol  Graham     NaN
</code></pre>

<p>I am guessing I need something like <code>~np.isnan</code> but the tilda does not work with strings.</p>
";;0;;2014-03-21T06:04:19.520;8.0;22551403;2014-03-21T09:46:20.713;;;;;3159981.0;;1;38;<python><pandas><dataframe>;Python pandas Filtering out nan from a data selection of a column of strings;55836.0
11790;11790;22588340.0;4.0;"<p>I have read some pricing data into a pandas dataframe the values appear as:</p>

<pre><code>$40,000*
$40000 conditions attached
</code></pre>

<p>I want to strip it down to just the numeric values.
I know I can loop through and apply regex </p>

<pre><code>[0-9]+
</code></pre>

<p>to each field then join the resulting list back together but is there a not loopy way?</p>

<p>Thanks</p>
";;0;;2014-03-23T07:48:50.477;3.0;22588316;2017-04-03T01:45:13.327;;;;;1687633.0;;1;25;<python><regex><pandas>;pandas applying regex to replace values;22689.0
11797;11797;22591267.0;2.0;"<p>I am filtering rows in a dataframe by values in two columns.</p>

<p>For some reason the OR operator behaves like I would expect AND operator to behave and vice versa.</p>

<p>My test code:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'a': range(5), 'b': range(5) })

# let's insert some -1 values
df['a'][1] = -1
df['b'][1] = -1
df['a'][3] = -1
df['b'][4] = -1

df1 = df[(df.a != -1) &amp; (df.b != -1)]
df2 = df[(df.a != -1) | (df.b != -1)]

print pd.concat([df, df1, df2], axis=1,
                keys = [ 'original df', 'using AND (&amp;)', 'using OR (|)',])
</code></pre>

<p>And the result:</p>

<pre><code>      original df      using AND (&amp;)      using OR (|)    
             a  b              a   b             a   b
0            0  0              0   0             0   0
1           -1 -1            NaN NaN           NaN NaN
2            2  2              2   2             2   2
3           -1  3            NaN NaN            -1   3
4            4 -1            NaN NaN             4  -1

[5 rows x 6 columns]
</code></pre>

<p>As you can see, the <code>AND</code> operator drops every row in which at least one value equals <code>-1</code>. On the other hand, the <code>OR</code> operator requires both values to be equal to <code>-1</code> to drop them. I would expect exactly the opposite result. Could anyone explain this behavior, please?</p>

<p>I am using pandas 0.13.1.</p>
";;1;;2014-03-23T12:52:59.133;14.0;22591174;2017-04-06T08:48:48.073;;;;;1407427.0;;1;26;<python><pandas><boolean-logic>;pandas: multiple conditions while indexing data frame - unexpected behavior;47480.0
11814;11814;22605281.0;1.0;"<p>In order to test some functionality I would like to create a <code>DataFrame</code> from a string. Let's say my testdata looks like:</p>

<pre><code>TESTDATA=""""""col1;col2;col3
1;4.4;99
2;4.5;200
3;4.7;65
4;3.2;140
""""""
</code></pre>

<p>What is the simplest way to read that data into a Pandas <code>DataFrame</code>?</p>
";;0;;2014-03-24T08:43:29.343;12.0;22604564;2016-10-14T14:50:10.613;2014-03-24T10:47:14.450;;355499.0;;355499.0;;1;76;<python><csv><pandas>;How to create a Pandas DataFrame from String;32697.0
11878;11878;22643040.0;5.0;"<p>I have a DataFrame (df1) with a dimension <code>2000 rows x 500 columns</code> (excluding the index) for which I want to divide each row by another DataFrame (df2) with dimension <code>1 rows X 500 columns</code>. Both have the same column headers. I tried:</p>

<p><code>df.divide(df2)</code> and 
<code>df.divide(df2, axis='index')</code> and multiple other solutions and I always get a df with <code>nan</code> values in every cell. What argument am I missing in the function <code>df.divide</code>?</p>
";;0;;2014-03-25T17:33:43.363;3.0;22642162;2017-03-30T06:48:28.407;;;;;1418224.0;;1;14;<python><pandas>;Python: Divide each row of a DataFrame by another DataFrame vector;20234.0
11897;11897;22650075.0;7.0;"<p>I can use <code>pandas</code> <code>dropna()</code> functionality to remove rows with some or all columns set as <code>NA</code>'s. Is there an equivalent function for dropping rows with all columns having value 0?</p>

<pre><code>P   kt  b   tt  mky depth
1   0   0   0   0   0
2   0   0   0   0   0
3   0   0   0   0   0
4   0   0   0   0   0
5   1.1 3   4.5 2.3 9.0
</code></pre>

<p>In this example, we would like to drop the first 4 rows from the data frame.</p>

<p>thanks!</p>
";;0;;2014-03-26T01:20:10.410;7.0;22649693;2017-02-05T17:58:57.613;2016-08-05T15:19:53.617;;3789866.0;;308827.0;;1;22;<python><pandas>;Drop rows with all zeros in pandas data frame;25347.0
11904;11904;;4.0;"<p>I would like to add a cumulative sum column to my Pandas dataframe so that:</p>

<pre><code>Jack | Monday | 10
Jack | Tuesday | 20
Jack | Tuesday | 10
Jack | Wednesday | 50
Jill | Monday | 40
Jill Wednesday | 110
</code></pre>

<p>becomes:</p>

<pre><code>Jack | Monday | 10 | 10
Jack | Tuesday | 30 | 40
Jack | Wednesday | 50 | 100
Jill | Monday | 40 | 40
Jill | Wednesday | 40 | 150
</code></pre>

<p>I tried various combos of <code>df.groupby</code> and <code>df.agg(lambda x: cumsum(x))</code> to no avail. Thanks in advance!</p>
";;0;;2014-03-26T03:17:42.550;6.0;22650833;2017-07-19T10:40:34.950;2014-06-25T13:57:12.960;;478288.0;;2974813.0;;1;14;<python><pandas>;Pandas groupby cumulative sum;13444.0
11963;11963;22676213.0;4.0;"<p>Suppose I have two DataFrames like so:</p>

<pre><code>left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})

right = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})
</code></pre>

<p>I want to merge them, so I try something like this:</p>

<pre><code>pd.merge(left, right, left_on='key1', right_on='key2')
</code></pre>

<p>And I'm happy</p>

<pre><code>    key1    lval    key2    rval
0   foo     1       foo     4
1   bar     2       bar     5
</code></pre>

<p>But I'm trying to use the join method, which I've been lead to believe is pretty similar. </p>

<pre><code>left.join(right, on=['key1', 'key2'])
</code></pre>

<p>And I get this:</p>

<pre><code>//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)
    406             if self.right_index:
    407                 if not ((len(self.left_on) == self.right.index.nlevels)):
--&gt; 408                     raise AssertionError()
    409                 self.right_on = [None] * n
    410         elif self.right_on is not None:

AssertionError: 
</code></pre>

<p>What am I missing?</p>
";;1;;2014-03-27T00:42:47.120;7.0;22676081;2017-01-17T21:54:15.537;;;;;633094.0;;1;45;<python><pandas>;Pandas - The difference between join and merge;22067.0
11984;11984;;4.0;"<p>I want to print the result of grouping with Pandas.</p>

<p>I have a dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})
print df

       A  B
0    one  0
1    one  1
2    two  2
3  three  3
4  three  4
5    one  5
</code></pre>

<p>When printing after grouping by 'A' I have the following:</p>

<pre><code>print df.groupby('A')

&lt;pandas.core.groupby.DataFrameGroupBy object at 0x05416E90&gt;
</code></pre>

<p>How can I print the dataframe grouped?</p>

<p>If I do:</p>

<pre><code>print df.groupby('A').head()
</code></pre>

<p>I obtain the dataframe as if it was not grouped:</p>

<pre><code>             A  B
A                
one   0    one  0
      1    one  1
two   2    two  2
three 3  three  3
      4  three  4
one   5    one  5
</code></pre>

<p>I was expecting something like:</p>

<pre><code>             A  B
A                
one   0    one  0
      1    one  1
      5    one  5
two   2    two  2
three 3  three  3
      4  three  4
</code></pre>
";;3;;2014-03-27T14:42:29.203;8.0;22691010;2017-04-09T15:08:44.150;2014-03-27T14:47:35.370;;3465658.0;;3465658.0;;1;25;<python><pandas>;How to print a groupby object;15872.0
11999;11999;22697903.0;3.0;"<p>I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:</p>

<pre><code>allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)
for y in allc:
    treat_numeric(agg[y])    

allc = list((agg.loc[:, (agg.dtypes!=np.float64)&amp;(agg.dtypes!=np.int)]).columns)
for y in allc:
    treat_str(agg[y])    
</code></pre>

<p>Is there a more elegant way to do this? E.g.</p>

<pre><code>for y in agg.columns:
    if(dtype(agg[y]) == 'string'):
          treat_str(agg[y])
    elif(dtype(agg[y]) != 'string'):
          treat_numeric(agg[y])
</code></pre>
";;1;;2014-03-27T19:49:31.313;7.0;22697773;2017-08-08T12:20:18.850;2016-12-26T23:29:10.537;;2662901.0;;209490.0;;1;31;<python><pandas>;how to check the dtype of a column in python pandas;62516.0
12018;12018;22702814.0;1.0;"<p>I would like to have:</p>

<pre><code>df[['income_1', 'income_2']] * df['mtaz_proportion']
</code></pre>

<p>return those columns multiplied by <code>df['mtaz_proportion']</code></p>

<p>so that I can set </p>

<pre><code>df[['mtaz_income_1', 'mtaz_income_2']] = 
df[['income_1', 'income_2']] * df['mtaz_proportion']
</code></pre>

<p>but instead I get: </p>

<pre><code>income_1    income_2    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  
0   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
1   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
2   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
</code></pre>

<p>ect...</p>

<p>what simple thing am I missing?</p>

<p>Thank you!</p>
";;0;;2014-03-28T01:56:23.540;5.0;22702760;2014-03-28T02:01:40.883;;;;;2588923.0;;1;17;<python><pandas>;how to multiply multiple columns by a column in Pandas;8143.0
12046;12046;22720823.0;1.0;"<p>From what I understand about a left outer join, the resulting table should never have more rows than the left table...Please let me know if this is wrong... </p>

<p>My left table is 192572 rows and 8 columns.</p>

<p>My right table is 42160 rows and 5 columns.</p>

<p>My Left table has a field called 'id' which matches with a column in my right table called 'key'.</p>

<p>Therefore I merge them as such:</p>

<pre><code>combined = pd.merge(a,b,how='left',left_on='id',right_on='key')
</code></pre>

<p>But then the combined shape is 236569.</p>

<p>What am I misunderstanding? </p>
";;2;;2014-03-28T18:39:48.923;1.0;22720739;2015-08-07T05:36:02.820;;;;;1455043.0;;1;17;<python><pandas>;Pandas Left Outer Join results in table larger than left table;6022.0
12151;12151;22845857.0;4.0;"<p>So here is how my data set looks like :</p>

<pre><code>In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])

In [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])

In [3]: df1
Out[3]: 
          I         J
A  0.675616  0.177597
B  0.675693  0.598682
C  0.631376  0.598966
D  0.229858  0.378817

In [4]: df2
Out[4]: 
          I         J
A  0.939620  0.984616
B  0.314818  0.456252
C  0.630907  0.656341
D  0.020994  0.538303
</code></pre>

<p><strong>I want to have stacked bar plot for each dataframe but since they have same index, I'd like to have 2 stacked bars per index.</strong></p>

<p>I've tried to plot both on the same axes :</p>

<pre><code>In [5]: ax = df1.plot(kind=""bar"", stacked=True)

In [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)
</code></pre>

<p>But it overlaps.</p>

<p>Then I tried to concat the two dataset first :</p>

<pre><code>pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)
</code></pre>

<p>but here everything is stacked</p>

<p>My best try is :</p>

<pre><code> pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)
</code></pre>

<p>Which gives :</p>

<p><img src=""https://i.stack.imgur.com/dSSsI.png"" alt=""enter image description here""></p>

<p>This is basically what I want, except that I want the bar ordered as</p>

<p>(df1,A) (df2,A) (df1,B) (df2,B) etc...</p>

<p>I guess there is a trick but I can't found it !</p>

<hr>

<p>After @bgschiller's answer I got this :</p>

<p><img src=""https://i.stack.imgur.com/8Uk5l.png"" alt=""enter image description here""> </p>

<p>Which is almost what I want. I would like the bar to be <strong>clustered by index</strong>, in order to have something visually clear.</p>

<p><em>Bonus</em> : Having the x-label not redundant, something like : </p>

<pre><code>df1 df2    df1 df2
_______    _______ ...
   A          B
</code></pre>

<p>Thanks for helping.</p>
";;0;;2014-04-01T13:22:11.730;9.0;22787209;2017-07-06T03:21:40.133;2017-02-10T19:52:24.277;;3297428.0;;3297428.0;;1;24;<python><pandas><matplotlib><plot><seaborn>;How to have clusters of stacked bars with python (Pandas);7938.0
12168;12168;22798911.0;1.0;"<p>Say I create a fully random <code>Dataframe</code> using the following:</p>

<pre><code>from pandas.util import testing
from random import randrange

def random_date(start, end):
    delta = end - start
    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds
    random_second = randrange(int_delta)
    return start + timedelta(seconds=random_second)

def rand_dataframe():
  df = testing.makeDataFrame()
  df['date'] = [random_date(datetime.date(2014,3,18),datetime.date(2014,4,1)) for x in xrange(df.shape[0])]
  df.sort(columns=['date'], inplace=True)      
  return df

df = rand_dataframe()
</code></pre>

<p>which results in the dataframe shown at the bottom of this post. I would like to plot my columns <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code> using the <a href=""http://www.stanford.edu/~mwaskom/software/seaborn/examples/timeseries_from_dataframe.html"" rel=""noreferrer"">timeseries</a> visualization features in <code>seaborn</code> so that I get something along these lines:</p>

<p><img src=""https://i.stack.imgur.com/pDYIh.png"" alt=""enter image description here""></p>

<p>How can I approach this problem? From what I read on <a href=""http://www.stanford.edu/~mwaskom/software/seaborn/timeseries_plots.html"" rel=""noreferrer"">this notebook</a>, the call should be:</p>

<pre><code>sns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")
</code></pre>

<p>but this seems to require that the dataframe is represented in a different way, with the columns somehow encoding <code>time</code>, <code>unit</code>, <code>condition</code> and <code>value</code>, which is not my case. How can I convert my dataframe (shown below) into this format?</p>

<p>Here is my dataframe:</p>

<pre><code>      date         A         B         C         D

2014-03-18  1.223777  0.356887  1.201624  1.968612
2014-03-18  0.160730  1.888415  0.306334  0.203939
2014-03-18 -0.203101 -0.161298  2.426540  0.056791
2014-03-18 -1.350102  0.990093  0.495406  0.036215
2014-03-18 -1.862960  2.673009 -0.545336 -0.925385
2014-03-19  0.238281  0.468102 -0.150869  0.955069
2014-03-20  1.575317  0.811892  0.198165  1.117805
2014-03-20  0.822698 -0.398840 -1.277511  0.811691
2014-03-20  2.143201 -0.827853 -0.989221  1.088297
2014-03-20  0.299331  1.144311 -0.387854  0.209612
2014-03-20  1.284111 -0.470287 -0.172949 -0.792020
2014-03-22  1.031994  1.059394  0.037627  0.101246
2014-03-22  0.889149  0.724618  0.459405  1.023127
2014-03-23 -1.136320 -0.396265 -1.833737  1.478656
2014-03-23 -0.740400 -0.644395 -1.221330  0.321805
2014-03-23 -0.443021 -0.172013  0.020392 -2.368532
2014-03-23  1.063545  0.039607  1.673722  1.707222
2014-03-24  0.865192 -0.036810 -1.162648  0.947431
2014-03-24 -1.671451  0.979238 -0.701093 -1.204192
2014-03-26 -1.903534 -1.550349  0.267547 -0.585541
2014-03-27  2.515671 -0.271228 -1.993744 -0.671797
2014-03-27  1.728133 -0.423410 -0.620908  1.430503
2014-03-28 -1.446037 -0.229452 -0.996486  0.120554
2014-03-28 -0.664443 -0.665207  0.512771  0.066071
2014-03-29 -1.093379 -0.936449 -0.930999  0.389743
2014-03-29  1.205712 -0.356070 -0.595944  0.702238
2014-03-29 -1.069506  0.358093  1.217409 -2.286798
2014-03-29  2.441311  1.391739 -0.838139  0.226026
2014-03-31  1.471447 -0.987615  0.201999  1.228070
2014-03-31 -0.050524  0.539846  0.133359 -0.833252
</code></pre>

<p>In the end, what I am looking for is an overlay of of plots (one per column), where each of them looks as follows (note that different values of CI get different values of alphas):</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=""https://i.stack.imgur.com/5AsIv.png"" alt=""enter image description here""></p>
";;7;;2014-04-01T19:38:35.727;4.0;22795348;2017-05-26T13:34:25.020;2014-04-02T00:14:40.650;;283296.0;;283296.0;;1;14;<python><matplotlib><pandas><seaborn>;Plotting time-series data with seaborn;22238.0
12187;12187;22799916.0;5.0;"<p>I have data in long format and am trying to reshape to wide, but there doesn't seem to be a straightforward way to do this using melt/stack/unstack:</p>

<pre><code>Salesman  Height   product      price
  Knut      6        bat          5
  Knut      6        ball         1
  Knut      6        wand         3
  Steve     5        pen          2
</code></pre>

<p>Becomes:</p>

<pre><code>Salesman  Height    product_1  price_1  product_2 price_2 product_3 price_3  
  Knut      6        bat          5       ball      1        wand      3
  Steve     5        pen          2        NA       NA        NA       NA
</code></pre>

<p>I think Stata can do something like this with the reshape command.</p>
";;0;;2014-04-01T23:37:44.700;9.0;22798934;2016-07-22T16:34:43.263;;;;;2985049.0;;1;11;<python><pandas>;Pandas long to wide reshape;11082.0
12191;12191;36816769.0;3.0;"<p>Sometimes I end up with a series of tuples/lists when using Pandas. This is common when, for example, doing a group-by and passing a function that has multiple return values:</p>

<pre><code>import numpy as np
from scipy import stats
df = pd.DataFrame(dict(x=np.random.randn(100),
                       y=np.repeat(list(""abcd""), 25)))
out = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)
print out

y
a       (1.3066417476, 0.203717485506)
b    (0.0801133382517, 0.936811414675)
c      (1.55784329113, 0.132360504653)
d     (0.267999459642, 0.790989680709)
dtype: object
</code></pre>

<p>What is the correct way to ""unpack"" this structure so that I get a DataFrame with two columns?</p>

<p>A related question is how I can unpack either this structure or the resulting dataframe into two Series/array objects. This almost works:</p>

<pre><code>t, p = zip(*out)
</code></pre>

<p>but it <code>t</code> is</p>

<pre><code> (array(1.3066417475999257),
 array(0.08011333825171714),
 array(1.557843291126335),
 array(0.267999459641651))
</code></pre>

<p>and one needs to take the extra step of squeezing it.</p>
";;1;;2014-04-02T00:14:32.223;4.0;22799300;2016-04-23T21:40:15.413;;;;;1533576.0;;1;14;<python><pandas>;How to unpack a Series of tuples in Pandas?;6603.0
12246;12246;22825954.0;2.0;"<p>I have the following:</p>

<pre><code>&gt; date1
Timestamp('2014-01-23 00:00:00', tz=None)

&gt; date2
datetime.date(2014, 3, 26)
</code></pre>

<p>and I read on <a href=""https://stackoverflow.com/a/13753918/283296"">this answer</a> that I could use <code>pandas.to_datetime()</code> to convert from <code>Timestamps</code> to <code>datetime</code> objects, but it doesn't seem to work:</p>

<pre><code>&gt; pd.to_datetime(date1)   
Timestamp('2014-01-23 00:00:00', tz=None)
</code></pre>

<p>Why? How can I convert between these two formats?</p>
";;0;;2014-04-02T23:58:09.037;1.0;22825349;2014-04-03T15:09:35.257;2017-05-23T10:30:49.210;;-1.0;;283296.0;;1;11;<python><datetime><pandas>;Converting between datetime and Pandas Timestamp objects;10290.0
12253;12253;22836353.0;1.0;"<p>I am trying to achieve differentiation by hatch pattern instead of by (just) colour. How do I do it using pandas?</p>

<p>It's possible in matplotlib, by passing the <code>hatch</code> optional argument as discussed <a href=""https://stackoverflow.com/questions/14279344/how-can-i-add-textures-to-my-bars-and-wedges"">here</a>. I know I can also pass that option to a pandas <code>plot</code>, but I don't know how to tell it to use a different hatch pattern for each <code>DataFrame</code> column.</p>

<pre><code>df = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])
df.plot(kind='bar', hatch='/');
</code></pre>

<p><img src=""https://i.stack.imgur.com/5HgTK.png"" alt=""enter image description here""></p>

<p>For colours, there is the <code>colormap</code> option described <a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html#colormaps"" rel=""nofollow noreferrer"">here</a>. Is there something similar for hatching? Or can I maybe set it manually by modifying the <code>Axes</code> object returned by <code>plot</code>?</p>
";;0;;2014-04-03T09:30:53.237;5.0;22833404;2015-11-19T17:34:14.323;2017-05-23T12:32:17.867;;-1.0;;544059.0;;1;11;<python><matplotlib><plot><pandas>;How do I plot hatched bars using pandas?;1573.0
12264;12264;;1.0;"<p>I have installed Python via Anaconda using the doc at <a href=""http://www.kevinsheppard.com/images/0/09/Python_introduction.pdf"">http://www.kevinsheppard.com/images/0/09/Python_introduction.pdf</a> and my Pandas version is 0.13.1.</p>

<p>However, since I presently have some issue with this version (no possibility to really calculate the mean using resample with DataFrame) I would like to know how I can quickly upgrade my version to 0.14.</p>

<p>I use to work with 'Python for Data Analysis from Wes McKinney' but I would like to know if it will not be worth to use eclipse for debugging (even if there are bugs with some eclipse version for the installation of PyDev) and how I can use it jointly with anaconda without any conflict.</p>
";;1;;2014-04-03T14:15:38.757;4.0;22840449;2016-04-02T16:21:33.793;2014-04-03T14:31:43.227;;541136.0;;3102882.0;;1;12;<python><eclipse><pandas>;How to update Pandas from Anaconda and is it possible to use eclipse with this last?;17480.0
12363;12363;;4.0;"<p>I have a pandas data frame with a 'date' column. Now i need to filter out all rows in the dataframe that have dates outside of the next two months.
Essentially, I only need to retain the row that are within the next two months. 
What is the best way to achieve this.</p>
";;0;;2014-04-06T19:24:42.437;13.0;22898824;2017-01-25T07:02:52.043;2014-04-06T19:34:34.503;;2096752.0;;11212.0;;1;23;<python><datetime><pandas><filtering><dataframe>;filtering pandas dataframes on dates;50394.0
12403;12403;22918691.0;3.0;"<p>If I want to drop duplicated index in a dataframe the following doesn't work for obvious reasons:</p>

<pre><code>myDF.drop_duplicates(cols=index)
</code></pre>

<p>and </p>

<pre><code>myDF.drop_duplicates(cols='index') 
</code></pre>

<p>looks for a column named 'index'</p>

<p>If I want to drop an index I have to do:</p>

<pre><code>myDF['index'] = myDF.index
myDF= myDF.drop_duplicates(cols='index')
myDF.set_index = myDF['index']
myDF= myDF.drop('index', axis =1)
</code></pre>

<p>Is there a more efficient way?</p>
";2016-01-29T15:13:35.627;2;;2014-04-07T16:39:54.477;5.0;22918212;2015-10-28T09:31:17.317;;;;;2662493.0;;1;13;<python><pandas><duplicate-removal>;Fastest Way to Drop Duplicated Index in a Pandas DataFrame;11999.0
12417;12417;22924683.0;2.0;"<p>I have two columns from and to date in a dataframe </p>

<p>when I try add new column diff with to find the difference between two date using</p>

<pre><code>df['diff'] = df['todate'] - df['fromdate']
</code></pre>

<p>I get the diff column in days if more than 24 hours.</p>

<pre><code>2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""
2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000
2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000
</code></pre>

<p>How do I convert my results only in hours and minutes ignoring days and even seconds. </p>
";;0;;2014-04-07T21:49:30.233;6.0;22923775;2016-12-27T06:52:54.437;2014-04-07T22:16:20.400;;2180567.0;;3508652.0;;1;21;<python><datetime><pandas>;Calculate Pandas DataFrame Time Difference Between Two Columns in Hours and Minutes;31742.0
12469;12469;22964673.0;4.0;"<p>What is the best way to create a zero-filled pandas data frame of a given size?</p>

<p>I have used: </p>

<pre><code>zero_data = np.zeros(shape=(len(data),len(feature_list)))
d = pd.DataFrame(zero_data, columns=feature_list)
</code></pre>

<p>Is there a better way to do it?</p>
";;3;;2014-04-09T12:56:30.883;5.0;22963263;2017-03-27T18:08:19.557;2015-02-24T16:04:54.053;;249341.0;;223443.0;;1;22;<python><pandas><dataframe>;Creating a zero-filled pandas data frame;29486.0
12535;12535;;8.0;"<p>I am trying to get the Adj Close prices from Yahoo Finance into a DataFrame. I have all the stocks I want but I am not able to sort on date.</p>

<pre><code>stocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']
ls_key = 'Adj Close'
start = datetime(2014,1,1)
end = datetime(2014,3,28)    
f = web.DataReader(stocks, 'yahoo',start,end)


cleanData = f.ix[ls_key]
dataFrame = pd.DataFrame(cleanData)

print dataFrame[:5]
</code></pre>

<p>I get the following result, which is almost perfect.</p>

<pre><code>              IBM   MSFT   ORCL    TSLA   YELP
Date                                           
2014-01-02  184.52  36.88  37.61  150.10  67.92
2014-01-03  185.62  36.64  37.51  149.56  67.66
2014-01-06  184.99  35.86  37.36  147.00  71.72
2014-01-07  188.68  36.14  37.74  149.36  72.66
2014-01-08  186.95  35.49  37.61  151.28  78.42
</code></pre>

<p>However, the Date is not an Item. so when I run: </p>

<pre><code>print dataFrame['Date']
</code></pre>

<p>I get the error:</p>

<pre><code>KeyError: u'no item named Date'
</code></pre>

<p>Hope anyone can help me adding the Date.</p>
";;3;;2014-04-10T14:51:59.753;5.0;22991567;2017-02-11T20:09:43.603;;;;;1948338.0;;1;11;<python><pandas>;Pandas yahoo finance DataReader;26300.0
12564;12564;23005564.0;4.0;"<p>I'd like to replace values in a <code>Pandas</code> <code>DataFrame</code> larger than an arbitrary number (100 in this case) with <code>NaN</code> (as values this large are indicative of a failed experiment). Previously I've used this to replace unwanted values:</p>

<pre><code>sve2_all[sve2_all[' Hgtot ng/l'] &gt; 100] = np.nan
</code></pre>

<p>However, I got the following error:</p>

<pre><code>-c:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
C:\Users\AppData\Local\Enthought\Canopy32\User\lib\site-packages\pandas\core\indexing.py:346: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
self.obj[item] = s
</code></pre>

<p>From <a href=""https://stackoverflow.com/questions/20625582/how-to-deal-with-this-pandas-warning"">this StackExchange question</a>, it seems that sometimes this warning can be ignored, but I can't follow the discussion well enough to be certain whether this applies to my situation. Is the warning basically letting me know that I'll be overwriting some of the values in my <code>DataFrame</code>?</p>

<p>Edit: As far as I can tell, everything behaved as it should. As a follow up is my method of replacing values non-standard? Is there a better way to replace values?</p>
";;0;;2014-04-11T03:04:49.710;10.0;23002762;2016-07-08T18:25:11.177;2017-05-23T12:02:20.613;;-1.0;;2694260.0;;1;17;<python><python-2.7><pandas>;Pandas: SettingWithCopyWarning;28360.0
12720;12720;23104436.0;1.0;"<p>There is <em>DataFrame.to_sql</em> method, but it works only for mysql, sqlite and oracle databases. I cant pass to this method postgres connection or sqlalchemy engine.</p>
";;0;;2014-04-16T08:29:32.080;5.0;23103962;2015-12-03T21:12:48.263;;;;;1185696.0;;1;16;<python><postgresql><pandas><sqlalchemy>;How to write DataFrame to postgres table?;19350.0
12737;12737;23112008.0;2.0;"<p>I have an <em>n</em>-by-<em>m</em> Pandas DataFrame <code>df</code> defined as follows. (I know this is not the best way to do it. It makes sense for what I'm trying to do in my actual code, but that would be TMI for this post so just take my word that this approach works in my particular scenario.)</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(columns=['col1'])
&gt;&gt;&gt; df.append(Series([None]), ignore_index=True)
&gt;&gt;&gt; df
Empty DataFrame
Columns: [col1]
Index: []
</code></pre>

<p>I stored lists in the cells of this DataFrame as follows.</p>

<pre><code>&gt;&gt;&gt; df['column1'][0] = [1.23, 2.34]
&gt;&gt;&gt; df
     col1
0  [1, 2]
</code></pre>

<p>For some reason, the DataFrame stored this list as a string instead of a list.</p>

<pre><code>&gt;&gt;&gt; df['column1'][0]
'[1.23, 2.34]'
</code></pre>

<p>I have 2 questions for you.</p>

<ol>
<li><strong>Why does the DataFrame store a list as a string and is there a way around this behavior?</strong></li>
<li><strong>If not, then is there a Pythonic way to convert this string into a list?</strong></li>
</ol>

<hr>

<p><strong>Update</strong></p>

<p>The DataFrame I was using had been saved and loaded from a CSV format. <em>This format, rather than the DataFrame itself, converted the list from a string to a literal.</em></p>
";;7;;2014-04-16T14:12:55.533;4.0;23111990;2016-06-16T17:16:22.557;2016-06-16T17:16:22.557;;2261276.0;;2932774.0;;1;17;<python><string><list><pandas><dataframe>;Pandas DataFrame stored list as string: How to convert back to list?;8688.0
12795;12795;23143081.0;2.0;"<p>Lets say I have a dataframe like this</p>

<pre><code>    A   B
0   a   b
1   c   d
2   e   f 
3   g   h
</code></pre>

<p>0,1,2,3 are times, a, c, e, g is one time series and b, d, f, h is another time series.
I need to be able to add two columns to the orignal dataframe which is got by computing the differences of consecutive rows for certain columns. </p>

<p>So i need something like this</p>

<pre><code>    A   B   dA
0   a   b  (a-c)
1   c   d  (c-e)
2   e   f  (e-g)
3   g   h   Nan
</code></pre>

<p>I saw something called diff on the dataframe/series but that does it slightly differently as in first element will become Nan.</p>
";;0;;2014-04-17T20:36:24.787;8.0;23142967;2014-04-17T20:45:00.443;;;;;11212.0;;1;26;<pandas><dataframe><series>;Adding a column thats result of difference in consecutive rows in pandas;15505.0
12804;12804;23146038.0;2.0;"<p>wowee.....how to use iterrows with python and pandas?  If I do a row iteration should I not be able to access a col with row['COL_NAME']?</p>

<p>Here are the col names: </p>

<pre><code>print df
Int64Index: 152 entries, 0 to 151
Data columns:
Date          152  non-null values
Time          152  non-null values
Time Zone     152  non-null values
Currency      152  non-null values
Event         152  non-null values
Importance    152  non-null values
Actual        127  non-null values
Forecast      86  non-null values
Previous      132  non-null values
dtypes: object(9)

for row in df.iterrows():
    print row['Date']

Traceback (most recent call last):
  File ""/home/ubuntu/workspace/calandar.py"", line 34, in &lt;module&gt;
    print row['Date']
TypeError: tuple indices must be integers, not str
</code></pre>

<p>if I print 1 row:</p>

<pre><code>(0, Date                                                 Sun Apr 13
Time                                                      17:30
Time Zone                                                   GMT
Currency                                                    USD
Event         USD Fed's Stein Speaks on Financial Stability ...
Importance                                                  Low
Actual                                                      NaN
Forecast                                                    NaN
Previous                                                    NaN
Name: 0)
</code></pre>
";;0;;2014-04-18T01:12:06.423;3.0;23145928;2014-04-18T17:51:59.323;2014-04-18T17:51:59.323;;704848.0;;1203556.0;;1;12;<python><pandas>;python and pandas - how to access a column using iterrows;18257.0
12809;12809;23151722.0;3.0;"<p>I have a df in pandas</p>

<pre><code>import pandas as pd
df = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])
</code></pre>

<p>I want to iterate over rows in df. For each row i want row<code>s value and next row</code>s value
Something like(it does not work):</p>

<pre><code>for i, row in df.iterrows():
     print row['value']
     i1, row1 = next(df.iterrows())
     print row1['value']
</code></pre>

<p>As a result I want </p>

<pre><code>'AA'
'BB'
'BB'
'CC'
'CC'
*Wrong index error here  
</code></pre>

<p>At this point i have mess way to solve this</p>

<pre><code>for i in range(0, df.shape[0])
   print df.irow(i)['value']
   print df.irow(i+1)['value']
</code></pre>

<p>Is there more efficient way to solve this issue? </p>
";;0;;2014-04-18T09:26:44.670;9.0;23151246;2015-06-19T14:07:17.827;;;;;2968805.0;;1;18;<python><pandas><next>;iterrows pandas get next rows value;40825.0
12851;12851;23178185.0;2.0;"<p>How do I get the min and max Date's from a dataframes major axis?</p>

<pre><code>           value
Date                                           
2014-03-13  10000.000 
2014-03-21   2000.000 
2014-03-27   2000.000 
2014-03-17    200.000 
2014-03-17      5.000 
2014-03-17     70.000 
2014-03-21    200.000 
2014-03-27      5.000 
2014-03-27     25.000 
2014-03-31      0.020 
2014-03-31     12.000 
2014-03-31      0.022
</code></pre>

<p>Essentially I want a way to get the min and max dates,i.e. <code>2014-03-13</code> and <code>2014-03-31</code>. I tried using <code>numpy.min</code> or <code>df.min(axis=0)</code>, I'm able to get the min or max value but thats not what I want</p>
";;0;;2014-04-20T03:32:13.090;2.0;23178129;2017-04-27T11:32:51.400;;;;;848277.0;;1;12;<python><datetime><pandas>;Getting min and max Date's from a pandas dataframe;11701.0
12878;12878;23198160.0;1.0;"<p>With the following DataFrame, how can I shift the ""beyer"" column based on the index without having Pandas assign the shifted value to a different index value?</p>

<pre><code>                  line_date  line_race  beyer
horse                                        
Last Gunfighter  2013-09-28         10     99
Last Gunfighter  2013-08-18         10    102
Last Gunfighter  2013-07-06          8    103
.....
Paynter          2013-09-28         10    103
Paynter          2013-08-31         10     88
Paynter          2013-07-27          8    100
</code></pre>

<p><code>df['beyer'].shift(1)</code> produces...</p>

<pre><code>                  line_date  line_race  beyer  beyer_shifted
horse                                                       
Last Gunfighter  2013-09-28         10     99            NaN
Last Gunfighter  2013-08-18         10    102             99
Last Gunfighter  2013-07-06          8    103            102
.....
Paynter          2013-09-28         10    103             71
Paynter          2013-08-31         10     88            103
Paynter          2013-07-27          8    100             88
</code></pre>

<p>The problem is that Paynter was given a beyer that Last Gunfighter (his first record) was assigned. Instead I want it to go like this...</p>

<pre><code>                  line_date  line_race  beyer  beyer_shifted
horse                                                       
Last Gunfighter  2013-09-28         10     99            NaN
Last Gunfighter  2013-08-18         10    102             99
Last Gunfighter  2013-07-06          8    103            102
.....
Paynter          2013-09-28         10    103            NaN
Paynter          2013-08-31         10     88            103
Paynter          2013-07-27          8    100             88
</code></pre>
";;0;;2014-04-21T13:05:45.553;8.0;23198053;2016-04-28T12:29:06.033;;;;;867549.0;;1;13;<python><pandas>;How do you shift Pandas DataFrame with a multiindex?;6890.0
12881;12881;;6.0;"<p>I have a pandas dataframe with few columns.</p>

<p>Now I know that certain rows are outliers based on a certain column value.</p>

<p>For instance columns - 'Vol' has all values around 12.xx and one value which is 4000</p>

<p>Now I would like to exclude those rows that have Vol Column like this.</p>

<p>So essentially I need to put a filter such that we select all rows wehre the values of a certain column are within say 3 standard deviations from mean.</p>

<p>Whats an elegant way to achieve this. </p>
";;1;;2014-04-21T14:51:37.383;28.0;23199796;2017-07-13T14:14:31.687;2014-04-21T15:47:21.870;;2487184.0;;11212.0;;1;51;<python><pandas><filtering><dataframe><outliers>;Detect and exclude outliers in Pandas dataframe;40223.0
12993;12993;;1.0;"<p>How can I calculate Principal Components Analysis from data in a pandas dataframe?</p>
";;1;;2014-04-25T00:22:48.223;5.0;23282130;2017-08-03T03:13:33.923;2016-10-04T19:31:18.680;;2241910.0;;3362813.0;;1;25;<python><pandas><pca><scientific-computing><principal-components>;Principal components analysis using pandas dataframe;13077.0
13011;13011;23296545.0;1.0;"<p>I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.</p>

<p>If I have, for example,</p>

<pre><code>df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])
</code></pre>

<p>I understand that a <code>query</code> returns a copy so that something like</p>

<pre><code>foo = df.query('2 &lt; index &lt;= 5')
foo.loc[:,'E'] = 40
</code></pre>

<p>will have no effect on the original dataframe, <code>df</code>. I also understand that scalar or named slices return a view, so that assignments to these, such as </p>

<pre><code>df.iloc[3] = 70
</code></pre>

<p>or </p>

<pre><code>df.ix[1,'B':'E'] = 222
</code></pre>

<p>will change <code>df</code>. But I'm lost when it comes to more complicated cases. For example, </p>

<pre><code>df[df.C &lt;= df.B]  = 7654321
</code></pre>

<p>changes <code>df</code>, but</p>

<pre><code>df[df.C &lt;= df.B].ix[:,'B':'E']
</code></pre>

<p>does not.</p>

<p>Is there a simple rule that Pandas is using that I'm just missing? What's going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I'm attempting to do in the last example above)?</p>

<hr>

<p>Note: This is not the same as <a href=""https://stackoverflow.com/q/17960511/656912"">this question</a>; and I have read <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">the documentation</a>, but am not enlightened by it. I've also read through the ""Related"" questions on this topic, but I'm still missing the simple rule Pandas is using, and how I'd apply it to  for example modify the values (or a subset of values) in a dataframe that satisfy a particular query.</p>
";;0;;2014-04-25T14:44:07.503;21.0;23296282;2015-08-15T11:12:44.370;2017-05-23T12:34:18.457;;-1.0;;656912.0;;1;39;<python><pandas><indexing><dataframe><slice>;What rules does Pandas use to generate a view vs a copy?;8967.0
13029;13029;23307361.0;6.0;"<p>I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. </p>

<p>I have tried the following:</p>

<pre><code>w['female']['female']='1'
w['female']['male']='0' 
</code></pre>

<p>But receive the exact same copy of the previous results.</p>

<p>I would ideally like to get some output which resembles the following loop element-wise.</p>

<pre><code>if w['female'] =='female':
    w['female'] = '1';
else:
    w['female'] = '0';
</code></pre>

<p>I've looked through the gotchas documentation (<a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html"">http://pandas.pydata.org/pandas-docs/stable/gotchas.html</a>) but cannot figure out why nothing happens.</p>

<p>Any help will be appreciated.</p>
";;1;;2014-04-26T06:04:24.873;14.0;23307301;2016-12-01T10:27:08.703;;;;;2771315.0;;1;33;<python><pandas>;Pandas: Replacing column values in dataframe;84170.0
13062;13062;23317595.0;2.0;"<p>I have a pandas dataframe with a column named 'City, State, Country'. I want to separate this column into three new columns, 'City, 'State' and 'Country'.</p>

<pre><code>0                 HUN
1                 ESP
2                 GBR
3                 ESP
4                 FRA
5             ID, USA
6             GA, USA
7    Hoboken, NJ, USA
8             NJ, USA
9                 AUS
</code></pre>

<p>Splitting the column into three columns is trivial enough:</p>

<pre><code>location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))
</code></pre>

<p>However, this creates left-aligned data:</p>

<pre><code>     0       1       2
0    HUN     NaN     NaN
1    ESP     NaN     NaN
2    GBR     NaN     NaN
3    ESP     NaN     NaN
4    FRA     NaN     NaN
5    ID      USA     NaN
6    GA      USA     NaN
7    Hoboken  NJ     USA
8    NJ      USA     NaN
9    AUS     NaN     NaN
</code></pre>

<p>How would one go about creating the new columns with the data right-aligned? Would I need to iterate through every row, count the number of commas and handle the contents individually?</p>
";;0;;2014-04-26T22:49:49.937;14.0;23317342;2017-01-27T15:59:44.397;;;;;3186581.0;;1;26;<python><split><pandas>;Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries;31355.0
13076;13076;;2.0;"<p>I have a pandas data frame that looks like this (its a pretty big one)</p>

<pre><code>           date      exer exp     ifor         mat  
1092  2014-03-17  American   M  528.205  2014-04-19 
1093  2014-03-17  American   M  528.205  2014-04-19 
1094  2014-03-17  American   M  528.205  2014-04-19 
1095  2014-03-17  American   M  528.205  2014-04-19    
1096  2014-03-17  American   M  528.205  2014-05-17 
</code></pre>

<p>now I would like to iterate row by row and as I go through each row, the value of <code>ifor</code>
in each row can change depending on some conditions and I need to lookup another dataframe.</p>

<p>Now, how do I update this as I iterate.
Tried a few things none of them worked.</p>

<pre><code>for i, row in df.iterrows():
    if &lt;something&gt;:
        row['ifor'] = x
    else:
        row['ifor'] = y

    df.ix[i]['ifor'] = x
</code></pre>

<p>None of these approaches seem to work. I don't see the values updated in the dataframe.</p>
";;3;;2014-04-28T00:19:49.537;8.0;23330654;2015-04-23T10:42:59.110;2015-04-23T10:42:59.110;;509706.0;;11212.0;;1;38;<python><pandas><updates><dataframe>;Update a dataframe in pandas while iterating row by row;31600.0
13127;13127;23354240.0;1.0;"<p>I have a pandas DataFrame, eg:</p>

<pre><code>x = DataFrame.from_dict({'farm' : ['A','B','A','B'], 
                         'fruit':['apple','apple','pear','pear'], 
                         '2014':[10,12,6,8], 
                         '2015':[11,13,7,9]})
</code></pre>

<p>ie:</p>

<pre><code>   2014  2015 farm  fruit
0    10    11    A  apple
1    12    13    B  apple
2     6     7    A   pear
3     8     9    B   pear
</code></pre>

<p>How can I convert it to this: ?</p>

<pre><code>  farm  fruit  value  year
0    A  apple     10  2014
1    B  apple     12  2014
2    A   pear      6  2014
3    B   pear      8  2014
4    A  apple     11  2015
5    B  apple     13  2015
6    A   pear      7  2015
7    B   pear      9  2015
</code></pre>

<p>I have tried <code>stack</code> and <code>unstack</code> but haven't been able to make it work.</p>

<p>Thanks!</p>
";;0;;2014-04-29T01:48:25.833;2.0;23354124;2014-04-29T02:05:49.027;;;;;905720.0;;1;14;<python><pandas><pivot-table>;"How can I ""unpivot"" specific columns from a pandas DataFrame?";4373.0
13158;13158;23377232.0;3.0;"<p>This is obviously simple, but as a numpy newbe I'm getting stuck.</p>

<p>I have a CSV file that contains 3 columns, the State, the Office ID, and the Sales for that office.</p>

<p>I want to calculate the percentage of sales per office in a given state (total of all percentages in each state is 100%).</p>

<pre><code>df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': range(1, 7) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})

df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
</code></pre>

<p>This returns:</p>

<pre><code>                  sales
state office_id        
AZ    2          839507
      4          373917
      6          347225
CA    1          798585
      3          890850
      5          454423
CO    1          819975
      3          202969
      5          614011
WA    2          163942
      4          369858
      6          959285
</code></pre>

<p>I can't seem to figure out how to ""reach up"" to the <code>state</code> level of the <code>groupby</code> to total up the <code>sales</code> for the entire <code>state</code> to calculate the fraction.</p>
";;0;;2014-04-29T23:30:40.203;13.0;23377108;2017-01-25T19:17:49.247;2017-01-25T18:26:26.710;;1840471.0;;126617.0;;1;29;<python><pandas>;Pandas percentage of total with groupby;34307.0
13196;13196;23394706.0;1.0;"<p>I'm using <code>groupby</code> on a pandas dataframe to drop all rows that don't have the minimum of a specific column. Something like this: </p>

<pre><code>df1 = df.groupby(""item"", as_index=False)[""diff""].min()
</code></pre>

<p>However, if I have more than those two columns, the other columns get dropped. Can I keep those columns using groupby, or am I going to have to find a different way to drop the rows?</p>

<p>My data looks like: </p>

<pre><code>    item    diff   otherstuff
   0   1       2            1
   1   1       1            2
   2   1       3            7
   3   2      -1            0
   4   2       1            3
   5   2       4            9
   6   2      -6            2
   7   3       0            0
   8   3       2            9
</code></pre>

<p>and should end up like:</p>

<pre><code>    item   diff  otherstuff
   0   1      1           2
   1   2     -6           2
   2   3      0           0
</code></pre>

<p>but what I'm getting is:</p>

<pre><code>    item   diff
   0   1      1           
   1   2     -6           
   2   3      0           
</code></pre>

<p>I've been looking through the documentation and can't find anything. I tried:</p>

<pre><code>df1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()

df1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]

df1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()
</code></pre>

<p>But none of those work (realized with the last one that the syntax is meant for aggregating after a group is created).</p>
";;0;;2014-04-30T17:29:50.187;12.0;23394476;2014-04-30T17:53:55.300;2014-04-30T17:53:55.300;;2375855.0;;2880319.0;;1;16;<python><pandas>;Keep other columns when using min() with groupby;8277.0
13244;13244;23428804.0;1.0;"<p>I am trying to create a stacked bar graph that replicates the picture, all my data is separate from that excel spreadsheet.</p>

<p><img src=""https://i.stack.imgur.com/z6Hzd.jpg"" alt=""enter image description here""></p>

<p>I cant figure out how to make a dataframe for it like pictured, nor can I figure out how to make the stacked bar chart. All examples I locate work in different ways to what I'm trying to create.</p>

<p>My dataframe is a csv of all values narrowed down to the following with a pandas dataframe.</p>

<pre><code>      Site Name    Abuse/NFF
0    NORTH ACTON       ABUSE
1    WASHINGTON         -
2    WASHINGTON        NFF
3    BELFAST            -
4    CROYDON            - 
</code></pre>

<p>I have managed to count the data with totals and get individual counts for each site, I just cant seem to combine it in a way to graph.</p>

<p>Would really appreciate some strong guidance.</p>

<p>Completed code, many thanks for the assistance completing.</p>

<pre><code>test5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)

test5.plot(kind='bar', stacked=True)
</code></pre>
";;1;;2014-05-01T19:53:27.373;3.0;23415500;2016-12-08T04:45:43.617;2014-05-02T20:54:29.560;;3565846.0;;3565846.0;;1;11;<python><matplotlib><pandas><ipython-notebook><python-3.4>;Pandas - Plotting a stacked Bar Chart;17781.0
13315;13315;;3.0;"<p>I am working my way through Wes's Python For Data Analysis, and I've run into a strange problem that is not addressed in the book.</p>

<p>In the code below, based on page 199 of his book, I create a dataframe and then use <code>pd.cut()</code> to create <code>cat_obj</code>. According to the book, <code>cat_obj</code> is </p>

<blockquote>
  <p>""a special Categorical object. You can treat it like an array of
  strings indicating the bin name; internally it contains a levels array
  indicating the distinct category names along with a labeling for the
  ages data in the labels attribute""</p>
</blockquote>

<p>Awesome! However, if I use the exact same <code>pd.cut()</code> code (In [5] below) to create a new column of the dataframe (called <code>df['cat']</code>), that column is not treated as a special <em>categorical variable</em> but simply as a regular pandas series.</p>

<p>How, then, do I create a column in a dataframe that is treated as a categorical variable?</p>

<pre><code>In [4]:

import pandas as pd

raw_data = {'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], 
        'score': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}
df = pd.DataFrame(raw_data, columns = ['name', 'score'])

bins = [0, 25, 50, 75, 100]
group_names = ['Low', 'Okay', 'Good', 'Great']

In [5]:
cat_obj = pd.cut(df['score'], bins, labels=group_names)
df['cat'] = pd.cut(df['score'], bins, labels=group_names)
In [7]:

type(cat_obj)
Out[7]:
pandas.core.categorical.Categorical
In [8]:

type(df['cat'])
Out[8]:
pandas.core.series.Series
</code></pre>
";;6;;2014-05-03T23:05:01.927;2.0;23450735;2015-11-16T12:15:31.457;;;;;2935984.0;;1;15;<python><pandas><categorical-data>;Categorical Variables In A Pandas Dataframe?;4988.0
13318;13318;23451304.0;2.0;"<p>I have a pandas dataframe with a column of real values that I want to zscore normalize:</p>

<pre><code>&gt;&gt; a
array([    nan,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,  0.6307,
        0.6599,  0.1065,  0.0508])
&gt;&gt; df = pandas.DataFrame({""a"": a})
</code></pre>

<p>The problem is that a single <code>nan</code> value makes all the array <code>nan</code>:</p>

<pre><code>&gt;&gt; from scipy.stats import zscore
&gt;&gt; zscore(df[""a""])
array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])
</code></pre>

<p>What's the correct way to apply <code>zscore</code> (or an equivalent function not from scipy) to a column of a pandas dataframe and have it ignore the <code>nan</code> values? I'd like it to be same dimension as original column with <code>np.nan</code> for values that can't be normalized</p>

<p><strong>edit</strong>: maybe the best solution is to use <code>scipy.stats.nanmean</code> and <code>scipy.stats.nanstd</code>? I don't see why the degrees of freedom need to be changed for <code>std</code> for this purpose:</p>

<pre><code>zscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)
</code></pre>
";;0;;2014-05-04T00:23:18.540;4.0;23451244;2014-05-04T17:01:28.167;2014-05-04T00:49:26.910;;248237.0;;248237.0;;1;13;<python><numpy><pandas><scipy>;how to zscore normalize pandas column with nans?;9629.0
13324;13324;23479973.0;9.0;"<p>I'm trying to create N balanced random subsamples of my large unbalanced dataset. Is there a way to do this simply with scikit-learn / pandas or do I have to implement it myself? Any pointers to code that does this?</p>

<p>These subsamples should be random and can be overlapping as I feed each to separate classifier in a very large ensemble of classifiers.</p>

<p>In Weka there is tool called spreadsubsample, is there equivalent in sklearn? 
<a href=""http://wiki.pentaho.com/display/DATAMINING/SpreadSubsample"">http://wiki.pentaho.com/display/DATAMINING/SpreadSubsample</a></p>

<p>(I know about weighting but that's not what I'm looking for.)</p>
";;2;;2014-05-04T11:31:38.177;10.0;23455728;2017-08-01T19:01:40.033;;;;;615985.0;;1;20;<python><pandas><scikit-learn><subsampling>;Scikit-learn balanced subsampling;13375.0
13381;13381;23483221.0;2.0;"<p>After reading through: <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.sort.html"">http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.sort.html</a></p>

<p>I still can't seem to figure out how to sort a column by a custom list.  Obviously, the default sort is alphabetical.  I'll give an example.  Here is my (very abridged) dataframe:</p>

<pre><code>             Player      Year   Age   Tm     G
2967     Cedric Hunter   1991    27  CHH     6
5335     Maurice Baker   2004    25  VAN     7
13950    Ratko Varda     2001    22  TOT     60
6141     Ryan Bowen      2009    34  OKC     52
6169     Adrian Caldwell 1997    31  DAL     81
</code></pre>

<p>I want to be able to sort by Player, Year and then Tm.  The default sort by Player and Year is fine for me, in normal order.  However, I do not want Team sorted alphabetically b/c I want TOT always at the top.</p>

<p>Here is the list I created:</p>

<pre><code>sorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL', 'DEN',
   'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',
   'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',
   'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',
   'WAS', 'WSB']
</code></pre>

<p>After reading through the link above, I thought this would work but it didn't:</p>

<pre><code>df.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])
</code></pre>

<p>It still has ATL at the top, meaning that it sorted alphabetically and not according to my custom list.  Any help would really be greatly appreciated, I just can't figure this out.</p>
";;2;;2014-05-05T22:04:47.910;6.0;23482668;2016-04-27T09:17:16.810;;;;;2989523.0;;1;15;<python><sorting><pandas>;sorting by a custom list in pandas;8245.0
13418;13418;23509622.0;1.0;"<p>I am trying to calculate time based aggregations in Pandas based on date values stored in a separate tables.</p>

<p>The top of the first table table_a looks like this:</p>

<pre><code>    COMPANY_ID  DATE            MEASURE
    1   2010-01-01 00:00:00     10
    1   2010-01-02 00:00:00     10
    1   2010-01-03 00:00:00     10
    1   2010-01-04 00:00:00     10
    1   2010-01-05 00:00:00     10
</code></pre>

<p>Here is the code to create the table:</p>

<pre><code>    table_a = pd.concat(\
    [pd.DataFrame({'DATE': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\
    'COMPANY_ID': 1 , 'MEASURE': 10}),\
    pd.DataFrame({'DATE': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\
    'COMPANY_ID': 2 , 'MEASURE': 10})])
</code></pre>

<p>The second table, table_b looks like this:</p>

<pre><code>        COMPANY     END_DATE
        1   2010-03-01 00:00:00
        1   2010-06-02 00:00:00
        2   2010-03-01 00:00:00
        2   2010-06-02 00:00:00
</code></pre>

<p>and the code to create it is:</p>

<pre><code>    table_b = pd.DataFrame({'END_DATE':pd.to_datetime(['03/01/2010','06/02/2010','03/01/2010','06/02/2010']),\
                    'COMPANY':(1,1,2,2)})
</code></pre>

<p>I want to be able to get the sum of the measure column for each COMPANY_ID for each 30 day period prior to the END_DATE in table_b.</p>

<p>This is (I think) the SQL equivalent:</p>

<pre><code>      select
 b.COMPANY_ID,
 b.DATE
 sum(a.MEASURE) AS MEASURE_TO_END_DATE
 from table_a a, table_b b
 where a.COMPANY = b.COMPANY and
       a.DATE &lt; b.DATE and
       a.DATE &gt; b.DATE - 30  
 group by b.COMPANY;
</code></pre>

<p>Thanks for any help</p>
";;2;;2014-05-07T03:46:02.330;2.0;23508351;2014-05-07T19:34:36.923;2014-05-07T04:08:19.757;;2362381.0;;2362381.0;;1;13;<python><join><pandas>;How to do a conditional join in python Pandas?;8135.0
13434;13434;23696169.0;2.0;"<p>Dataframes in Pandas have a <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.boxplot.html"" rel=""nofollow noreferrer"">boxplot</a> method, but is there any way to create <strong>dot-boxplots</strong> in Pandas, or otherwise with <a href=""http://www.stanford.edu/~mwaskom/software/seaborn/index.html"" rel=""nofollow noreferrer"">seaborn</a>? </p>

<p>By a dot-boxplot, I mean a boxplot that <strong>shows the actual data points</strong> (or a relevant sample of them) inside the plot, e.g. like the example below (obtained in R). </p>

<p><img src=""https://i.stack.imgur.com/3IkgQ.png"" alt=""enter image description here""></p>
";;6;;2014-05-07T13:25:09.327;1.0;23519135;2015-02-17T16:16:43.903;2014-05-07T13:34:42.747;;283296.0;;283296.0;;1;11;<python><matplotlib><pandas><seaborn>;Dot-boxplots from DataFrames;4100.0
13439;13439;;3.0;"<p>My current code is shown below - I'm importing a MAT file and trying to create a DataFrame from variables within it:</p>

<pre><code>mat = loadmat(file_path)  # load mat-file
Variables = mat.keys()    # identify variable names

df = pd.DataFrame         # Initialise DataFrame

for name in Variables:

    B = mat[name]
    s = pd.Series (B[:,1])
</code></pre>

<p>So within the loop I can create a series of each variable (they're arrays with two columns - so the values I need are in column 2)</p>

<p>My question is how do I append the series to the dataframe? I've looked through the documentation and none of the examples seem to fit what I'm trying to do.</p>

<p>Best Regards,</p>

<p>Ben</p>
";;0;;2014-05-07T15:06:40.637;1.0;23521511;2015-06-28T09:38:02.737;;;;;3215592.0;;1;11;<python><pandas><mat>;Pandas: Creating DataFrame from Series;16290.0
13490;13490;23544011.0;1.0;"<p>I have a pandas dataframe that has two datetime64 columns and one timedelta64 column that is the difference between the two columns. I'm trying to plot a histogram of the timedelta column to visualize the time differences between the two events.</p>

<p>However, just using <code>df['time_delta']</code> results in:
<code>TypeError: ufunc add cannot use operands with types dtype('&lt;m8[ns]') and dtype('float64')</code></p>

<p>Trying to convert the timedelta column to : <code>float--&gt; df2 = df1['time_delta'].astype(float)</code> 
results in:
<code>TypeError: cannot astype a timedelta from [timedelta64[ns]] to [float64]</code></p>

<p>How would one create a histogram of pandas timedelta data?</p>
";;1;;2014-05-08T13:57:39.613;4.0;23543909;2014-05-08T14:02:51.167;2014-05-08T14:02:51.167;;1398915.0;;3325052.0;;1;18;<python><matplotlib><pandas>;Plotting pandas timedelta;9760.0
13514;13514;23549599.0;4.0;"<p>I am sure there is an obvious way to do this but cant think of anything slick right now.</p>

<p>Basically instead of raising exception I would like to get <code>True</code> or <code>False</code> to see if a value exists in pandas <code>df</code> index.</p>

<pre><code>df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])

df.loc['g']  # (should give False)
</code></pre>

<p>What I have working now is the following</p>

<pre><code>sum(df.index == 'g')
</code></pre>
";;1;;2014-05-08T17:59:49.123;5.0;23549231;2017-06-05T19:51:07.230;2017-01-28T21:28:39.593;;1478537.0;;369541.0;;1;41;<python><pandas><ipython>;Check if a value exists in pandas dataframe index;45304.0
13517;13517;;4.0;"<p>I've been following 'python for data analysis'. On pg. 345, you get to this code to plot returns across a variety of stocks. However, the plotting function does not work for me. I get 
FigureCanvasAgg' object has no attribute 'invalidate' ?</p>

<pre><code>names = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work
def get_px(stock, start, end):
    return web.get_data_yahoo(stock, start, end)['Adj Close']
px = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})

#fillna method pad uses last valid observation to fill
px = px.asfreq('B').fillna(method='pad')
rets = px.pct_change()
df2 = ((1 + rets).cumprod() - 1)

df2.ix[0] = 1

df2.plot()
</code></pre>

<p>UPDATE: full traceback</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-122-df192c0432be&gt; in &lt;module&gt;()
      6 df2.ix[0] = 1
      7 
----&gt; 8 df2.plot()

//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, **kwds)
   1634                      logy=logy, sort_columns=sort_columns,
   1635                      secondary_y=secondary_y, **kwds)
-&gt; 1636     plot_obj.generate()
   1637     plot_obj.draw()
   1638     if subplots:

//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in generate(self)
    854         self._compute_plot_data()
    855         self._setup_subplots()
--&gt; 856         self._make_plot()
    857         self._post_plot_logic()
    858         self._adorn_subplots()

//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_plot(self)
   1238         if not self.x_compat and self.use_index and self._use_dynamic_x():
   1239             data = self._maybe_convert_index(self.data)
-&gt; 1240             self._make_ts_plot(data, **self.kwds)
   1241         else:
   1242             lines = []

//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)
   1319                 self._maybe_add_color(colors, kwds, style, i)
   1320 
-&gt; 1321                 _plot(data[col], i, ax, label, style, **kwds)
   1322 
   1323         self._make_legend(lines, labels)

//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _plot(data, col_num, ax, label, style, **kwds)
   1293         def _plot(data, col_num, ax, label, style, **kwds):
   1294             newlines = tsplot(data, plotf, ax=ax, label=label,
-&gt; 1295                                 style=style, **kwds)
   1296             ax.grid(self.grid)
   1297             lines.append(newlines[0])

//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in tsplot(series, plotf, **kwargs)
     79 
     80     # set date formatter, locators and rescale limits
---&gt; 81     format_dateaxis(ax, ax.freq)
     82     left, right = _get_xlim(ax.get_lines())
     83     ax.set_xlim(left, right)

//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in format_dateaxis(subplot, freq)
    258     subplot.xaxis.set_major_formatter(majformatter)
    259     subplot.xaxis.set_minor_formatter(minformatter)
--&gt; 260     pylab.draw_if_interactive()

//anaconda/lib/python2.7/site-packages/IPython/utils/decorators.pyc in wrapper(*args, **kw)
     41     def wrapper(*args,**kw):
     42         wrapper.called = False
---&gt; 43         out = func(*args,**kw)
     44         wrapper.called = True
     45         return out

//anaconda/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.pyc in draw_if_interactive()
    227         figManager =  Gcf.get_active()
    228         if figManager is not None:
--&gt; 229             figManager.canvas.invalidate()
    230 
    231 

AttributeError: 'FigureCanvasAgg' object has no attribute 'invalidate'
</code></pre>
";;3;;2014-05-08T18:43:40.227;4.0;23550056;2016-03-10T16:25:10.073;2014-05-08T21:45:57.020;;3314418.0;;3314418.0;;1;14;<python><matplotlib><pandas><financial>;FigureCanvasAgg' object has no attribute 'invalidate' ? python plotting;5778.0
13598;13598;23600844.0;2.0;"<p>Given a dictionary of data frames like:</p>

<pre><code>dict = {'ABC': df1, 'XYZ' : df2}   # of any length...
</code></pre>

<p>where each data frame has the same columns and similar index, for example:</p>

<pre><code>data           Open     High      Low    Close   Volume
Date                                                   
2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833
2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866
2002-01-21  0.19523  0.20970  0.19162  0.20608   771149
</code></pre>

<p>What is the simplest way to combine all the data frames into one, with a multi-index like:</p>

<pre><code>symbol         ABC                                       XYZ
data           Open     High      Low    Close   Volume  Open ...
Date                                                   
2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  ...
2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  ...
2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  ...
</code></pre>

<p>I've tried a few methods - eg for each data frame replace the columns with a multi-index like <code>.from_product(['ABC', columns])</code> and then concatenate along <code>axis=1</code>, without success.</p>
";;0;;2014-05-12T03:23:03.830;7.0;23600582;2014-05-12T14:28:14.100;2014-05-12T14:28:14.100;;247702.0;;424153.0;;1;17;<python><pandas><multi-index>;Concatenate Pandas columns under new multi-index level;6817.0
13711;13711;34272155.0;3.0;"<p>The <code>pandas</code> <code>drop_duplicates</code> function is great for ""uniquifying"" a dataframe. However, one of the keyword arguments to pass is <code>take_last=True</code> or <code>take_last=False</code>, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible?</p>

<pre><code>    A   B   C
0   foo 0   A
1   foo 1   A
2   foo 1   B
3   bar 1   A
</code></pre>

<p>As an example, I would like to drop rows which match on columns <code>A</code> and <code>C</code> so this should drop rows 0 and 1.</p>
";;1;;2014-05-15T00:31:47.783;3.0;23667369;2015-12-14T16:38:02.883;2014-05-15T01:28:05.467;;1706564.0;;1706564.0;;1;16;<python><pandas><duplicates>;Drop all duplicate rows in Python Pandas;36128.0
13715;13715;23671390.0;5.0;"<p>I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. </p>

<p>How can I ""join"" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name?</p>

<p>The <code>join()</code> function in pandas specifies that I need a multiindex, but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index. </p>
";;3;;2014-05-15T02:51:40.553;18.0;23668427;2017-04-18T22:17:31.207;2014-05-15T03:21:23.157;;712997.0;;712997.0;;1;34;<python><pandas>;pandas joining multiple dataframes on columns;46068.0
13750;13750;23691168.0;2.0;"<p>I want to split the following dataframe based on column ZZ</p>

<pre><code>df = 
        N0_YLDF  ZZ        MAT
    0  6.286333   2  11.669069
    1  6.317000   6  11.669069
    2  6.324889   6  11.516454
    3  6.320667   5  11.516454
    4  6.325556   5  11.516454
    5  6.359000   6  11.516454
    6  6.359000   6  11.516454
    7  6.361111   7  11.516454
    8  6.360778   7  11.516454
    9  6.361111   6  11.516454
</code></pre>

<p>As output, I want a new dataframe with the 'N0_YLDF' column split into 4, one new column for each unique value of ZZ. How do I go about this? I can do groupby, but do not know what to do with the grouped object.</p>
";;0;;2014-05-16T01:10:06.967;4.0;23691133;2017-03-13T02:55:17.570;;;;;308827.0;;1;14;<python><pandas>;Split pandas dataframe based on groupby;6273.0
13803;13803;23732825.0;2.0;"<p>I am trying to read data from a csv file into a pandas dataframe, and access the first column 'Date'</p>

<pre><code>import pandas as pd
df_ticks=pd.read_csv('values.csv', delimiter=',')
print(df_ticks.columns)
df_ticks['Date']
</code></pre>

<p>produces the following result</p>

<pre><code>Index([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')
KeyError: u'no item named Date'
</code></pre>

<p>If I try to acces any other column like 'Open' or 'Volume' it is working as expected</p>
";;1;;2014-05-19T07:00:48.320;4.0;23731564;2017-05-18T06:15:22.310;2017-05-18T06:15:22.310;;1478537.0;;984308.0;;1;11;<python><pandas>;KeyError when indexing Pandas dataframe;30310.0
13812;13812;23743582.0;1.0;"<p>I have table <code>x</code>:</p>

<pre><code>        website
0   http://www.google.com/
1   http://www.yahoo.com
2   None
</code></pre>

<p>I want to replace python None with pandas NaN. I tried:</p>

<pre><code>x.replace(to_replace=None, value=np.nan)
</code></pre>

<p>But I got:</p>

<pre><code>TypeError: 'regex' must be a string or a compiled regular expression or a list or dict of strings or regular expressions, you passed a 'bool'
</code></pre>

<p>How should I go about it? </p>
";;0;;2014-05-19T17:10:12.977;2.0;23743460;2017-07-07T05:55:29.480;;;;;927667.0;;1;17;<pandas>;Replacing None with NaN in pandas;8731.0
13820;13820;23747587.0;3.0;"<p>I have a df like this:</p>

<pre><code>    a b           c
    1 NaT         w
    2 2014-02-01  g
    3 NaT         x   

    df=df[df.b=='2014-02-01']
</code></pre>

<p>will give me</p>

<pre><code>    a  b          c
    2 2014-02-01  g
</code></pre>

<p>I want a database of all rows with NaT in column b?</p>

<pre><code>   df=df[df.b==None] #Doesn't work
</code></pre>

<p>I want this:</p>

<pre><code>    a b           c
    1 NaT         w
    3 NaT         x    
</code></pre>
";;2;;2014-05-19T21:23:00.497;1.0;23747451;2016-10-31T09:47:25.540;2014-05-19T21:26:45.737;;345480.0;;3371523.0;;1;13;<python><pandas><dataframe>;Filtering all rows with NaT in a column in Dataframe python;16311.0
13824;13824;;4.0;"<p>I am pulling a subset of data from a column based on conditions in another column being met.</p>

<p>I can get the correct values back but it is in pandas.core.frame.DataFrame.  How do I convert that to list?</p>

<pre><code>import pandas as pd

tst = pd.read_csv('C:\\SomeCSV.csv')

lookupValue = tst['SomeCol'] == ""SomeValue""
ID = tst[lookupValue][['SomeCol']]
#How To convert ID to a list
</code></pre>
";;1;;2014-05-20T00:00:09.007;15.0;23748995;2017-08-10T14:33:52.383;;;;;3646105.0;;1;43;<python><pandas><tolist>;Pandas DataFrame to list;103159.0
13906;13906;23787275.0;1.0;"<p>I am trying to join to dataframe on the same column ""Date"", the code is as follow:</p>

<pre><code>import pandas as pd
from datetime import datetime
df_train_csv = pd.read_csv('./train.csv',parse_dates=['Date'],index_col='Date')

start = datetime(2010, 2, 5)
end = datetime(2012, 10, 26)

df_train_fly = pd.date_range(start, end, freq=""W-FRI"")
df_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=['Date'])

merged = df_train_csv.join(df_train_fly.set_index(['Date']), on = ['Date'], how = 'right', lsuffix='_x')
</code></pre>

<p>It complains dataframe df_train_csv has no column named ""Date"". I'd like to set ""Date"" in both dataframe as index and I am wondering what is the best way to join dataframe with date as the index?</p>

<h1>UPDATE:</h1>

<p>That is the sample data</p>

<pre><code>Date,Weekly_Sales
2010-02-05,24924.5
2010-02-12,46039.49
2010-02-19,41595.55
2010-02-26,19403.54
2010-03-05,21827.9
2010-03-12,21043.39
2010-03-19,22136.64
2010-03-26,26229.21
2010-04-02,57258.43
2010-04-09,42960.91
2010-04-16,17596.96
2010-04-23,16145.35
2010-04-30,16555.11
2010-05-07,17413.94
2010-05-14,18926.74
2010-05-21,14773.04
2010-05-28,15580.43
2010-06-04,17558.09
2010-06-11,16637.62
2010-06-18,16216.27
2010-06-25,16328.72
2010-07-02,16333.14
2010-07-09,17688.76
2010-07-16,17150.84
2010-07-23,15360.45
2010-07-30,15381.82
2010-08-06,17508.41
2010-08-13,15536.4
2010-08-20,15740.13
2010-08-27,15793.87
2010-09-03,16241.78
2010-09-10,18194.74
2010-09-17,19354.23
2010-09-24,18122.52
2010-10-01,20094.19
2010-10-08,23388.03
2010-10-15,26978.34
2010-10-22,25543.04
2010-10-29,38640.93
2010-11-05,34238.88
2010-11-12,19549.39
2010-11-19,19552.84
2010-11-26,18820.29
2010-12-03,22517.56
2010-12-10,31497.65
2010-12-17,44912.86
2010-12-24,55931.23
2010-12-31,19124.58
2011-01-07,15984.24
2011-01-14,17359.7
2011-01-21,17341.47
2011-01-28,18461.18
2011-02-04,21665.76
2011-02-11,37887.17
2011-02-18,46845.87
2011-02-25,19363.83
2011-03-04,20327.61
2011-03-11,21280.4
2011-03-18,20334.23
2011-03-25,20881.1
2011-04-01,20398.09
2011-04-08,23873.79
2011-04-15,28762.37
2011-04-22,50510.31
2011-04-29,41512.39
2011-05-06,20138.19
2011-05-13,17235.15
2011-05-20,15136.78
2011-05-27,15741.6
2011-06-03,16434.15
2011-06-10,15883.52
2011-06-17,14978.09
2011-06-24,15682.81
2011-07-01,15363.5
2011-07-08,16148.87
2011-07-15,15654.85
2011-07-22,15766.6
2011-07-29,15922.41
2011-08-05,15295.55
2011-08-12,14539.79
2011-08-19,14689.24
2011-08-26,14537.37
2011-09-02,15277.27
2011-09-09,17746.68
2011-09-16,18535.48
2011-09-23,17859.3
2011-09-30,18337.68
2011-10-07,20797.58
2011-10-14,23077.55
2011-10-21,23351.8
2011-10-28,31579.9
2011-11-04,39886.06
2011-11-11,18689.54
2011-11-18,19050.66
2011-11-25,20911.25
2011-12-02,25293.49
2011-12-09,33305.92
2011-12-16,45773.03
2011-12-23,46788.75
2011-12-30,23350.88
2012-01-06,16567.69
2012-01-13,16894.4
2012-01-20,18365.1
2012-01-27,18378.16
2012-02-03,23510.49
2012-02-10,36988.49
2012-02-17,54060.1
2012-02-24,20124.22
2012-03-02,20113.03
2012-03-09,21140.07
2012-03-16,22366.88
2012-03-23,22107.7
2012-03-30,28952.86
2012-04-06,57592.12
2012-04-13,34684.21
2012-04-20,16976.19
2012-04-27,16347.6
2012-05-04,17147.44
2012-05-11,18164.2
2012-05-18,18517.79
2012-05-25,16963.55
2012-06-01,16065.49
2012-06-08,17666
2012-06-15,17558.82
2012-06-22,16633.41
2012-06-29,15722.82
2012-07-06,17823.37
2012-07-13,16566.18
2012-07-20,16348.06
2012-07-27,15731.18
2012-08-03,16628.31
2012-08-10,16119.92
2012-08-17,17330.7
2012-08-24,16286.4
2012-08-31,16680.24
2012-09-07,18322.37
2012-09-14,19616.22
2012-09-21,19251.5
2012-09-28,18947.81
2012-10-05,21904.47
2012-10-12,22764.01
2012-10-19,24185.27
2012-10-26,27390.81
</code></pre>

<p>I will read it from a csv file. But sometimes, some weeks may be missing. Therefore, I am trying to generate a date range like this:</p>

<pre><code>df_train_fly = pd.date_range(start, end, freq=""W-FRI"")
</code></pre>

<p>This generated dataframe contains all weeks in the range so I need to merge those two dataframe into one.</p>

<p>If I check df_train_csv['Date'] and df_train_fly['Date'] from the iPython console, they both showed as dtype: datetime64[ns]</p>
";;8;;2014-05-21T15:04:47.137;3.0;23787072;2014-05-21T17:24:41.800;2014-05-21T17:24:41.800;;3291475.0;;3291475.0;;1;15;<python><pandas><indexing><data-analysis>;Python Pandas join dataframes on index;33759.0
14001;14001;23836353.0;2.0;"<p>I have a pandas data frame where the first 3 columns are strings:</p>

<pre><code>         ID        text1    text 2
0       2345656     blah      blah
1          3456     blah      blah
2        541304     blah      blah        
3        201306       hi      blah        
4   12313201308    hello      blah         
</code></pre>

<p>I want to add leading zeros to the ID:</p>

<pre><code>                ID    text1    text 2
0  000000002345656     blah      blah
1  000000000003456     blah      blah
2  000000000541304     blah      blah        
3  000000000201306       hi      blah        
4  000012313201308    hello      blah 
</code></pre>

<p>I have tried:</p>

<pre><code>df['ID'] = df.ID.zfill(15))
df['ID'] = '{0:0&gt;15}'.format(df['ID']
</code></pre>
";;0;;2014-05-23T18:36:54.430;5.0;23836277;2015-07-22T04:18:59.840;;;;;3272980.0;;1;12;<python><pandas>;Add Leading Zeros to Strings in Pandas Dataframe;6942.0
14044;14044;23853569.0;1.0;"<p>I have a very large data set and I can't afford to read the entire data set in. So, I'm thinking of reading only one chunk of it to train but I have no idea how to do it. Any thought will be appreciated.</p>
";;0;;2014-05-25T08:50:00.203;5.0;23853553;2017-04-11T15:14:16.593;;;;;1334657.0;;1;23;<python><pandas>;Python Pandas: How to read only first n rows of CSV files in?;18338.0
14100;14100;23887956.0;3.0;"<p>This is my data frame that should be repeated for 5 times:</p>

<pre><code>&gt;&gt;&gt; x = pd.DataFrame({'a':1,'b':2},index = range(1))
&gt;&gt;&gt; x
   a  b
0  1  2
</code></pre>

<p>I wanna have the result like this:</p>

<pre><code>&gt;&gt;&gt; x.append(x).append(x).append(x)
   a  b
0  1  2
0  1  2
0  1  2
0  1  2
</code></pre>

<p>But there must be a way smarter than keep appending.. Actually the data frame Im working on should be repeated for 50 times..</p>

<p>I haven't found anything practical, including those like <code>np.repeat</code> ---- it just doesnt work on data frame.</p>

<p>Could anyone help?</p>
";;0;;2014-05-27T11:09:41.023;4.0;23887881;2016-12-29T22:48:24.873;;;;;1670180.0;;1;12;<python><pandas><duplicates><dataframe><repeat>;How to repeat Pandas data frame?;8822.0
14232;14232;23966229.0;2.0;"<p>I have a timeseries dataframe  <code>df</code> looks like this (the time seris happen within same day, but across different hours:</p>

<pre><code>                                id               val 
 time                    
2014-04-03 16:01:53             23              14389      
2014-04-03 16:01:54             28              14391             
2014-04-03 16:05:55             24              14393             
2014-04-03 16:06:25             23              14395             
2014-04-03 16:07:01             23              14395             
2014-04-03 16:10:09             23              14395             
2014-04-03 16:10:23             26              14397             
2014-04-03 16:10:57             26              14397             
2014-04-03 16:11:10             26              14397              
</code></pre>

<p>I need to create group every 5 minutes from starting from <code>16:00:00</code>. That is all the rows with in the range <code>16:00:00</code> to <code>16:05:00</code>, its value of the new column <code>period</code> is 1. (the number of rows within each group is irregular, so i can't simply cut the group)</p>

<p>Eventually, the data should look like this:</p>

<pre><code>                                id               val           period 
time            
2014-04-03 16:01:53             23              14389             1
2014-04-03 16:01:54             28              14391             1
2014-04-03 16:05:55             24              14393             2
2014-04-03 16:06:25             23              14395             2
2014-04-03 16:07:01             23              14395             2
2014-04-03 16:10:09             23              14395             3
2014-04-03 16:10:23             26              14397             3
2014-04-03 16:10:57             26              14397             3
2014-04-03 16:11:10             26              14397             3
</code></pre>

<p>The purpose is to perform some <code>groupby</code> operation, but the operation I need to do is not included in <code>pd.resample(how=' ')</code> method. So I have to create a <code>period</code> column to identify each group, then do <code>df.groupby('period').apply(myfunc)</code>.</p>

<p>Any help or comments are highly appreciated.</p>

<p>Thanks!</p>
";;1;;2014-05-31T03:38:17.133;2.0;23966152;2014-06-01T15:29:44.537;;;;;3576212.0;;1;11;<python><datetime><numpy><pandas>;how to create a group ID based on 5 minutes interval in pandas timeseries?;3577.0
14292;14292;;2.0;"<p>I'm using Pandas' <code>to_sql</code> function to write to MySQL, which is timing out due to large frame size (1M rows, 20 columns).</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html</a></p>

<p>Is there a more official way to chunk through the data and write rows in blocks?  I've written my own code, which seems to work.  I'd prefer an official solution though.  Thanks!</p>

<pre><code>def write_to_db(engine, frame, table_name, chunk_size):

    start_index = 0
    end_index = chunk_size if chunk_size &lt; len(frame) else len(frame)

    frame = frame.where(pd.notnull(frame), None)
    if_exists_param = 'replace'

    while start_index != end_index:
        print ""Writing rows %s through %s"" % (start_index, end_index)
        frame.iloc[start_index:end_index, :].to_sql(con=engine, name=table_name, if_exists=if_exists_param)
        if_exists_param = 'append'

        start_index = min(start_index + chunk_size, len(frame))
        end_index = min(end_index + chunk_size, len(frame))

engine = sqlalchemy.create_engine('mysql://...') #database details omited
write_to_db(engine, frame, 'retail_pendingcustomers', 20000)
</code></pre>
";;4;;2014-06-03T05:31:08.797;1.0;24007762;2014-09-04T17:50:21.043;2014-09-04T17:50:21.043;;653364.0;;2804588.0;;1;12;<python><mysql><sql><pandas><sqlalchemy>;Python Pandas - Using to_sql to write large data frames in chunks;8266.0
14335;14335;24029921.0;3.0;"<p>If the data look like:</p>

<pre><code>Store,Dept,Date,Weekly_Sales,IsHoliday
1,1,2010-02-05,24924.5,FALSE
1,1,2010-02-12,46039.49,TRUE
1,1,2010-02-19,41595.55,FALSE
1,1,2010-02-26,19403.54,FALSE
1,1,2010-03-05,21827.9,FALSE
1,1,2010-03-12,21043.39,FALSE
1,1,2010-03-19,22136.64,FALSE
1,1,2010-03-26,26229.21,FALSE
1,1,2010-04-02,57258.43,FALSE
</code></pre>

<p>And I wanna duplicate rows with IsHoliday equal to TRUE, I can do:</p>

<pre><code>is_hol = df['IsHoliday'] == True
df_try = df[is_hol]
df=df.append(df_try*10)
</code></pre>

<p>But is there a better way to do this as I need to duplicate holiday rows by 5 times, and I have to append 5 times if using above way.</p>
";;0;;2014-06-04T05:26:20.503;4.0;24029659;2016-12-29T22:58:08.023;;;;;3291475.0;;1;14;<python><pandas><dataframe>;Python Pandas replicate rows in dataframe;13950.0
14353;14353;24043138.0;3.0;"<p>With the nice indexing methods in Pandas I have no problems extracting data in various ways. On the other hand I am still confused about how to change data in an existing DataFrame. </p>

<p>In the following code I have two DataFrames and my goal is to update values in a specific row in the first df from values of the second df. How can I achieve this?</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'filename' :  ['test0.dat', 'test2.dat'], 
                                  'm': [12, 13], 'n' : [None, None]})
df2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])

# this overwrites the first row but we want to update the second
# df.update(df2)

# this does not update anything
df.loc[df.filename == 'test2.dat'].update(df2)

print(df)
</code></pre>

<p>gives </p>

<pre><code>   filename   m     n
0  test0.dat  12  None
1  test2.dat  13  None

[2 rows x 3 columns]
</code></pre>

<p>but how can I achieve this:</p>

<pre><code>    filename   m     n
0  test0.dat  12  None
1  test2.dat  13  16

[2 rows x 3 columns]
</code></pre>
";;0;;2014-06-04T12:02:26.290;2.0;24036911;2017-03-01T19:44:08.107;;;;;407197.0;;1;16;<python><pandas>;How to update values in a specific row in a Python Pandas DataFrame?;26260.0
14356;14356;24037972.0;2.0;"<pre><code>import pandas as pd

path1 = ""/home/supertramp/Desktop/100&amp;life_180_data.csv""

mydf =  pd.read_csv(path1)

numcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}

print mydf['Cigarettes']

mydf['CigarNum'] = mydf['Cigarettes'].apply(numcigar.get).astype(float)

print mydf['CigarNum']

mydf.to_csv('/home/supertramp/Desktop/powerRangers.csv')
</code></pre>

<p>The csv file ""100&amp;life_180_data.csv"" contains columns like age, bmi,Cigarettes,Alocohol etc.</p>

<pre><code>No                int64
Age               int64
BMI             float64
Alcohol          object
Cigarettes       object
dtype: object
</code></pre>

<p>Cigarettes column contains ""Never"" ""1-5 Cigarettes/day"",""10-20 Cigarettes/day"".
I want to assign weights to these object (Never,1-5 Cigarettes/day ,....)</p>

<p>The expected output is new column CigarNum appended which consists only numbers 0,1,2
CigarNum is as expected till 8 rows and then shows Nan till last row in CigarNum column</p>

<pre><code>0                     Never
1                     Never
2        1-5 Cigarettes/day
3                     Never
4                     Never
5                     Never
6                     Never
7                     Never
8                     Never
9                     Never
10                    Never
11                    Never
12     10-20 Cigarettes/day
13       1-5 Cigarettes/day
14                    Never
...
167                    Never
168                    Never
169     10-20 Cigarettes/day
170                    Never
171                    Never
172                    Never
173                    Never
174                    Never
175                    Never
176                    Never
177                    Never
178                    Never
179                    Never
180                    Never
181                    Never
Name: Cigarettes, Length: 182, dtype: object
</code></pre>

<p>The output I get shoudln't give NaN after few first rows.</p>

<pre><code>0      0
1      0
2      1
3      0
4      0
5      0
6      0
7      0
8      0
9      0
10   NaN
11   NaN
12   NaN
13   NaN
14     0
...
167   NaN
168   NaN
169   NaN
170   NaN
171   NaN
172   NaN
173   NaN
174   NaN
175   NaN
176   NaN
177   NaN
178   NaN
179   NaN
180   NaN
181   NaN
Name: CigarNum, Length: 182, dtype: float64
</code></pre>
";;2;;2014-06-04T12:31:34.173;2.0;24037507;2017-01-30T15:25:08.027;2014-06-04T12:35:30.383;;3504055.0;;3504055.0;;1;16;<python><csv><pandas>;Converting string objects to int/float using pandas;46157.0
14360;14360;24040239.0;1.0;"<p>I don't know why this puts NaN into 'new' column?</p>

<pre><code>df['new'] = pd.Series([0 for x in range(len(df.index))])
</code></pre>

<p>EDIT:</p>

<pre><code>df['new'] = 0 
</code></pre>

<p>works :)</p>
";;5;;2014-06-04T13:39:10.240;4.0;24039023;2017-02-09T10:57:08.027;2014-06-04T13:46:40.457;;2006977.0;;2006977.0;;1;32;<python><pandas>;add column with constant value to pandas dataframe;32111.0
14363;14363;24041761.0;1.0;"<p>I have a <code>DataFrame</code> that looks like</p>

<pre><code>  Emp1    Empl2           date       Company
0    0        0     2012-05-01         apple
1    0        1     2012-05-29         apple
2    0        1     2013-05-02         apple
3    0        1     2013-11-22         apple
18   1        0     2011-09-09        google
19   1        0     2012-02-02        google
20   1        0     2012-11-26        google
21   1        0     2013-05-11        google
</code></pre>

<p>I want to pass the company and date for setting a <code>MultiIndex</code> for this <code>DataFrame</code>. Currently it has a default index. I am using <code>df.set_index(['Company', 'date'], inplace=True)</code></p>

<pre><code>df = pd.DataFrame()
for c in company_list:
        row = pd.DataFrame([dict(company = '%s' %s, date = datetime.date(2012, 05, 01))])
        df = df.append(row, ignore_index = True)
        for e in emp_list:
            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = '""+s+""' and emp_name = '""+b+""' group by company, date, name LIMIT 5 "", con)
                if len(dataset) == 0:
                row = pd.DataFrame([dict(sitename='%s' %s, name = '%s' %b, date = datetime.date(2012, 05, 01), count = np.nan)])
                dataset = dataset.append(row, ignore_index=True)
            dataset = dataset.rename(columns = {'count': '%s' %b})
            dataset = dataset.groupby(['company', 'date', 'emp_name'], as_index = False).sum()

            dataset = dataset.drop('emp_name', 1)
            df = pd.merge(df, dataset, how = '')
            df = df.sort('date', ascending = True)
            df.fillna(0, inplace = True)

df.set_index(['Company', 'date'], inplace=True)            
print df
</code></pre>

<p>But when I print this <code>DataFrame</code>, it prints <code>None</code>. I saw this solution from stackoverflow it self. Is this not the correct way of doing it. Also I want to shuffle the positions of the columns company and date so that company becomes the first index, and date becomes the second in Hierarchy. Any ideas on this?</p>
";;2;;2014-06-04T15:22:44.767;5.0;24041436;2016-12-15T14:35:13.033;2016-12-15T14:35:13.033;;2087463.0;;3527975.0;;1;21;<python><pandas>;set multi index of an existing data frame in pandas;21648.0
14453;14453;24082767.0;4.0;"<p>I have the following data in a pandas dataframe</p>

<pre><code>       date  template     score
0  20140605         0  0.138786
1  20140605         1  0.846441
2  20140605         2  0.766636
3  20140605         3  0.259632
4  20140605         4  0.497366
5  20140606         0  0.138139
6  20140606         1  0.845320
7  20140606         2  0.762876
8  20140606         3  0.261035
9  20140606         4  0.498010
</code></pre>

<p>For every day there will be 5 templates and each template will have a score.</p>

<p>I want to plot the date in the x axis and score in the y axis and a separate line graph for each template in the same figure.</p>

<p>Is it possible to do this using matplotlib?</p>
";;2;;2014-06-06T11:02:21.247;5.0;24080275;2015-12-05T20:34:54.947;;;;;24949.0;;1;15;<python><matplotlib><plot><pandas>;Plotting multiple line graph using pandas and matplotlib;25416.0
14455;14455;24083253.0;1.0;"<p>Consider a csv file:</p>

<pre><code>string,date,number
a string,2/5/11 9:16am,1.0
a string,3/5/11 10:44pm,2.0
a string,4/22/11 12:07pm,3.0
a string,4/22/11 12:10pm,4.0
a string,4/29/11 11:59am,1.0
a string,5/2/11 1:41pm,2.0
a string,5/2/11 2:02pm,3.0
a string,5/2/11 2:56pm,4.0
a string,5/2/11 3:00pm,5.0
a string,5/2/14 3:02pm,6.0
a string,5/2/14 3:18pm,7.0
</code></pre>

<p>I can read this in, and reformat the date column into datetime format:</p>

<pre><code>b=pd.read_csv('b.dat')
b['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')
</code></pre>

<p>I have been trying to group the data by month. It seems like there should be an obvious way of accessing the month and grouping by that. But I can't seem to do it. Does anyone know how?</p>

<p>What I am currently trying is re-indexing by the date:</p>

<pre><code>b.index=b['date']
</code></pre>

<p>I can access the month like so:</p>

<pre><code>b.index.month
</code></pre>

<p>However I can't seem to find a function to lump together by month.</p>
";;0;;2014-06-06T13:15:41.713;12.0;24082784;2017-07-11T14:47:11.337;2015-04-29T16:04:22.723;;1461850.0;;1461850.0;;1;27;<python><datetime><pandas>;pandas dataframe groupby datetime month;26219.0
14552;14552;24242333.0;6.0;"<p>I took a new clean install of OSX 10.9.3 and installed pip, and then did</p>

<pre>
pip install pandas
pip install numpy
</pre>

<p>Both installs seemed to be perfectly happy, and ran without any errors (though there were a zillion warnings).  When I tried to run a python script with import pandas, I got the following error:</p>

<pre>

    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): 
    File ""./moen.py"", line 7, in  import pandas File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in  from . import hashtable, tslib, lib 
    File ""numpy.pxd"", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) 
    ValueError: numpy.dtype has the wrong size, try recompiling

</pre>

<p>How do I fix this error and get pandas to load properly?</p>
";;3;;2014-06-09T14:50:19.740;2.0;24122850;2016-02-22T21:55:19.823;2014-06-09T14:56:33.810;;1536722.0;;1536722.0;;1;16;<python><numpy><pandas>;pandas ValueError: numpy.dtype has the wrong size, try recompiling;23340.0
14553;14553;24656474.0;4.0;"<p>I'm trying to preform recursive feature elimination using <code>scikit-learn</code> and a random forest classifier, with OOB ROC as the method of scoring each subset created during the recursive process.</p>

<p>However, when I try to use the <code>RFECV</code> method, I get an error saying <code>AttributeError: 'RandomForestClassifier' object has no attribute 'coef_'</code>  </p>

<p>Random Forests don't have coefficients per se, but they do have rankings by Gini score.  So, I'm wondering how to get arround this problem.</p>

<p>Please note that I want to use a method that will explicitly tell me what features from my <code>pandas</code> DataFrame were selected in the optimal grouping as I am using recursive feature selection to try to minimize the amount of data I will input into the final classifier. </p>

<p>Here's some example code: </p>

<pre><code>from sklearn import datasets
import pandas as pd
from pandas import Series
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV

iris = datasets.load_iris()
x=pd.DataFrame(iris.data, columns=['var1','var2','var3', 'var4'])
y=pd.Series(iris.target, name='target')
rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)
rfecv = RFECV(estimator=rf, step=1, cv=10, scoring='ROC', verbose=2)
selector=rfecv.fit(x, y)

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 336, in fit
    ranking_ = rfe.fit(X_train, y_train).ranking_
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 148, in fit
    if estimator.coef_.ndim &gt; 1:
AttributeError: 'RandomForestClassifier' object has no attribute 'coef_'
</code></pre>
";;7;;2014-06-09T15:26:35.577;7.0;24123498;2015-07-10T12:30:45.400;;;;;1884063.0;;1;14;<python><pandas><scikit-learn><random-forest><feature-selection>;Recursive feature elimination on Random Forest using scikit-learn;9257.0
14602;14602;24147363.0;12.0;"<p>I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.</p>

<p>Thanks!</p>
";;0;;2014-06-10T17:24:57.857;41.0;24147278;2017-08-17T08:23:27.397;;;;;3712008.0;;1;97;<python><python-2.7><pandas><dataframe>;How do I create test and train samples from one dataframe with pandas?;81694.0
14709;14709;24283087.0;5.0;"<p>Say I have data about 3 trading strategies, each with and without transaction costs.  I want to plot, on the same axes, the time series of each of the 6 variants (3 strategies * 2 trading costs).  I would like the ""with transaction cost"" lines to be plotted with <code>alpha=1</code> and <code>linewidth=1</code> while I want the ""no transaction costs"" to be plotted with <code>alpha=0.25</code> and <code>linewidth=5</code>.  But I would like the color to be the same for both versions of each strategy. </p>

<p>I would like something along the lines of:</p>

<pre><code>fig, ax = plt.subplots(1, 1, figsize=(10, 10))

for c in with_transaction_frame.columns:
    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)

****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE

for c in no_transaction_frame.columns:
    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)

ax.legend()
</code></pre>

<p>What is the appropriate code to put on the indicated line to reset the color cycle so it is ""back to the start"" when the second loop is invoked?</p>
";;0;;2014-06-12T20:13:44.943;4.0;24193174;2017-02-17T13:59:38.217;;;;;2501018.0;;1;26;<python><matplotlib><pandas>;Reset color cycle in Matplotlib;10416.0
14716;14716;24196288.0;2.0;"<p>Is there an existing function to estimate fixed effect (one-way or two-way) from Pandas or Statsmodels.</p>

<p>There used to be a function in Statsmodels but it seems discontinued. And in Pandas, there is something called <code>plm</code>, but I can't import it or run it using <code>pd.plm()</code>.</p>
";;14;;2014-06-12T23:23:38.237;4.0;24195432;2017-06-29T22:57:20.297;2014-06-12T23:35:45.190;;1763356.0;;3576212.0;;1;11;<python><pandas><regression><statsmodels>;Fixed effect in Pandas or Statsmodels;5937.0
14731;14731;24205468.0;3.0;"<h2>EDIT</h2>

<p>I found a quite nice solution and posted it below as an answer.
The result will look like this:</p>

<p><img src=""https://i.stack.imgur.com/J0BXc.png"" alt=""enter image description here""></p>

<hr>

<p>Some example data you can generate for this problem:</p>

<pre><code>codes = list('ABCDEFGH'); 
dates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); 
dates = dates.append(dates)
dates.sort()
df = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})
</code></pre>

<p>resulting in:</p>

<pre><code>In [55]: df
Out[55]:
    amount col1 col2       date
0        1    D    E 2013-11-01
0        5    E    B 2013-11-01
1        5    G    A 2013-11-02
1        7    D    H 2013-11-02
2        5    E    G 2013-11-03
2        4    H    G 2013-11-03
3        7    A    F 2013-11-04
3        3    A    A 2013-11-04
4        1    E    G 2013-11-05
4        7    D    C 2013-11-05
5        5    C    A 2013-11-06
5        7    H    F 2013-11-06
6        1    G    B 2013-11-07
6        8    D    A 2013-11-07
7        1    B    H 2013-11-08
7        8    F    H 2013-11-08
8        3    A    E 2013-11-09
8        1    H    D 2013-11-09
9        3    B    D 2013-11-10
9        1    H    G 2013-11-10
10       6    E    E 2013-11-11
10       6    F    E 2013-11-11
11       2    G    B 2013-11-12
11       5    H    H 2013-11-12
12       5    F    G 2013-11-13
12       5    G    B 2013-11-13
13       8    H    B 2013-11-14
13       6    G    F 2013-11-14
14       9    F    C 2013-11-15
14       4    H    A 2013-11-15
..     ...  ...  ...        ...
77       9    A    B 2014-01-17
77       7    E    B 2014-01-17
78       4    F    E 2014-01-18
78       6    B    E 2014-01-18
79       6    A    H 2014-01-19
79       3    G    D 2014-01-19
80       7    E    E 2014-01-20
80       6    G    C 2014-01-20
81       9    H    G 2014-01-21
81       9    C    B 2014-01-21
82       2    D    D 2014-01-22
82       7    D    A 2014-01-22
83       6    G    B 2014-01-23
83       1    A    G 2014-01-23
84       9    B    D 2014-01-24
84       7    G    D 2014-01-24
85       7    A    F 2014-01-25
85       9    B    H 2014-01-25
86       9    C    D 2014-01-26
86       5    E    B 2014-01-26
87       3    C    H 2014-01-27
87       7    F    D 2014-01-27
88       3    D    G 2014-01-28
88       4    A    D 2014-01-28
89       2    F    A 2014-01-29
89       8    D    A 2014-01-29
90       1    A    G 2014-01-30
90       6    C    A 2014-01-30
91       6    H    C 2014-01-31
91       2    G    F 2014-01-31

[184 rows x 4 columns]
</code></pre>

<p>I'd like to group by calendar-week and by value of <code>col1</code>. Like this:</p>

<pre><code>kw = lambda x: x.isocalendar()[1]
grouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'})
</code></pre>

<p>resulting in:</p>

<pre><code>In [58]: grouped
Out[58]:
           amount
date col1
44   D          8
     E         10
     G          5
     H          4
45   D         15
     E          1
     G          1
     H          9
     A         13
     C          5
     B          4
     F          8
46   E          7
     G         13
     H         17
     B          9
     F         23
47   G         14
     H          4
     A         40
     C          7
     B         16
     F         13
48   D          7
     E         16
     G          9
     H          2
     A          7
     C          7
     B          2
...           ...
1    H         14
     A         14
     B         15
     F         19
2    D         13
     H         13
     A         13
     B         10
     F         32
3    D          8
     E         18
     G          3
     H          6
     A         30
     C          9
     B          6
     F          5
4    D          9
     E         12
     G         19
     H          9
     A          8
     C         18
     B         18
5    D         11
     G          2
     H          6
     A          5
     C          9
     F          9

[87 rows x 1 columns]
</code></pre>

<p>Then I want a plot to be generated like this:
<img src=""https://i.stack.imgur.com/rsfmn.png"" alt=""enter image description here"">
That means: calendar-week and year (datetime) on the x-axis and for each of the grouped <code>col1</code> one bar.</p>

<p>The problem I'm facing is: I only have integers describing the calendar week (KW in the plot), but I somehow have to merge back the date on it to get the ticks labeled by year as well. Furthermore I can't only plot the grouped calendar week because I need a correct order of the items (kw 47, kw 48 (year 2013) have to be on the left side of kw 1 (because this is 2014)).</p>

<hr>

<h2>EDIT</h2>

<p>I figured out from here:
<a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-barplot"">http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-barplot</a> that grouped bars need to be columns instead of rows. So I thought about how to transform the data and found the method <code>pivot</code> which turns out to be a great function. <code>reset_index</code> is needed to transform the multiindex into columns. At the end I fill <code>NaN</code>s by zero:</p>

<pre><code>A = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0)
</code></pre>

<p>transforms the data into:</p>

<pre><code>col1   A   B   C   D   E   F   G   H
date
1      4  31   0   0   0  18  13   8
2      0  12  13  22   1  17   0   8
3      3  10   4  13  12   8   7   6
4     17   0  10   7   0  25   7   4
5      7   0   7   9   8   6   0   7
44     0   0   2  11   7   0   0   2
45     9   3   2  14   0  16  21   2
46     0  14   7   2  17  13  11   8
47     5  13   0  15  19   7   5  10
48    15   8  12   2  20   4   7   6
49    20   0   0  18  22  17  11   0
50     7  11   8   6   5   6  13  10
51     8  26   0   0   5   5  16   9
52     8  13   7   5   4  10   0  11
</code></pre>

<p>which looks like the example data in the docs to be plotted in grouped bars:</p>

<pre><code>A. plot(kind='bar')
</code></pre>

<p>gets this:</p>

<p><img src=""https://i.stack.imgur.com/uDozx.png"" alt=""enter image description here""></p>

<p>whereas I have the problem with the axis as it is now sorted (from 1-52), which is actually wrong, because calendar week 52 belongs to year 2013 in this case... Any ideas on how to merge back the real datetime for the calendar-weeks and use them as x-axis ticks?</p>
";;2;;2014-06-13T10:22:17.080;7.0;24203106;2015-01-06T15:39:18.500;2014-06-13T12:37:12.480;;701049.0;;701049.0;;1;11;<python><datetime><pandas><calendar><group-by>;Pandas: Group by calendar-week, then plot grouped barplots for the real datetime;12490.0
14764;14764;;1.0;"<p>I'm trying do something that should be really simple in pandas, but it seems anything but. I'm trying to add a column to an existing pandas dataframe that is a mapped value based on another (existing) column. Here is a small test case:</p>

<pre><code>import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001]} )
df[""B""] = equiv(df[""A""])
print(df)
</code></pre>

<p>I was hoping the following would result:</p>

<pre><code>      A   B
0  7001   1
1  8001   2
2  9001   3
</code></pre>

<p>Instead, I get an error telling me that equiv is not a callable function. Fair enough, it's a dictionary, but even if I wrap it in a function I still get frustration. So I tried to use a map function that seems to work with other operations, but it also is defeated by use of a dictionary:</p>

<pre><code>df[""B""] = df[""A""].map(lambda x:equiv[x])
</code></pre>

<p>In this case I just get KeyError: 8001. I've read through documentation and previous posts, but have yet to come across anything that suggests how to mix dictionaries with pandas dataframes. Any suggestions would be greatly appreciated.</p>
";;0;;2014-06-14T03:53:49.400;10.0;24216425;2016-03-04T22:38:11.840;2014-06-14T04:11:22.727;;2487184.0;;3739614.0;;1;28;<python><pandas>;Adding a new pandas column with mapped value from a dictionary;15904.0
14849;14849;27232309.0;4.0;"<p>When calling</p>

<pre><code>df = pd.read_csv('somefile.csv')
</code></pre>

<p>I get:</p>

<blockquote>
  <p>/Users/josh/anaconda/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py:1130:
  DtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtype
  option on import or set low_memory=False.</p>
</blockquote>

<p>Why is the <code>dtype</code> option related to <code>low_memory</code>, and why would making it <code>False</code> help with this problem?</p>
";;2;;2014-06-16T19:56:47.607;31.0;24251219;2016-11-24T08:06:48.383;2016-04-15T06:38:03.700;;3730397.0;;1732769.0;;1;81;<python><parsing><numpy><pandas><dataframe>;Pandas read_csv low_memory and dtype options;53257.0
14892;14892;24273597.0;1.0;"<p>Is that any way that I can get first element of Seires without have information on index.</p>

<p>For example,We have a Series</p>

<pre><code>    import pandas as pd
    key='MCS096'
    SUBJECTS=pd.DataFrame({'ID':Series([146],index=[145]),\
                   'study':Series(['MCS'],index=[145]),\
                   'center':Series(['Mag'],index=[145]),\
                   'initials':Series(['MCS096'],index=[145])
                   })
</code></pre>

<p>prints out SUBJECTS:</p>

<pre><code>    print (SUBJECTS[SUBJECTS.initials==key]['ID'])
    145    146
    Name: ID, dtype: int64
</code></pre>

<p>How can I get the value here 146 without using index 145?</p>

<p>Thank you very much</p>
";;3;;2014-06-17T20:55:43.537;2.0;24273130;2014-06-18T03:25:49.170;2014-06-18T03:25:49.170;;1552748.0;;2999675.0;;1;12;<python><pandas>;Get first element of Series without have information on index;17707.0
14908;14908;;3.0;"<p>I have a dataframe..</p>

<pre><code>s1 = pd.Series([5, 6, 7])
s2 = pd.Series([7, 8, 9])

df = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])

   A  B  C
0  5  6  7
1  7  8  9

[2 rows x 3 columns]
</code></pre>

<p>and I need to add a first row [2, 3, 4] to get..</p>

<pre><code>   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>

<p>I've tried append() and concat() functions but somehow I can't find the right way how to do that.</p>

<p>Any ideas?
Is there any direct way how to add/insert series to dataframe?</p>
";;2;;2014-06-18T11:27:35.493;6.0;24284342;2016-11-04T20:15:22.633;2014-09-09T22:00:16.953;;759866.0;;55129.0;;1;27;<python><pandas>;Insert a row to pandas dataframe;87343.0
15122;15122;24386746.0;2.0;"<p>If I add two columns to create a third, any columns containing NaN (representing missing data in my world) cause the resulting output column to be NaN as well. Is there a way to skip NaNs without explicitly setting the values to 0 (which would lose the notion that those values are ""missing"")?</p>

<pre><code>In [42]: frame = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})

In [44]: frame['c'] = frame['a'] + frame['b']

In [45]: frame
Out[45]: 
    a   b   c
0   1   3   4
1   2 NaN NaN
2 NaN   4 NaN
</code></pre>

<p>In the above, I would like column c to be [4, 2, 4].</p>

<p>Thanks...</p>
";;0;;2014-06-24T12:27:19.100;2.0;24386638;2014-06-24T12:52:45.620;;;;;2073538.0;;1;11;<pandas>;Pandas sum two columns, skipping NaN;7005.0
15149;15149;24418294.0;2.0;"<p>Are there any examples of how to pass parameters with an SQL query in Pandas?</p>

<p>In particular I'm using an SQLAlchemy engine to connect to a PostgreSQL database.  So far I've found that the following works:</p>

<pre><code>df = psql.read_sql(('select ""Timestamp"",""Value"" from ""MyTable"" '
                     'where ""Timestamp"" BETWEEN %s AND %s'),
                   db,params=[datetime(2014,6,24,16,0),datetime(2014,6,24,17,0)],
                   index_col=['Timestamp'])
</code></pre>

<p>The Pandas documentation says that params can also be passed as a dict, but I can't seem to get this to work having tried for instance:</p>

<pre><code>df = psql.read_sql(('select ""Timestamp"",""Value"" from ""MyTable"" '
                     'where ""Timestamp"" BETWEEN :dstart AND :dfinish'),
                   db,params={""dstart"":datetime(2014,6,24,16,0),""dfinish"":datetime(2014,6,24,17,0)},
                   index_col=['Timestamp'])
</code></pre>

<p>What is the recommended way of running these types of queries from Pandas?</p>
";;0;;2014-06-25T12:21:42.360;10.0;24408557;2014-06-26T07:29:33.393;2014-06-26T07:29:33.393;;653364.0;;3775114.0;;1;19;<python><sql><pandas>;Pandas read_sql with parameters;28919.0
15199;15199;24436783.0;2.0;"<p>Does anyone know if it is possible to use the DataFrame.loc method to select from a MultiIndex? I have the following DataFrame and would like to be able to access the values located in the 'Dwell' columns, at the indices of <code>('at', 1)</code>, <code>('at', 3)</code>, <code>('at', 5)</code>, and so on (non-sequential).</p>

<p>I'd love to be able to do something like <code>data.loc[['at',[1,3,5]], 'Dwell']</code>, similar to the <code>data.loc[[1,3,5], 'Dwell']</code> syntax for a regular index (which returns a 3-member series of Dwell values).</p>

<p>My purpose is to select an arbitrary subset of the data, perform some analysis only on that subset, and then update the new values with the results of the analysis. I plan on using the same syntax to set new values for these data, so chaining selectors wouldn't really work in this case.</p>

<p>Here is a slice of the DataFrame I'm working with: </p>

<pre><code>         Char    Dwell  Flight  ND_Offset  Offset
QGram                                                           
at    0     a      100     120   0.000000       0  
      1     t      180       0   0.108363       5  
      2     a      100     120   0.000000       0 
      3     t      180       0   0.108363       5 
      4     a       20     180   0.000000       0  
      5     t       80     120   0.108363       5
      6     a       20     180   0.000000       0   
      7     t       80     120   0.108363       5  
      8     a       20     180   0.000000       0  
      9     t       80     120   0.108363       5   
      10    a      120     180   0.000000       0  
</code></pre>

<p>Thanks!</p>
";;0;;2014-06-26T16:31:32.327;1.0;24435788;2017-05-05T17:41:12.847;;;;;3199122.0;;1;11;<python><pandas><dataframe><multi-index>;Using .loc with a MultiIndex in pandas?;8091.0
15243;15243;;9.0;"<p>I'm trying to use scikit-learn's <code>LabelEncoder</code> to encode a pandas <code>DataFrame</code> of string labels. As the dataframe has many (50+) columns, I want to avoid creating a <code>LabelEncoder</code> object for each column; I'd rather just have one big <code>LabelEncoder</code> objects that works across <em>all</em> my columns of data.  </p>

<p>Throwing the entire <code>DataFrame</code> into <code>LabelEncoder</code> creates the below error.  Please bear in mind that I'm using dummy data here; in actuality I'm dealing with about 50 columns of string labeled data, so need a solution that doesn't reference any columns by name. </p>

<pre><code>import pandas
from sklearn import preprocessing 

df = pandas.DataFrame({'pets':['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 'owner':['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 'location':['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 'New_York']})
le = preprocessing.LabelEncoder()

le.fit(df)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit
    y = column_or_1d(y, warn=True)
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d
    raise ValueError(""bad input shape {0}"".format(shape))
ValueError: bad input shape (6, 3)
</code></pre>

<p>Any thoughts on how to get around this problem? </p>
";;2;;2014-06-27T18:29:40.543;37.0;24458645;2017-07-26T18:57:37.927;;;;;1884063.0;;1;67;<python><pandas><scikit-learn>;Label encoding across multiple columns in scikit-learn;27993.0
15282;15282;24475214.0;4.0;"<p>I have a pandas dataframe I would like to se the diagonal to 0</p>

<pre><code>import numpy
import pandas

df = pandas.DataFrame(numpy.random.rand(5,5))
df

Out[6]:
     0           1           2           3               4
0    0.536596    0.674319    0.032815    0.908086    0.215334
1    0.735022    0.954506    0.889162    0.711610    0.415118
2    0.119985    0.979056    0.901891    0.687829    0.947549
3    0.186921    0.899178    0.296294    0.521104    0.638924
4    0.354053    0.060022    0.275224    0.635054    0.075738
5 rows  5 columns
</code></pre>

<p>now I want to set the diagonal to 0:</p>

<pre><code>for i in range(len(df.index)):
    for j in range(len(df.columns)):
        if i==j:
            df.loc[i,j] = 0
df
Out[9]:
     0           1           2           3           4
0    0.000000    0.674319    0.032815    0.908086    0.215334
1    0.735022    0.000000    0.889162    0.711610    0.415118
2    0.119985    0.979056    0.000000    0.687829    0.947549
3    0.186921    0.899178    0.296294    0.000000    0.638924
4    0.354053    0.060022    0.275224    0.635054    0.000000
5 rows  5 columns
</code></pre>

<p>but there must be a more pythonic way than that!?</p>
";;0;;2014-06-29T10:16:03.597;3.0;24475094;2016-12-04T00:25:36.270;;;;;819718.0;;1;14;<python><numpy><pandas>;Set values on the diagonal of pandas.DataFrame;4336.0
15309;15309;24496435.0;3.0;"<p>Say you have this MultiIndex-ed DataFrame:</p>

<pre><code>df = pd.DataFrame({'co':['DE','DE','FR','FR'],
                   'tp':['Lake','Forest','Lake','Forest'],
                   'area':[10,20,30,40],
                   'count':[7,5,2,3]})
df = df.set_index(['co','tp'])
</code></pre>

<p>Which looks like this:</p>

<pre><code>           area  count
co tp
DE Lake      10      7
   Forest    20      5
FR Lake      30      2
   Forest    40      3
</code></pre>

<p>I would like to <strong>retrieve the unique values per index level</strong>. This can be accomplished using</p>

<pre><code>df.index.levels[0]  # returns ['DE', 'FR]
df.index.levels[1]  # returns ['Lake', 'Forest']
</code></pre>

<p>What I would <em>really</em> like to do, is to retrieve these lists by <strong>addressing the levels by their name</strong>, i.e. <code>'co'</code> and <code>'tp'</code>. The shortest two ways I could find looks like this:</p>

<pre><code>list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']
df.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']
</code></pre>

<p>But non of them are very elegant. Is there a shorter way?</p>
";;1;;2014-06-30T17:30:57.923;8.0;24495695;2016-12-05T13:03:41.940;2015-11-04T09:30:40.227;;2375855.0;;2375855.0;;1;30;<python><pandas>;Pandas: Get unique MultiIndex level values by label;18489.0
15354;15354;34466473.0;3.0;"<p>I am running 'describe()' on a dataframe and getting summaries of only int columns (pandas 14.0). </p>

<p>The documentation says that for object columns frequency of most common value, and additional statistics would be returned. What could be wrong? (no error message is returned by the way)</p>

<p><strong>Edit:</strong></p>

<p>I think it's how the function is set to behave on mixed column types in a dataframe. Although the documentation fails to mention it.</p>

<p><strong>Example code:</strong></p>

<pre><code>df_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})
df_test.dtypes
df_test.describe()
df_test['$a'] = df_test['$a'].astype(str)
df_test.describe()
df_test['$a'].describe()
df_test['$b'].describe()
</code></pre>

<p><strong>My ugly work around in the meanwhile:</strong></p>

<pre><code>def my_df_describe(df):
    objects = []
    numerics = []
    for c in df:
        if (df[c].dtype == object):
            objects.append(c)
        else:
            numerics.append(c)

    return df[numerics].describe(), df[objects].describe()
</code></pre>
";;7;;2014-07-02T06:15:43.097;5.0;24524104;2017-08-01T21:55:09.340;2014-07-22T07:33:15.610;;553095.0;;2808117.0;;1;11;<python><pandas>;Pandas 'describe' is not returning summary of all columns;13696.0
15446;15446;24609894.0;3.0;"<p>This may seem to be a useless feature but it would be very helpful for me. I would like to save the output I get inside Canopy IDE. I would not think this is specific to Canopy but for the sake of clarity that is what I use. For example, my console Out[2] is what I would want from this:</p>

<p><img src=""https://i.stack.imgur.com/Zfni3.jpg"" alt=""enter image description here""></p>

<p>I think that the formatting is quite nice and to reproduce this each time instead of just saving the output would be a waste of time. So my question is, how can I get a handle on this figure? Ideally the implimentation would be similar to standard methods, such that it could be done like this:</p>

<pre><code>from matplotlib.backends.backend_pdf import PdfPages

pp = PdfPages('Output.pdf')
fig = plt.figure() 
ax = fig.add_subplot(1, 1, 1)
df.plot(how='table')
pp.savefig()
pp.close()
</code></pre>

<p>NOTE: I realize that a very similar question has been asked before ( <a href=""https://stackoverflow.com/questions/19726663/how-to-save-the-pandas-dataframe-series-data-as-a-figure"">How to save the Pandas dataframe/series data as a figure?</a> ) but it never received an answer and I think I have stated the question more clearly.</p>
";;4;;2014-07-04T13:00:56.973;2.0;24574976;2015-06-16T19:56:39.417;2017-05-23T12:33:28.450;;-1.0;;3767276.0;;1;18;<python><matplotlib><pandas><ipython><canopy>;"Save the ""Out[]"" table of a pandas dataframe as a figure";6231.0
15577;15577;32662331.0;3.0;"<p>I want to print the whole dataframe, but I don't want to print the index</p>

<p>Besides, one column is datetime type, I just want to print time, not date.</p>

<p>The dataframe looks like:</p>

<pre><code>   User ID           Enter Time   Activity Number
0      123  2014-07-08 00:09:00              1411
1      123  2014-07-08 00:18:00               893
2      123  2014-07-08 00:49:00              1041
</code></pre>

<p>I want it print as</p>

<pre><code>User ID   Enter Time   Activity Number
123         00:09:00              1411
123         00:18:00               893
123         00:49:00              1041
</code></pre>
";;4;;2014-07-09T02:50:29.920;5.0;24644656;2017-08-17T16:51:42.363;2017-08-02T14:09:49.100;;4909087.0;;3804098.0;;1;32;<python><datetime><pandas><dataframe>;How to print dataframe without index;21964.0
15579;15579;36475297.0;4.0;"<p>I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured out a way to do that yet.  I've written the following code that works:</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn import preprocessing

scaler = preprocessing.MinMaxScaler()

dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})
min_max_scaler = preprocessing.MinMaxScaler()

def scaleColumns(df, cols_to_scale):
    for col in cols_to_scale:
        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])
    return df

dfTest

    A   B   C
0    14.00   103.02  big
1    90.20   107.26  small
2    90.95   110.35  big
3    96.27   114.23  small
4    91.21   114.68  small

scaled_df = scaleColumns(dfTest,['A','B'])
scaled_df

A   B   C
0    0.000000    0.000000    big
1    0.926219    0.363636    small
2    0.935335    0.628645    big
3    1.000000    0.961407    small
4    0.938495    1.000000    small
</code></pre>

<p>I'm curious if this is the preferred/most efficient way to do this transformation.  Is there a way I could use df.apply that would be better?  </p>

<p>I'm also surprised I can't get the following code to work:</p>

<p><code>bad_output = min_max_scaler.fit_transform(dfTest['A'])</code></p>

<p>If I pass an entire dataframe to the scaler it works:</p>

<p><code>dfTest2 = dfTest.drop('C', axis = 1)
good_output = min_max_scaler.fit_transform(dfTest2)
good_output</code></p>

<p>I'm confused why passing a series to the scaler fails.  In my full working code above I had hoped to just pass a series to the scaler then set the dataframe column = to the scaled series.  I've seen this question asked a few other places, but haven't found a good answer.  Any help understanding what's going on here would be greatly appreciated!</p>
";;3;;2014-07-09T03:57:55.123;9.0;24645153;2016-06-20T09:04:33.377;;;;;2255198.0;;1;25;<python><pandas><scikit-learn><dataframe>;pandas dataframe columns scaling with sklearn;19762.0
15856;15856;24775756.0;2.0;"<p>I would like the element-wise logical OR operator. I know ""or"" itself is not what I am looking for.</p>

<p>For AND I want to use &amp; as explained <a href=""https://stackoverflow.com/questions/21415661/logic-operator-for-boolean-indexing-in-pandas"" title=""here"">here</a>. For NOT I want to use np.invert() as explained <a href=""https://stackoverflow.com/questions/15998188/how-can-i-obtain-the-element-wise-logical-not-of-a-pandas-series"">here</a>. So what is the equivalent for OR?</p>
";;0;;2014-07-16T08:19:48.903;1.0;24775648;2017-05-12T21:35:45.777;2017-05-23T10:31:12.887;;-1.0;;3647167.0;;1;32;<python><pandas><boolean-logic><logical-operators><boolean-operations>;Element-wise logical OR in Pandas;17268.0
15867;15867;24792087.0;1.0;"<p>So I have a 'Date' column in my data frame where the dates have the format like this</p>

<pre><code>0    1998-08-26 04:00:00 
</code></pre>

<p>If I only want the Year month and day how do I drop the trivial hour?</p>
";;4;;2014-07-16T16:40:36.903;4.0;24786209;2014-08-11T16:17:52.047;;;;;2926266.0;;1;13;<python><datetime><pandas><dataframe>;Dropping time from datetime <[M8] in Pandas;11942.0
15957;15957;24826569.0;1.0;"<p>Using the Pandas package in python, I would to sum (marginalize) over one level in a series with a 3-level multiindex to produce a series with a 2 level multiindex. For example, if I have the following:</p>

<pre><code>ind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]
mi = pd.MultiIndex.from_tuples(ind)
data = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)

A  B  C    264
      c     13
   b  C     29
      c      8
a  B  C    152
      c      7
   b  C     15
      c      1
</code></pre>

<p>I would like to sum over the variable C to produce the following output:</p>

<pre><code>A  B    277
   b     37
a  B    159
   b     16
</code></pre>

<p>What is the best way in Pandas to do this?</p>
";;0;;2014-07-18T13:37:08.027;4.0;24826368;2014-07-18T13:46:44.137;;;;;3829299.0;;1;16;<python><pandas><statistics><multi-index>;Summing over a multiindex level in a pandas series;7387.0
16041;16041;24870404.0;3.0;"<p>Is there a way to check if a column exists in a Pandas DataFrame?</p>

<p>Suppose that I have the following DataFrame:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from random import randint
&gt;&gt;&gt; df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                       'B': [randint(1, 9)*10 for x in xrange(10)],
                       'C': [randint(1, 9)*100 for x in xrange(10)]})
&gt;&gt;&gt; df
   A   B    C
0  3  40  100
1  6  30  200
2  7  70  800
3  3  50  200
4  7  50  400
5  4  10  400
6  3  70  500
7  8  30  200
8  3  40  800
9  6  60  200
</code></pre>

<p>and I want to calculate <code>df['sum'] = df['A'] + df['C']</code></p>

<p>but first I want to check if <code>df['A']</code> exists, and if not, I want to calculate <code>df['sum'] = df['B'] + df['C']</code> instead.</p>

<p>Thanks for the help. </p>
";;0;;2014-07-21T16:43:02.963;5.0;24870306;2017-07-10T13:57:34.433;;;;;3321229.0;;1;71;<python><pandas><dataframe>;How to check if a column exists in Pandas;39441.0
16045;16045;24871316.0;4.0;"<p>I have noticed very poor performance when using iterrows from pandas.</p>

<p>Is this something that is experienced by others? Is it specific to iterrows and should this function be avoided for data of a certain size (I'm working with 2-3 million rows)?</p>

<p><a href=""https://github.com/pydata/pandas/issues/7683"">This discussion</a> on GitHub led me to believe it is caused when mixing dtypes in the dataframe, however the simple example below shows it is there even when using one dtype (float64). This takes 36 seconds on my machine:</p>

<pre><code>import pandas as pd
import numpy as np
import time

s1 = np.random.randn(2000000)
s2 = np.random.randn(2000000)
dfa = pd.DataFrame({'s1': s1, 's2': s2})

start = time.time()
i=0
for rowindex, row in dfa.iterrows():
    i+=1
end = time.time()
print end - start
</code></pre>

<p>Why are vectorized operations like apply so much quicker? I imagine there must be some row by row iteration going on there too. </p>

<p>I cannot figure out how to not use iterrows in my case (this I'll save for a future question). Therefore I would appreciate hearing if you have consistently been able to avoid this iteration. I'm making calculations based on data in separate dataframes. Thank you!</p>

<p>---Edit: simplified version of what I want to run has been added below---</p>

<pre><code>import pandas as pd
import numpy as np

#%% Create the original tables
t1 = {'letter':['a','b'],
      'number1':[50,-10]}

t2 = {'letter':['a','a','b','b'],
      'number2':[0.2,0.5,0.1,0.4]}

table1 = pd.DataFrame(t1)
table2 = pd.DataFrame(t2)

#%% Create the body of the new table
table3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])

#%% Iterate through filtering relevant data, optimizing, returning info
for row_index, row in table1.iterrows():   
    t2info = table2[table2.letter == row['letter']].reset_index()
    table3.ix[row_index,] = optimize(t2info,row['number1'])

#%% Define optimization
def optimize(t2info, t1info):
    calculation = []
    for index, r in t2info.iterrows():
        calculation.append(r['number2']*t1info)
    maxrow = calculation.index(max(calculation))
    return t2info.ix[maxrow]
</code></pre>
";;6;;2014-07-21T17:19:17.380;17.0;24870953;2017-08-15T15:03:32.380;2014-07-21T17:31:21.977;;3232824.0;;3232824.0;;1;22;<python><performance><pandas><iteration>;Does iterrows have performance issues?;7339.0
16105;16105;24902313.0;3.0;"<p>I'm using df.columns.values to make a list of column names which I then iterate over and make charts, etc... but when I set this up I overlooked the non-numeric columns in the df. Now, I'd much rather not simply drop those columns from the df (or a copy of it). Instead, I would like to find a slick way to eliminate them from the list of column names. </p>

<p>Now I have: </p>

<pre><code>names = df.columns.values 
</code></pre>

<p>what I'd like to get to is something that behaves like: </p>

<pre><code>names = df.columns.values(column_type=float64) 
</code></pre>

<p>Is there any slick way to do this? I suppose I could make a copy of the df, and drop those non-numeric columns before doing columns.values, but that strikes me as clunky.</p>

<p>Welcome any inputs/suggestions. Thanks. </p>
";;0;;2014-07-23T04:17:22.373;2.0;24901766;2015-09-24T11:40:57.207;2014-07-23T07:20:02.873;;704848.0;;3730522.0;;1;20;<python><pandas>;python: How to get column names from pandas dataframe - but only for continuous data type?;63185.0
16237;16237;24980809.0;1.0;"<p>I'm trying to write a function to aggregate and perform various stats calcuations on a dataframe in Pandas and then merge it to the original dataframe however, I'm running to issues.  This is code equivalent in SQL:</p>

<pre><code>SELECT EID,
       PCODE,
       SUM(PVALUE) AS PVALUE,
       SUM(SQRT(SC*EXP(SC-1))) AS SC,
       SUM(SI) AS SI,
       SUM(EE) AS EE
INTO foo_bar_grp
FROM foo_bar
GROUP BY EID, PCODE 
</code></pre>

<p>And then join on the original table:</p>

<pre><code>SELECT *
FROM foo_bar_grp INNER JOIN 
foo_bar ON foo_bar.EID = foo_bar_grp.EID 
        AND foo_bar.PCODE = foo_bar_grp.PCODE
</code></pre>

<p><strong>Here are the steps:  Loading the data</strong>
IN:>></p>

<pre><code>pol_dict = {'PID':[1,1,2,2],
             'EID':[123,123,123,123],
             'PCODE':['GU','GR','GU','GR'],
             'PVALUE':[100,50,150,300],
             'SI':[400,40,140,140],
             'SC':[230,23,213,213],
             'EE':[10000,10000,2000,30000],
             }


pol_df = DataFrame(pol_dict)

pol_df
</code></pre>

<p>OUT:>></p>

<pre><code>   EID    EE PCODE  PID  PVALUE   SC   SI
0  123  10000    GU    1     100  230  400
1  123  10000    GR    1      50   23   40
2  123   2000    GU    2     150  213  140
3  123  30000    GR    2     300  213  140
</code></pre>

<p><strong>Step 2:  Calculating and Grouping on the data:</strong></p>

<p>My pandas code is as follows:</p>

<pre><code>#create aggregation dataframe
poagg_df = pol_df
del poagg_df['PID']
po_grouped_df = poagg_df.groupby(['EID','PCODE'])

#generate acc level aggregate
acc_df = po_grouped_df.agg({
    'PVALUE' : np.sum,
    'SI' : lambda x: np.sqrt(np.sum(x * np.exp(x-1))),
    'SC' : np.sum,
    'EE' : np.sum
})
</code></pre>

<p>This works fine until I want to join on the original table:</p>

<p>IN:>></p>

<pre><code>po_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))
</code></pre>

<p>OUT:>>
KeyError: u'no item named EID'</p>

<p>For some reason, the grouped dataframe can't join back to the original table.  I've looked at ways of trying to convert the groupby columns to actual columns but that doesn't seem to work. </p>

<p>Please note, the end goal is to be able to find the percentage for each column (PVALUE, SI, SC, EE)  IE:</p>

<pre><code>pol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)
</code></pre>

<p>Thanks!</p>
";;0;;2014-07-27T11:25:56.883;4.0;24980437;2014-07-27T12:10:29.183;;;;;3879804.0;;1;12;<python><python-2.7><pandas>;Pandas - GroupBy and then Merge on original table;10276.0
16247;16247;24988227.0;2.0;"<p>Say I have a dictionary that looks like this:</p>

<pre><code>dictionary = {'A' : {'a': [1,2,3,4,5],
                     'b': [6,7,8,9,1]},

              'B' : {'a': [2,3,4,5,6],
                     'b': [7,8,9,1,2]}}
</code></pre>

<p>and I want a dataframe that looks something like this:</p>

<pre><code>     A   B
     a b a b
  0  1 6 2 7
  1  2 7 3 8
  2  3 8 4 9
  3  4 9 5 1
  4  5 1 6 2
</code></pre>

<p>Is there a convenient way to do this? If I try:</p>

<pre><code>In [99]:

DataFrame(dictionary)

Out[99]:
     A               B
a   [1, 2, 3, 4, 5] [2, 3, 4, 5, 6]
b   [6, 7, 8, 9, 1] [7, 8, 9, 1, 2]
</code></pre>

<p>I get a dataframe where each element is a list. What I need is a multiindex where each level corresponds to the keys in the nested dict and the rows corresponding to each element in the list as shown above. I think I can work a very crude solution but I'm hoping there might be something a bit simpler. </p>
";;0;;2014-07-28T03:43:02.213;7.0;24988131;2017-06-28T18:35:32.453;2014-07-28T03:53:20.210;;2593236.0;;2593236.0;;1;17;<python><dictionary><pandas><dataframe><multi-index>;Nested dictionary to multiindex dataframe where dictionary keys are column labels;6855.0
16319;16319;25025065.0;2.0;"<p>I have a dataframe that looks like:</p>

<pre><code>data = {'index': ['2014-06-22 10:46:00', '2014-06-24 19:52:00', '2014-06-25 17:02:00', '2014-06-25 17:55:00', '2014-07-02 11:36:00', '2014-07-06 12:40:00', '2014-07-05 12:46:00', '2014-07-27 15:12:00'],
    'type': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'C'],
    'sum_col': [1, 2, 3, 1, 1, 3, 2, 1]}
df = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])
df['index'] = pd.to_datetime(df['index'])
df = df.set_index('index')
df['weekofyear'] = df.index.weekofyear
df['date'] = df.index.date
df['date'] = pd.to_datetime(df['date'])



                     type sum_col weekofyear   date
index               
2014-06-22 10:46:00    A    1       25      2014-06-22
2014-06-24 19:52:00    B    2       26      2014-06-24
2014-06-25 17:02:00    C    3       26      2014-06-25
2014-06-25 17:55:00    A    1       26      2014-06-25
2014-07-02 11:36:00    B    1       27      2014-07-02
2014-07-06 12:40:00    C    3       27      2014-07-06
2014-07-05 12:46:00    A    2       27      2014-07-05
2014-07-27 15:12:00    C    1       30      2014-07-27
</code></pre>

<p>I'm looking to groupby the weekofyear, then sum up the sum_col. In addition, I need to find the earliest, and the latest date for the week. The first part is pretty easy:</p>

<pre><code>gb = df.groupby(['type', 'weekofyear'])
gb['sum_col'].agg({'sum_col' : np.sum})
</code></pre>

<p>I've tried to find the min/max date with this, but haven't been successful:</p>

<pre><code>gb = df.groupby(['type', 'weekofyear'])
gb.agg({'sum_col' : np.sum,
        'date' : np.min,
        'date' : np.max})
</code></pre>

<p>How would one find the earliest/latest date that appears?</p>
";;0;;2014-07-29T20:52:45.057;3.0;25024797;2016-09-03T23:40:16.463;;;;;3325052.0;;1;15;<python><pandas><dataframe>;Max and Min date in pandas groupby;11334.0
16347;16347;;5.0;"<p>say df is a pandas DataFrame.
I would like to find all columns of numeric type.
something like:</p>

<pre><code>isNumeric = is_numeric(df)
</code></pre>
";;1;;2014-07-30T14:36:21.093;7.0;25039626;2017-05-15T14:59:09.477;;;;;2424587.0;;1;37;<python><types><pandas>;find numeric columns in pandas (python);15852.0
16371;16371;25050179.0;2.0;"<p>I have a pandas dataframe (df), and I want to do something like:</p>

<pre><code>newdf = df[(df.var1 == 'a') &amp; (df.var2 == NaN)]
</code></pre>

<p>I've tried replacing NaN with <code>np.NaN</code>, or <code>'NaN'</code> or <code>'nan'</code> etc, but nothing evaluates to True. There's no <code>pd.NaN</code>.</p>

<p>I can use <code>df.fillna(np.nan)</code> before evaluating the above expression but that feels hackish and I wonder if it will interfere with other pandas operations that rely on being able to identify pandas-format NaN's later.</p>

<p>I get the feeling there should be an easy answer to this question, but somehow it has eluded me. Any advice is appreciated. Thank you.</p>
";;2;;2014-07-31T02:57:26.547;3.0;25050141;2015-03-13T00:53:06.667;;;;;2412518.0;;1;18;<python><pandas><nan>;How to filter in NaN (pandas)?;13993.0
16378;16378;25057724.0;1.0;"<p>Dataframe.resample() works only with timeseries data. I cannot find a way of getting every nth row from non-timeseries data. What is the best method?</p>
";;0;;2014-07-31T09:44:55.827;5.0;25055712;2014-07-31T11:25:16.297;;;;;1014352.0;;1;23;<pandas><resampling>;Pandas every nth row;10589.0
16383;16383;25058102.0;5.0;"<p>I'm generating a number of dataframes with the same shape, and I want to compare them to one another. I want to be able to get the mean and median across the dataframes.</p>

<pre><code>         Source.0  Source.1  Source.2  Source.3
cluster                                        
0        0.001182  0.184535  0.814230  0.000054
1        0.000001  0.160490  0.839508  0.000001
2        0.000001  0.173829  0.826114  0.000055
3        0.000432  0.180065  0.819502  0.000001
4        0.000152  0.157041  0.842694  0.000113
5        0.000183  0.174142  0.825674  0.000001
6        0.000001  0.151556  0.848405  0.000038
7        0.000771  0.177583  0.821645  0.000001
8        0.000001  0.202059  0.797939  0.000001
9        0.000025  0.189537  0.810410  0.000028
10       0.006142  0.003041  0.493912  0.496905
11       0.003739  0.002367  0.514216  0.479678
12       0.002334  0.001517  0.529041  0.467108
13       0.003458  0.000001  0.532265  0.464276
14       0.000405  0.005655  0.527576  0.466364
15       0.002557  0.003233  0.507954  0.486256
16       0.004161  0.000001  0.491271  0.504568
17       0.001364  0.001330  0.528311  0.468996
18       0.002886  0.000001  0.506392  0.490721
19       0.001823  0.002498  0.509620  0.486059

         Source.0  Source.1  Source.2  Source.3
cluster                                        
0        0.000001  0.197108  0.802495  0.000396
1        0.000001  0.157860  0.842076  0.000063
2        0.094956  0.203057  0.701662  0.000325
3        0.000001  0.181948  0.817841  0.000210
4        0.000003  0.169680  0.830316  0.000001
5        0.000362  0.177194  0.822443  0.000001
6        0.000001  0.146807  0.852924  0.000268
7        0.001087  0.178994  0.819564  0.000354
8        0.000001  0.202182  0.797333  0.000485
9        0.000348  0.181399  0.818252  0.000001
10       0.003050  0.000247  0.506777  0.489926
11       0.004420  0.000001  0.513927  0.481652
12       0.006488  0.001396  0.527197  0.464919
13       0.001510  0.000001  0.525987  0.472502
14       0.000001  0.000001  0.520737  0.479261
15       0.000001  0.001765  0.515658  0.482575
16       0.000001  0.000001  0.492550  0.507448
17       0.002855  0.000199  0.526535  0.470411
18       0.000001  0.001952  0.498303  0.499744
19       0.001232  0.000001  0.506612  0.492155
</code></pre>

<p>Then I want to get the mean of these two dataframes.</p>

<p>What is the easiest way to do this?</p>

<p>Just to clarify I want to get the mean for each particular cell when the indexes and columns of all the dataframes are exactly the same.</p>

<p>So in the example I gave, the average for <code>[0,Source.0]</code> would be (0.001182 + 0.000001) / 2 = 0.0005915.</p>
";;2;;2014-07-31T11:32:09.220;3.0;25057835;2016-10-24T19:32:50.290;2016-04-19T14:18:56.080;;4346285.0;;819718.0;;1;12;<python><r><numpy><pandas>;Get the mean across multiple Pandas DataFrames;5655.0
16502;16502;25122293.0;2.0;"<p>Here is my df:</p>

<pre><code>                             Net   Upper   Lower  Mid  Zsore
Answer option                                                
More than once a day          0%   0.22%  -0.12%   2    65 
Once a day                    0%   0.32%  -0.19%   3    45
Several times a week          2%   2.45%   1.10%   4    78
Once a week                   1%   1.63%  -0.40%   6    65
</code></pre>

<p>How can I move a column by name (""Mid"") to the front of the table, index 0. This is what it needs to look like:</p>

<pre><code>                             Mid   Upper   Lower  Net  Zsore
Answer option                                                
More than once a day          2   0.22%  -0.12%   0%    65 
Once a day                    3   0.32%  -0.19%   0%    45
Several times a week          4   2.45%   1.10%   2%    78
Once a week                   6   1.63%  -0.40%   1%    65
</code></pre>

<p>My current code moves the column by index via ""df.columns.tolist()"" but Id like to shift it by Name. </p>
";;0;;2014-08-04T15:21:31.893;5.0;25122099;2017-06-27T11:09:46.847;2015-11-13T00:24:55.667;;4805990.0;;2342399.0;;1;18;<python><pandas><move><dataframe><shift>;Move column by name to front of table in pandas;14833.0
16530;16530;25129655.0;3.0;"<p>Assume I have a DataFrame <code>sales</code> of timestamp values:</p>

<pre><code>timestamp               sales_office
2014-01-01 09:01:00     Cincinnati
2014-01-01 09:11:00     San Francisco
2014-01-01 15:22:00     Chicago
2014-01-01 19:01:00     Chicago
</code></pre>

<p>I would like to create a new column <code>time_hour</code>. I can create it by writing a short function as so and using <code>apply()</code> to apply it iteratively:</p>

<pre><code>def hr_func(ts):
    return ts.hour

sales['time_hour'] = sales['timestamp'].apply(hr_func)
</code></pre>

<p>I would then see this result:</p>

<pre><code>timestamp               sales_office         time_hour
2014-01-01 09:01:00     Cincinnati           9
2014-01-01 09:11:00     San Francisco        9
2014-01-01 15:22:00     Chicago              15
2014-01-01 19:01:00     Chicago              19
</code></pre>

<p>What I'd <em>like</em> to achieve is some shorter transformation like this (which I know is erroneous but gets at the spirit):</p>

<pre><code>sales['time_hour'] = sales['timestamp'].hour
</code></pre>

<p>Obviously the column is of type <code>Series</code> and as such doesn't have those attributes, but it seems there's a simpler way to make use of matrix operations.</p>

<p>Is there a more-direct approach?</p>
";;8;;2014-08-04T23:38:33.807;4.0;25129144;2016-12-15T20:24:58.693;;;;;2276583.0;;1;14;<python><datetime><pandas>;Pandas: Return Hour from Datetime Column Directly;20259.0
16571;16571;25146337.0;7.0;"<p>I have a Dataframe, df, with the following column:</p>

<pre><code>df['ArrivalDate'] =
...
936   2012-12-31
938   2012-12-29
965   2012-12-31
966   2012-12-31
967   2012-12-31
968   2012-12-31
969   2012-12-31
970   2012-12-29
971   2012-12-31
972   2012-12-29
973   2012-12-29
...
</code></pre>

<p>The elements of the column are pandas.tslib.Timestamp.</p>

<p>I want to just include the year and month.  I thought there would be simple way to do it, but I can't figure it out.</p>

<p>Here's what I've tried:</p>

<pre><code>df['ArrivalDate'].resample('M', how = 'mean')
</code></pre>

<p>I got the following error:</p>

<pre><code>Only valid with DatetimeIndex or PeriodIndex 
</code></pre>

<p>Then I tried:</p>

<pre><code>df['ArrivalDate'].apply(lambda(x):x[:-2])
</code></pre>

<p>I got the following error:</p>

<pre><code>'Timestamp' object has no attribute '__getitem__' 
</code></pre>

<p>Any suggestions?</p>

<p>Edit: I sort of figured it out.  </p>

<pre><code>df.index = df['ArrivalDate']
</code></pre>

<p>Then, I can resample another column using the index.</p>

<p>But I'd still like a method for reconfiguring the entire column.  Any ideas?</p>
";;1;;2014-08-05T18:44:30.347;22.0;25146121;2017-07-21T05:17:12.517;;;;;3494704.0;;1;53;<python><pandas>;Extracting just Month and Year from Pandas Datetime column (Python);78275.0
16662;16662;25190070.0;3.0;"<p>I have the following pd.DataFrame:</p>

<pre><code>Name    0                       1                      ...
Col     A           B           A            B         ...
0       0.409511    -0.537108   -0.355529    0.212134  ...
1       -0.332276   -1.087013    0.083684    0.529002  ...
2       1.138159    -0.327212    0.570834    2.337718  ...
</code></pre>

<p>It has MultiIndex columns with <code>names=['Name', 'Col']</code> and hierarchical levels. The <code>Name</code> label goes from 0 to n, and for each label, there are two <code>A</code> and <code>B</code> columns. </p>

<p>I would like to subselect all the <code>A</code> (or <code>B</code>) columns of this DataFrame. </p>
";;1;;2014-08-07T18:28:15.267;7.0;25189575;2017-05-19T13:53:10.393;2017-05-19T13:53:10.393;;4720935.0;;3287799.0;;1;15;<python><pandas><hierarchical><multi-index>;pandas dataframe select columns in multiindex;11209.0
16688;16688;;1.0;"<p>Say I want to do a <strong>stratified</strong> sample from a dataframe in Pandas so that I get <code>5%</code> of rows for every value of a given column. How can I do that?</p>

<p>For example, in the dataframe below, I would like to sample <code>5%</code> of the rows associated with each value of the column <code>Z</code>. Is there any way to <strong>sample groups</strong> from a dataframe loaded in memory?</p>

<pre><code>&gt; df 

   X   Y  Z
   1 123  a
   2  89  b
   1 234  a
   4 893  a
   6 234  b
   2 893  b
   3 200  c
   5 583  c
   2 583  c
   6 100  c
</code></pre>

<p>More generally, what if I this dataframe in disk in a huge file (e.g. 8 GB of a csv file). Is there any way to do this sampling without having to load the entire dataframe in memory?</p>
";;8;;2014-08-08T12:46:24.683;3.0;25203883;2014-08-11T23:36:22.460;2014-08-11T12:53:29.273;;283296.0;;283296.0;;1;12;<python><pandas>;Sampling groups in Pandas;2102.0
16718;16718;25211834.0;1.0;"<p>I want to preview a Pandas dataframe.  I would use head(mymatrix) in R, but I do not know how to do this in Pandas Python.</p>

<p>When I type </p>

<p>df.head(10) I get...</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 10 entries, 0 to 9
Data columns (total 14 columns):
#Book_Date            10  non-null values
Item_Qty              10  non-null values
Item_id               10  non-null values
Location_id           10  non-null values
MFG_Discount          10  non-null values
Sale_Revenue          10  non-null values
Sales_Flg             10  non-null values
Sell_Unit_Cost        5  non-null values
Store_Discount        10  non-null values
Transaction_Id        10  non-null values
Unit_Cost_Amt         10  non-null values
Unit_Received_Cost    5  non-null values
Unnamed: 0            10  non-null values
Weight                10  non-null values
</code></pre>
";;10;;2014-08-08T19:50:16.677;2.0;25211220;2014-08-09T04:08:25.667;2014-08-08T21:30:51.310;;2690949.0;;2690949.0;;1;14;<python><r><view><pandas>;Python equivalent of R's head and tail function;20701.0
16726;16726;25213438.0;2.0;"<p>I'm using Seaborn's lmplot to plot a linear regression, dividing my dataset into two groups with a categorical variable.</p>

<p>For both x and y, I'd like to manually set the <em>lower bound</em> on both plots, but leave the <em>upper bound</em> at the Seaborn default.
Here's a simple example:</p>

<pre><code>import pandas as pd
import seaborn as sns
import random

n = 200
random.seed(2014)
base_x = [random.random() for i in range(n)]
base_y = [2*i for i in base_x]
errors = [random.uniform(0,1) for i in range(n)]
y = [i+j for i,j in zip(base_y,errors)]

df = pd.DataFrame({'X': base_x,
                   'Y': y,
                   'Z': ['A','B']*(n/2)})

mask_for_b = df.Z == 'B'
df.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2

sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)
</code></pre>

<p>This outputs the following:
<img src=""https://i.stack.imgur.com/Pqw2j.png"" alt=""enter image description here""></p>

<p>But in this example, I'd like the xlim and the ylim to be (0,*) . I tried using sns.plt.ylim and sns.plt.xlim but those only affect the right-hand plot. 
Example:</p>

<pre><code>sns.plt.ylim(0,)
sns.plt.xlim(0,)
</code></pre>

<p><img src=""https://i.stack.imgur.com/s9jpF.png"" alt=""enter image description here""></p>

<p>How can I access the xlim and ylim for each plot in the FacetGrid?</p>
";;2;;2014-08-08T22:12:00.260;9.0;25212986;2014-08-08T23:19:53.093;;;;;3393459.0;;1;30;<python><pandas><seaborn>;How to set some xlim and ylim in Seaborn lmplot facetgrid;25892.0
16768;16768;25253780.0;3.0;"<p>I have a time series in pandas that looks like this:</p>

<pre><code>                     Values
1992-08-27 07:46:48    28.0  
1992-08-27 08:00:48    28.2  
1992-08-27 08:33:48    28.4  
1992-08-27 08:43:48    28.8  
1992-08-27 08:48:48    29.0  
1992-08-27 08:51:48    29.2  
1992-08-27 08:53:48    29.6  
1992-08-27 08:56:48    29.8  
1992-08-27 09:03:48    30.0
</code></pre>

<p>I would like to resample it to a regular time series with 15 min times steps where the values are linearly interpolated. Basically I would like to get:</p>

<pre><code>                     Values
1992-08-27 08:00:00    28.2  
1992-08-27 08:15:00    28.3  
1992-08-27 08:30:00    28.4  
1992-08-27 08:45:00    28.8  
1992-08-27 09:00:00    29.9
</code></pre>

<p>However using the resample method (df.resample('15Min')) from Pandas I get:</p>

<pre><code>                     Values
1992-08-27 08:00:00   28.20  
1992-08-27 08:15:00     NaN  
1992-08-27 08:30:00   28.60  
1992-08-27 08:45:00   29.40  
1992-08-27 09:00:00   30.00  
</code></pre>

<p>I have tried the resample method with different 'how' and 'fill_method' parameters but never got exactly the results I wanted. Am I using the wrong method?</p>

<p>I figure this is a fairly simple query, but I have searched the web for a while and couldn't find an answer.</p>

<p>Thanks in advance for any help I can get.</p>
";;0;;2014-08-11T02:04:51.033;7.0;25234941;2016-09-27T17:18:23.793;;;;;3928036.0;;1;12;<python><pandas><time-series><linear-interpolation>;Python regularise irregular time series with linear interpolation;3459.0
16773;16773;25562948.0;4.0;"<p>I've got pandas data with some columns of text type. There are some NaN values along with these text columns. What I'm trying to do is to impute those NaN's by sklearn.preprocessing. Imputer (replacing NaN by the most frequent value). The problem is in implementation.
Suppose there is a Pandas dataframe df with 30 columns, 10 of which are of categorical nature.
Once I run</p>

<pre><code>from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)
imp.fit(df) 
</code></pre>

<p>Python generates an error: 'could not convert string to float: 'run1'', where 'run1' is an ordinary (non-missing) value from the first column with categorical data.</p>

<p>Any help would be very welcome</p>
";;3;;2014-08-11T09:26:41.750;20.0;25239958;2017-07-24T07:46:49.360;;;;;3058525.0;;1;23;<python><pandas><scikit-learn>;Impute categorical missing values in scikit-learn;10847.0
16811;16811;25254087.0;3.0;"<p>This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.</p>

<p>So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).</p>

<p>For example, let's say I want to pull the 1.2 value in Btime as a variable. </p>

<p>Whats the right way to do this?</p>

<p>df_test = </p>

<pre><code>  ATime   X   Y   Z   Btime  C   D   E
0    1.2  2  15   2    1.2  12  25  12
1    1.4  3  12   1    1.3  13  22  11
2    1.5  1  10   6    1.4  11  20  16
3    1.6  2   9  10    1.7  12  29  12
4    1.9  1   1   9    1.9  11  21  19
5    2.0  0   0   0    2.0   8  10  11
6    2.4  0   0   0    2.4  10  12  15
</code></pre>
";;1;;2014-08-11T23:30:16.683;6.0;25254016;2017-03-25T13:17:35.087;;;;;2860565.0;;1;62;<python><pandas>;Pandas - Get first row value of a given column;90823.0
16886;16886;25289109.0;3.0;"<p>I intend to plot multiple columns in a <code>pandas dataframe</code>, all grouped by another column using <code>groupby</code> inside <code>seaborn.boxplot</code>. There is a nice answer here, for a similar problem in <code>matplotlib</code> <a href=""https://stackoverflow.com/questions/16592222/matplotlib-group-boxplots"">matplotlib: Group boxplots</a> but given the fact that <code>seaborn.boxplot</code> comes with <code>groupby</code> option I thought it could be much easier to do this in <code>seaborn</code>. </p>

<p>Here we go with a reproducible example that fails:</p>

<pre><code>import seaborn as sns
import pandas as pd
df = pd.DataFrame(
[
[2, 4, 5, 6, 1],
[4, 5, 6, 7, 2],
[5, 4, 5, 5, 1],
[10, 4, 7, 8, 2],
[9, 3, 4, 6, 2],
[3, 3, 4, 4, 1]
], columns=['a1', 'a2', 'a3', 'a4', 'b'])

#Plotting by seaborn
sns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)
</code></pre>

<p>What I get is something that completely ignores <code>groupby</code> option: </p>

<p><img src=""https://i.stack.imgur.com/2zAei.png"" alt=""Failed groupby""></p>

<p>Whereas if I do this with one column it works thanks to another SO question <a href=""https://stackoverflow.com/questions/25279810/seaborn-groupby-pandas-series/25282470?noredirect=1#comment39402836_25282470"">Seaborn groupby pandas Series</a> :</p>

<pre><code>sns.boxplot(df.a1, groupby=df.b)
</code></pre>

<p><img src=""https://i.stack.imgur.com/nwUkU.png"" alt=""seaborn that does not fail""></p>

<p>So I would like to get all my columns in one plot (all columns come in a similar scale).</p>

<p>EDIT:</p>

<p>The above SO question was edited and now includes a 'not clean' answer to this problem, but it would be nice if someone has a better idea for this problem. </p>
";;0;;2014-08-13T11:24:00.483;5.0;25284859;2016-12-18T15:49:05.223;2017-05-23T10:30:59.773;;-1.0;;2462394.0;;1;15;<matplotlib><pandas><seaborn>;Grouping boxplots in seaborn when input is a DataFrame;10099.0
17034;17034;25352191.0;2.0;"<p>I converted a pandas dataframe to an html output using the <code>DataFrame.to_html</code> function. When I save this to a separate html file, the file shows truncated output.</p>

<p>For example, in my TEXT column, </p>

<p><code>df.head(1)</code> will show </p>

<p><i>The film was an excellent effort...</i></p>

<p>instead of </p>

<p><i>The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.</i></p>

<p>This rendition is fine in the case of a screen-friendly format of a massive pandas dataframe, but I need an html file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet. </p>

<p>How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the html version of the information? I would imagine that the html table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the <code>DataFrame.to_html</code> function.</p>
";;0;;2014-08-17T17:52:52.150;8.0;25351968;2017-02-17T09:32:05.943;2014-08-17T18:06:23.800;;3950509.0;;3950509.0;;1;21;<python><html><pandas>;How to display full (non-truncated) dataframe information in html when converting from pandas dataframe to html?;13484.0
17093;17093;25412939.0;2.0;"<p>After performing a groupby.sum() on a dataframe I'm having some trouble trying to create my intended plot.</p>

<p><img src=""https://i.stack.imgur.com/y9L5u.png"" alt=""grouped dataframe with multi-index""></p>

<p>How can I create a subplot (kind='bar') for each 'Code', where the x-axis is the 'Month' and the bars are ColA and ColB?</p>
";;0;;2014-08-19T15:05:48.973;5.0;25386870;2014-08-20T19:28:55.840;;;;;1183875.0;;1;25;<python><matplotlib><pandas><multi-index>;Pandas Plotting with Multi-Index;12929.0
17172;17172;;1.0;"<p>I ran up against unexpected behavior in pandas when comparing two series.  I wanted to know if this is intended or a bug.</p>

<p>suppose I:</p>

<pre><code>import pandas as pd
x = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')
y = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')

x &gt; y
</code></pre>

<p>yields:</p>

<pre><code>a     True
b    False
c     True
d    False
e    False
f    False
Name: Value, dtype: bool
</code></pre>

<p>which isn't what I wanted.  Clearly, I expected the indexes to line up.  But I have to explicitly line them up to get the desired results.</p>

<pre><code>x &gt; y.reindex_like(x)
</code></pre>

<p>yields:</p>

<pre><code>a     True
b     True
c     True
d    False
e    False
f    False
Name: Value, dtype: bool
</code></pre>

<p>Which is what I expected.</p>

<p>What's worse is if I:</p>

<pre><code>x + y
</code></pre>

<p>I get:</p>

<pre><code>a    1
b    1
c    1
d    2
e    2
f    2
Name: Value, dtype: int64
</code></pre>

<p>So when operating, the indexes line up.  When comparing, they do not.  Is my observation accurate?  Is this intended for some purpose?</p>

<p>Thanks,</p>

<p>-PiR</p>
";;4;;2014-08-21T20:19:11.947;;25435229;2014-09-29T14:57:58.810;2014-08-21T21:31:50.820;;2487184.0;;2336654.0;;1;17;<python><pandas><compare><series>;What happens when you compare 2 pandas Series;1056.0
17181;17181;25440505.0;3.0;"<p>I have a df like so:</p>

<pre><code>import pandas
a=[['1/2/2014', 'a', '6', 'z1'], 
   ['1/2/2014', 'a', '3', 'z1'], 
   ['1/3/2014', 'c', '1', 'x3'],
   ]
df = pandas.DataFrame.from_records(a[1:],columns=a[0])
</code></pre>

<p>I want to flatten the df so it is one continuous list like so:</p>

<p><code>['1/2/2014', 'a', '6', 'z1', '1/2/2014', 'a', '3', 'z1','1/3/2014', 'c', '1', 'x3']</code></p>

<p>I can loop through the rows and <code>extend</code> to a list, but is a much easier way to do it?</p>
";;2;;2014-08-22T05:17:57.967;2.0;25440008;2014-08-22T06:03:06.743;2014-08-22T06:03:06.743;;832621.0;;1744744.0;;1;12;<python><list><numpy><pandas><dataframe>;python pandas flatten a dataframe to a list;11919.0
17202;17202;25449186.0;2.0;"<p>I looking for a way to annotate my bars in a Pandas bar plot with the values (rounded) in my DataFrame.</p>

<pre><code>&gt;&gt;&gt; df=pd.DataFrame({'A':np.random.rand(2),'B':np.random.rand(2)},index=['value1','value2'] )         
&gt;&gt;&gt; df
                 A         B
  value1  0.440922  0.911800
  value2  0.588242  0.797366
</code></pre>

<p>I would like to get something like this:</p>

<p><img src=""https://i.stack.imgur.com/aJ7dS.png"" alt=""bar plot annotation example""></p>

<p>I tried with this, but the annotations are all centered on the xthicks:</p>

<pre><code>&gt;&gt;&gt; ax = df.plot(kind='bar') 
&gt;&gt;&gt; for idx, label in enumerate(list(df.index)): 
        for acc in df.columns:
            value = np.round(df.ix[idx][acc],decimals=2)
            ax.annotate(value,
                        (idx, value),
                         xytext=(0, 15), 
                         textcoords='offset points')
</code></pre>
";;1;;2014-08-22T13:01:37.407;10.0;25447700;2017-03-27T13:30:05.363;2014-08-22T15:31:04.843;;3325052.0;;1613796.0;;1;18;<python><matplotlib><plot><pandas><dataframe>;Annotate bars with values on Pandas bar plots;9512.0
17270;17270;25478896.0;1.0;"<p>I am doing some geocoding work that I used <code>selenium</code> to screen scrape the x-y coordinate I need for address of a location, I imported an xls file to panda dataframe and want to use explicit loop to update the rows which do not have the x-y coordinate, like below:</p>

<pre><code>for index, row in rche_df.iterrows():
    if isinstance(row.wgs1984_latitude, float):
        row = row.copy()
        target = row.address_chi        
        dict_temp = geocoding(target)
        row.wgs1984_latitude = dict_temp['lat']
        row.wgs1984_longitude = dict_temp['long']
</code></pre>

<p>I have read <a href=""https://stackoverflow.com/questions/15972264/why-doesnt-this-function-take-after-i-iterrows-over-a-pandas-dataframe"">Why doesn&#39;t this function &quot;take&quot; after I iterrows over a pandas DataFrame?</a> and am fully aware that iterrow only gives us a view rather than a copy for editing, but what if I really to update the value row by row? Is <code>lambda</code> feasible?</p>
";;2;;2014-08-25T03:10:55.810;3.0;25478528;2014-08-25T04:04:16.833;2017-05-23T12:24:23.243;;-1.0;;373908.0;;1;11;<python><loops><pandas><explicit>;Updating value in iterrow for pandas;12347.0
17274;17274;;1.0;"<p>I am trying to create a column which contains only the minimum of the one row and a few columns, for example:</p>

<pre><code>    A0      A1      A2      B0      B1      B2      C0      C1
0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75
1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39
2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73
3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03
4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61
5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53
6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72
</code></pre>

<p>Here I am trying to create a column which contains the minimum for each row of columns B0, B1, B2.</p>

<p>The output would look like this:</p>

<pre><code>    A0      A1      A2      B0      B1      B2      C0      C1      Minimum
0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75    0.42
1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39    0.39
2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73    0.00
3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03    0.51
4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61    0.51
5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53    0.17
6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72    0.01
</code></pre>

<p>Here is part of the code, but it is not doing what I want it to do:</p>

<pre><code>for i in range(0,2):
    df['Minimum'] = df.loc[0,'B'+str(i)].min()
</code></pre>
";;0;;2014-08-25T05:40:04.413;2.0;25479607;2014-08-25T07:58:36.440;2014-08-25T05:50:59.120;;3943055.0;;3943055.0;;1;12;<python><pandas><row><minimum><calculated-columns>;Pandas min() of selected row and columns;15665.0
17291;17291;25493765.0;2.0;"<p>I have the following 2 dataframes </p>

<pre><code>Example1
sku loc flag  
122  61 True 
123  61 True
113  62 True 
122  62 True 
123  62 False
122  63 False
301  63 True 

Example2 
sku dept 
113 a
122 b
123 b
301 c 
</code></pre>

<p>I want to perform a merge, or join opertation using Pandas (or whichever Python operator is best) to produce the below data frame. </p>

<pre><code>Example3
sku loc flag   dept  
122  61 True   b
123  61 True   b
113  62 True   a
122  62 True   b
123  62 False  b
122  63 False  b
301  63 True   c

Both 
df_Example1.join(df_Example2,lsuffix='_ProdHier')
df_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')
</code></pre>

<p>Aren't working.
What am I doing wrong? </p>
";;0;;2014-08-25T20:12:08.170;15.0;25493625;2017-03-03T15:08:22.547;2014-08-25T20:31:18.133;;704848.0;;3967806.0;;1;22;<python><join><pandas><vlookup>;vlookup in Pandas using join;21751.0
17319;17319;;2.0;"<p>I am using pandas to analyse the large data files here: <a href=""http://www.nielda.co.uk/betfair/data/"" rel=""noreferrer"">http://www.nielda.co.uk/betfair/data/</a> They are around 100 megs in size.</p>

<p>Each load from csv takes a few seconds, and then more time to convert the dates.</p>

<p>I have tried loading the files, converting the dates from strings to datetimes, and then re-saving them as pickle files. But loading those takes a few seconds as well.</p>

<p>What fast methods could I use to load/save the data from disk?</p>
";;6;;2014-08-26T14:34:35.880;5.0;25508510;2014-08-26T17:06:17.123;;;;;847663.0;;1;15;<python><pandas>;Fastest way to parse large CSV files in Pandas;11193.0
17397;17397;;1.0;"<p>I want to use the Pandas dataframe to breakdown the variance in one variable.</p>

<p>For example, if I have a column called 'Degrees', and I have this indexed for various dates, cities, and night vs. day, I want to find out what fraction of the variation in this series is coming from cross-sectional city variation, how much is coming from time series variation, and how much is coming from night vs. day. </p>

<p>In Stata I would use Fixed effects and look at the R^2.   Hopefully my question makes sense.</p>

<p>Basically, what I want to do, is find the ANOVA breakdown of ""Degrees"" by three other columns.  </p>
";;6;;2014-08-27T21:41:10.063;3.0;25537399;2015-11-26T20:08:59.263;2014-08-27T23:43:38.940;;2690949.0;;2690949.0;;1;21;<python><pandas><scipy><statsmodels><anova>;ANOVA in python using pandas dataframe with statsmodels or scipy?;10835.0
17481;17481;26394108.0;4.0;"<p>Is there a way to do this?  I cannot seem an easy way to interface pandas series with plotting a CDF.  </p>
";;4;;2014-08-29T23:05:46.837;4.0;25577352;2016-09-21T23:52:51.393;;;;;2690949.0;;1;17;<python><pandas><series><cdf>;Plotting CDF of a pandas series in python;14521.0
17537;17537;38750433.0;1.0;"<p>I want to bring some data into a pandas DataFrame and I want to assign dtypes for each column on import.  I want to be able to do this for larger datasets with many different columns, but, as an example:</p>

<pre><code>myarray = np.random.randint(0,5,size=(2,2))
mydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])
mydf.dtypes
</code></pre>

<p>results in:</p>

<pre><code>TypeError: data type not understood
</code></pre>

<p>I tried a few other methods such as:</p>

<pre><code>mydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})

TypeError: object of type 'type' has no len()
</code></pre>

<p>If I put <code>dtype=(float,int)</code> it applies a float format to both columns.</p>

<p>In the end I would like to just be able to pass it a list of datatypes the same way I can pass it a list of column names.</p>
";;6;;2014-09-01T17:27:36.507;3.0;25610592;2017-08-15T10:45:08.647;2016-11-29T16:03:06.660;;202229.0;;3971910.0;;1;17;<python><pandas><types>;How to set dtypes by column in pandas DataFrame;5800.0
17565;17565;25630681.0;1.0;"<p>I'm relatively new to using the PyCharm IDE, and have been unable to find a way to better shape the output when in a built-in console session. I'm typically working with pretty wide dataframes, that would fit easily across my monitor, but the display is cutting and wrapping them much sooner than needed.</p>

<p>Does anyone know of a setting to change this behavior to take advantage of the full width of my screen?</p>

<p>Edit: I don't have enough reputation to post a screenshot, but link is below:
<a href=""http://imgur.com/iiBK3iU"">http://imgur.com/iiBK3iU</a></p>

<p>I would like to prevent it from wrapping after only a few columns (for example, the column 'ReadmitRate' should be immediately to the right of 'SNFDaysPerSNFCase')</p>
";;2;;2014-09-02T16:53:04.247;5.0;25628496;2016-04-10T14:21:26.657;2016-04-10T14:21:26.657;;541208.0;;3866328.0;;1;13;<python><pandas><ipython><pycharm>;Getting wider output in PyCharm's built-in console;4476.0
17568;17568;;1.0;"<p>The following code works well. Just checking: am I using and timing Pandas correctly and is there any faster way? Thanks.</p>

<pre><code>$ python3
Python 3.4.0 (default, Apr 11 2014, 13:05:11) 
[GCC 4.8.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import timeit
&gt;&gt;&gt; pd.__version__
'0.14.1'

def randChar(f, numGrp, N) :
   things = [f%x for x in range(numGrp)]
   return [things[x] for x in np.random.choice(numGrp, N)]

def randFloat(numGrp, N) :
   things = [round(100*np.random.random(),4) for x in range(numGrp)]
   return [things[x] for x in np.random.choice(numGrp, N)]

N=int(1e8)
K=100
DF = pd.DataFrame({
  'id1' : randChar(""id%03d"", K, N),       # large groups (char)
  'id2' : randChar(""id%03d"", K, N),       # large groups (char)
  'id3' : randChar(""id%010d"", N//K, N),   # small groups (char)
  'id4' : np.random.choice(K, N),         # large groups (int)
  'id5' : np.random.choice(K, N),         # large groups (int)
  'id6' : np.random.choice(N//K, N),      # small groups (int)            
  'v1' :  np.random.choice(5, N),         # int in range [1,5]
  'v2' :  np.random.choice(5, N),         # int in range [1,5]
  'v3' :  randFloat(100,N)                # numeric e.g. 23.5749
})
</code></pre>

<p>Now time 5 different groupings, repeating each one twice to confirm the timing. [I realise <code>timeit(2)</code> runs it twice, but then it reports the total. I'm interested in the time of the first and second run separately.] Python uses about 10G of RAM according to <code>htop</code> during these tests.</p>

<pre><code>&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1']).agg({'v1':'sum'})""                            ,""from __main__ import DF"").timeit(1)
5.604133386000285
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1']).agg({'v1':'sum'})""                            ,""from __main__ import DF"").timeit(1)
5.505057081000359

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1','id2']).agg({'v1':'sum'})""                      ,""from __main__ import DF"").timeit(1)
14.232032927000091
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1','id2']).agg({'v1':'sum'})""                      ,""from __main__ import DF"").timeit(1)
14.242601240999647

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})""               ,""from __main__ import DF"").timeit(1)
22.87025260900009
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})""               ,""from __main__ import DF"").timeit(1)
22.393589012999655

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})"" ,""from __main__ import DF"").timeit(1)
2.9725865330001398
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})"" ,""from __main__ import DF"").timeit(1)
2.9683854739996605

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})""    ,""from __main__ import DF"").timeit(1)
12.776488024999708
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})""    ,""from __main__ import DF"").timeit(1)
13.558292575999076
</code></pre>

<p>Here is system info :</p>

<pre><code>$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 62
Stepping:              4
CPU MHz:               2500.048
BogoMIPS:              5066.38
Hypervisor vendor:     Xen
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              25600K
NUMA node0 CPU(s):     0-7,16-23
NUMA node1 CPU(s):     8-15,24-31

$ free -h
             total       used       free     shared    buffers     cached
Mem:          240G        74G       166G       372K        33M       550M
-/+ buffers/cache:        73G       166G
Swap:           0B         0B         0B
</code></pre>

<p>I don't believe it's relevant but just in case, the <code>randChar</code> function above is a workaround for a memory error in <code>mtrand.RandomState.choice</code> :</p>

<p><a href=""https://stackoverflow.com/questions/25627161/how-to-solve-memory-error-in-mtrand-randomstate-choice"">How to solve memory error in mtrand.RandomState.choice?</a></p>
";;16;;2014-09-02T19:39:28.213;8.0;25631076;2015-02-25T14:17:41.623;2017-05-23T12:02:23.730;;-1.0;;403310.0;;1;46;<python><performance><numpy><pandas>;Is this the fastest way to group in Pandas?;2357.0
17586;17586;;2.0;"<p>I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do something more manual?</p>

<p><strong>timedelta column</strong>                </p>

<blockquote>
  <p>7 days, 23:29:00</p>
</blockquote>

<p><strong>day integer column</strong></p>

<blockquote>
  <p>7</p>
</blockquote>
";;1;;2014-09-03T13:53:18.993;9.0;25646200;2017-02-15T10:50:14.973;;;;;4004150.0;;1;27;<python><pandas><timedelta>;Python: Convert timedelta to int in a dataframe;31951.0
17717;17717;25698756.0;3.0;"<p>I have a pandas dataframe with about 20 columns.</p>

<p>It is possible to replace all occurrences of a string (here a newline) by manually writing all column names:</p>

<pre><code>df['columnname1'] = df['columnname1'].str.replace(""\n"",""&lt;br&gt;"")
df['columnname2'] = df['columnname2'].str.replace(""\n"",""&lt;br&gt;"")
df['columnname3'] = df['columnname3'].str.replace(""\n"",""&lt;br&gt;"")
...
df['columnname20'] = df['columnname20'].str.replace(""\n"",""&lt;br&gt;"")
</code></pre>

<p>This unfortunately does not work:</p>

<pre><code>df = df.replace(""\n"",""&lt;br&gt;"")
</code></pre>

<p>Is there any other, more elegant solution?</p>
";;0;;2014-09-06T09:15:43.023;5.0;25698710;2016-09-01T09:48:38.303;;;;;1351281.0;;1;19;<python><replace><pandas><dataframe>;Replace all occurrences of a string in a pandas dataframe (Python);36857.0
17720;17720;25703030.0;4.0;"<p>I have a large dataframe (several million rows).</p>

<p>I want to be able to do a groupby operation on it, but just grouping by arbitrary consecutive (preferably equal-sized) subsets of rows, rather than using any particular property of the individual rows to decide which group they go to.</p>

<p>The use case: I want to apply a function to each row via a parallel map in IPython. It doesn't matter which rows go to which back-end engine, as the function calculates a result based on one row at a time. (Conceptually at least; in reality it's vectorized.)</p>

<p>I've come up with something like this:</p>

<pre><code># Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to
max_idx = dataframe.index.max()
tenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)

# Use this value to perform a groupby, yielding 10 consecutive chunks
groups = [g[1] for g in dataframe.groupby(tenths)]

# Process chunks in parallel
results = dview.map_sync(my_function, groups)
</code></pre>

<p>But this seems very long-winded, and doesn't guarantee equal sized chunks. Especially if the index is sparse or non-integer or whatever.</p>

<p>Any suggestions for a better way?</p>

<p>Thanks!</p>
";;0;;2014-09-06T10:46:50.930;12.0;25699439;2016-11-23T02:45:14.517;2014-09-06T17:22:46.517;;152386.0;;152386.0;;1;15;<python><pandas><parallel-processing><ipython>;How to iterate over consecutive chunks of Pandas dataframe efficiently;10644.0
17799;17799;25748826.0;2.0;"<p>I have the following DataFrame:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})
</code></pre>

<p>I would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.</p>

<p>Going across forums, I thought something like this would work:</p>

<pre><code>df['e'] = df[['a','b','d']].map(sum)
</code></pre>

<p>But no!</p>

<p>I would like to realize the operation having the list of columns <code>['a','b','d']</code> and <code>df</code> as inputs.</p>
";;0;;2014-09-09T15:36:03.770;13.0;25748683;2017-01-02T14:28:05.513;2017-01-02T11:31:54.060;;3923281.0;;3628236.0;;1;35;<python><pandas><dataframe><sum>;Pandas: sum DataFrame rows for given columns;88000.0
17852;17852;25774395.0;2.0;"<p>I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])
&gt;&gt;&gt; df
   col1  col2  col3  col4
0     1     1     1     1
1     2     2     2     2
2     3     3     3     3
</code></pre>

<p>So if we call <code>df.mean(axis=1)</code>, we'll get a mean across the rows:</p>

<pre><code>&gt;&gt;&gt; df.mean(axis=1)
0    1
1    2
2    3
</code></pre>

<p>However, if we call <code>df.drop(name, axis=1)</code>, we actually <strong>drop a column</strong>, not a row:</p>

<pre><code>&gt;&gt;&gt; df.drop(""col4"", axis=1)
   col1  col2  col3
0     1     1     1
1     2     2     2
2     3     3     3
</code></pre>

<p>Can someone help me understand what is meant by an ""axis"" in pandas/numpy/scipy?</p>

<p>A side note, <code>DataFrame.mean</code> just might be defined wrong. It says in the documentation for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html""><code>DataFrame.mean</code></a> that <code>axis=1</code> is supposed to mean a mean over the columns, not the rows...</p>
";;0;;2014-09-10T19:11:14.883;45.0;25773245;2017-04-14T13:56:46.890;2017-01-02T16:06:26.100;;3923281.0;;2014591.0;;1;58;<python><arrays><pandas><numpy><dataframe>;"Ambiguity in Pandas Dataframe / Numpy Array ""axis"" definition";11210.0
17884;17884;38885898.0;4.0;"<p>When saving a Pandas DataFrame to csv, some integers are getting converted in floats.
It happens where a column of floats has missing values (<code>np.nan</code>). </p>

<p>Is there a simple way to avoid it?
(Especially in an automatic way - I often deal with many columns of various data types.)</p>

<p>For example</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame([[1,2],[3,np.nan],[5,6]],
                  columns=[""a"",""b""],
                  index=[""i_1"",""i_2"",""i_3""])
df.to_csv(""file.csv"")
</code></pre>

<p>yields</p>

<pre><code>,a,b
i_1,1,2.0
i_2,3,
i_3,5,6.0
</code></pre>

<p>What I would like to get is</p>

<pre><code>,a,b
i_1,1,2
i_2,3,
i_3,5,6
</code></pre>

<p>EDIT: I am fully aware of <a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"">Support for integer NA - Pandas Caveats and Gotchas</a>. The question is what is a nice workaround (especially in case if there are many other columns of various types and I do not know in advance which ""integer"" columns have missing values).</p>
";;5;;2014-09-11T13:55:50.857;1.0;25789354;2016-08-11T01:25:49.373;2015-06-28T21:59:56.530;;907575.0;;907575.0;;1;13;<csv><pandas><int><nan><missing-data>;Exporting ints with missing values to csv in Pandas;1388.0
17886;17886;25789512.0;1.0;"<p>I want to create a new column in Pandas using a string sliced for another column in the dataframe.</p>

<p>For example.</p>

<pre><code>Sample  Value  New_sample
AAB     23     A
BAB     25     B
</code></pre>

<p>Where <code>New_sample</code> is a new column formed from a simple <code>[:1]</code> slice of <code>Sample</code></p>

<p>I've tried a number of things to no avail - I feel I'm missing something simple.</p>

<p>What's the most efficient way of doing this?</p>
";;0;;2014-09-11T13:59:14.817;1.0;25789445;2014-09-11T14:21:13.137;;;;;1510846.0;;1;12;<python><pandas>;Pandas make new column from string slice of another column;12335.0
17912;17912;25797313.0;1.0;"<p>I have a normal df.index that I would like to add some hours to it.</p>

<pre><code>In [1]: test[1].index
Out[2]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2010-03-11, ..., 2014-08-14]
Length: 52, Freq: None, Timezone: None
</code></pre>

<p>This is how the first element looks like:</p>

<pre><code>In [1]: test[1].index[0]
Out[2]: Timestamp('2010-03-11 00:00:00')
</code></pre>

<p>So I try this to add the hours:</p>

<pre><code>In [1]: test[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')
</code></pre>

<p>However I get this:</p>

<pre><code>Out[2]: Timestamp('2010-03-11 00:00:00.000000016')
</code></pre>

<p>But I would like to get this:</p>

<pre><code>Out[2]: Timestamp('2010-03-11 16:00:00')
</code></pre>

<p>What I am missing?. The enviroment is Anaconda (latest) Python 2.7.7, iPython 2.2</p>

<p>Thanks a lot</p>
";;0;;2014-09-11T21:30:13.723;1.0;25797245;2014-09-11T22:22:24.727;;;;;3396911.0;;1;12;<python><pandas><indexing><time-series><dataframe>;How to properly add hours to a pandas.tseries.index.DatetimeIndex?;5659.0
18138;18138;25916109.0;3.0;"<p>I'm new to Python and Pandas so there might be a simple solution which I don't see. </p>

<p>I have a number of discontinuous datasets which look like this:  </p>

<pre><code>ind A    B  C  
0   0.0  1  3  
1   0.5  4  2  
2   1.0  6  1  
3   3.5  2  0  
4   4.0  4  5  
5   4.5  3  3  
</code></pre>

<p>I now look for a solution to get the following:  </p>

<pre><code>ind A    B  C  
0   0.0  1  3  
1   0.5  4  2  
2   1.0  6  1  
3   1.5  NAN NAN  
4   2.0  NAN NAN  
5   2.5  NAN NAN  
6   3.0  NAN NAN  
7   3.5  2  0  
8   4.0  4  5  
9   4.5  3  3  
</code></pre>

<p>The problem is,that the gap in A varies from dataset to dataset in position and length...</p>
";;1;;2014-09-18T10:17:23.810;3.0;25909984;2017-03-24T14:07:05.650;2014-09-18T20:52:37.273;;4053508.0;;4053508.0;;1;11;<python><numpy><pandas>;Missing data, insert rows in Pandas and fill with NAN;5165.0
18191;18191;25935024.0;2.0;"<p>I have a data frame df which looks like this. Date and Time are 2 multilevel index</p>

<pre><code>                           observation1   observation2
date          Time                             
2012-11-02    9:15:00      79.373668      224
              9:16:00      130.841316     477
2012-11-03    9:15:00      45.312814      835
              9:16:00      123.776946     623
              9:17:00      153.76646      624
              9:18:00      463.276946     626
              9:19:00      663.176934     622
              9:20:00      763.77333      621
2012-11-04    9:15:00      115.449437     122
              9:16:00      123.776946     555
              9:17:00      153.76646      344
              9:18:00      463.276946     212
</code></pre>

<p>I want to have do some complex process over daily data block.</p>

<p>Psuedo code would look like</p>

<pre><code> for count in df(level 0 index) :
     new_df = get only chunk for count
     complex_process(new_df)
</code></pre>

<p>So, first of all, I could not find a way to access only blocks for a date</p>

<pre><code>2012-11-03    9:15:00      45.312814      835
              9:16:00      123.776946     623
              9:17:00      153.76646      624
              9:18:00      463.276946     626
              9:19:00      663.176934     622
              9:20:00      763.77333      621
</code></pre>

<p>and then send it for processing. I am doing this in for loop as I am not sure if there is any way to do it without mentioning exact value of level 0 column. I did some basic search and able to get df.index.get_level_values(0), but it returns me all the values and that causes loop to run multiple times for a day. I want to create a dataframe per day and send it for processing.</p>
";;0;;2014-09-19T08:15:06.447;6.0;25929319;2015-04-26T03:11:15.987;;;;;3907612.0;;1;17;<python><pandas>;How to iterate over pandas multiindex dataframe using index;10633.0
18256;18256;;5.0;"<p>I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:</p>

<pre><code>MemoryError                               Traceback (most recent call last)
&lt;ipython-input-58-67a72687871b&gt; in &lt;module&gt;()
----&gt; 1 data=pd.read_csv('aphro.csv',sep=';')

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)
    450                     infer_datetime_format=infer_datetime_format)
    451 
--&gt; 452         return _read(filepath_or_buffer, kwds)
    453 
    454     parser_f.__name__ = name

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in _read(filepath_or_buffer, kwds)
    242         return parser
    243 
--&gt; 244     return parser.read()
    245 
    246 _parser_defaults = {

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
    693                 raise ValueError('skip_footer not supported for iteration')
    694 
--&gt; 695         ret = self._engine.read(nrows)
    696 
    697         if self.options.get('as_recarray'):

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
   1137 
   1138         try:
-&gt; 1139             data = self._reader.read(nrows)
   1140         except StopIteration:
   1141             if nrows is None:

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader.read (pandas\parser.c:7145)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\parser.c:7369)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._read_rows (pandas\parser.c:8194)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\parser.c:9402)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\parser.c:10057)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:10361)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser._try_int64 (pandas\parser.c:17806)()

MemoryError: 
</code></pre>

<p>Any help on this?? </p>
";;1;;2014-09-21T17:46:43.317;20.0;25962114;2017-08-03T18:56:53.103;2014-09-21T17:48:49.047;;1258041.0;;4064040.0;;1;37;<python><memory><numpy><pandas>;How to read a 6 GB csv file with pandas;40201.0
18398;18398;26240208.0;2.0;"<p>Why does pandas make a distinction between a <code>Series</code> and a single-column <code>DataFrame</code>?<br>
In other words: what is the reason of existence of the <code>Series</code> class? </p>

<p>I'm mainly using time series with datetime index, maybe that helps to set the context. </p>
";;4;;2014-09-25T20:09:21.210;15.0;26047209;2016-08-19T08:22:00.327;2014-09-25T20:28:29.710;;566942.0;;566942.0;;1;43;<python><pandas>;What is the difference between a pandas Series and a single-column DataFrame?;19282.0
18418;18418;26064898.0;1.0;"<p>I have a csv file from this <a href=""http://www.exoplanet.eu/catalog/?f=%22radial%22+IN+detection+OR+%22astrometry%22+IN+detection+OR+%22transit%22+IN+detection"">webpage</a>.
I want to read some of the columns in the downloaded file (the csv version can be downloaded in the upper right corner).</p>

<p>Let's say I want 2 columns:</p>

<ul>
<li>59 which in the header is <code>star_name</code></li>
<li>60 which in the header is <code>ra</code>.</li>
</ul>

<p>However, for some reason the authors of the webpage sometimes decide to move the columns around.</p>

<p>In the end I want something like this, keeping in mind that values can be missing.</p>

<pre><code>data = #read data in a clever way
names = data['star_name']
ras = data['ra']
</code></pre>

<p>This will prevent my program to malfunction when the columns are changed again in the future, if they keep the name correct.</p>

<p>Until now I have tried various ways using the <code>csv</code> module and resently the <code>pandas</code> module. Both without any luck.</p>

<p>EDIT (added two lines + the header of my datafile. Sorry, but it's extremely long.)</p>

<pre><code># name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field
11 Com b,19.4,1.5,1.5,,,,326.03,0.32,0.32,1.29,0.05,0.05,0.231,0.005,0.005,0.011664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2008,2011-12-23,94.8,1.5,1.5,2452899.6,1.6,1.6,Radial Velocity,,,,,11 Com,185.1791667,17.7927778,4.74,,,,,110.6,-0.35,2.7,19.0,G8 III,,4742.0,,
11 UMi b,10.5,2.47,2.47,,,,516.22,3.25,3.25,1.54,0.07,0.07,0.08,0.03,0.03,0.012887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2009,2009-08-13,117.63,21.06,21.06,2452861.05,2.06,2.06,Radial Velocity,,,,,11 UMi,229.275,71.8238889,5.02,,,,,119.5,0.04,1.8,24.08,K4III,1.56,4340.0,,
</code></pre>
";;6;;2014-09-26T15:35:48.373;1.0;26063231;2014-09-26T17:14:44.877;2014-09-26T17:03:18.133;;1340702.0;;1340702.0;;1;17;<python><csv><pandas>;Read specific columns with pandas or other python module;25214.0
18427;18427;29321298.0;3.0;"<p>When importing pandas I would get the following error: </p>

<p><code>Numpy.dtype has the wrong size, try recompiling</code></p>

<p>I am running Python 2.7.5, with Pandas 0.14.1, and Numpy 1.9.0. I have tried installing older versions of both using pip, with major errors every time. I am a beginner when it comes to Python so any help here would be much appreciated. :)</p>

<p>EDIT: running OS X 10.9.4</p>

<p>EDIT 2: here is a link to a video of me uninstalling and reinstalling Numpy + Pandas, and then running a .py file: <a href=""https://www.dropbox.com/s/sx9l288jijokrar/numpy%20issue.mov?dl=0"">https://www.dropbox.com/s/sx9l288jijokrar/numpy%20issue.mov?dl=0</a></p>
";;12;;2014-09-26T20:18:10.457;4.0;26067692;2016-08-21T09:44:14.360;2014-09-26T23:06:56.970;;1776039.0;;1776039.0;;1;11;<python><numpy><pandas>;Numpy.dtype has the wrong size, try recompiling;14293.0
18475;18475;26098292.0;2.0;"<p>I have a Pandas series sf:</p>

<pre><code>email
email1@email.com    [1.0, 0.0, 0.0]
email2@email.com    [2.0, 0.0, 0.0]
email3@email.com    [1.0, 0.0, 0.0]
email4@email.com    [4.0, 0.0, 0.0]
email5@email.com    [1.0, 0.0, 3.0]
email6@email.com    [1.0, 5.0, 0.0]
</code></pre>

<p>And I would like to transform it to the following DataFrame:</p>

<pre><code>index | email             | list
_____________________________________________
0     | email1@email.com  | [1.0, 0.0, 0.0]
1     | email2@email.com  | [2.0, 0.0, 0.0]
2     | email3@email.com  | [1.0, 0.0, 0.0]
3     | email4@email.com  | [4.0, 0.0, 0.0]
4     | email5@email.com  | [1.0, 0.0, 3.0]
5     | email6@email.com  | [1.0, 5.0, 0.0]
</code></pre>

<p>I found a way to do it, but I doubt it's the more efficient one:</p>

<pre><code>df1 = pd.DataFrame(data=sf.index, columns=['email'])
df2 = pd.DataFrame(data=sf.values, columns=['list'])
df = pd.merge(df1, df2, left_index=True, right_index=True)
</code></pre>
";;3;;2014-09-29T10:38:16.747;5.0;26097916;2017-01-05T21:32:43.490;;;;;1754181.0;;1;15;<python><pandas><dataframe><series>;python, best way to convert a pandas series into a pandas dataframe;19075.0
18534;18534;26121238.0;1.0;"<p>I am following the <a href=""http://nbviewer.ipython.org/urls/bitbucket.org/hrojas/learn-pandas/raw/master/lessons/01%20-%20Lesson.ipynb"">Pandas tutorials</a></p>

<p>The tutorials are written using python 2.7 and I am doing them in python 3.4</p>

<p>Here is my version details.</p>

<pre><code>In [11]: print('Python version ' + sys.version)
Python version 3.4.1 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 17:27:11)
[MSC v.1600 64 bit (AMD64)]

In [12]: print('Pandas version ' + pd.__version__)
Pandas version 0.14.1
</code></pre>

<p>I create the zip as per  the tutorial</p>

<pre><code>In [13]: names = ['Bob','Jessica','Mary','John','Mel']

In [14]: births = [968, 155, 77, 578, 973]

In [15]: zip?
Type:            type
String form:     &lt;class 'zip'&gt;
Namespace:       Python builtin
Init definition: zip(self, *args, **kwargs)
Docstring:
zip(iter1 [,iter2 [...]]) --&gt; zip object

Return a zip object whose .__next__() method returns a tuple where
the i-th element comes from the i-th iterable argument.  The .__next__()
method continues until the shortest iterable in the argument sequence
is exhausted and then it raises StopIteration.

In [16]: BabyDataSet = zip(names,births)
</code></pre>

<p>But after creation the first error shows that I cannot see the contents of the zip.</p>

<pre><code>In [17]: BabyDataSet
Out[17]: &lt;zip at 0x4f28848&gt;

In [18]: print(BabyDataSet)
&lt;zip object at 0x0000000004F28848&gt;
</code></pre>

<p>Then when I go to create the dataframe I get this iterator error.</p>

<pre><code>In [21]: df = pd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-21-636a49c94b6e&gt; in &lt;module&gt;()
----&gt; 1 df = pd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])

c:\Users\Sayth\Anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self
, data, index, columns, dtype, copy)
    255                                          copy=copy)
    256         elif isinstance(data, collections.Iterator):
--&gt; 257             raise TypeError(""data argument can't be an iterator"")
    258         else:
    259             try:

TypeError: data argument can't be an iterator

In [22]:
</code></pre>

<p>Is  this a python 3 gotcha where I need to do it differently? Or other?</p>
";;2;;2014-09-30T12:34:15.000;3.0;26121009;2014-09-30T12:44:58.770;;;;;461887.0;;1;14;<python><python-3.x><pandas>;Python 3 - Zip is an iterator in a pandas dataframe;12260.0
18574;18574;26133621.0;2.0;"<p>Is there a way to round a single column in pandas without affecting the rest of the dataframe?</p>

<pre><code> df:
      item  value1  value2
    0    a    1.12     1.3
    1    a    1.50     2.5
    2    a    0.10     0.0
    3    b    3.30    -1.0
    4    b    4.80    -1.0
</code></pre>

<p>df.value1.apply(np.round)
gives</p>

<pre><code>0    1
1    2
2    0
3    3
4    5
5    5
</code></pre>

<p>What is the correct way to make data look like this:</p>

<pre><code>  item  value1  value2
0    a       1     1.3
1    a       2     2.5
2    a       0     0.0
3    b       3    -1.0
4    b       5    -1.0
5    c       5     5.0
</code></pre>
";;0;;2014-10-01T03:11:41.610;3.0;26133538;2017-05-10T16:15:31.487;;;;;1653558.0;;1;11;<python><pandas>;round a single column in pandas;9137.0
18579;18579;26139658.0;4.0;"<p>I have this data frame <code>diamonds</code> which is composed of variables like <code>(carat, price, color)</code>, and I want to draw a scatter plot of <code>price</code> to <code>carat</code> for each <code>color</code>, which means different <code>color</code> has different color in the plot.</p>

<p>This is easy in <code>R</code> with <code>ggplot</code>:</p>

<pre><code>ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors
       data=diamonds) + geom_point(stat='summary', fun.y=median)
</code></pre>

<p><img src=""https://i.stack.imgur.com/HW43K.png"" alt=""enter image description here""></p>

<p>I wonder how could this be done in Python using <code>matplotlib</code> ?</p>

<p>PS:</p>

<p>I know about auxiliary plotting packages, such as <code>seaborn</code> and <code>ggplot for python</code>, and I donot prefer them, just want to find out if it is possible to do the job using <code>matplotlib</code> alone, ;P</p>
";;0;;2014-10-01T10:37:29.157;11.0;26139423;2017-07-06T04:56:03.930;2014-10-01T10:57:29.663;;2235936.0;;2235936.0;;1;35;<matplotlib><pandas><visualization>;plot different color for different categorical levels using matplotlib;28342.0
18595;18595;26147330.0;2.0;"<p>The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?</p>

<p>I want to do something like:</p>

<pre><code>header = df[df['old_header_name1'] == 'new_header_name1']

df.columns = header
</code></pre>
";;0;;2014-10-01T17:33:44.280;8.0;26147180;2017-03-15T22:56:58.257;2014-10-01T18:16:16.083;;1763356.0;;3128336.0;;1;15;<python><pandas><rename><dataframe>;Convert row to column header for Pandas DataFrame,;24734.0
18673;18673;27027632.0;3.0;"<p>I have used rosetta.parallel.pandas_easy to parallelize apply after group by, for example:</p>

<pre><code>from rosetta.parallel.pandas_easy import groupby_to_series_to_frame
df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])
groupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)
</code></pre>

<p>However, has anyone figured out how to parallelize a function that returns a dataframe? This code fails for rosetta, as expected.</p>

<pre><code>def tmpFunc(df):
    df['c'] = df.a + df.b
    return df

df.groupby(df.index).apply(tmpFunc)
groupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)
</code></pre>
";;0;;2014-10-03T22:43:28.453;28.0;26187759;2015-11-12T08:13:44.327;;;;;4043526.0;;1;22;<python><pandas><parallel-processing><rosetta>;Parallelize apply after pandas groupby;11709.0
18701;18701;26206622.0;4.0;"<p>I have the following table.  I want to calculate a weighted average grouped by each date based on the formula below.  I can do this using some standard conventional code, but assuming that this data is in a pandas dataframe, is there any easier way to achieve this rather than through iteration?</p>

<pre><code>Date        ID      wt      value   w_avg
01/01/2012  100     0.50    60      0.791666667
01/01/2012  101     0.75    80
01/01/2012  102     1.00    100
01/02/2012  201     0.50    100     0.722222222
01/02/2012  202     1.00    80
</code></pre>

<p>01/01/2012 w_avg = 0.5 * ( 60/ sum(60,80,100)) + .75 * (80/ sum(60,80,100)) + 1.0 * (100/sum(60,80,100))
01/02/2012 w_avg = 0.5 * ( 100/ sum(100,80)) + 1.0 * ( 80/ sum(100,80))</p>
";;1;;2014-10-05T18:36:05.140;5.0;26205922;2016-03-05T06:58:02.577;;;;;480118.0;;1;15;<python><numpy><pandas>;Calculate weighted average using a pandas/dataframe;16744.0
18737;18737;;4.0;"<p>I am quite new to pandas, I am attempting to concatenate a set of dataframes and I am getting this error:</p>

<pre><code>ValueError: Plan shapes are not aligned
</code></pre>

<p>My understanding of concat is that it will join where columns are the same, but for those that it can't find it will fill with NA. This doesn't seem to be the case here.</p>

<p>Heres the concat statement</p>

<pre><code>dfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]
alpha = pd.concat(dfs)
</code></pre>
";;4;;2014-10-06T23:21:31.263;2.0;26226343;2017-08-21T13:15:36.933;2017-08-14T14:01:20.623;;4981721.0;;585492.0;;1;17;<python><pandas><concat>;Pandas concat gives error ValueError: Plan shapes are not aligned;7420.0
18759;18759;26244925.0;3.0;"<p>I'd like to be able to compute descriptive statistics on data in a Pandas DataFrame, but I only care about duplicated entries. For example, let's say I have the DataFrame created by:</p>

<pre><code>import pandas as pd
data={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}
frame=pd.DataFrame(data,columns=['key1','key2','data'])
print frame


     key1  key2  data
0     1     2     5
1     2     2     6
2     3     1     2
3     1     2     6
4     2     2     1
5     3     4     6
6     2     2     2
7     2     2     8
</code></pre>

<p>As you can see, rows 0,1,3,4,6, and 7 are all duplicates (using 'key1' and 'key2'. However, if I index this DataFrame like so:</p>

<pre><code>frame[frame.duplicated(['key1','key2'])]
</code></pre>

<p>I get </p>

<pre><code>   key1  key2  data
3     1     2     6
4     2     2     1
6     2     2     2
7     2     2     8
</code></pre>

<p>(i.e., the 1st and 2nd rows do not show up because they are not indexed to True by the duplicated method). </p>

<p>That is my first problem. My second problems deals with how to extract the descriptive statistics from this information. Forgetting the missing duplicate for the moment, let's say I want to compute the .min() and .max() for the duplicate entries (so that I can get a range). I can use groupby and these methods on the groupby object like so:</p>

<pre><code>a.groupby(['key1','key2']).min()
</code></pre>

<p>which gives </p>

<pre><code>           key1  key2  data
key1 key2                  
1    2        1     2     6
2    2        2     2     1
</code></pre>

<p>The data I want is obviously here, but what's the best way for me to extract it?  How do I index the resulting object to get what I want (which is the key1,key2,data info)?</p>
";;1;;2014-10-07T20:04:08.113;9.0;26244309;2016-02-25T11:09:37.227;2015-09-21T16:56:49.963;;4168397.0;;1961582.0;;1;18;<python><pandas><dataframe>;How to analyze all duplicate entries in this Pandas DataFrame?;22157.0
18801;18801;26266031.0;3.0;"<p>Best I can come up with is</p>

<pre><code>df = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below
s = pd.Series({'s1':5, 's2':6})

for name in s.index:
    df[name] = s[name]

   a  b  s1  s2
0  1  3   5   6
1  2  4   5   6
</code></pre>

<p>Can anybody suggest better syntax / faster method? </p>

<p>My attempts:</p>

<pre><code>df.merge(s)
AttributeError: 'Series' object has no attribute 'columns'
</code></pre>

<p>and</p>

<pre><code>df.join(s)
ValueError: Other Series must have a name
</code></pre>

<p><em>EDIT</em> The first two answers posted highlighted a problem with my question, so please use the following to construct <code>df</code>:</p>

<pre><code>df = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])
</code></pre>

<p>with the final result</p>

<pre><code>    a  b  s1  s2
3 NaN  4   5   6
5   2  5   5   6
6   3  6   5   6
</code></pre>
";;0;;2014-10-08T20:27:27.287;;26265819;2016-11-23T11:03:11.060;2014-10-10T22:38:05.503;;3417592.0;;3417592.0;;1;16;<python><pandas>;How to merge Series to DataFrame as columns, broadcasting;20918.0
18804;18804;;9.0;"<p>I have data, in which I want to find number of NaN, so that if it is less than some threshold, I will drop this columns. I looked, but didn't able to find any function for this. there is count_values(), but it would be slow for me, because most of values are distinct and I want count of NaN only.</p>
";;0;;2014-10-08T21:00:19.823;32.0;26266362;2017-06-02T06:58:22.087;;;;;3799307.0;;1;110;<python><pandas>;How to count the Nan values in the column in Panda Data frame;83763.0
18827;18827;26301947.0;2.0;"<p>I have a Python Pandas <code>DataFrame</code> object containing textual data. My problem is, that when I use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer""><code>to_html()</code></a> function, it truncates the strings in the output.</p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas
df = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})
print (df.to_html())
</code></pre>

<p>The output is truncated at <code>adapis...</code></p>

<pre><code>&lt;table border=""1"" class=""dataframe""&gt;
  &lt;thead&gt;
    &lt;tr style=""text-align: right;""&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt; Lorem ipsum dolor sit amet, consectetur adipis...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>

<p>There is a related question on SO, but it uses placeholders and search/replace functionality to postprocess the HTML, which I would like to avoid:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/25070758/writing-full-contents-of-pandas-dataframe-to-html-table"">Writing full contents of Pandas dataframe to HTML table</a></li>
</ul>

<p>Is there a simpler solution to this problem? I could not find anything related from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
";;5;;2014-10-09T11:52:12.383;4.0;26277757;2016-09-29T06:24:04.177;2017-05-23T10:31:29.493;;-1.0;;621917.0;;1;32;<python><html><pandas>;Pandas to_html() truncates string contents;7560.0
18887;18887;;5.0;"<p>So I have initialized an empty pandas DataFrame and I would like to iteratively append lists (or Series) as rows in this DataFrame. What is the best way of doing this?</p>
";;1;;2014-10-11T00:53:18.913;4.0;26309962;2016-09-03T02:51:57.063;;;;;1628702.0;;1;21;<python><pandas><append><dataframe>;Appending a list or series to a pandas DataFrame as a row?;40866.0
18967;18967;26347456.0;1.0;"<p>I am trying to drop multiple columns (column 2 and 70 in my data set, indexed as 1 and 69 respectively) by index number in a pandas data frame with the following code:</p>

<pre><code>df.drop([df.columns[[1, 69]]], axis=1, inplace=True)
</code></pre>

<p>I get the following error: </p>

<pre><code>TypeError: unhashable type: 'Index'
</code></pre>

<p>And in my code the [1, 69] is highlighted and says:</p>

<pre><code>Expected type 'Integral', got 'list[int]' instead
</code></pre>

<p>The following code does what I want it to do successfully, but on two lines of repetitive code (first dropping col index 69, then 1, and order does matter because dropping earlier columns changes the index of later columns). I thought I could specify more than one column index simply as a list, but perhaps I have something wrong above?</p>

<pre><code>df.drop([df.columns[69]], axis=1, inplace=True)
df.drop([df.columns[1]], axis=1, inplace=True)
</code></pre>

<p>Is there a way that I can do this on one line similar to the first code snippet above?</p>
";;0;;2014-10-13T19:24:44.390;7.0;26347412;2017-07-23T10:06:15.620;2017-07-23T10:06:15.620;;1033581.0;;4080773.0;;1;35;<python><pandas>;Drop multiple columns in pandas;66476.0
19093;19093;26415620.0;6.0;"<p>I have a data frame in pandas in which each column has different value range. For example:</p>

<p>df:</p>

<pre><code>A     B   C
1000  10  0.5
765   5   0.35
800   7   0.09
</code></pre>

<p>Any idea how I can normalize the columns of this data frame where each value is between 0 and 1?</p>

<p>My desired output is:</p>

<pre><code>A     B    C
1     1    1
0.765 0.5  0.7
0.8   0.7  0.18(which is 0.09/0.5)
</code></pre>
";;1;;2014-10-16T22:24:42.070;8.0;26414913;2017-04-21T15:06:22.387;2016-10-05T15:34:00.757;;1007939.0;;2827771.0;;1;17;<python><pandas><normalize>;Normalize columns of pandas data frame;28751.0
19147;19147;26465555.0;3.0;"<p>I've frequented used pandas' <code>agg()</code> function to run summary statistics on every column of a data.frame.  For example, here's how you would produce the mean and standard deviation:</p>

<pre><code>df = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],
                   'B': [10, 12, 10, 25, 10, 12],
                   'C': [100, 102, 100, 250, 100, 102]})

&gt;&gt;&gt; df
[output]
        A   B    C
0  group1  10  100
1  group1  12  102
2  group2  10  100
3  group2  25  250
4  group3  10  100
5  group3  12  102
</code></pre>

<p>In both of those cases, the order that individual rows are sent to the agg function does not matter.  But consider the following example, which:</p>

<pre><code>df.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])

[output]

        mean  &lt;lambda&gt;  mean  &lt;lambda&gt;
A                                     
group1  11.0        12   101       102
group2  17.5        25   175       250
group3  11.0        12   101       102
</code></pre>

<p>In this case the lambda functions as intended, outputting the second row in each group.  However, I have not been able to find anything in the pandas documentation that implies that this is guaranteed to be true in all cases.  I want use <code>agg()</code> along with a weighted average function, so I want to be sure that the rows that come into the function will be in the same order as they appear in the original data frame.</p>

<p>Does anyone know, ideally via somewhere in the docs or pandas source code, if this is guaranteed to be the case?</p>
";;2;;2014-10-19T22:31:41.470;2.0;26456125;2016-12-03T17:22:15.477;2014-10-19T23:36:57.733;;3124423.0;;3124423.0;;1;15;<python><pandas><aggregate>;Python Pandas: Is Order Preserved When Using groupby() and agg()?;3852.0
19150;19150;26457238.0;2.0;"<p>A pandas DataFrame column <code>duration</code> contains <code>timedelta64[ns]</code> as shown. How can you convert them to seconds?</p>

<pre><code>0   00:20:32
1   00:23:10
2   00:24:55
3   00:13:17
4   00:18:52
Name: duration, dtype: timedelta64[ns]
</code></pre>

<p>I tried the following</p>

<pre><code>print df[:5]['duration'] / np.timedelta64(1, 's')
</code></pre>

<p>but got the error</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 16, in &lt;module&gt;
    print df[0:5]['duration'] / np.timedelta64(1, 's')
  File ""C:\Python27\lib\site-packages\pandas\core\series.py"", line 130, in wrapper
    ""addition and subtraction, but the operator [%s] was passed"" % name)
TypeError: can only operate on a timedeltas for addition and subtraction, but the operator [__div__] was passed
</code></pre>

<p>Also tried</p>

<pre><code>print df[:5]['duration'].astype('timedelta64[s]')
</code></pre>

<p>but received the error</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 17, in &lt;module&gt;
    print df[:5]['duration'].astype('timedelta64[s]')
  File ""C:\Python27\lib\site-packages\pandas\core\series.py"", line 934, in astype
    values = com._astype_nansafe(self.values, dtype)
  File ""C:\Python27\lib\site-packages\pandas\core\common.py"", line 1653, in _astype_nansafe
    raise TypeError(""cannot astype a timedelta from [%s] to [%s]"" % (arr.dtype,dtype))
TypeError: cannot astype a timedelta from [timedelta64[ns]] to [timedelta64[s]]
</code></pre>
";;3;;2014-10-20T00:09:44.143;8.0;26456825;2017-07-19T11:22:57.317;;;;;741099.0;;1;12;<python><python-2.7><numpy><pandas>;Convert timedelta64[ns] column to seconds in Python Pandas DataFrame;9843.0
19181;19181;26474062.0;14.0;"<p>I have a freshly installed Ubuntu on a freshly built computer. I just installed python-pip using apt-get. Now when I try to pip install Numpy and Pandas, it gives the following error.</p>

<p>I've seen this error mentioned in quite a few places on SO and Google, but I haven't been able to find a solution. Some people mention it's a bug, some threads are just dead... What's going on?</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/bin/pip"", line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.4', 'console_scripts', 'pip')()
  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main
    return command.main(cmd_args)
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main
    text = '\n'.join(complete_log)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 72: ordinal not in range(128)
</code></pre>
";;4;;2014-10-20T19:46:18.363;9.0;26473681;2017-08-22T20:22:50.583;;;;;3884713.0;;1;59;<python><numpy><pandas><pip>;"PIP Install Numpy throws an error ""ascii codec can't decode byte 0xe2""";46327.0
19208;19208;26510251.0;1.0;"<p>I have a list 'abc' and a dataframe 'df':</p>

<pre><code>abc = ['foo', 'bar']
df =
    A  B
0  12  NaN
1  23  NaN
</code></pre>

<p>I want to insert the list into cell 1B, so I want this result:</p>

<pre><code>    A  B
0  12  NaN
1  23  ['foo', 'bar']
</code></pre>

<p>Ho can I do that?</p>

<p>1) If I use this:</p>

<pre><code>df.ix[1,'B'] = abc
</code></pre>

<p>I get the following error message:</p>

<pre><code>ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>

<p>because it tries to insert the list (that has two elements) into a row / column but not into a cell.</p>

<p>2) If I use this:</p>

<pre><code>df.ix[1,'B'] = [abc]
</code></pre>

<p>then it inserts a list that has only one element that is the 'abc' list ( <code>[['foo', 'bar']]</code> ).</p>

<p>3) If I use this:</p>

<pre><code>df.ix[1,'B'] = ', '.join(abc)
</code></pre>

<p>then it inserts a string: ( <code>foo, bar</code> ) but not a list.</p>

<p>4) If I use this:</p>

<pre><code>df.ix[1,'B'] = [', '.join(abc)]
</code></pre>

<p>then it inserts a list but it has only one element ( <code>['foo, bar']</code> ) but not two as I want ( <code>['foo', 'bar']</code> ).</p>

<p>Thanks for help!</p>

<hr>

<h2>EDIT</h2>

<p>My new dataframe and the old list:</p>

<pre><code>abc = ['foo', 'bar']
df2 =
    A    B         C
0  12  NaN      'bla'
1  23  NaN  'bla bla'
</code></pre>

<p>Another dataframe:</p>

<pre><code>df3 =
    A    B         C                    D
0  12  NaN      'bla'  ['item1', 'item2']
1  23  NaN  'bla bla'        [11, 12, 13]
</code></pre>

<p>I want insert the 'abc' list into <code>df2.loc[1,'B']</code> and/or <code>df3.loc[1,'B']</code>.</p>

<p>If the dataframe has columns only with integer values and/or NaN values and/or list values then inserting a list into a cell works perfectly. If the dataframe has columns only with string values and/or NaN values and/or list values then inserting a list into a cell works perfectly. But if the dataframe has columns with integer and string values and other columns then the error message appears if I use this: <code>df2.loc[1,'B'] = abc</code> or <code>df3.loc[1,'B'] = abc</code>.</p>

<p>Another dataframe:</p>

<pre><code>df4 =
          A     B
0      'bla'  NaN
1  'bla bla'  NaN
</code></pre>

<p>These inserts work perfectly: <code>df.loc[1,'B'] = abc</code> or <code>df4.loc[1,'B'] = abc</code>.</p>
";;3;;2014-10-21T09:26:25.280;2.0;26483254;2016-06-30T22:22:14.440;2016-06-30T22:22:14.440;;1534017.0;;2740380.0;;1;22;<python><list><pandas><insert><dataframe>;Python pandas insert list into a cell;17970.0
19214;19214;33782239.0;4.0;"<p>The quantile functions gives us the quantile of a given pandas series <strong>s</strong>,</p>

<p>E.g.</p>

<blockquote>
  <p>s.quantile(0.9) is 4.2</p>
</blockquote>

<p>Is there the inverse function (i.e. cumulative distribution) which finds the value x such that </p>

<blockquote>
  <p>s.quantile(x)=4</p>
</blockquote>

<p>Thanks</p>
";;1;;2014-10-21T14:25:14.773;3.0;26489134;2016-11-07T18:35:03.353;2014-10-22T08:17:03.973;;1232506.0;;1232506.0;;1;11;<python><pandas><quantile>;what's the inverse of the quantile function on a pandas Series?;3802.0
19233;19233;26495839.0;2.0;"<p>What's the difference between:</p>

<p><code>pandas.DataFrame.from_csv</code>, doc link: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.from_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.from_csv.html</a></p>

<p>and</p>

<p><code>pandas.read_csv</code>, doc link: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>
";;2;;2014-10-21T20:10:07.877;3.0;26495408;2017-04-07T05:13:06.953;;;;;3659451.0;;1;15;<python><csv><pandas>;Pandas - pandas.DataFrame.from_csv vs pandas.read_csv;5198.0
19303;19303;26521726.0;3.0;"<p>I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs in that large file. One of the tabs has a ton of data and the other is just a few square cells.</p>

<p>When I use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html"">pd.read_excel()</a> on <em>any</em> worksheet, it looks to me like the whole file is loaded (not just the worksheet I'm interested in). So when I use the method twice (once for each sheet), I effectively have to suffer the whole workbook being read in twice (even though we're only using the specified sheet).</p>

<p>Am I using it wrong or is it just limited in this way?</p>

<p>Thank you!</p>
";;0;;2014-10-23T04:21:45.470;4.0;26521266;2017-02-11T19:37:17.173;;;;;3127764.0;;1;18;<python><excel><pandas><dataframe>;Using Pandas to pd.read_excel() for multiple worksheets of the same workbook;22157.0
19317;19317;26535881.0;2.0;"<p>Say I have a dataframe <code>df</code> with a column <code>value</code> holding some float values and some <code>NaN</code>. How can I get the part of the dataframe where we have <code>NaN</code> <strong>using the query syntax</strong>?</p>

<p>The following, for example, does not work:</p>

<pre><code>df.query( '(value &lt; 10) or (value == NaN)' )
</code></pre>

<p>I get <code>name NaN is not defined</code> (same for <code>df.query('value ==NaN')</code>)</p>

<p>Generally speaking, is there any way to use numpy names in query, such as <code>inf</code>, <code>nan</code>, <code>pi</code>, <code>e</code>, etc.?</p>
";;0;;2014-10-23T19:09:28.330;5.0;26535563;2014-10-23T19:28:19.147;;;;;283296.0;;1;12;<python><pandas>;Querying for NaN and other names in Pandas;5041.0
19328;19328;26538379.0;4.0;"<p>I have read a csv file and pivoted it to get to following structure.</p>

<pre><code>pivoted = df.pivot('user_id', 'group', 'value')
lookup = df.drop_duplicates('user_id')[['user_id', 'group']]
lookup.set_index(['user_id'], inplace=True)
result = pivoted.join(lookup)
result = result.fillna(0) 
</code></pre>

<p>Section of the result:</p>

<pre><code>             0     1     2    3     4    5   6  7    8   9  10  11  12  13  group
user_id                                                                      
2        33653  2325   916  720   867  187  31  0    6   3  42  56  92  15    l-1
4        18895   414  1116  570  1190   55  92  0  122  23  78   6   4   2    l-2 
16        1383    70    27   17    17    1   0  0    0   0   1   0   0   0    l-2
50         396    72    34    5    18    0   0  0    0   0   0   0   0   0    l-3
51        3915  1170   402  832  2791  316  12  5  118  51  32   9  62  27    l-4
</code></pre>

<p>I want to sum across column 0 to column 13 by each row and divide each cell by the sum of that row. I am still getting used to pandas, If I understand correctly we should try to avoid for loops when doing things like this? So How can I do this pandas way? </p>
";;1;;2014-10-23T21:35:20.467;3.0;26537878;2016-12-08T12:54:49.237;2014-10-23T21:46:11.620;;200317.0;;200317.0;;1;12;<python><pandas><dataframe>;Pandas sum across columns and divide each cell from that value;19153.0
19389;19389;26577689.0;2.0;"<p>Is there any function that would be the equivalent of a combination of <code>df.isin()</code> and <code>df[col].str.contains()</code>? </p>

<p>For example, say I have the series
<code>s = pd.Series(['cat','hat','dog','fog','pet'])</code>, and I want to find all places where <code>s</code> contains any of <code>['og', 'at']</code>, I would want to get everything but pet.</p>

<p>I have a solution, but it's rather inelegant:</p>

<pre><code>searchfor = ['og', 'at']
found = [s.str.contains(x) for x in searchfor]
result = pd.DataFrame[found]
result.any()
</code></pre>

<p>Is there a better way to do this?</p>
";;0;;2014-10-26T20:23:37.160;5.0;26577516;2017-02-28T20:44:28.480;2017-02-28T20:44:28.480;;3923281.0;;1628722.0;;1;15;<python><string><pandas><dataframe><match>;pandas: test if string contains one of the substrings in a list;8720.0
19516;19516;26640189.0;4.0;"<p>So assume I have a dataframe with rownames that aren't a column of their own per se such as the following:</p>

<pre><code>        X  Y
 Row 1  0  5
 Row 2  8  1
 Row 3  3  0
</code></pre>

<p>How would I extract these row names as a list, if I have their index?
For example, it would look something like: </p>

<p>function_name(dataframe[indices])</p>

<blockquote>
  <p>['Row 1', 'Row 2']</p>
</blockquote>

<p>Thanks for your help!</p>
";;0;;2014-10-29T20:34:16.333;3.0;26640145;2017-05-05T16:40:42.073;2016-12-22T15:00:26.130;;4195814.0;;4195814.0;;1;24;<python><pandas><dataframe>;Python Pandas: How to get the row names from index of a dataframe?;41938.0
19531;19531;26654201.0;3.0;"<p>I have following 2 data frames:</p>

<pre><code>df_a =

     mukey  DI  PI
0   100000  35  14
1  1000005  44  14
2  1000006  44  14
3  1000007  43  13
4  1000008  43  13

df_b = 
    mukey  niccdcd
0  190236        4
1  190237        6
2  190238        7
3  190239        4
4  190240        7
</code></pre>

<p>When I try to join these 2 dataframes:</p>

<pre><code>join_df = df_a.join(df_b,on='mukey',how='left')
</code></pre>

<p>I get the error:</p>

<pre><code>*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')
</code></pre>

<p>Why is this so? The dataframes do have common 'mukey' values.</p>
";;5;;2014-10-30T05:09:05.673;5.0;26645515;2017-07-24T23:51:52.213;2017-07-24T23:51:52.213;;3388962.0;;308827.0;;1;36;<python><join><pandas>;Pandas join issue: columns overlap but no suffix specified;30832.0
19534;19534;26649199.0;3.0;"<p>I have the following dataframe:</p>

<pre><code>Date        abc    xyz
01-Jun-13   100    200
03-Jun-13   -20    50
15-Aug-13   40     -5
20-Jan-14   25     15
21-Feb-14   60     80
</code></pre>

<p>I need to group the data by year and month. ie: Group by Jan 2013, Feb 2013, Mar 2013 etc...
I will be using the newly grouped data to create a plot showing abc vs xyz per year/month.</p>

<p>I've tried various combinations of groupby and sum but just can't seem to get anything to work.</p>

<p>Thank you for any assistance.</p>
";;0;;2014-10-30T06:10:51.123;11.0;26646191;2016-11-23T17:09:48.500;;;;;4169229.0;;1;16;<python><pandas>;Pandas groupby month and year;19480.0
19554;19554;26658301.0;1.0;"<p>I am trying to access the index of a row in a function applied across an entire <code>DataFrame</code> in Pandas. I have something like this:</p>

<pre><code>df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
&gt;&gt;&gt; df
   a  b  c
0  1  2  3
1  4  5  6
</code></pre>

<p>and I'll define a function that access elements with a given row</p>

<pre><code>def rowFunc(row):
    return row['a'] + row['b'] * row['c']
</code></pre>

<p>I can apply it like so:</p>

<pre><code>df['d'] = df.apply(rowFunc, axis=1)
&gt;&gt;&gt; df
   a  b  c   d
0  1  2  3   7
1  4  5  6  34
</code></pre>

<p>Awesome! Now what if I want to incorporate the index into my function?
The index of any given row in this <code>DataFrame</code> before adding <code>d</code> would be <code>Index([u'a', u'b', u'c', u'd'], dtype='object')</code>, but I want the 0 and 1. So I can't just access <code>row.index</code>.</p>

<p>I know I could create a temporary column in the table where I store the index, but I""m wondering if it is sotred in the row object somewhere.</p>
";;2;;2014-10-30T16:22:13.837;2.0;26658240;2014-10-30T16:33:39.140;;;;;2055368.0;;1;26;<python-2.7><pandas><dataframe>;getting the index of a row in a pandas apply function;9080.0
19583;19583;26667043.0;3.0;"<p>[EDIT: Wrong subject/title of post corrected]</p>

<p>I have a dataframe with some columns like this:</p>

<pre><code>A   B   C  
0   
4
5
6
7
7
6
5
</code></pre>

<p>The <em>possible range of values in A are only from 0 to 7</em>. </p>

<p>Also, I have a list of 8 elements like this:</p>

<pre><code>List=[2,5,6,8,12,16,26,32]  //There are only 8 elements in this list
</code></pre>

<p>If the element in column A is <em>n</em>, I need to insert the <em>n</em> th element from the List in a new column, say 'D'.</p>

<p>How can I do this in one go without looping over the whole dataframe? </p>

<p>The resulting dataframe would look like this:</p>

<pre><code>A   B   C   D
0           2
4           12
5           16
6           26
7           32
7           32
6           26
5           16
</code></pre>

<p>(Note: The dataframe is huge and iteration is the last option option. But I can also arrange the elements in 'List' in any other data structure like dict if necessary)</p>
";;1;;2014-10-31T03:00:30.587;1.0;26666919;2016-07-20T20:58:22.283;2014-10-31T03:22:40.303;;677588.0;;677588.0;;1;18;<python><pandas><dataframe>;python pandas add column in dataframe from list;27924.0
19604;19604;;2.0;"<p>Is it possible to export a Pandas dataframe as an image file? Something like <code>df.to_png()</code> or <code>df.to_table().savefig('table.png')</code>.</p>

<p>At the moment I export a dataframe using <code>df.to_csv()</code>. I then open this csv file in Excel to make the data look pretty and then copy / paste the Excel table into Powerpoint as an image. I see matplotlib has a <code>.table()</code> method, but I'm having trouble getting it to work with my df.</p>

<p>The df I'm using has 5 columns &amp; 5 rows and each 'cell' is a number.</p>

<p>Thanks in advance.</p>
";;3;;2014-10-31T15:43:13.330;6.0;26678467;2017-05-18T10:05:16.620;;;;;2370852.0;;1;11;<python><pandas>;Export a Pandas dataframe as a table image;5767.0
19655;19655;26716774.0;4.0;"<p>I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be <code>keys</code> and the elements of other columns in same row be <code>values</code>. </p>

<p>DataFrame:   </p>

<pre><code>    ID   A   B   C
0   p    1   3   2
1   q    4   3   2
2   r    4   0   9  
</code></pre>

<p>Output should be like this:</p>

<p>Dictionary:</p>

<pre><code>{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}
</code></pre>
";;2;;2014-11-03T14:47:53.253;10.0;26716616;2016-12-11T17:14:51.747;2016-12-11T17:14:51.747;;3923281.0;;3676196.0;;1;28;<python><pandas><dictionary><dataframe>;Convert a Pandas DataFrame to a dictionary;34674.0
19676;19676;26724581.0;2.0;"<p>Python 3.4 and Pandas 0.15.0</p>

<p>df is a dataframe and col1 is a column. With the code below, I'm checking for the presence of the value 10 and replacing such values with 1000. </p>

<pre><code>df.col1[df.col1 == 10] = 1000
</code></pre>

<p>Here's another example. This time, I'm changing values in col2 based on index.</p>

<pre><code>df.col2[df.index == 151] = 500
</code></pre>

<p>Both these produce the warning below:</p>

<pre><code>-c:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
</code></pre>

<p>Finally,</p>

<pre><code>cols = ['col1', 'col2', 'col3']
df[cols] = df[cols].applymap(some_function)
</code></pre>

<p>This produces a similar warning, with an added suggestion:</p>

<pre><code>Try using .loc[row_indexer,col_indexer] = value instead
</code></pre>

<p>I'm not sure I understand the discussion pointed to in the warnings. What would be a better way to write these three lines of code?</p>

<p>Note that the operations worked.</p>
";2017-07-18T20:03:46.770;0;;2014-11-03T22:23:20.833;6.0;26724378;2017-07-10T15:17:05.150;2014-11-04T03:53:27.543;;1927583.0;;1927583.0;;1;16;<python><pandas><warnings>;Pandas SettingWithCopyWarning;31586.0
19741;19741;26763793.0;3.0;"<p>I have one field in a pandas DataFrame that was imported as string format. 
It should be a datetime variable.
How do I convert it to a datetime column and then filter based on date.</p>

<p>Example:</p>

<ul>
<li>DataFrame Name: <strong>raw_data</strong>    </li>
<li>Column Name: <strong>Mycol</strong>    </li>
<li>Value
Format in Column: <strong>'05SEP2014:00:00:00.000'</strong></li>
</ul>
";;0;;2014-11-05T17:24:34.143;15.0;26763344;2017-03-29T07:03:16.780;2014-11-05T17:25:51.540;;2867928.0;;3971910.0;;1;43;<python><datetime><pandas>;Convert Pandas Column to DateTime;63841.0
19783;19783;26778637.0;1.0;"<p>I want to replicate rows in a Pandas Dataframe. Each row should be repeated n times, where n is a field of each row. </p>

<pre><code>import pandas as pd

what_i_have = pd.DataFrame(data={
  'id': ['A', 'B', 'C'],
  'n' : [  1,   2,   3],
  'v' : [ 10,  13,   8]
})

what_i_want = pd.DataFrame(data={
  'id': ['A', 'B', 'B', 'C', 'C', 'C'],
  'v' : [ 10,  13,  13,   8,   8,   8]
})
</code></pre>

<p>Is this possible?</p>
";;5;;2014-11-06T11:01:59.267;4.0;26777832;2014-11-06T11:42:27.623;;;;;4222508.0;;1;12;<python><pandas>;Replicating rows in a pandas data frame by a column value;3782.0
19800;19800;;2.0;"<p>I'm trying to use multiprocessing with pandas dataframe, that is split the dataframe to 8 parts. apply some function to each part using apply (with each part processed in different process). </p>

<p>EDIT:
Here's the solution I finally found:</p>

<pre><code>import multiprocessing as mp
import pandas.util.testing as pdt

def process_apply(x):
    # do some stuff to data here

def process(df):
    res = df.apply(process_apply, axis=1)
    return res

if __name__ == '__main__':
    p = mp.Pool(processes=8)
    split_dfs = np.array_split(big_df,8)
    pool_results = p.map(aoi_proc, split_dfs)
    p.close()
    p.join()

    # merging parts processed by different processes
    parts = pd.concat(pool_results, axis=0)

    # merging newly calculated parts to big_df
    big_df = pd.concat([big_df, parts], axis=1)

    # checking if the dfs were merged correctly
    pdt.assert_series_equal(parts['id'], big_df['id'])
</code></pre>
";;6;;2014-11-06T16:15:29.550;4.0;26784164;2016-08-12T18:24:17.870;2015-07-14T19:38:33.703;;3425536.0;;2006977.0;;1;12;<python><pandas><multiprocessing>;pandas multiprocessing apply;5109.0
19809;19809;26787032.0;2.0;"<p>I'm trying to create csv with pandas , but when I export to csv it gave me one extra row </p>

<pre><code>d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}
df0_fa = pd.DataFrame(d)
df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')
</code></pre>

<p>so my result would be : </p>

<pre><code>,one,two
0,1.0,1.0
1,2.0,2.0
2,3.0,3.0
3,,4.0
</code></pre>

<p>But what I want is</p>

<pre><code>one,two
1.0,1.0
2.0,2.0
3.0,3.0
,4.0
</code></pre>
";;0;;2014-11-06T18:39:38.867;5.0;26786960;2016-11-09T09:30:59.483;;;;;918460.0;;1;28;<python><csv><pandas>;pandas to_csv first extra column remove, how to?;14508.0
19918;19918;26838140.0;3.0;"<p>I have a Pandas Dataframe as shown below:</p>

<pre><code>    1    2       3
 0  a  NaN    read
 1  b    l  unread
 2  c  NaN    read
</code></pre>

<p>I want to remove the NaN values with an empty string so that it looks like so:</p>

<pre><code>    1    2       3
 0  a   """"    read
 1  b    l  unread
 2  c   """"    read
</code></pre>
";;1;;2014-11-10T06:29:26.410;10.0;26837998;2017-07-25T10:09:18.557;;;;;1452759.0;;1;40;<python><pandas><nan>;Pandas Replace NaN with blank/empty string;36883.0
19989;19989;;4.0;"<p>I'm plotting two data series with Pandas with seaborn imported. Ideally I would like the horizontal grid lines shared between both the left and the right y-axis, but I'm under the impression that this is hard to do.</p>

<p>As a compromise I would like to remove the grid lines all together. The following code however produces the horizontal gridlines for the secondary y-axis.</p>

<pre><code>import pandas as pd
import numpy as np
import seaborn as sns


data = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])
data.plot(secondary_y=['B'],grid=False)
</code></pre>

<p><img src=""https://i.stack.imgur.com/4l0Z2.png"" alt=""gridlines that I want to get rid of""></p>
";;5;;2014-11-11T15:24:56.750;3.0;26868304;2016-02-17T10:19:56.887;2014-11-11T19:43:28.953;;2998998.0;;2998998.0;;1;12;<python><matplotlib><pandas><plot><seaborn>;How to get rid of grid lines when plotting with Seaborn + Pandas with secondary_y;14203.0
20007;20007;29665452.0;4.0;"<p>I am using iPython notebook.  When I do this:</p>

<pre><code>df
</code></pre>

<p>I get a beautiful table with cells.  However, if i do this:</p>

<pre><code>df1
df2 
</code></pre>

<p>it doesn't print the first beautiful table.  If I try this:</p>

<pre><code>print df1
print df2
</code></pre>

<p>It prints out the table in a different format that spills columns over and makes the output very tall.  </p>

<p>Is there a way to force it to print out the beautiful tables for both datasets?</p>
";;2;;2014-11-11T19:40:53.933;24.0;26873127;2017-04-14T23:04:13.130;2016-11-18T15:48:44.680;;202229.0;;3971910.0;;1;78;<pandas><printing><ipython-notebook><jupyter-notebook><display>;Show DataFrame as table in iPython Notebook;52192.0
20032;20032;26878532.0;3.0;"<p>This is more of a conceptual question, I do not have a specific problem.</p>

<p>I am learning python for data analysis, but I am very familiar with R - one of the great things about R is plyr (and of course ggplot2) and even better dplyr. Pandas of course has split-apply as well however in R I can do things like (in dplyr, a bit different in plyr, and I can see now how dplyr mimics the . notation from object programming)</p>

<pre><code>   data %.% group_by(c(.....)) %.% summarise(new1 = ...., new2 = ...., ..... newn=....)
</code></pre>

<p>in which I create multiple summary calculations at the same time</p>

<p>How do I do that in python, because </p>

<pre><code>df[...].groupby(.....).sum() only sums columns, 
</code></pre>

<p>while on R I can have one mean, one sum, one special function, etc. on one call</p>

<p>I realize I can do all my operations separately and merge them, and that is fine if I am using python, but when it comes down to choosing a tool, any line of code you do not have to type and check and validate adds up in time</p>

<p>in addition, in dplyr you can also add mutate statements as well, so it seems to me it is way more powerful - so what am I missing about pandas or python - </p>

<p>My goal is to learn, I have spent a lot of effort to learn python and it is a worthy investment, but still the question remains</p>
";;0;;2014-11-12T02:55:11.400;6.0;26878476;2016-12-30T07:21:49.167;2016-12-30T07:21:49.167;;2360798.0;;1617979.0;;1;14;<python><r><pandas><plyr><dplyr>;plyr or dplyr in Python;7935.0
20034;20034;26893083.0;2.0;"<p>Is there an easy way to check whether two data frames are different copies or views of the same underlying data that doesn't involve manipulations? I'm trying to get a grip on when each is generated, and given how idiosyncratic the rules seem to be, I'd like an easy way to test. </p>

<p>For example, I thought ""id(df.values)"" would be stable across views, but they don't seem to be:</p>

<pre><code># Make two data frames that are views of same data.
df = pd.DataFrame([[1,2,3,4],[5,6,7,8]], index = ['row1','row2'], 
       columns = ['a','b','c','d'])
df2 = df.iloc[0:2,:]

# Demonstrate they are views:
df.iloc[0,0] = 99
df2.iloc[0,0]
Out[70]: 99

# Now try and compare the id on values attribute
# Different despite being views! 

id(df.values)
Out[71]: 4753564496

id(df2.values)
Out[72]: 4753603728

# And we can of course compare df and df2
df is df2
Out[73]: False
</code></pre>

<p>Other answers I've looked up that try to give rules, but don't seem consistent, and also don't answer this question of how to test:</p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy"">What rules does Pandas use to generate a view vs a copy?</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/17960511/pandas-subindexing-dataframes-copies-vs-views"">Pandas: Subindexing dataframes: Copies vs views</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/14192741/understanding-pandas-dataframe-indexing"">Understanding pandas dataframe indexing</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/22537112/re-assignment-in-pandas-copy-or-view"">Re-assignment in Pandas: Copy or view?</a></p></li>
</ul>

<p>And of course: 
 - <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy</a></p>

<p><strong>UPDATE:</strong> Comments below seem to answer the question -- looking at the <code>df.values.base</code> attribute rather than <code>df.values</code> attribute does it, as does a reference to the <code>df._is_copy</code> attribute (though the latter is probably very bad form since it's an internal).</p>
";;5;;2014-11-12T04:05:05.813;7.0;26879073;2017-01-18T20:43:13.783;2017-05-23T12:31:52.760;;-1.0;;2302819.0;;1;16;<python><pandas>;Checking whether data frame is copy or view in Pandas;3172.0
20056;20056;26887820.0;1.0;"<p>I've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a ""1"" in another ethnicity column they still are counted as Hispanic not a two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(accept for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. </p>

<p>Its almost like doing a for loop through each row and if each record meets a criteria they are added to one list and eliminated from the original.  </p>

<p>From the dataframe below I need to calculate a new column based off of the following:</p>

<p>=========================  CRITERIA  ===============================</p>

<pre><code>IF [ERI_Hispanic] = 1 THEN RETURN Hispanic
ELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) &gt; 1 THEN RETURN Two or More
ELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN A/I AK Native
ELSE IF [ERI_Asian] = 1 THEN RETURN Asian
ELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN Black/AA
ELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN Haw/Pac Isl.
ELSE IF [ERI_White] = 1 THEN RETURN White
</code></pre>

<p>Comment: If the ERI Flag for Hispanic is True (1), then employee is classified as Hispanic</p>

<p>Comment: If more than 1 non-Hispanic ERI Flag are true, return Two or More</p>

<p>======================  DATAFRAME ===========================</p>

<p>In [13]: df1</p>

<p>Out [13]: </p>

<pre><code>     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined
0    MOST           JEFF        E       0               0           0               0               0               1           White
1    CRUISE         TOM         E       0               0           0               1               0               0           White
2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown
3    DICAP          LEO                 0               0           0               0               0               1           Unknown
4    BRANDO         MARLON      E       0               0           0               0               0               0           White
5    HANKS          TOM         0                       0           0               0               0               1           Unknown
6    DENIRO         ROBERT      E       0               1           0               0               0               1           White
7    PACINO         AL          E       0               0           0               0               0               1           White
8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White
9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White
</code></pre>
";;0;;2014-11-12T12:08:12.910;22.0;26886653;2014-11-12T13:11:09.120;;;;;2201603.0;;1;52;<python><numpy><pandas>;pandas create new column based on values from other columns;68245.0
20071;20071;26893443.0;2.0;"<p>I have have a column of a pandas dataframe that I got from a database query with blanck cells. The blank cells become ""None"" and I want to check if each of the rows is None:         </p>

<pre><code>In [325]: yes_records_sample['name']
Out[325]: 
41055    John J Murphy Professional Building
25260                                   None
41757             Armand Bayou Nature Center
31397                                   None
33104               Hubert Humphrey Building
16891                         Williams Hall
29618                                   None
3770                          Covenant House
39618                                   None
1342       Bhathal Student Services Building
20506                                   None
</code></pre>

<p>My understanding per the documentation is that I can check if each row is null with <code>isnull()</code> command <a href=""http://pandas.pydata.org/pandas-docs/dev/missing_data.html#values-considered-missing"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/dev/missing_data.html#values-considered-missing</a></p>

<p>That function, however, is not working for me:  </p>

<pre><code>In [332]: isnull(yes_records_sample['name'])
</code></pre>

<p>I get the following error:</p>

<pre><code>NameError Traceback (most recent call last)
&lt;ipython-input-332-55873906e7e6&gt; in &lt;module&gt;()
----&gt; 1 isnull(yes_records_sample['name'])
NameError: name 'isnull' is not defined
</code></pre>

<p>I also saw that someone just replaced the ""None"" strings, but neither of these variations on that approach worked for me:
<a href=""https://stackoverflow.com/questions/24619145/rename-none-value-in-pandas"">Rename &quot;None&quot; value in Pandas</a></p>

<pre><code>yes_records_sample['name'].replace('None', ""--no value--"")
yes_records_sample['name'].replace(None, ""--no value--"")
</code></pre>

<p>I was ultimately able to use the <code>fillna</code> function and fill each of those rows with an empty string <code>yes_records_sample.fillna('')</code> as a workaround and then I could check <code>yes_records_sample['name']==''</code> But I am profoundly confused by how 'None' works and what it means.  Is there a way to easily just check if a cell in a dataframe is 'None'? </p>
";;0;;2014-11-12T17:57:19.947;3.0;26893419;2016-11-14T15:55:28.507;2017-05-23T12:34:31.827;;-1.0;;1042614.0;;1;12;<python><pandas><null><na>;Selecting pandas cells with None value;12598.0
20265;20265;26977495.0;5.0;"<pre><code>df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],
                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],
                   'Col3': np.random.random(5)})
</code></pre>

<p>What is the best way to return the unique values of 'Col1' and 'Col2'?</p>

<p>The desired output is </p>

<pre><code>'Bob', 'Joe', 'Bill', 'Mary', 'Steve'
</code></pre>
";;0;;2014-11-17T16:21:12.120;18.0;26977076;2017-08-18T01:56:56.700;2016-12-11T17:15:53.187;;3923281.0;;2333196.0;;1;35;<python><pandas><dataframe><unique>;pandas unique values multiple columns;43537.0
20357;20357;27018394.0;2.0;"<p>If I have a dataframe df with multiple columns ['x', 'y', 'z'] how do I forward fill only one column 'x'
or a group of columns ['x','y']?</p>

<p>I only know to do it by axis.</p>
";;0;;2014-11-19T08:31:39.580;3.0;27012151;2015-12-05T17:56:00.673;;;;;2776506.0;;1;13;<python><pandas>;forward fill specific columns in pandas dataframe;8448.0
20445;20445;27050186.0;3.0;"<p>I have a list of dicts in the following form that I generate from pandas. I want to convert it to a json format.</p>

<pre><code>list_val = [{1.0: 685}, {2.0: 8}]
output = json.dumps(list_val)
</code></pre>

<p>However, json.dumps throws an error: TypeError: 685 is not JSON serializable</p>

<p>I am guessing it's a type conversion issue from numpy to python(?).</p>

<p>However, when I convert the values v of each dict in the array using np.int32(v) it still throws the error.</p>

<p>EDIT: Here's the full code</p>

<pre><code>            new = df[df[label] == label_new] 
            ks_dict = json.loads(content)
            ks_list = ks_dict['variables']
            freq_counts = []

            for ks_var in ks_list:

                    freq_var = dict()
                    freq_var[""name""] = ks_var[""name""]
                    ks_series = new[ks_var[""name""]]
                    temp_df = ks_series.value_counts().to_dict()
                    freq_var[""new""] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]            
                    freq_counts.append(freq_var)

           out = json.dumps(freq_counts)
</code></pre>
";;6;;2014-11-20T21:42:32.353;4.0;27050108;2015-08-02T08:44:31.817;2014-11-20T21:53:23.007;;3972080.0;;3972080.0;;1;12;<python><json><numpy><pandas>;Convert numpy type to python;6926.0
20464;20464;27060328.0;2.0;"<p>I have a pandas dataframe df as illustrated below:</p>

<pre><code>BrandName Specialty
A          H
B          I
ABC        J
D          K
AB         L
</code></pre>

<p>I want to replace 'ABC' and 'AB' in column BrandName by A.
Can someone help with this?</p>
";;0;;2014-11-21T11:17:27.283;7.0;27060098;2017-07-19T19:16:57.707;;;;;4201311.0;;1;16;<python><replace><pandas><dataframe>;Replacing few values in a pandas dataframe column with another value;28802.0
20485;20485;27066284.0;2.0;"<p>I am trying to do a pandas merge and get the above error from the title when I try to run it. I am using 3 columns to match on whereas just before I do  similar merge on only 2 columns and it works fine.</p>

<pre><code>df = pd.merge(df, c, how=""left"",
        left_on=[""section_term_ps_id"", ""section_school_id"", ""state""],
        right_on=[""term_ps_id"", ""term_school_id"", ""state""])
</code></pre>

<p>columns for the two dataframes</p>

<p>df:</p>

<blockquote>
  <p>Index([u'section_ps_id', u'section_school_id', u'section_course_number', u'secti
  on_term_ps_id', u'section_staff_ps_id', u'section_number', u'section_expression'
  , u'section_grade_level', u'state', u'sections_id', u'course_ps_id', u'course_sc
  hool_id', u'course_number', u'course_schd_dept', u'courses_id', u'school_ps_id',
   u'course_school_id', u'school_name', u'school_abbr', u'school_low_grade', u'sch
  ool_high_grade', u'school_alt_school_number', u'school_state', u'school_phone',
  u'school_fax', u'school_principal', u'school_principal_phone', u'school_principa
  l_email', u'school_asst_principal', u'school_asst_principal_phone', u'school_ass
  t_principal_email'], dtype='object')</p>
</blockquote>

<p>c:</p>

<blockquote>
  <p>Index([u'term_ps_id', u'term_school_id', u'term_portion',
  u'term_start_date', u' term_end_date', u'term_abbreviation',
  u'term_name', u'state', u'terms_id', u'sch ool_ps_id',
  u'term_school_id', u'school_name', u'school_abbr', u'school_low_grad
  e', u'school_high_grade', u'school_alt_school_number',
  u'school_state', u'school
  _phone', u'school_fax', u'school_principal', u'school_principal_phone', u'school
  _principal_email', u'school_asst_principal', u'school_asst_principal_phone', u's chool_asst_principal_email'],
  dtype='object')</p>
</blockquote>

<p>Is it possible to merge on three columns like this? Is there anything wrong from the merge call here?</p>
";;1;;2014-11-21T15:46:00.277;2.0;27065133;2017-08-25T01:10:21.493;2017-08-25T01:10:21.493;;8144295.0;;387191.0;;1;17;<python><pandas><dataframe><data-structures>;"Pandas merge giving error ""Buffer has wrong number of dimensions (expected 1, got 2)""";10573.0
20589;20589;27139421.0;1.0;"<p>What exactly is the <code>lexsort_depth</code> of a multi-index dataframe? Why does it have to be sorted for indexing?</p>

<p>For example, I have noticed that, after manually building a multi-index dataframe <code>df</code> with columns organized in three levels, if I try to do:</p>

<pre><code>idx = pd.IndexSlice
df[idx['foo', 'bar']]
</code></pre>

<p>I get:</p>

<pre><code>KeyError: 'Key length (2) was greater than MultiIndex lexsort depth (0)'
</code></pre>

<p>and at this point, <code>df.columns.lexsort_depth</code> is <code>0</code></p>

<p><strong>However,</strong> if I do, as recommended <a href=""https://stackoverflow.com/questions/19981518/sorting-multi-index-to-full-depth-pandas"">here</a> and <a href=""https://groups.google.com/forum/#!topic/pydata/x5vHWNkhVeM"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>df = df.sortlevel(0,axis=1)
</code></pre>

<p>then the cross-section indexing works. Why? What exactly is <code>lexsort_depth</code>, and why sorting with <code>sortlevel</code> fixes this type of indexing?</p>
";;0;;2014-11-25T00:26:36.977;2.0;27116739;2014-12-19T22:30:54.150;2017-05-23T12:34:24.333;;-1.0;;283296.0;;1;11;<python><numpy><pandas>;What exactly is the lexsort_depth of a multi-index Dataframe?;4723.0
20594;20594;27117982.0;2.0;"<p>I have the following dataframe:</p>

<pre><code>     col
0    pre
1    post
2    a
3    b
4    post
5    pre
6    pre
</code></pre>

<p>I want to replace all rows in the dataframe which do not contain 'pre' to become 'nonpre', so dataframe looks like:        </p>

<pre><code>     col
0    pre
1    nonpre
2    nonpre
3    nonpre
4    nonpre
5    pre
6    pre
</code></pre>

<p>I can do this using a dictionary and pandas replace, however I want to just select the elements which are not 'pre' and replace them with 'nonpre'. is there a better way to do that without listing all possible col values in a dictionary?</p>
";;0;;2014-11-25T02:24:42.567;2.0;27117773;2014-11-25T02:46:37.600;;;;;308827.0;;1;12;<python><replace><pandas>;Pandas replace values;20684.0
20606;20606;27126593.0;2.0;"<p>I have a problem with adding columns in pandas.
I have DataFrame, dimensional is nxk. And in process I wiil need add columns with dimensional mx1, where m = [1,n], but I don't know m.</p>

<p>When I try do it:   </p>

<pre><code>df['Name column'] = data    
# type(data) = list
</code></pre>

<p>result:</p>

<pre><code>AssertionError: Length of values does not match length of index   
</code></pre>

<p>Can I add columns with different length?</p>
";;0;;2014-11-25T12:16:17.850;2.0;27126511;2017-08-14T15:26:42.830;;;;;4051810.0;;1;11;<python><pandas>;add columns different length pandas;8799.0
20781;20781;27203245.0;2.0;"<p>I have a 100M line csv file (actually many separate csv files) totaling 84GB. I need to convert it to a HDF5 file with a single float dataset. I used <strong>h5py</strong> in testing without any problems, but now I can't do the final dataset without running out of memory.</p>

<p>How can I write to HDF5 without having to store the whole dataset in memory? I'm expecting actual code here, because it should be quite simple.</p>

<p>I was just looking into <strong>pytables</strong>, but it doesn't look like the array class (which corresponds to a HDF5 dataset) can be written to iteratively. Similarly, <strong>pandas</strong> has <code>read_csv</code> and <code>to_hdf</code> methods in its <code>io_tools</code>, but I can't load the whole dataset at one time so that won't work. Perhaps you can help me solve the problem correctly with other tools in pytables or pandas.</p>
";;0;;2014-11-29T14:08:35.250;6.0;27203161;2014-12-01T14:10:38.483;;;;;484488.0;;1;17;<python><csv><pandas><hdf5><pytables>;Convert large csv to hdf5;8628.0
20852;20852;27242735.0;2.0;"<p>I am getting a <code>ValueError: cannot reindex from a duplicate axis</code> when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.</p>

<p>Here is my session inside of <code>ipdb</code> trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create <code>sum</code> index for sum of all columns I am getting <code>ValueError: cannot reindex from a duplicate axis</code> error. I created a small DataFrame with the same characteristics, but was not able to reproduce the problem, what could I be missing?</p>

<p>I don't really understand what <code>ValueError: cannot reindex from a duplicate axis</code>means, what does this error message mean? Maybe this will help me diagnose the problem, and this is most answerable part of my question.</p>

<pre><code>ipdb&gt; type(affinity_matrix)
&lt;class 'pandas.core.frame.DataFrame'&gt;
ipdb&gt; affinity_matrix.shape
(333, 10)
ipdb&gt; affinity_matrix.columns
Int64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')
ipdb&gt; affinity_matrix.index
Index([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')

ipdb&gt; affinity_matrix.values.dtype
dtype('float64')
ipdb&gt; 'sums' in affinity_matrix.index
False
</code></pre>

<p>Here is the error:</p>

<pre><code>ipdb&gt; affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)
*** ValueError: cannot reindex from a duplicate axis
</code></pre>

<p>I tried to reproduce this with a simple example, but I failed</p>

<pre><code>In [32]: import pandas as pd

In [33]: import numpy as np

In [34]: a = np.arange(35).reshape(5,7)

In [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))

In [36]: df.values.dtype
Out[36]: dtype('int64')

In [37]: df.loc['sums'] = df.sum(axis=0)

In [38]: df
Out[38]: 
      10  11  12  13  14  15   16
x      0   1   2   3   4   5    6
y      7   8   9  10  11  12   13
u     14  15  16  17  18  19   20
z     21  22  23  24  25  26   27
w     28  29  30  31  32  33   34
sums  70  75  80  85  90  95  100
</code></pre>
";;4;;2014-12-01T20:00:51.787;6.0;27236275;2016-06-06T10:26:39.810;2016-01-19T17:54:09.403;;3651127.0;;1078084.0;;1;56;<python><pandas>;What does `ValueError: cannot reindex from a duplicate axis` mean?;48018.0
20925;20925;27266225.0;3.0;"<p>I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple
values in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (with the same values in all other columns). So if I have:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(
    {'trial_num': [1, 2, 3, 1, 2, 3],
     'subject': [1, 1, 1, 2, 2, 2],
     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]
    }
)

df
Out[10]: 
                 samples  subject  trial_num
0    [0.57, -0.83, 1.44]        1          1
1    [-0.01, 1.13, 0.36]        1          2
2   [1.18, -1.46, -0.94]        1          3
3  [-0.08, -4.22, -2.05]        2          1
4     [0.72, 0.79, 0.53]        2          2
5    [0.4, -0.32, -0.13]        2          3
</code></pre>

<p>How do I convert to long form, e.g.:</p>

<pre><code>   subject  trial_num  sample  sample_num
0        1          1    0.57           0
1        1          1   -0.83           1
2        1          1    1.44           2
3        1          2   -0.01           0
4        1          2    1.13           1
5        1          2    0.36           2
6        1          3    1.18           0
# etc.
</code></pre>

<p>The index is not important, it's OK to set existing
columns as the index and the final ordering isn't
important.</p>
";;0;;2014-12-03T04:44:44.443;14.0;27263805;2016-12-23T14:38:56.897;;;;;1222578.0;;1;31;<python><pandas>;pandas: When cell contents are lists, create a row for each element in the list;8222.0
20945;20945;27275344.0;4.0;"<p>I have a DataFrame:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],
                   'foo.bars': [0, 0, 0, 0, 0, 1],
                   'bar.baz': [5, 5, 6, 5, 5.6, 6.8],
                   'foo.fox': [2, 4, 1, 0, 0, 5],
                   'nas.foo': ['NA', 0, 1, 0, 0, 0],
                   'foo.manchu': ['NA', 0, 0, 0, 0, 0],})
</code></pre>

<p>I want to select values of 1 in columns starting with <code>foo.</code>. Is there a better way to do it other than:</p>

<pre><code>df2 = df[(df['foo.aa'] == 1)|
(df['foo.fighters'] == 1)|
(df['foo.bars'] == 1)|
(df['foo.fox'] == 1)|
(df['foo.manchu'] == 1)
]
</code></pre>

<p>Something similar to writing something like:</p>

<pre><code>df2= df[df.STARTS_WITH_FOO == 1]
</code></pre>

<p>The answer should print out a DataFrame like this:</p>

<pre><code>   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo
0      5.0     1.0         0             0        2         NA      NA
1      5.0     2.1         0             1        4          0       0
2      6.0     NaN         0           NaN        1          0       1
5      6.8     6.8         1             0        5          0       0

[4 rows x 7 columns]
</code></pre>
";;0;;2014-12-03T15:15:54.533;6.0;27275236;2017-08-25T19:27:20.310;2017-01-02T11:56:56.490;;3923281.0;;3159981.0;;1;18;<python><pandas><dataframe><selection>;pandas: best way to select all columns starting with X;13469.0
21054;21054;27325729.0;1.0;"<p>I'm having trouble figuring out how to skip n rows in a csv file but keep the header which is the 1 row.</p>

<p>What I want to do is iterate but keep the header from the first row.  <code>skiprows</code> makes the header the first row after the skipped rows.  What is the best way of doing this?</p>

<pre><code>data = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)
</code></pre>
";;0;;2014-12-05T22:24:32.623;3.0;27325652;2016-03-25T10:48:17.883;2016-03-25T10:48:17.883;;3923281.0;;1072661.0;;1;19;<python><csv><pandas>;Python Pandas read_csv skip rows but keep header;12332.0
21112;21112;29036738.0;4.0;"<p>I've taken my Series and coerced it to a datetime column of dtype=<code>datetime64[ns]</code> (though only need day resolution...not sure how to change). </p>

<pre><code>import pandas as pd
df = pd.read_csv('somefile.csv')
column = df['date']
column = pd.to_datetime(column, coerce=True)
</code></pre>

<p>but plotting doesn't work:</p>

<pre><code>ipdb&gt; column.plot(kind='hist')
*** TypeError: ufunc add cannot use operands with types dtype('&lt;M8[ns]') and dtype('float64')
</code></pre>

<p>I'd like to plot a histogram that just <strong>shows the count of dates by week, month, or year</strong>.</p>

<p>Surely there is a way to do this in <code>pandas</code>?</p>
";;1;;2014-12-08T19:32:45.060;10.0;27365467;2017-03-21T18:06:02.983;2016-05-02T10:30:19.783;;3297428.0;;712997.0;;1;32;<python><pandas><matplotlib><time-series>;python pandas: plot histogram of dates?;17415.0
21170;21170;27422749.0;2.0;"<p>DataFrame:</p>

<pre><code>  c_os_family_ss c_os_major_is l_customer_id_i
0      Windows 7                         90418
1      Windows 7                         90418
2      Windows 7                         90418
</code></pre>

<p>Code:</p>

<pre><code>print df
for name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):
    print name
    print group
</code></pre>

<p>I'm trying to just loop over the aggregated data, but I get the error:</p>

<blockquote>
  <p>ValueError: too many values to unpack</p>
</blockquote>

<p>@EdChum, here's the expected output:</p>

<pre><code>                                                    c_os_family_ss  \
l_customer_id_i
131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...
135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...

                                                     c_os_major_is
l_customer_id_i
131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...
135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...
</code></pre>

<p>The output is not the problem, I wish to loop over every group.</p>
";;7;;2014-12-10T16:01:40.593;4.0;27405483;2015-10-21T21:15:18.667;2014-12-10T16:11:33.933;;184379.0;;184379.0;;1;29;<python><pandas>;How to loop over grouped Pandas dataframe?;27652.0
21297;21297;30355286.0;2.0;"<p>I receive a DataFrame from somewhere and want to to create other DataFrame with the same number and names of columns and rows (indexes). For example, suppose that the original data frame was created as</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame([[11,12],[21,22]],columns=['c1','c2'],index=['i1','i2'])
</code></pre>

<p>I copied the structure by explicitly defining the columns and names:</p>

<pre><code>df2 = pd.DataFrame(columns=df1.columns,index=df1.index)    
</code></pre>

<p>I don't want to copy the data, otherwise I could just write <code>df2 = df1.copy()</code>. In other words, after df2 being created it must contain only NaN elements:</p>

<pre><code>In [23]: df1
Out[23]: 
    c1  c2
i1  11  12
i2  21  22

In [24]: df2
Out[24]: 
     c1   c2
i1  NaN  NaN
i2  NaN  NaN
</code></pre>

<p>Is there a more idiomatic way of doing it?</p>
";;3;;2014-12-14T08:49:35.527;2.0;27467730;2017-01-05T17:48:57.360;2017-01-05T17:48:57.360;;3482418.0;;3482418.0;;1;16;<python><pandas><dataframe>;Is there a way to copy only the structure (not the data) of a Pandas DataFrame?;6323.0
21303;21303;;5.0;"<p>I have a dataframe with two columns of datetime.time's.  I'd like to scatter plot them.  I'd also like the axes to display the times, ideally.  But</p>

<pre><code>df.plot(kind='scatter', x='T1', y='T2')
</code></pre>

<p>dumps a bunch of internal plot errors ending with a KeyError on 'T1'.</p>

<p>Alternatively, I try</p>

<pre><code>plt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])
plt.show()
</code></pre>

<p>and I get 'Exception in Tkinter callback' with a long stack crawl ending in</p>

<pre><code>return _from_ordinalf(x, tz)
  File ""/usr/lib/python3/dist-packages/matplotlib/dates.py"", line 224, in _from_ordinalf
microsecond, tzinfo=UTC).astimezone(tz)
TypeError: tzinfo argument must be None or of a tzinfo subclass, not type 'str'
</code></pre>

<p>Any pointers?</p>
";;4;;2014-12-14T18:27:48.190;2.0;27472548;2017-06-15T16:01:02.340;;;;;833300.0;;1;12;<python><python-3.x><matplotlib><pandas>;pandas scatter plotting datetime;5660.0
21309;21309;27475514.0;5.0;"<p>Using this as a starting point:</p>

<pre><code>a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]
df = pd.DataFrame(a, columns=['one', 'two', 'three'])

Out[8]: 
  one  two three
0   10  1.2   4.2
1   15  70   0.03
2    8   5     0
</code></pre>

<p>I want to use something like an <code>if</code> statement within pandas. </p>

<pre><code>if df['one'] &gt;= df['two'] and df['one'] &lt;= df['three']:
    df['que'] = df['one']
</code></pre>

<p>Basically, check each row via the <code>if</code> statement, create new column. </p>

<p>The docs say to use <code>.all</code> but there is no example...</p>
";;4;;2014-12-14T22:33:45.890;13.0;27474921;2017-01-02T11:29:33.820;2017-01-02T11:29:33.820;;3923281.0;;428862.0;;1;19;<python><pandas><if-statement><dataframe>;Compare two columns using pandas;35053.0
21335;21335;27489248.0;3.0;"<p>For example I have following table:</p>

<pre><code>index,A,B
0,0,0
1,0,8
2,0,8
3,1,0
4,1,5
</code></pre>

<p>After grouping by <code>A</code>:</p>

<pre><code>0:
index,A,B
0,0,0
1,0,8
2,0,8

1:
index,A,B
3,1,5
4,1,3
</code></pre>

<p>What I need is to drop rows from each group, where the number in column <code>B</code> is less than maximum value from all rows from group's column <code>B</code>. Well I have a problem translating and formulating this problem to English so here is the example:</p>

<p>Maximum value from rows in column <code>B</code> in group <code>0</code>: <strong>8</strong></p>

<p>So I want to drop row with index <code>0</code> and keep rows with indexes <code>1</code> and <code>2</code></p>

<p>Maximum value from rows in column <code>B</code> in group <code>1</code>: <strong>5</strong></p>

<p>So I want to drop row with index <code>4</code> and keep row with index <code>3</code></p>

<p>I have tried to use pandas filter function, but the problem is that it is operating on all rows in group at one time:</p>

<pre><code>data = &lt;example table&gt;
grouped = data.groupby(""A"")
filtered = grouped.filter(lambda x: x[""B""] == x[""B""].max())
</code></pre>

<p><strong>So what I ideally need is some filter, which iterates through all rows in group.</strong> </p>

<p>Thanks for help!</p>

<p>P.S. Is there also way to only delete rows in groups and do not return <code>DataFrame</code> object?</p>
";;3;;2014-12-15T15:59:19.243;6.0;27488080;2017-07-07T18:02:47.360;2014-12-16T07:38:16.560;;1928742.0;;1928742.0;;1;13;<python><pandas><filter><lambda><group-by>;Python pandas - filter rows after groupby;21194.0
21404;21404;27951930.0;1.0;"<p>Consider the following dataframe:</p>

<pre><code>     A      B         C         D
0  foo    one  0.162003  0.087469
1  bar    one -1.156319 -1.526272
2  foo    two  0.833892 -1.666304
3  bar  three -2.026673 -0.322057
4  foo    two  0.411452 -0.954371
5  bar    two  0.765878 -0.095968
6  foo    one -0.654890  0.678091
7  foo  three -1.789842 -1.130922
</code></pre>

<p>The following commands work:</p>

<pre><code>&gt; df.groupby('A').apply(lambda x: (x['C'] - x['D']))
&gt; df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())
</code></pre>

<p>but none of the following work:</p>

<pre><code>&gt; df.groupby('A').transform(lambda x: (x['C'] - x['D']))
ValueError: could not broadcast input array from shape (5) into shape (5,3)

&gt; df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())
 TypeError: cannot concatenate a non-NDFrame object
</code></pre>

<p><strong>Why?</strong> <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation"" rel=""noreferrer"">The example on the documentation</a> seems to suggest that calling <code>transform</code> on a group allows one to do row-wise operation processing:</p>

<pre><code># Note that the following suggests row-wise operation (x.mean is the column mean)
zscore = lambda x: (x - x.mean()) / x.std()
transformed = ts.groupby(key).transform(zscore)
</code></pre>

<p>In other words, I thought that transform is essentially a specific type of apply (the one that does not aggregate). Where am I wrong?</p>

<p>For reference, below is the construction of the original dataframe above:</p>

<pre><code>df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                          'foo', 'bar', 'foo', 'foo'],
                   'B' : ['one', 'one', 'two', 'three',
                         'two', 'two', 'one', 'three'],
                   'C' : randn(8), 'D' : randn(8)})
</code></pre>
";;2;;2014-12-17T02:27:43.740;30.0;27517425;2016-11-30T17:36:37.717;2014-12-22T18:30:19.690;;283296.0;;283296.0;;1;38;<python><pandas>;Apply vs transform on a group object;19045.0
21670;21670;27667801.0;1.0;"<p>I'm learning the Python pandas library. Coming from an R background, the indexing and selecting functions seem more complicated than they need to be. My understanding it that .loc() is only label based and .iloc() is only integer based. </p>

<p><strong>Why should I ever use .loc() and .iloc() if .ix() is faster and supports integer and label access?</strong> </p>
";;1;;2014-12-27T13:35:22.070;17.0;27667759;2017-03-22T09:22:59.917;;;;;2469211.0;;1;53;<python><r><pandas>;Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?;38433.0
21674;21674;27680109.0;3.0;"<p>When you are selecting a sub dataframe from a parent dataframe. I notice some programmers are making a copy of the data frame using .copy() method. Why are they making a copy of the data frame? What will happen if I dont make a copy??</p>
";;2;;2014-12-28T02:22:27.573;7.0;27673231;2017-01-20T13:22:43.417;;;;;4273266.0;;1;36;<pandas>;why should I make a copy of a data frame in pandas;28042.0
21823;21823;27759140.0;3.0;"<p>I would like to know if there is someway of replacing all DataFrame negative numbers by zeros?</p>
";;1;;2015-01-03T20:14:14.083;2.0;27759084;2017-07-27T20:09:59.257;2016-09-25T18:50:51.323;;4203807.0;;4203807.0;;1;12;<python><replace><pandas>;How to replace negative numbers in Pandas Data Frame by zero;20051.0
21877;21877;27787977.0;1.0;"<p>This would be useful so I know how many unique groups I have to perform calculations on.  Thank you.</p>

<p>Suppose groupby object is called <code>dfgroup</code>. </p>
";;0;;2015-01-05T21:08:28.870;;27787930;2015-01-05T21:12:03.193;;;;;2690949.0;;1;14;<python><pandas><grouping>;How to get number of groups in a groupby object in pandas?;5235.0
21965;21965;27844045.0;3.0;"<p>I want to group my dataframe by two columns and then sort the aggregated results within the groups.</p>

<pre><code>In [167]:
df

Out[167]:
count   job source
0   2   sales   A
1   4   sales   B
2   6   sales   C
3   3   sales   D
4   7   sales   E
5   5   market  A
6   3   market  B
7   2   market  C
8   4   market  D
9   1   market  E

In [168]:
df.groupby(['job','source']).agg({'count':sum})

Out[168]:
            count
job     source  
market  A   5
        B   3
        C   2
        D   4
        E   1
sales   A   2
        B   4
        C   6
        D   3
        E   7
</code></pre>

<p>I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:</p>

<pre><code>            count
job     source  
market  A   5
        D   4
        B   3
sales   E   7
        C   6
        B   4
</code></pre>
";;0;;2015-01-08T14:37:19.673;25.0;27842613;2017-07-05T10:54:22.157;;;;;3198411.0;;1;43;<python><sorting><pandas><group-by>;pandas groupby sort within groups;37771.0
22040;22040;27889674.0;2.0;"<p>I want to query a PostgreSQL database and return the output as a Pandas dataframe.</p>

<p>I use <code>sqlalchemy</code> to create a connection the the database:</p>

<pre><code>from sqlalchemy import create_engine
engine = create_engine('postgresql://user@localhost:5432/mydb')
</code></pre>

<p>I write a Pandas dataframe to a database table:</p>

<pre><code>i=pd.read_csv(path)
i.to_sql('Stat_Table',engine,if_exists='replace')
</code></pre>

<p>Based upon the <a href=""http://pandas.pydata.org/pandas-docs/version/0.14.0/generated/pandas.read_sql_query.html"" rel=""noreferrer"">docs</a>, looks like pd.read_sql_query() should accept a SQLAlchemy engine:</p>

<pre><code>a=pd.read_sql_query('select * from Stat_Table',con=engine)
</code></pre>

<p>But it throws an error:</p>

<pre><code>ProgrammingError: (ProgrammingError) relation ""stat_table"" does not exist
</code></pre>

<p>I'm using Pandas version 0.14.1. </p>

<p>What's the right way to do this?</p>
";;0;;2015-01-11T05:27:49.787;4.0;27884268;2015-01-11T17:06:03.063;;;;;3422206.0;;1;19;<python><postgresql><pandas><sqlalchemy>;Return Pandas dataframe from PostgreSQL query with sqlalchemy;16640.0
22056;22056;38951835.0;1.0;"<p>I've been reading a tab-delimited data file in Windows with Pandas/Python without any problems. The data file contains notes in first three lines and then follows with a header. </p>

<pre><code>df = pd.read_csv(myfile,sep='\t',skiprows=(0,1,2),header=(0))
</code></pre>

<p>I'm now trying to read this file with my Mac. (My first time using Python on Mac.) I get the following error.</p>

<pre><code>pandas.parser.CParserError: Error tokenizing data. C error: Expected 1
fields in line 8, saw 39
</code></pre>

<p>If set the <em>error_bad_lines</em> argument for <em>read_csv</em> to <em>False</em>, I get the following information, which continues until the end of the last row.</p>

<pre><code>Skipping line 8: expected 1 fields, saw 39
Skipping line 9: expected 1 fields, saw 125
Skipping line 10: expected 1 fields, saw 125
Skipping line 11: expected 1 fields, saw 125
Skipping line 12: expected 1 fields, saw 125
Skipping line 13: expected 1 fields, saw 125
Skipping line 14: expected 1 fields, saw 125
Skipping line 15: expected 1 fields, saw 125
Skipping line 16: expected 1 fields, saw 125
Skipping line 17: expected 1 fields, saw 125
...
</code></pre>

<p>Do I need to specify a value for the <em>encoding</em> argument? It seems as though I shouldn't have to because reading the file works fine on Windows.</p>
";;6;;2015-01-12T06:05:53.593;3.0;27896214;2017-08-05T15:13:38.097;;;;;3062149.0;;1;13;<python><osx><pandas><import><tab-delimited>;Reading tab-delimited file with Pandas - works on Windows, but not on Mac;23577.0
22069;22069;27905350.0;5.0;"<p>Suppose I have a DataFrame with some <code>NaN</code>s:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
&gt;&gt;&gt; df
    0   1   2
0   1   2   3
1   4 NaN NaN
2 NaN NaN   9
</code></pre>

<p>What I need to do is replace every <code>NaN</code> with the first non-<code>NaN</code> value in the same column above it. It is assumed that the first row will never contain a <code>NaN</code>. So for the previous example the result would be</p>

<pre><code>   0  1  2
0  1  2  3
1  4  2  3
2  4  2  9
</code></pre>

<p>I can just loop through the whole DataFrame column-by-column, element-by-element and set the values directly, but is there an easy (optimally a loop-free) way of achieving this?</p>
";;0;;2015-01-12T15:22:48.147;5.0;27905295;2017-06-01T22:53:24.467;2016-12-30T15:26:58.967;;3923281.0;;461202.0;;1;23;<python><python-3.x><pandas><dataframe><nan>;How to replace NaNs by preceding values in pandas DataFrame?;14351.0
22225;22225;27975230.0;4.0;"<p>Assume we have a data frame in Python Pandas that looks like this:</p>

<pre><code>df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})
</code></pre>

<p>Or, in table form:</p>

<pre class=""lang-none prettyprint-override""><code>ids    vals
aball   1
bball   2
cnut    3
fball   4
</code></pre>

<p>How do I filter rows which contain the key word ""ball?"" For example, the output should be:</p>

<pre class=""lang-none prettyprint-override""><code>ids    vals
aball   1
bball   2
fball   4
</code></pre>
";;1;;2015-01-15T23:44:22.450;8.0;27975069;2017-05-31T15:14:24.067;2015-01-15T23:54:58.777;;2359271.0;;4383051.0;;1;26;<python><pandas>;How to filter rows containing a string pattern from a Pandas dataframe;35532.0
22307;22307;28006809.0;2.0;"<p>It's easy to turn a list of lists into a pandas dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([[1,2,3],[3,4,5]])
</code></pre>

<p>But how do I turn df back into a list of lists?</p>

<pre><code>lol = df.what_to_do_now?
print lol
# [[1,2,3],[3,4,5]]
</code></pre>
";;1;;2015-01-18T03:14:21.700;5.0;28006793;2015-01-18T03:59:46.763;;;;;2004922.0;;1;27;<python><pandas>;Pandas DataFrame to List of Lists;27158.0
22313;22313;28009526.0;1.0;"<p>I have a DataFrame <code>df</code> like the following (excerpt, 'Timestamp' are the index):</p>

<pre><code>Timestamp              Value
2012-06-01 00:00:00     100
2012-06-01 00:15:00     150
2012-06-01 00:30:00     120
2012-06-01 01:00:00     220
2012-06-01 01:15:00      80
...and so on.
</code></pre>

<p>I need a new column <code>df['weekday']</code> with the respective weekday/day-of-week of the timestamps.</p>

<p>How can I get this?</p>
";;0;;2015-01-18T11:54:32.367;2.0;28009370;2015-11-12T14:18:17.943;2015-11-12T14:18:17.943;;1025391.0;;4079532.0;;1;16;<python><pandas>;Get weekday/day-of-week for Datetime column of DataFrame;17538.0
22329;22329;28020783.0;2.0;"<p>dataset is pandas dataframe. This is sklearn.cluster.KMeans</p>

<pre><code> km = KMeans(n_clusters = n_Clusters)

 km.fit(dataset)

 prediction = km.predict(dataset)
</code></pre>

<p>This is how I decide which entity belongs to which cluster:</p>

<pre><code> for i in range(len(prediction)):
     cluster_fit_dict[dataset.index[i]] = prediction[i]
</code></pre>

<p>This is how dataset looks:</p>

<pre><code> A 1 2 3 4 5 6
 B 2 3 4 5 6 7
 C 1 4 2 7 8 1
 ...
</code></pre>

<p>where A,B,C are indices</p>

<p>Is this the correct way of using k-means?</p>
";;1;;2015-01-19T02:17:48.563;8.0;28017091;2016-12-28T18:27:00.117;2016-12-28T18:27:00.117;;2683.0;;3718867.0;;1;17;<python><pandas><scikit-learn><cluster-analysis><k-means>;Will pandas dataframe object work with sklearn kmeans clustering?;15064.0
22583;22583;28135445.0;1.0;"<p>I need to concatenate two dataframes <code>df_a</code> and<code>df_b</code> having equal number of rows (<code>nRow</code>) one after another without any consideration of keys. This function is similar to <code>cbind</code> in <code>R programming language</code>. The number of columns in each dataframe may be different. </p>

<p>The resultant dataframe will have the same number of rows <code>nRow</code> and number of columns equal to the sum of number of columns in both the dataframes. In othe words, this is a blind columnar concatenation of two dataframes. </p>

<pre><code>import pandas as pd
dict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}
df_a = pd.DataFrame(dict_data)
dict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}
df_b = pd.DataFrame(dict_data)
</code></pre>
";;0;;2015-01-25T10:35:54.480;5.0;28135436;2017-07-16T09:43:32.027;2017-07-16T09:43:32.027;;5225453.0;;1140126.0;;1;18;<python><pandas><dataframe>;Concatenate rows of two dataframes in pandas;37294.0
22602;22602;;3.0;"<p>Many spreadsheets have formulas and formatting that Python tools for reading and writing Excel files cannot faithfully reproduce. That means that any file I want to create programmatically must be something I basically create from scratch, and then other Excel files (with the aforementioned sophistication) have to refer to that file (which creates a variety of other dependency issues).</p>

<p>My understanding of Excel file 'tabs' is that they're actually just a collection of XML files. Well, is it possible to use pandas (or one of the underlying read/write engines such as xlsxwriter or openpyxl to modify just one of the tabs, leaving other tabs (with more wicked stuff in there) intact?</p>

<p>EDIT: I'll try to further articulate the problem with an example.</p>

<ul>
<li>Excel Sheet test.xlsx has four tabs (aka worksheets): Sheet1, Sheet2, Sheet3, Sheet4</li>
<li>I read Sheet3 into a DataFrame (let's call it df) using pandas.read_excel()</li>
<li>Sheet1 and Sheet2 contain formulas, graphs, and various formatting that neither openpyxl nor xlrd can successfully parse, and Sheet4 contains other data. I don't want to touch those tabs at all.</li>
<li>Sheet2 actually has some references to cells on Sheet3</li>
<li>I make some edits to df and now want to write it back to sheet3, leaving the other sheets untouched (and the references to it from other worksheets in the workbook intact)</li>
</ul>

<p>Can I do that and, if so, how?</p>
";;0;;2015-01-25T22:38:05.990;4.0;28142420;2017-01-23T01:54:39.117;2015-02-01T13:43:57.037;;3127764.0;;3127764.0;;1;11;<python><excel><pandas>;Can Pandas read and modify a single Excel file worksheet (tab) without modifying the rest of the file?;3756.0
22644;22644;28161433.0;2.0;"<p>I have a pandas dataframe as follows:</p>

<pre><code>Symbol  Date
A       02/20/2015
A       01/15/2016
A       08/21/2015
</code></pre>

<p>I want to sort it by <code>Date</code>, but the column is just an <code>object</code>.</p>

<p>I tried to make the column a date object, but I ran into an issue where that format is not the format needed. The format needed is <code>2015-02-20,</code> etc.</p>

<p>So now I'm trying to figure out how to have numpy convert the 'American' dates into the ISO standard, so that I can make them date objects, so that I can sort by them.</p>

<p>How would I convert these american dates into ISO standard, or is there a more straight forward method I'm missing within pandas?</p>
";;0;;2015-01-27T00:35:37.240;3.0;28161356;2016-07-05T09:44:57.160;2015-08-06T13:38:01.903;;2071807.0;;3697550.0;;1;16;<python><pandas>;Sort Pandas Dataframe by Date;25652.0
22735;22735;28199556.0;6.0;"<p>I currently came up with some work arounds to count the number of missing values in a pandas <code>DataFrame</code>. Those are quite ugly and I am wondering if there is a better way to do it.</p>

<p>Let's create an example <code>DataFrame</code>:</p>

<pre><code>from numpy.random import randn
df = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],
               columns=['one', 'two', 'three'])
df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
</code></pre>

<p><img src=""https://i.stack.imgur.com/PiluQ.png"" alt=""enter image description here""></p>

<p>What I currently have is</p>

<p>a) Counting cells with missing values:</p>

<pre><code>&gt;&gt;&gt; sum(df.isnull().values.ravel())
9
</code></pre>

<p>b) Counting rows that have missing values somewhere:</p>

<pre><code>&gt;&gt;&gt; sum([True for idx,row in df.iterrows() if any(row.isnull())])
3
</code></pre>
";;0;;2015-01-28T18:17:45.780;4.0;28199524;2016-12-25T09:59:18.903;;;;;2489252.0;;1;11;<python><pandas><missing-data>;Best way to count the number of rows with missing values in a pandas DataFrame;14012.0
22796;22796;32558621.0;7.0;"<p>I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a <code>pandas</code> dataframe and run a regression with each.</p>

<p>Here's what I'm doing:</p>

<pre><code>all_data = {}
for ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:
    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')

prices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  
returns = prices.pct_change()
</code></pre>

<p>I know I can run a regression like this:</p>

<pre><code>regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()
</code></pre>

<p>but suppose I want to do this for each column in the dataframe. In particular, I want to regress FIUIX on FSTMX, and then FSAIX on FSTMX, and then FSAVX on FSTMX. After each regression I want to store the residuals.</p>

<p>I've tried various versions of the following, but I must be getting the syntax wrong:</p>

<pre><code>resids = {}
for k in returns.keys():
    reg = sm.OLS(returns[k],returns.FSTMX).fit()
    resids[k] = reg.resid
</code></pre>

<p>I think the problem is I don't know how to refer to the returns column by key, so <code>returns[k]</code> is probably wrong.</p>

<p>Any guidance on the best way to do this would be much appreciated. Perhaps there's a common pandas approach I'm missing.</p>
";;1;;2015-01-29T15:42:25.967;9.0;28218698;2017-07-29T17:07:08.427;;;;;368311.0;;1;42;<python><pandas><statsmodels>;How to iterate over columns of pandas dataframe to run regression;71906.0
22798;22798;;1.0;"<p>I wanted to bring this up, just because it's crazy weird.  Maybe Wes has some idea. The file is pretty regular: 1100 rows x ~3M columns, data are tab-separated, consisting solely of the integers 0, 1, and 2.  Clearly this is not expected.</p>

<p>If I prepopulate a dataframe as below, it consumes ~26GB of RAM.</p>

<pre><code>h = open(""ms.txt"")
header = h.readline().split(""\t"")
h.close()
rows=1100
df = pd.DataFrame(columns=header, index=range(rows), dtype=int)
</code></pre>

<p>System info:</p>

<ul>
<li>python 2.7.9 </li>
<li>ipython 2.3.1 </li>
<li>numpy 1.9.1 </li>
<li>pandas 0.15.2.</li>
</ul>

<p>Any ideas welcome.  </p>
";;23;;2015-01-29T16:38:24.460;0.0;28219902;2016-11-10T10:19:30.943;2016-11-10T10:19:30.943;;3730397.0;;1027577.0;;1;11;<python><parsing><pandas><numpy><ipython>;Pandas read_csv on 6.5 GB file consumes more than 170GB RAM;878.0
22826;22826;28227679.0;2.0;"<p>I have the following code:</p>

<pre><code>rows =[]
for dt in new_info:
    x =  dt['state']
    est = dt['estimates']

    col_R = [val['choice'] for val in est if val['party'] == 'Rep']
    col_D = [val['choice'] for val in est if val['party'] == 'Dem']

    incumb = [val['party'] for val in est if val['incumbent'] == True ]

    rows.append((x, col_R, col_D, incumb))
</code></pre>

<p>Now I want to convert my rows list into a pandas data frame. Structure of my  rows list is shown below and my list has 32 entries. </p>

<p><img src=""https://i.stack.imgur.com/qgt2s.png"" alt=""enter image description here""></p>

<p>When I convert this into a pandas data frame, I get the entries in the data frame as a list. :</p>

<pre><code>pd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  
</code></pre>

<p><img src=""https://i.stack.imgur.com/dx0sv.png"" alt=""enter image description here""></p>

<p>But I want my data frame like this </p>

<p><img src=""https://i.stack.imgur.com/Qb6Ce.png"" alt=""enter image description here""></p>

<p>The new info variable looks like this 
<img src=""https://i.stack.imgur.com/fY81U.png"" alt=""enter image description here""></p>
";;0;;2015-01-30T01:06:08.543;3.0;28227612;2015-01-30T22:53:20.757;2015-01-30T02:20:31.390;;4273266.0;;4273266.0;;1;13;<python><pandas>;how to convert a list into a pandas dataframe;70273.0
22831;22831;28229188.0;2.0;"<p>I'm trying to inner join DataFrame A to DataFrame B and am running into an error.</p>

<p>Here's my join statement:</p>

<pre><code>merged = DataFrameA.join(DataFrameB, on=['Code','Date'])
</code></pre>

<p>And here's the error:</p>

<pre><code>ValueError: len(left_on) must equal the number of levels in the index of ""right""
</code></pre>

<p>I'm not sure the column order matters (they aren't truly ""ordered"" are they?), but just in case, the DataFrames are organized like this:</p>

<pre><code>DataFrameA:  Code, Date, ColA, ColB, ColC, ..., ColG, ColH (shape: 80514, 8 - no index)
DataFrameB:  Date, Code, Col1, Col2, Col3, ..., Col15, Col16 (shape: 859, 16 - no index)
</code></pre>

<p>Do I need to correct my join statement?  Or is there another, better way to get the intersection (or inner join) of these two DataFrames?</p>
";;2;;2015-01-30T03:31:13.960;;28228781;2017-02-03T13:02:23.657;2015-01-30T03:38:44.600;;2166252.0;;4504093.0;;1;15;<python><join><pandas><inner-join>;Python Pandas inner join;13222.0
22844;22844;28236391.0;1.0;"<p>Suppose I have a column like so:</p>

<pre><code>a   b  
1   5   
1   7
2   3
1   3
2   5
</code></pre>

<p>I want to sum up the values for <code>b</code> where <code>a = 1</code>, for example. This would give me <code>5 + 7 + 3 = 15</code>.</p>

<p>How do I do this in pandas?</p>
";;1;;2015-01-30T12:48:32.360;7.0;28236305;2016-12-11T17:21:25.870;2016-12-11T17:21:25.870;;3923281.0;;3375198.0;;1;21;<python><pandas><dataframe><data-analysis>;How do I sum values in a column that match a given condition using pandas?;27037.0
22915;22915;28267291.0;2.0;"<p>Let's assume that I have an XML like this:</p>

<pre><code>&lt;type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com""&gt;
    &lt;count=""N""&gt;
        &lt;KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...] [...]
]]&gt;
        &lt;/document&gt;
    &lt;/documents&gt;
&lt;/author&gt;
</code></pre>

<p>I would like to read this xml file and convert it to a pandas dataframe:</p>

<pre><code>key                                         type     language    feature            web                             data
e95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]
e95324a9a6c790ecb95e46cf15bE232ee517651     XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]
19e71144c50a8b9160b3cvdf2324f0955e906fce    XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]
21d4af9021a174f61b8erf284606c74d9e42        XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]
28a45eb2460823499763d70vdf9ca00ddbb665       XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]
</code></pre>

<p>This is what I all ready tried, but I am getting some errors and probably there is a more efficient way for doing this task:</p>

<pre><code>from lxml import objectify
import pandas as pd

path = 'file_path'
xml = objectify.parse(open(path))
root = xml.getroot()
root.getchildren()[0].getchildren()
df = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))

for i in range(0,len(xml)):
    obj = root.getchildren()[i].getchildren()
    row = dict(zip(['key','type', 'language', 'feature', 'web', 'data'], [obj[0].text, obj[1].text]))
    row_s = pd.Series(row)
    row_s.name = i
    df = df.append(row_s)
</code></pre>

<p>Could anybody provide me a better aproach for this problem?, thanks in advance.</p>
";;2;;2015-02-01T03:58:53.723;1.0;28259301;2017-06-06T06:16:58.953;;;;;4506080.0;;1;16;<python><xml><python-2.7><parsing><pandas>;How to convert an XML file to nice pandas dataframe?;22013.0
22931;22931;28272238.0;6.0;"<p>I have a Series like this after doing groupby('name') and used mean() function on other column</p>

<pre><code>name
383      3.000000
663      1.000000
726      1.000000
737      9.000000
833      8.166667
</code></pre>

<p>Could anyone please show me how to filter out the rows with 1.000000 mean values? Thank you and I greatly appreciate your help.</p>
";;1;;2015-02-02T06:21:20.667;4.0;28272137;2017-04-13T20:36:34.367;2015-02-02T06:22:55.613;user554546;;;3926352.0;;1;13;<python><pandas>;Pandas How to filter a Series;16334.0
22978;22978;;1.0;"<p>In Pandas, I am doing:</p>

<pre><code>bp = p_df.groupby('class').plot(kind='kde')
</code></pre>

<p><code>p_df</code> is a <code>dataframe</code> object.</p>

<p>However, this is producing two plots, one for each class.
How do I force one plot with both classes in the same plot?</p>
";;0;;2015-02-03T06:54:23.450;4.0;28293028;2017-05-28T19:49:21.840;2017-05-28T19:49:21.840;;3923281.0;;4522999.0;;1;18;<python><pandas><plot>;Plotting grouped data in same plot using Pandas;14669.0
23031;23031;28312011.0;1.0;"<p>I want to find rows that contain a string, like so:</p>

<pre><code>DF[DF.col.str.contains(""foo"")]
</code></pre>

<p>However, this fails because some elements are NaN:</p>

<blockquote>
  <p>ValueError: cannot index with vector containing NA / NaN values</p>
</blockquote>

<p>So I resort to the obfuscated</p>

<pre><code>DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]
</code></pre>

<p>Is there a better way?</p>
";;0;;2015-02-04T00:57:17.270;1.0;28311655;2015-02-04T01:35:31.080;;;;;832188.0;;1;24;<pandas>;Ignoring NaNs with str.contains;6219.0
23157;23157;28371611.0;3.0;"<p>I have a sorting request per example below.</p>

<p>Do i need to reset_index(), then sort() and then set_index() or is there a slick way to do this?</p>

<pre><code>l = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]
df = pd.DataFrame(l,columns = ['idx1','idx2','col1'])
df.set_index(['idx1','idx2'],inplace=True)

# assume data has been received like this...
print df

           col1
idx1 idx2      
1    A       99
     B      102
     C      105
     D       97
2    A       19
     B       14
     C       10
     D       17

# I'd like to sort descending on col1, partitioning within index level = 'idx2'

           col1
idx1 idx2      
1    C      105
     B      102
     A       99
     D       97

2    A       19
     D       17
     B       14
     C       10
</code></pre>

<hr>

<p>Thank you for the answer
Note I change the data slightly:</p>

<pre><code>l = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]
df = pd.DataFrame(l,columns = ['idx1','idx2','col1'])
df.set_index(['idx1','idx2'],inplace=True)
df = df.sort_index(by='col1', ascending=False)
</code></pre>

<p>however the output is </p>

<pre><code>idx1 idx2      
1    C      105
     A       99
     D       97
2    A       19
     D       17
     B       14
1    B       11
2    C       10
</code></pre>

<p>i would have wanted it to be </p>

<pre><code>idx1 idx2      
1    C      105
     A       99
     D       97
     B       11

2    A       19
     D       17
     B       14
     C       10
</code></pre>
";;1;;2015-02-06T17:22:39.357;;28371308;2017-07-18T10:45:16.973;2017-07-18T10:45:16.973;;3861108.0;;2028710.0;;1;17;<python><pandas><sorting>;Sort by column within multi index level in pandas;1573.0
23198;23198;28384887.0;4.0;"<p>I'm finding it difficult to understand how to fix a Pipeline I created (read: largely pasted from a tutorial). It's python 3.4.2:</p>

<pre><code>df = pd.DataFrame
df = DataFrame.from_records(train)

test = [blah1, blah2, blah3]

pipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])

pipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))
predicted = pipeline.predict(test)
</code></pre>

<p>When I run it, I get:</p>

<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>This is for the line <code>pipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))</code>.</p>

<p>I've experimented a lot with solutions through numpy, scipy, and so forth, but I still don't know how to fix it. And yes, similar questions have come up before, but not inside a pipeline.
Where is it that I have to apply <code>toarray</code> or <code>todense</code>?</p>
";;0;;2015-02-07T16:43:17.853;9.0;28384680;2016-10-04T21:22:06.553;;;;;4540977.0;;1;14;<python><numpy><pandas><scikit-learn>;Scikit-Learn's Pipeline: A sparse matrix was passed, but dense data is required;7573.0
23274;23274;28417338.0;3.0;"<p>When using R it's handy to load ""practice"" datasets using </p>

<pre><code>data(iris)
</code></pre>

<p>or</p>

<pre><code>data(mtcars)
</code></pre>

<p>Is there something similar for Pandas? I know I can load using any other method, just curious if there's anything builtin</p>
";;1;;2015-02-09T19:02:07.153;6.0;28417293;2017-06-06T10:31:47.083;;;;;414104.0;;1;19;<pandas>;Sample Datasets in Pandas;7140.0
23390;23390;28466662.0;1.0;"<p>Suppose I have a data frame <code>data</code> with strings that I want converted to indicators. I use <code>pandas.get_dummies(data)</code> to convert this to a dataset that I can now use for building a model.</p>

<p>Now I have a single new observation that I want to run through my model. Obviously I can't use <code>pandas.get_dummies(new_data)</code> because it doesn't contain all of the classes and won't make the same indicator matrices. Is there a good way to do this?</p>
";;0;;2015-02-11T22:18:11.460;2.0;28465633;2016-11-02T22:50:35.160;;;;;2643154.0;;1;11;<python><pandas>;Easy way to apply transformation from `pandas.get_dummies` to new data?;1296.0
23495;23495;28538738.0;5.0;"<p>I have some data and when I import it I get the following unneeded columns I'm looking for an easy way to delete all of these</p>

<pre><code>   'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',
   'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',
   'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',
   'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',
   'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',
   'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',
   'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',
   'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',
   'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',
   'Unnamed: 60'
</code></pre>

<p>They are indexed by 0-indexing so I tried something like </p>

<pre><code>    df.drop(df.columns[[22, 23, 24, 25, 
    26, 27, 28, 29, 30, 31, 32 ,55]], axis=1, inplace=True)
</code></pre>

<p>But this isn't very efficient. I tried writing some for loops but this struck me as bad Pandas behaviour. Hence i ask the question here.</p>

<p>I've seen some examples which are similar (<a href=""https://stackoverflow.com/questions/26347412/drop-multiple-columns-pandas"">Drop multiple columns pandas</a>) but this doesn't answer my question. </p>
";;3;;2015-02-16T09:47:30.270;5.0;28538536;2017-07-28T15:16:51.560;2017-07-28T15:16:51.560;;2610971.0;;2610971.0;;1;19;<python><pandas>;Deleting multiple columns in Pandas;22682.0
23599;23599;28590865.0;1.0;"<p>I'm trying to take a dataframe and transform it into a partcular json format.</p>

<p>Here's my dataframe example:</p>

<pre><code>DataFrame name: Stops
id    location
0     [50, 50]
1     [60, 60]
2     [70, 70]
3     [80, 80]
</code></pre>

<p>Here's the json format I'd like to transform into:</p>

<pre><code>""stops"":
[
{
    ""id"": 1,
    ""location"": [50, 50]
},
{
    ""id"": 2,
    ""location"": [60, 60]
},
... (and so on)
]
</code></pre>

<p>Notice it's a list of dicts.  I have it nearly there with the following code:</p>

<p><code>df.reset_index().to_json(orient='index)</code></p>

<p>However, that line also includes the index like this:</p>

<pre><code>""stops"":
{
""0"":
    {
        ""id"": 0,
        ""location"": [50, 50]
    },
""1"":
    {
        ""id"": 1,
        ""location"": [60, 60]
    },
... (and so on)
}
</code></pre>

<p>Notice this is a dict of dicts and also includes the index twice (in the first dict and as the ""id"" in the second dict!  Any help would be appreciated.</p>
";;0;;2015-02-18T18:04:14.847;7.0;28590663;2015-02-18T18:13:49.687;;;;;3146833.0;;1;16;<python><json><pandas>;Pandas dataframe to json without index;12308.0
23611;23611;28595765.0;1.0;"<p>suppose I have two dataframes: </p>

<pre><code>import pandas
....
....
test1 = pandas.DataFrame([1,2,3,4,5])
....
....
test2 = pandas.DataFrame([4,2,1,3,7])
....
</code></pre>

<p>I tried <code>test1.append(test2)</code> but it is the equivalent of R's <code>rbind</code>.</p>

<p>How can I combine the two as two columns of a dataframe similar to the <code>cbind</code> function in R? </p>
";;0;;2015-02-18T23:07:08.940;2.0;28595701;2015-08-29T04:34:51.587;2015-08-29T04:34:51.587;;202229.0;;2526657.0;;1;16;<python-3.x><pandas><concat><cbind>;pandas equivalent of R's cbind (concatenate/stack vectors vertically);12260.0
23757;23757;28652153.0;3.0;"<p>I am trying to unstack a multi-index with pandas and I am keep getting:</p>

<pre><code>ValueError: Index contains duplicate entries, cannot reshape
</code></pre>

<p>Given a dataset with four columns:</p>

<ul>
<li>id (string)</li>
<li>date (string)</li>
<li>location (string)</li>
<li>value (float)</li>
</ul>

<p>I first set a three-level multi-index:</p>

<pre><code>In [37]: e.set_index(['id', 'date', 'location'], inplace=True)

In [38]: e
Out[38]: 
                                    value
id           date       location       
id1          2014-12-12 loc1        16.86
             2014-12-11 loc1        17.18
             2014-12-10 loc1        17.03
             2014-12-09 loc1        17.28
</code></pre>

<p>Then I try to unstack the location:</p>

<pre><code>In [39]: e.unstack('location')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-39-bc1e237a0ed7&gt; in &lt;module&gt;()
----&gt; 1 e.unstack('location')
...
C:\Anaconda\envs\sandbox\lib\site-packages\pandas\core\reshape.pyc in _make_selectors(self)
    143 
    144         if mask.sum() &lt; len(self.index):
--&gt; 145             raise ValueError('Index contains duplicate entries, '
    146                              'cannot reshape')
    147 

ValueError: Index contains duplicate entries, cannot reshape
</code></pre>

<p>What is going on here?</p>
";;4;;2015-02-21T20:40:22.533;4.0;28651079;2017-01-02T20:28:33.297;;;;;348501.0;;1;17;<python><pandas>;Pandas unstack problems: ValueError: Index contains duplicate entries, cannot reshape;11491.0
23787;23787;;1.0;"<p>Is there a way I could use a scipy function like <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"" rel=""noreferrer""><code>norm.cdf</code></a> <strong>in place</strong> on a <code>numpy.array</code> (or <code>pandas.DataFrame</code>), using a variant of <code>numpy.apply</code>, <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_along_axis.html"" rel=""noreferrer""><code>numpy.apply_along_axs</code></a>, etc?</p>

<hr>

<p>The background is, I have a table of z-score values that I would like to convert to CDF values of the norm distribution. I'm currently using <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"" rel=""noreferrer""><code>norm.cdf</code></a> from <code>scipy</code> for this.</p>

<p>I'm currently manipulating a dataframe that has non-numeric values.</p>

<pre><code>      Name      Val1      Val2      Val3      Val4 
0        A -1.540369 -0.077779  0.979606 -0.667112   
1        B -0.787154  0.048412  0.775444 -0.510904   
2        C -0.477234  0.414388  1.250544 -0.411658   
3        D -1.430851  0.258759  1.247752 -0.883293   
4        E -0.360181  0.485465  1.123589 -0.379157
</code></pre>

<p>(Making the <code>Name</code> variable an index is a solution, but in my actual dataset, the names are not alphabetical characters.)</p>

<p>To modify only the numeric data, I'm using <a href=""http://nullege.com/codes/search/pandas.DataFrame._data.get_numeric_data"" rel=""noreferrer""><code>df._get_numeric_data()</code></a> a private function that returns a dataframe containing a dataframe's numeric data. However, there is no <code>set</code> function. Hence, if I call</p>

<pre><code>norm.cdf(df._get_numeric_data)
</code></pre>

<p>this won't change <code>df</code>'s original data.</p>

<p>I'm trying to circumvent this by applying <code>norm.cdf</code> to the numeric dataframe <strong>inplace</strong>, so this changes my original dataset.</p>
";;1;;2015-02-22T18:01:39.423;3.0;28661258;2015-02-27T23:58:09.427;2015-02-22T18:08:02.823;;2014591.0;;2014591.0;;1;11;<python><pandas><scipy><vectorization>;Python Pandas: Apply function to dataframe in place;10910.0
23833;23833;28677358.0;4.0;"<p><strong>Problem:</strong></p>

<p>What'd I like to do is step-by-step reduce a value in a <code>Series</code> by a continuously decreasing base figure.</p>

<p>I'm not sure of the terminology for this - I did think I could do something with <code>cumsum</code> and <code>diff</code> but I think I'm leading myself on a wild goose chase there...</p>

<p><strong>Starting code:</strong></p>

<pre><code>import pandas as pd

ALLOWANCE = 100
values = pd.Series([85, 10, 25, 30])
</code></pre>

<p><strong>Desired output:</strong></p>

<pre><code>desired = pd.Series([0, 0, 20, 30])
</code></pre>

<p><strong>Rationale:</strong></p>

<p>Starting with a base of <code>ALLOWANCE</code> - each value in the <code>Series</code> is reduced by the amount remaining, as is the allowance itself, so the following steps occur:</p>

<ul>
<li>Start with 100, we can completely remove <code>85</code> so it becomes <code>0</code>, we now have <code>15</code> left as <code>ALLOWANCE</code></li>
<li>The next value is <code>10</code> and we still have <code>15</code> available, so this becomes <code>0</code> again and we have <code>5</code> left.</li>
<li>The next value is <code>25</code> - we only have <code>5</code> left, so this becomes <code>20</code> and now we have no further allowance.</li>
<li>The next value is <code>30</code>, and since there's no allowance, the value remains as <code>30</code>.</li>
</ul>
";;1;;2015-02-23T15:13:16.033;;28676916;2015-03-10T16:05:02.537;2015-03-07T12:43:53.570;;100297.0;;1252759.0;;1;14;<python><pandas>;Calculate new value based on decreasing value;632.0
23847;23847;28680078.0;3.0;"<p>I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.</p>

<p>For example, I want to drop all rows which have the string ""XYZ"" as a substring in the column C of the data frame.</p>

<p>Can this be implemented in an efficient way using .drop() method?</p>
";;0;;2015-02-23T17:43:01.053;7.0;28679930;2017-08-14T18:58:12.800;;;;;492372.0;;1;23;<python><pandas>;How to drop rows from pandas data frame that contains a particular string in a particular column?;20370.0
24058;24058;;3.0;"<p>Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in <code>Pandas</code>. I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options. </p>

<p>Is there a reason why I should ever use <code>.loc</code> or <code>.iloc</code> over the most general option <code>.ix</code>? </p>

<p>I understand that <code>.loc</code>, <code>iloc</code>, <code>at</code>, and <code>iat</code> may provide some guaranteed correctness that <code>.ix</code> can't offer, but I've also read where <code>.ix</code> tends to be the fastest solution across the board. </p>

<p>Can someone please explain the real world, best practices reasoning behind utilizing anything other than <code>.ix</code>?</p>
";;5;;2015-02-27T04:12:39.900;39.0;28757389;2017-05-14T21:16:43.390;2015-07-31T10:35:35.167;;3923281.0;;4613042.0;;1;71;<python><pandas>;Loc vs. iloc vs. ix vs. at vs. iat?;31094.0
24092;24092;28783971.0;2.0;"<p>I have a <code>pandas.DataFrame</code> called <code>df</code> which has an automatically generated index, with a column <code>dt</code>:</p>

<pre><code>df['dt'].dtype, df['dt'][0]
# (dtype('&lt;M8[ns]'), Timestamp('2014-10-01 10:02:45'))
</code></pre>

<p>What I'd like to do is create a new column truncated to hour precision. I'm currently using:</p>

<pre><code>df['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))
</code></pre>

<p>This works, so that's fine. However, I've an inkling there's some nice way using <code>pandas.tseries.offsets</code> or creating a <code>DatetimeIndex</code> or similar. </p>

<p><strong>So if possible, is there some <code>pandas</code> wizardry to do this?</strong> </p>
";;2;;2015-02-27T20:03:51.560;5.0;28773342;2017-07-06T20:38:19.110;2017-04-28T16:59:44.890;;3923281.0;;1252759.0;;1;13;<python><pandas><datetime><dataframe>;Truncate `TimeStamp` column to hour precision in pandas `DataFrame`;4070.0
24419;24419;28902170.0;8.0;"<p>I've two pandas data frames which have some rows in common.</p>

<p>Suppose dataframe2 is a subset of dataframe1.</p>

<p><strong>How can I get the rows of dataframe1 which are not in dataframe2?</strong></p>

<pre><code>df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) 
df2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})
</code></pre>
";;0;;2015-03-06T15:10:28.400;13.0;28901683;2017-08-02T23:01:14.633;2015-03-06T15:30:26.847;;704848.0;;3965770.0;;1;44;<python><pandas>;pandas get rows which are NOT in other dataframe;26226.0
24506;24506;;1.0;"<p>I got stuck on something that feels like should be relatively easy. The code I bring below is a sample based on a larger project I'm working on. I saw no reason to post all the details, so please accept the data structures I bring as is.</p>

<p>Basically, I'm creating a bar chart, and I just can figure out how to add value labels on the bars (in the center of the bar, or just above it). Been looking at samples around the web but with no success implementing on my own code. I believe the solution is either with 'text' or 'annotate', but I:
a) don't know which one to use (and generally speaking, haven't figured out when to use which).
b) can't see to get either to present the value labels.
Would appreciate your help, my code below.
Thanks in advance!</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.mpl_style', 'default') 
%matplotlib inline


frequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data

freq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.

x_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]

# now to plot the figure...
plt.figure(figsize=(12, 8))
fig = freq_series.plot(kind='bar')
fig.set_title(""Amount Frequency"")
fig.set_xlabel(""Amount ($)"")
fig.set_ylabel(""Frequency"")
fig.set_xticklabels(x_labels)
</code></pre>
";;0;;2015-03-08T20:00:53.223;8.0;28931224;2017-02-16T23:19:00.707;;;;;2594546.0;;1;28;<python><python-2.7><pandas><matplotlib><data-visualization>;Adding value labels on a matplotlib bar chart;35505.0
24649;24649;28986536.0;1.0;"<p>I have a column in my dataframe like this:</p>

<pre><code>range
(2,30)
(50,290)
(400,1000)
... 
</code></pre>

<p>and I want to replace the ',' comma with '-' dash. I am currently using this method but nothing is changed. </p>

<pre><code>org_info_exc['range'].replace(',','-',inplace=True)
</code></pre>

<p>Can anybody help?</p>
";;0;;2015-03-11T12:19:59.483;3.0;28986489;2015-03-11T13:16:46.777;;;;;2058811.0;;1;11;<python><replace><pandas><dataframe>;Python Pandas: How to replace a characters in a column of a dataframe?;15525.0
25105;25105;29177664.0;2.0;"<p>I am working with this Pandas DataFrame in Python 2.7.</p>

<pre><code>File    heat    Farheit Temp_Rating
   1    YesQ    75      N/A
   1    NoR     115     N/A
   1    YesA    63      N/A
   1    NoT     83      41
   1    NoY     100     80
   1    YesZ    56      12
   2    YesQ    111     N/A
   2    NoR     60      N/A
   2    YesA    19      N/A
   2    NoT     106     77
   2    NoY     45      21
   2    YesZ    40      54
   3    YesQ    84      N/A
   3    NoR     67      N/A
   3    YesA    94      N/A
   3    NoT     68      39
   3    NoY     63      46
   3    YesZ    34      81
</code></pre>

<p>I need to replace all NaNs in the <code>Temp_Rating</code> column with the value from the <code>Farheit</code> column.</p>

<p>This is what I need:</p>

<pre><code>File        heat    Observation
   1        YesQ    75
   1        NoR     115
   1        YesA    63
   1        YesQ    41
   1        NoR     80
   1        YesA    12
   2        YesQ    111
   2        NoR     60
   2        YesA    19
   2        NoT     77
   2        NoY     21
   2        YesZ    54
   3        YesQ    84
   3        NoR     67
   3        YesA    94
   3        NoT     39
   3        NoY     46
   3        YesZ    81
</code></pre>

<p>If I do a Boolean selection, I can pick out only one of these columns at a time. The problem is if I then try to join them, I am not able to do this while preserving the correct order.</p>

<p>How can I only find <code>Temp_Rating</code> rows with the <code>NaN</code>s and replace them with the value in the same row of the <code>Farheit</code> column?</p>
";;0;;2015-03-20T23:43:01.930;11.0;29177498;2017-07-07T14:40:49.313;;;;;4057186.0;;1;28;<python-2.7><pandas><dataframe><nan>;Python Pandas replace NaN in one column with value from corresponding row of second column;14402.0
25156;25156;29218694.0;1.0;"<p>I have created a TimeSeries in pandas:</p>

<pre><code>In [346]: from datetime import datetime

In [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),

 .....: datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]

In [348]: ts = Series(np.random.randn(6), index=dates)

In [349]: ts

Out[349]: 

2011-01-02 0.690002

2011-01-05 1.001543

2011-01-07 -0.503087

2011-01-08 -0.622274

2011-01-10 -0.921169

2011-01-12 -0.726213
</code></pre>

<p>I'm following on the example from 'Python for Data Analysis' book. </p>

<p>In the following paragraph, the author checks the index type:</p>

<pre><code>In [353]: ts.index.dtype

Out[353]: dtype('datetime64[ns]')
</code></pre>

<p>When I do exactly the same operation in the console I get:</p>

<pre><code>ts.index.dtype
dtype('&lt;M8[ns]')
</code></pre>

<p>What is the difference between two types <code>'datetime64[ns]'</code> and <code>'&lt;M8[ns]'</code> ?</p>

<p>And why do I get a different type?</p>
";;1;;2015-03-23T09:17:10.037;5.0;29206612;2017-06-18T18:44:58.837;2015-03-23T09:47:52.597;;653364.0;;2758414.0;;1;19;<python><numpy><pandas><datetime64>;Difference between data type 'datetime64[ns]' and '<M8[ns]'?;10521.0
25190;25190;29233999.0;1.0;"<p>I'm a beginner of Spark-DataFrame API. </p>

<p>I use this code to load csv tab-separated into Spark Dataframe</p>

<pre><code>lines = sc.textFile('tail5.csv')
parts = lines.map(lambda l : l.strip().split('\t'))
fnames = *some name list*
schemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])
ddf = sqlContext.createDataFrame(parts,schemaData)
</code></pre>

<p>Suppose I create DataFrame with Spark from new files, and convert it to pandas using built-in method toPandas(),</p>

<ul>
<li>Does it store the Pandas object to local memory?</li>
<li>Does Pandas low-level computation handled all by Spark?</li>
<li>Does it exposed all pandas dataframe functionality?(I guess yes)</li>
<li>Can I convert it toPandas and just be done with it, without so much touching DataFrame API? </li>
</ul>
";;0;;2015-03-24T06:22:11.687;6.0;29226210;2015-03-24T16:01:58.347;2015-03-24T16:01:58.347;;564538.0;;1296136.0;;1;21;<python><pandas><apache-spark><pyspark>;What is the Spark DataFrame method `toPandas` actually doing?;16117.0
25216;25216;29233885.0;2.0;"<p>I have a dataframe that looks like the following</p>

<pre><code>   color  x   y
0    red  0   0
1    red  1   1
2    red  2   2
3    red  3   3
4    red  4   4
5    red  5   5
6    red  6   6
7    red  7   7
8    red  8   8
9    red  9   9
10  blue  0   0
11  blue  1   1
12  blue  2   4
13  blue  3   9
14  blue  4  16
15  blue  5  25
16  blue  6  36
17  blue  7  49
18  blue  8  64
19  blue  9  81
</code></pre>

<p>I ultimately want two lines, one blue, one red.  The red line should essentially be y=x and the blue line should be y=x^2</p>

<p>When I do the following:</p>

<pre><code>df.plot(x='x', y='y')
</code></pre>

<p>The output is this:</p>

<p><img src=""https://i.stack.imgur.com/SlPtV.png"" alt=""""></p>

<p>Is there a way to make pandas know that there are two sets?  And group them accordingly.  I'd like to be able to specify the column 'color' as the set differentiator</p>
";;0;;2015-03-24T12:57:58.880;4.0;29233283;2015-03-24T13:29:24.083;;;;;2236401.0;;1;15;<python><pandas><plot>;Plotting multiple lines with pandas dataframe;11996.0
25241;25241;29242900.0;1.0;"<p>I've been looking around for ways to select columns through the python documentation and the forums but every example on indexing columns are too simplistic. </p>

<p>Suppose I have a 10 x 10 dataframe</p>

<pre><code>df = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])
</code></pre>

<p>So far, all the documentations gives is just a simple example of indexing like</p>

<pre><code>subset = df.loc[:,'A':'C']
</code></pre>

<p>or</p>

<pre><code>subset = df.loc[:,'C':]
</code></pre>

<p>But I get an error when I try index multiple, non-sequential columns, like this</p>

<pre><code>subset = df.loc[:,('A':'C', 'E')]
</code></pre>

<p>How would I index in Pandas if I wanted to select column A to C, E, and G to I? It appears that this logic will not work</p>

<pre><code>subset = df.loc[:,('A':'C', 'E', 'G':'I')]
</code></pre>

<p>I feel that the solution is pretty simple, but I can't get around this error. Thanks!</p>
";;2;;2015-03-24T20:03:23.373;3.0;29241836;2015-04-23T15:22:16.700;;;;;4487457.0;;1;15;<python><pandas>;select multiple columns by labels pandas;20035.0
25377;25377;29287549.0;2.0;"<p>How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do <code>usecols</code></p>
";;4;;2015-03-26T19:27:49.120;3.0;29287224;2015-03-26T19:48:50.207;;;;;308827.0;;1;31;<python><pandas>;Pandas read in table without headers;32704.0
25470;25470;29314880.0;3.0;"<p>I have a pd.DataFrame that was created by parsing some excel spreadsheets. A column of which has empty cells. For example, below is the output for the frequency of that column, 32320 records have missing values for Tenant.</p>

<pre><code>   In [67]: value_counts(Tenant,normalize=False)
   Out[67]:
                              32320
   Thunderhead                8170
   Big Data Others            5700
   Cloud Cruiser              5700
   Partnerpedia               5700
   Comcast                    5700
   SDP                        5700
   Agora                      5700
   dtype: int64
</code></pre>

<p>I am trying to drop rows where Tenant is missing, however isnull option does not recognize the missing values. </p>

<pre><code>   In [71]: df['Tenant'].isnull().sum()
   Out[71]: 0
</code></pre>

<p>The column has data type ""Object"". What is happening in this case ? How can I drop records where Tenant is missing?</p>
";;0;;2015-03-28T05:30:48.690;5.0;29314033;2017-05-31T18:10:48.120;2015-03-28T16:23:31.630;;202229.0;;3923448.0;;1;14;<python><pandas>;Python Pandas DataFrame remove Empty Cells;20419.0
25506;25506;29334672.0;2.0;"<p>I have a very big csv file so that I can not read them all into the memory. I only want to read and process a few lines in it. So I am seeking a function in Pandas which could handle this task, which the basic python can handle this well:</p>

<pre><code>with open('abc.csv') as f:
    line = f.readline()
    # pass until it reaches a particular line number....
</code></pre>

<p>However, if I do this in pandas, I always read the first line:</p>

<pre><code>datainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )
datainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )
</code></pre>

<p>I am looking for some easier way to handle this task in pandas. For example, if I want to read rows from 1000 to 2000. How can I do this quickly? </p>

<p>The reason I want to use pandas, because I want to read data into the data frame</p>
";;0;;2015-03-29T20:29:18.227;1.0;29334463;2017-07-06T20:17:18.733;2017-01-11T07:01:55.167;;562769.0;;3804098.0;;1;12;<python><pandas>;How can I partially read a huge CSV file?;10245.0
25611;25611;29370182.0;3.0;"<p>I am creating a dataframe from a csv as follows:</p>

<pre><code>stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)
</code></pre>

<p>The dataframe has a date column. Is there a way to create a new dataframe (or just overwrite the existing one) which only containes rows that fall between a specific date range?</p>
";;0;;2015-03-31T13:38:06.717;22.0;29370057;2017-06-19T14:36:51.780;2015-03-31T13:53:06.027;;4099593.0;;4169229.0;;1;42;<python><pandas>;Select dataframe rows between two dates;55170.0
25613;25613;29370709.0;3.0;"<p>I have the following <code>DataFrame</code>, where <code>Track ID</code> is the row index. How can I split the string in the <code>stats</code> column into 5 columns of numbers?</p>

<pre><code>Track ID    stats
14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)
28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)
42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)
56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)
70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)
84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)
</code></pre>

<p>Thanks in advance,</p>

<p>T</p>
";;4;;2015-03-31T13:51:07.010;4.0;29370211;2017-01-26T13:17:16.540;;;;;4593423.0;;1;16;<python><pandas><split>;pandas split string into columns;20576.0
25807;25807;29432741.0;4.0;"<p>I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using <code>dataframe.corr()</code> function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?</p>
";;0;;2015-04-03T12:57:22.830;17.0;29432629;2017-08-22T19:07:16.187;2017-08-22T19:07:16.187;;3670871.0;;1983512.0;;1;33;<python><pandas><matplotlib><data-visualization><information-visualization>;Correlation matrix using pandas;39252.0
25880;25880;29461151.0;2.0;"<p>I have an excel file (.xls format) with 5 sheets, I want to replace the contents of sheet 5 with contents of my pandas data frame.</p>
";;0;;2015-04-05T16:24:49.843;4.0;29459461;2015-04-05T19:10:12.690;;;;;4450260.0;;1;11;<python><pandas>;Pandas Dataframe to excel sheet;18975.0
25898;25898;29464365.0;2.0;"<p>I have this DataFrame (<code>df1</code>) in Pandas:</p>

<pre><code>df1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))
print df1

       A         B         C         D
0.860379  0.726956  0.394529  0.833217
0.014180  0.813828  0.559891  0.339647
0.782838  0.698993  0.551252  0.361034
0.833370  0.982056  0.741821  0.006864
0.855955  0.546562  0.270425  0.136006
0.491538  0.445024  0.971603  0.690001
0.911696  0.065338  0.796946  0.853456
0.744923  0.545661  0.492739  0.337628
0.576235  0.219831  0.946772  0.752403
0.164873  0.454862  0.745890  0.437729
</code></pre>

<p>I would like to check if any row (all columns) from another dataframe (<code>df2</code>) are present in <code>df1</code>. Here is <code>df2</code>:</p>

<pre><code>df2 = df1.ix[4:8]
df2.reset_index(drop=True,inplace=True)
df2.loc[-1] = [2, 3, 4, 5]
df2.loc[-2] = [14, 15, 16, 17]
df2.reset_index(drop=True,inplace=True)
print df2

           A         B         C         D
    0.855955  0.546562  0.270425  0.136006
    0.491538  0.445024  0.971603  0.690001
    0.911696  0.065338  0.796946  0.853456
    0.744923  0.545661  0.492739  0.337628
    0.576235  0.219831  0.946772  0.752403
    2.000000  3.000000  4.000000  5.000000
   14.000000 15.000000 16.000000 17.000000
</code></pre>

<p>I tried using <code>df.lookup</code> to search for one row at a time. I did it this way:</p>

<pre><code>list1 = df2.ix[0].tolist()
cols = df1.columns.tolist()
print df1.lookup(list1, cols)
</code></pre>

<p>but I got this error message:</p>

<pre><code>  File ""C:\Users\test.py"", line 19, in &lt;module&gt;
    print df1.lookup(list1, cols)
  File ""C:\python27\lib\site-packages\pandas\core\frame.py"", line 2217, in lookup
    raise KeyError('One or more row labels was not found')
KeyError: 'One or more row labels was not found'
</code></pre>

<p>I also tried <code>.all()</code> using:</p>

<pre><code>print (df2 == df1).all(1).any()
</code></pre>

<p>but I got this error message:</p>

<pre><code>  File ""C:\Users\test.py"", line 12, in &lt;module&gt;
    print (df2 == df1).all(1).any()
  File ""C:\python27\lib\site-packages\pandas\core\ops.py"", line 884, in f
    return self._compare_frame(other, func, str_rep)
  File ""C:\python27\lib\site-packages\pandas\core\frame.py"", line 3010, in _compare_frame
    raise ValueError('Can only compare identically-labeled '
ValueError: Can only compare identically-labeled DataFrame objects
</code></pre>

<p>I also tried <code>isin()</code> like this:</p>

<pre><code>print df2.isin(df1)
</code></pre>

<p>but I got <code>False</code> everywhere, which is not correct:</p>

<pre><code>    A      B      C      D
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
False  False  False  False
</code></pre>

<p>Is it possible to search for a set of rows in a DataFrame, by comparing it to another dataframe's rows?</p>

<p>EDIT:
Is is possible to drop <code>df2</code> rows if those rows are also present in <code>df1</code>?</p>
";;0;;2015-04-06T01:30:50.050;5.0;29464234;2015-04-11T06:56:05.583;2015-04-06T20:39:08.817;;4057186.0;;4057186.0;;1;13;<python><pandas><rows><matching>;Compare Python Pandas DataFrames for matching rows;25048.0
25967;25967;29499109.0;1.0;"<p>Assuming i have a <code>DataFrame</code> that looks like this:</p>

<pre><code>Hour | V1 | V2 | A1 | A2
 0   | 15 | 13 | 25 | 37  
 1   | 26 | 52 | 21 | 45 
 2   | 18 | 45 | 45 | 25 
 3   | 65 | 38 | 98 | 14
</code></pre>

<p>Im trying to create a bar plot to compare columns <code>V1</code> and <code>V2</code> by the <code>Hour</code>.
When I do:</p>

<pre><code>import matplotlib.pyplot as plt
ax = df.plot(kind='bar', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)
ax.set_xlabel(""Hour"",fontsize=12)
ax.set_ylabel(""V"",fontsize=12)
</code></pre>

<p>I get a plot and a legend with all the columns' values and names. How can I modify my code so the plot and legend only displays the columns <code>V1</code> and <code>V2</code>  </p>
";;2;;2015-04-07T18:30:59.760;2.0;29498652;2017-05-09T20:55:51.163;;;;;4760190.0;;1;11;<python><pandas><plot>;Plot bar graph from Pandas DataFrame;28656.0
26016;26016;31026736.0;5.0;"<p>Getting the following error when trying to install Pandas (0.16.0), which is in my requirements.txt file, on AWS Elastic Beanstalk EC2 instance:</p>

<pre><code>  building 'pandas.msgpack' extension

  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D__LITTLE_ENDIAN__=1 -Ipandas/src/klib -Ipandas/src -I/opt/python/run/venv/local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/msgpack.cpp -o build/temp.linux-x86_64-2.7/pandas/msgpack.o

  gcc: error trying to exec 'cc1plus': execvp: No such file or directory

  error: command 'gcc' failed with exit status 1
</code></pre>

<p>I'm running on <code>64bit Amazon Linux 2015.03 v1.3.0 running Python 2.7</code> and previously ran into this same error on a t1.micro instance, which was resolved when I change to a m3.medium, but I'm running an m3.xlarge so can't be a memory issue.</p>

<p>I have also ensured that gcc is installed as a package in <code>.ebextensions/00_gcc.config</code>:</p>

<pre><code>packages:
   yum:
      gcc: []
      gcc-c++: []
</code></pre>
";;0;;2015-04-08T13:30:45.587;;29516084;2017-07-21T12:09:29.260;;;;;1177562.0;;1;11;<python><amazon-web-services><pandas><amazon-ec2><elastic-beanstalk>;'gcc' failed during pandas build on AWS Elastic Beanstalk;2250.0
26020;26020;29517089.0;3.0;"<p>I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.</p>

<p>Existing df:</p>

<pre><code>Date, Open, High, Low, Close
01-01-2015, 565, 600, 400, 450
</code></pre>

<p>New df:</p>

<pre><code>Name, Date, Open, High, Low, Close
abc, 01-01-2015, 565, 600, 400, 450
</code></pre>

<p>I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.</p>

<p>Im not entirely sure how to do that.</p>
";;0;;2015-04-08T14:09:22.313;4.0;29517072;2017-06-20T15:42:00.087;;;;;4169229.0;;1;28;<python><pandas>;Add column to dataframe with default value;24041.0
26054;26054;29528804.0;4.0;"<p>This topic hasn't been addressed in a while, here or elsewhere. Is there a solution converting a SQLAlchemy <code>&lt;Query object&gt;</code> to a pandas DataFrame?</p>

<p>Pandas has the capability to use <code>pandas.read_sql</code> but this requires use of raw SQL. I have two reasons for wanting to avoid it: 1) I already have everything using the ORM (a good reason in and of itself) and 2) I'm using python lists as part of the query (eg: <code>.db.session.query(Item).filter(Item.symbol.in_(add_symbols)</code> where <code>Item</code> is my model class and <code>add_symbols</code> is a list). This is the equivalent of SQL <code>SELECT ... from ... WHERE ... IN</code>. </p>

<p>Is anything possible?</p>
";;0;;2015-04-08T21:36:34.073;23.0;29525808;2017-03-17T09:00:53.033;;;;;1556228.0;;1;37;<python><pandas><sqlalchemy><flask-sqlalchemy>;SQLAlchemy ORM conversion to pandas DataFrame;9483.0
26069;26069;29530601.0;8.0;"<p>In python pandas, what's the best way to check whether a DataFrame has one (or more) NaN values?</p>

<p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=""https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas"">This post</a> right here doesn't exactly answer my question either.</p>
";;1;;2015-04-09T05:09:39.597;52.0;29530232;2017-08-23T01:48:20.603;2017-05-23T12:34:26.463;;-1.0;;2014591.0;;1;113;<python><pandas><nan>;Python pandas: check if any value is NaN in DataFrame;136260.0
26080;26080;29533502.0;2.0;"<p>Consider the following code running in iPython/Jupyter Notebook:</p>

<pre><code>from pandas import *
%matplotlib inline

ys = [[0,1,2,3,4],[4,3,2,1,0]]
x_ax = [0,1,2,3,4]

for y_ax in ys:
    ts = Series(y_ax,index=x_ax)
    ts.plot(kind='bar', figsize=(15,5))
</code></pre>

<p>I would expect to have 2 separate plots as output, instead I got the two series merged in one single plot.
Why is that? How can I get two separate plots keeping the <code>for</code> loop?</p>
";;0;;2015-04-09T07:55:00.753;3.0;29532894;2017-03-29T19:12:25.543;2017-03-29T19:12:25.543;;3190076.0;;3190076.0;;1;13;<python><pandas><matplotlib><plot><ipython-notebook>;iPython/Jupyter Notebook and Pandas, how to plot multiple graphs in a for loop?;10329.0
26118;26118;;4.0;"<p>Each row in a Pandas dataframe contains lat/lng coordinates of 2 points. Using the Python code below, calculating the distances between these 2 points for many (millions) of rows takes a very long time!</p>

<p>Considering that the 2 points are under 50 miles apart and accuracy is not very important, is it possible to make the calculation faster?</p>

<pre><code>from math import radians, cos, sin, asin, sqrt
def haversine(lon1, lat1, lon2, lat2):
    """"""
    Calculate the great circle distance between two points 
    on the earth (specified in decimal degrees)
    """"""
    # convert decimal degrees to radians 
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    # haversine formula 
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a)) 
    km = 6367 * c
    return km


for index, row in df.iterrows():
    df.loc[index, 'distance'] = haversine(row['a_longitude'], row['a_latitude'], row['b_longitude'], row['b_latitude'])
</code></pre>
";;10;;2015-04-09T18:00:08.417;3.0;29545704;2017-07-30T03:30:22.087;2016-12-24T19:19:20.210;;2683.0;;741099.0;;1;12;<python><numpy><pandas><gis><haversine>;Fast Haversine Approximation (Python/Pandas);3266.0
26134;26134;29550458.0;3.0;"<p>I have a pandas dataframe (this is only a little piece)</p>

<pre><code>&gt;&gt;&gt; d1
   y norm test  y norm train  len(y_train)  len(y_test)  \
0    64.904368    116.151232          1645          549   
1    70.852681    112.639876          1645          549   

                                    SVR RBF  \
0   (35.652207342877873, 22.95533537448393)   
1  (39.563683797747622, 27.382483096332511)   

                                        LCV  \
0  (19.365430594452338, 13.880062435173587)   
1  (19.099614489458364, 14.018867136617146)   

                                   RIDGE CV  \
0  (4.2907610988480362, 12.416745648065584)   
1    (4.18864306788194, 12.980833914392477)   

                                         RF  \
0   (9.9484841581029428, 16.46902345373697)   
1  (10.139848213735391, 16.282141345406522)   

                                           GB  \
0  (0.012816232716538605, 15.950164822266007)   
1  (0.012814519804493328, 15.305745202851712)   

                                             ET DATA  
0  (0.00034337162272515505, 16.284800366214057)  j2m  
1  (0.00024811554516431878, 15.556506191784194)  j2m  
&gt;&gt;&gt; 
</code></pre>

<p>I want to split all the columns that contain tuples. For example I want to replace the column <code>LCV</code> with the columns <code>LCV-a</code> and <code>LCV-b</code> . </p>

<p>How can I do that?</p>

<p>EDIT:</p>

<p>The proposed solution does not work why??</p>

<pre><code>&gt;&gt;&gt; d1['LCV'].apply(pd.Series)
                                          0
0  (19.365430594452338, 13.880062435173587)
1  (19.099614489458364, 14.018867136617146)
&gt;&gt;&gt; 
</code></pre>

<p>EDIT:
This seems to be working</p>

<pre><code>&gt;&gt;&gt; d1['LCV'].apply(eval).apply(pd.Series)
           0          1
0  19.365431  13.880062
1  19.099614  14.018867
&gt;&gt;&gt; 
</code></pre>
";;4;;2015-04-09T22:50:38.133;5.0;29550414;2017-05-26T08:20:14.630;2016-05-02T03:44:37.210;;2230844.0;;2411173.0;;1;18;<python><numpy><pandas><dataframe><tuples>;how to split column of tuples in pandas dataframe?;8089.0
26188;26188;34879805.0;3.0;"<p>I have the following DataFrame:</p>

<pre><code>    Col1  Col2  Col3  Type
0      1     2     3     1
1      4     5     6     1
...
20     7     8     9     2
21    10    11    12     2
...
45    13    14    15     3
46    16    17    18     3
...
</code></pre>

<p>The DataFrame is read from a csv file. All rows which have <code>Type</code> 1 are on top, followed by the rows with <code>Type</code> 2, followed by the rows with <code>Type</code> 3, etc.</p>

<p>I would like to shuffle the DataFrame's rows, so that all <code>Type</code>'s are mixed. A possible result could be:</p>

<pre><code>    Col1  Col2  Col3  Type
0      7     8     9     2
1     13    14    15     3
...
20     1     2     3     1
21    10    11    12     2
...
45     4     5     6     1
46    16    17    18     3
...
</code></pre>

<p>As can be seen from the result, the order of the rows is shuffled, but the columns remain the same. I don't know if I am explaining this clearly. Let me know if I don't.</p>

<p>How can I achieve this?</p>
";;0;;2015-04-11T09:47:57.653;7.0;29576430;2016-09-24T07:42:10.263;;;;;1833854.0;;1;60;<python><pandas><dataframe><permutation><shuffle>;Shuffle DataFrame rows;36846.0
26243;26243;;1.0;"<p>I have a PostgreSQL db. Pandas has a 'to_sql' function to write the records of a dataframe into a database. But I haven't found any documentation on how to update an existing database row using pandas when im finished with the dataframe.</p>

<p>Currently I am able to read a database table into a dataframe using pandas read_sql_table. I then work with the data as necessary. However I haven't been able to figure out how to write that dataframe back into the database to update the original rows.</p>

<p>I dont want to have to overwrite the whole table. I just need to update the rows that were originally selected.</p>
";;6;;2015-04-13T14:01:01.780;5.0;29607222;2016-02-10T16:42:27.000;;;;;4169229.0;;1;15;<python><postgresql><pandas>;Update existing row in database from pandas df;3593.0
26373;26373;29673192.0;2.0;"<p>I have an adjacency matrix stored as a <code>pandas.DataFrame</code>:</p>

<pre><code>node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],
    index=node_names, columns=node_names)
a_numpy = a.as_matrix()
</code></pre>

<p>I'd like to create an <code>igraph.Graph</code> from either the <code>pandas</code> or the <code>numpy</code> adjacency matrices. In an ideal world the nodes would be named as expected.</p>

<p>Is this possible? <a href=""http://igraph.org/python/doc/tutorial/tutorial.html"">The tutorial</a> seems to be silent on the issue.</p>
";;2;;2015-04-15T15:57:19.507;6.0;29655111;2015-04-16T11:21:46.177;2015-04-15T16:12:42.653;;2071807.0;;2071807.0;;1;13;<python><numpy><pandas><igraph>;igraph Graph from numpy or pandas adjacency matrix;5110.0
26485;26485;;1.0;"<p>I have a dataframe with ca 155,000 rows and 12 columns.
If I export it to csv with dataframe.to_csv , the output is an 11MB file (which is produced instantly).</p>

<p>If, however, I export to a Microsoft SQL Server with the to_sql method, it takes between 5 and 6 minutes!
No columns are text: only int, float, bool and dates. I have seen cases where ODBC drivers set nvarchar(max) and this slows down the data transfer, but it cannot be the case here.</p>

<p>Any suggestions on how to speed up the export process? Taking 6 minutes to export 11 MBs of data makes the ODBC connection practically unusable.</p>

<p>Thanks!</p>

<p>My code is:</p>

<pre><code>import pandas as pd
from sqlalchemy import create_engine, MetaData, Table, select
ServerName = ""myserver""
Database = ""mydatabase""
TableName = ""mytable""

engine = create_engine('mssql+pyodbc://' + ServerName + '/' + Database)
conn = engine.connect()

metadata = MetaData(conn)

my_data_frame.to_sql(TableName,engine)
</code></pre>
";;5;;2015-04-17T17:55:11.857;4.0;29706278;2015-08-13T13:37:17.353;2015-08-13T13:37:17.353;;3730397.0;;4045275.0;;1;11;<python><sql><pandas><sqlalchemy><pyodbc>;python pandas to_sql with sqlalchemy : how to speed up exporting to MS SQL?;8173.0
26696;26696;29763653.0;4.0;"<p>I have a dataframe look like this:</p>

<pre><code>    import pandas
    import numpy as np
    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))
    df
          a         b         c         d
    0  0.418762  0.042369  0.869203  0.972314
    1  0.991058  0.510228  0.594784  0.534366
    2  0.407472  0.259811  0.396664  0.894202
    3  0.726168  0.139531  0.324932  0.906575
</code></pre>

<p>How I can get all columns except <code>column b</code> using <code>df.ix</code></p>
";2017-05-04T21:53:34.647;0;;2015-04-21T05:24:59.340;15.0;29763620;2017-07-01T01:17:21.583;2015-04-21T05:27:55.243;;4744765.0;;4744765.0;;1;29;<python><pandas>;How to select all columns, except one column in pandas using .ix;26374.0
26704;26704;29765839.0;3.0;"<p>I have a dataframe like this one:</p>

<pre><code>In [10]: df
Out[10]: 
         Column 1
foo              
Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>

<p>How to remove <code>index name</code> <code>foo</code> from that dataframe?
The desired output is like this:</p>

<pre><code>In [10]: df
Out[10]: 
         Column 1             
Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>
";;0;;2015-04-21T07:24:16.367;3.0;29765548;2016-03-15T14:37:13.043;2015-04-21T07:29:20.420;;4744765.0;;4744765.0;;1;17;<python><pandas>;Remove index name in pandas;14140.0
26797;26797;29794993.0;3.0;"<p>I would like to add a column 'D' to a dataframe like this:</p>

<pre><code>U,L
111,en
112,en
112,es
113,es
113,ja
113,zh
114,es
</code></pre>

<p>based on the following Dictionary:</p>

<pre><code>d = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}
</code></pre>

<p>so that the resulting dataframe appears as:</p>

<pre><code>U,L,D
111,en,en
112,en,en
112,es,en
113,es,es
113,ja,es
113,zh,es
114,es,es
</code></pre>

<p>So far I tried the <code>pd.join()</code> method but I can't figured out how it works with Dictionaries.</p>
";;0;;2015-04-22T10:39:21.800;8.0;29794959;2017-03-14T23:31:19.357;;;;;2699288.0;;1;16;<python><pandas>;pandas - add new column to dataframe from dictionary;11620.0
26868;26868;29815523.0;3.0;"<p>I have the following DataFrame:</p>

<pre>
customer    item1      item2    item3
1           apple      milk     tomato
2           water      orange   potato
3           juice      mango    chips
</pre>

<p>which I want to translate it to list of dictionaries per row</p>

<pre><code>rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>
";;1;;2015-04-23T06:12:18.060;14.0;29815129;2017-02-14T06:16:46.470;2017-02-14T06:16:46.470;;3510736.0;;1057443.0;;1;38;<python><list><dictionary><pandas><dataframe>;Pandas DataFrame to List of Dictionaries;15409.0
26936;26936;29836852.0;1.0;"<p>I'm working in Python with a pandas DataFrame of video games, each with a genre. I'm trying to remove any video game with a genre that appears less than some number of times in the DataFrame, but I have no clue how to go about this. I did find <a href=""https://stackoverflow.com/questions/6796569/how-to-filter-a-dataframe-based-on-category-counts"">a StackOverflow question</a> that seems to be related, but I can't decipher the solution at all (possibly because I've never heard of R and my memory of functional programming is rusty at best).</p>

<p>Help?</p>
";;0;;2015-04-24T00:48:31.980;1.0;29836836;2015-04-24T00:50:54.607;2017-05-23T11:54:37.807;;-1.0;;2503283.0;;1;11;<python><pandas><filtering><dataframe>;How do I filter a pandas DataFrame based on value counts?;5042.0
27121;27121;;3.0;"<p>I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.</p>

<pre><code>df = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})
</code></pre>

<p>Now when I try to print the same, I do not see the full string I rather see only part of the string.</p>

<p>I tried following options </p>

<ul>
<li>using print(df.iloc[2]) </li>
<li>using to_html</li>
<li>using to_string</li>
<li>One of the stackoverflow answer suggested to increase column width by
using pandas display option, that did not work either.</li>
<li>I also did not get how set_printoptions will help me.</li>
</ul>

<p>Any ideas appreciated. Looks very simple, but not able to get it!  </p>
";;0;;2015-04-27T17:54:00.980;2.0;29902714;2017-08-02T03:31:43.610;;;;;3907612.0;;1;16;<python><string><pandas>;Print the complete string of a pandas dataframe;10013.0
27169;27169;29919489.0;3.0;"<p>I have a DataFrame like this one:</p>

<pre><code>In [7]:
frame.head()
Out[7]:
Communications and Search   Business    General Lifestyle
0   0.745763    0.050847    0.118644    0.084746
0   0.333333    0.000000    0.583333    0.083333
0   0.617021    0.042553    0.297872    0.042553
0   0.435897    0.000000    0.410256    0.153846
0   0.358974    0.076923    0.410256    0.153846
</code></pre>

<p>In here, I want to ask how to get column name which has maximum value for each row, the desired output is like this:</p>

<pre><code>In [7]:
    frame.head()
    Out[7]:
    Communications and Search   Business    General Lifestyle   Max
    0   0.745763    0.050847    0.118644    0.084746           Communications 
    0   0.333333    0.000000    0.583333    0.083333           Business  
    0   0.617021    0.042553    0.297872    0.042553           Communications 
    0   0.435897    0.000000    0.410256    0.153846           Communications 
    0   0.358974    0.076923    0.410256    0.153846           Business 
</code></pre>
";;0;;2015-04-28T12:18:57.087;7.0;29919306;2016-12-13T12:46:57.547;2016-12-13T12:46:57.547;;3923281.0;;4744765.0;;1;30;<python><pandas><dataframe><max>;Find the column name which has the maximum value for each row;10758.0
27284;27284;29955358.0;1.0;"<p>I'm reading through the Pandas documentation, and the term ""broadcasting"" is <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#flexible-binary-operations"">used extensively</a>, but never really defined or explained.</p>

<p>What does it mean?</p>
";;4;;2015-04-29T20:50:04.660;4.0;29954263;2015-09-15T14:46:52.180;2015-04-29T23:03:50.520;;704848.0;;916907.0;;1;16;<python><numpy><pandas>;"What does the term ""broadcasting"" mean in Pandas documentation?";2181.0
27323;27323;;1.0;"<p>I have a really big DataFrame and I was wondering if there was short (one or two liner) way to get the a count of non-NaN entries in a DataFrame. I don't want to do this one column at a time as I have close to 1000 columns. </p>

<pre><code>df1 = pd.DataFrame([(1,2,None),(None,4,None),(5,None,7),(5,None,None)], 
                    columns=['a','b','d'], index = ['A', 'B','C','D'])

    a   b   d
A   1   2 NaN
B NaN   4 NaN
C   5 NaN   7
D   5 NaN NaN
</code></pre>

<p>Output:</p>

<pre><code>a: 3
b: 2
d: 1
</code></pre>
";;3;;2015-04-30T14:57:40.623;1.0;29971075;2017-05-26T12:09:56.143;2017-05-26T12:09:56.143;;3923281.0;;2108615.0;;1;20;<python><pandas><dataframe><count><nan>;Count number of non-NaN entries in every column of Dataframe;14021.0
27442;27442;30010004.0;1.0;"<p>I have a data frame that looks like this:</p>

<pre><code>company  Amazon  Apple  Yahoo
name
A             0    130      0
C           173      0      0
Z             0      0    150
</code></pre>

<p>It was created using this code:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'name' : ['A', 'Z','C'],
                   'company' : ['Apple', 'Yahoo','Amazon'],
                   'height' : [130, 150,173]})

df = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)
</code></pre>

<p>What I want to do is to sort the row (with index <code>name</code>) according to a predefined list <code>[""Z"", ""C"", ""A""]</code>.  Resulting in this :</p>

<pre><code>company  Amazon  Apple  Yahoo
name
Z             0      0    150
C           173      0      0
A             0    130      0
</code></pre>

<p>How can I achieve that?</p>
";;0;;2015-05-03T03:34:33.970;4.0;30009948;2017-08-09T06:55:01.253;;;;;67405.0;;1;12;<python><pandas>;How to reorder indexed rows based on a list in Pandas data frame;12941.0
27466;27466;30065040.0;1.0;"<p>Im triying to obtain the most informative features from a <a href=""http://pastebin.com/3qYc9mfZ"" rel=""noreferrer"">textual corpus</a>. From this well answered <a href=""https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c"">question</a> I know that this task could be done as follows:</p>

<pre><code>def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):
    labelid = list(classifier.classes_).index(classlabel)
    feature_names = vectorizer.get_feature_names()
    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]

    for coef, feat in topn:
        print classlabel, feat, coef
</code></pre>

<p>Then:</p>

<pre><code>most_informative_feature_for_class(tfidf_vect, clf, 5)
</code></pre>

<p>For this classfier:</p>

<pre><code>X = tfidf_vect.fit_transform(df['content'].values)
y = df['label'].values


from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                    y, test_size=0.33)
clf = SVC(kernel='linear', C=1)
clf.fit(X, y)
prediction = clf.predict(X_test)
</code></pre>

<p>The problem is the output of <code>most_informative_feature_for_class</code>:</p>

<pre><code>5 a_base_de_bien bastante   (0, 2451)   -0.210683496368
  (0, 3533) -0.173621065386
  (0, 8034) -0.135543062425
  (0, 10346)    -0.173621065386
  (0, 15231)    -0.154148294738
  (0, 18261)    -0.158890483047
  (0, 21083)    -0.297476572586
  (0, 434)  -0.0596263855375
  (0, 446)  -0.0753492277856
  (0, 769)  -0.0753492277856
  (0, 1118) -0.0753492277856
  (0, 1439) -0.0753492277856
  (0, 1605) -0.0753492277856
  (0, 1755) -0.0637950312345
  (0, 3504) -0.0753492277856
  (0, 3511) -0.115802483001
  (0, 4382) -0.0668983049212
  (0, 5247) -0.315713152154
  (0, 5396) -0.0753492277856
  (0, 5753) -0.0716096348446
  (0, 6507) -0.130661516772
  (0, 7978) -0.0753492277856
  (0, 8296) -0.144739048504
  (0, 8740) -0.0753492277856
  (0, 8906) -0.0753492277856
  : :
  (0, 23282)    0.418623443832
  (0, 4100) 0.385906085143
  (0, 15735)    0.207958503155
  (0, 16620)    0.385906085143
  (0, 19974)    0.0936828782325
  (0, 20304)    0.385906085143
  (0, 21721)    0.385906085143
  (0, 22308)    0.301270427482
  (0, 14903)    0.314164150621
  (0, 16904)    0.0653764031957
  (0, 20805)    0.0597723455204
  (0, 21878)    0.403750815828
  (0, 22582)    0.0226150073272
  (0, 6532) 0.525138162099
  (0, 6670) 0.525138162099
  (0, 10341)    0.525138162099
  (0, 13627)    0.278332617058
  (0, 1600) 0.326774799211
  (0, 2074) 0.310556919237
  (0, 5262) 0.176400451433
  (0, 6373) 0.290124806858
  (0, 8593) 0.290124806858
  (0, 12002)    0.282832270298
  (0, 15008)    0.290124806858
  (0, 19207)    0.326774799211
</code></pre>

<p>It is not returning the label nor the words. Why this is happening and how can I print the words and the labels?. Do you guys this is happening since I am using pandas to read the data?. Another thing I tried is the following, form this <a href=""https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"">question</a>:</p>

<pre><code>def print_top10(vectorizer, clf, class_labels):
    """"""Prints features with the highest coefficient values, per class""""""
    feature_names = vectorizer.get_feature_names()
    for i, class_label in enumerate(class_labels):
        top10 = np.argsort(clf.coef_[i])[-10:]
        print(""%s: %s"" % (class_label,
              "" "".join(feature_names[j] for j in top10)))


print_top10(tfidf_vect,clf,y)
</code></pre>

<p>But I get this traceback:</p>

<p>Traceback (most recent call last):</p>

<pre><code>  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 237, in &lt;module&gt;
    print_top10(tfidf_vect,clf,5)
  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 231, in print_top10
    for i, class_label in enumerate(class_labels):
TypeError: 'int' object is not iterable
</code></pre>

<p>Any idea of how to solve this, in order to get the features with the highest coefficient values?.</p>
";;0;;2015-05-03T18:07:12.767;3.0;30017491;2015-05-05T23:41:41.733;2015-05-05T21:05:15.897;;4099593.0;;4114372.0;;1;15;<python><pandas><machine-learning><nlp><scikit-learn>;Problems obtaining most informative features with scikit learn?;1804.0
27483;27483;30025025.0;1.0;"<p>I was trying to split the sample dataset using Scikit-learn's Stratified Shuffle Split. I followed the example shown on the Scikit-learn documentation <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"">here</a>  </p>

<pre><code>import pandas as pd
import numpy as np
# UCI's wine dataset
wine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")

# separate target variable from dataset
target = wine['quality']
data = wine.drop('quality',axis = 1)

# Stratified Split of train and test data
from sklearn.cross_validation import StratifiedShuffleSplit
sss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)

for train_index, test_index in sss:
    xtrain, xtest = data[train_index], data[test_index]
    ytrain, ytest = target[train_index], target[test_index]

# Check target series for distribution of classes
ytrain.value_counts()
ytest.value_counts()
</code></pre>

<p>However, upon running this script, I get the following error:</p>

<pre><code>IndexError: indices are out-of-bounds
</code></pre>

<p>Could someone please point out what I am doing wrong here? Thanks!</p>
";;1;;2015-05-04T06:35:04.070;7.0;30023927;2015-05-04T09:37:53.040;2015-05-04T09:34:16.843;;3962124.0;;3962124.0;;1;17;<python><pandas><scikit-learn>;"sklearn.cross_validation.StratifiedShuffleSplit - error: ""indices are out-of-bounds""";6029.0
27487;27487;32773145.0;3.0;"<p>I have a pandas data frame <code>mydf</code> that has two columns,and both columns are datetime datatypes: <code>mydate</code> and <code>mytime</code>. I want to add three more columns: <code>hour</code>, <code>weekday</code>, and <code>weeknum</code>. </p>

<pre><code>def getH(t): #gives the hour
    return t.hour
def getW(d): #gives the week number
    return d.isocalendar()[1] 
def getD(d): #gives the weekday
    return d.weekday() # 0 for Monday, 6 for Sunday

mydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)
mydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)
mydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)
</code></pre>

<p>The snippet works, but it's not computationally efficient as it loops through the data frame at least three times. I would just like to know if there's a faster and/or more optimal way to do this. For example, using <code>zip</code> or <code>merge</code>? If, for example, I just create one function that returns three elements, how should I implement this? To illustrate, the function would be:</p>

<pre><code>def getHWd(d,t):
    return t.hour, d.isocalendar()[1], d.weekday()
</code></pre>
";;1;;2015-05-04T09:32:39.863;8.0;30026815;2015-09-25T00:53:03.280;;;;;2867097.0;;1;15;<python><pandas>;Add Multiple Columns to Pandas Dataframe from Function;15980.0
27556;27556;30053507.0;2.0;"<p>I want to create a Pandas DataFrame filled with NaNs. During my research I found <a href=""https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it"">an answer</a>:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(index=range(0,4),columns=['A'])
</code></pre>

<p>This code results in a DataFrame filled with NaNs of type ""object"". So they cannot be used later on for example with the <code>interpolate()</code> method. Therefore, I created the DataFrame with this complicated code (inspired by <a href=""https://stackoverflow.com/questions/1704823/initializing-numpy-matrix-to-something-other-than-zero-or-one"">this answer</a>):</p>

<pre><code>import pandas as pd
import numpy as np

dummyarray = np.empty((4,1))
dummyarray[:] = np.nan

df = pd.DataFrame(dummyarray)
</code></pre>

<p>This results in a DataFrame filled with NaN of type ""float"", so it can be used later on with <code>interpolate()</code>. Is there a more elegant way to create the same result?</p>
";;1;;2015-05-05T12:44:09.800;6.0;30053329;2017-01-27T12:34:25.580;2017-05-23T12:26:17.050;;-1.0;;4866038.0;;1;20;<python><pandas><numpy><dataframe><nan>;Elegant way to create empty pandas DataFrame with NaN of type float;43287.0
27570;27570;30059290.0;1.0;"<p>I've got a dataset with a big number of rows. Some of the values are NaN, like this:</p>

<pre><code>In [91]: df
Out[91]:
 1    3      1      1      1
 1    3      1      1      1
 2    3      1      1      1
 1    1    NaN    NaN    NaN
 1    3      1      1      1
 1    1      1      1      1
</code></pre>

<p>And I want to count the number of NaN values in each string, it would be like this:</p>

<pre><code>In [91]: list = &lt;somecode with df&gt;
In [92]: list
    Out[91]:
     [0,
      0,
      0,
      3,
      0,
      0]
</code></pre>

<p>What is the best and fastest way to do it?</p>
";;1;;2015-05-05T17:14:08.567;4.0;30059260;2016-11-17T10:59:16.560;2016-11-17T10:59:16.560;;202229.0;;3010179.0;;1;12;<pandas><count><row><dataframe><nan>;Python/Pandas: counting the number of missing/NaN in each row;9681.0
27650;27650;41226605.0;2.0;"<p>I am trying to read in a json file into pandas data frame. Here is the first line line of the json file:</p>

<pre><code>{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}
</code></pre>

<p>I am trying do the following:<code>df = pd.read_json(path)</code>
I am getting the following error (with full traceback):</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 198, in read_json
    date_unit).parse()
  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 266, in parse
    self._parse_no_numpy()
  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 483, in _parse_no_numpy
    loads(json, precise_float=self.precise_float), dtype=None)
ValueError: Trailing data
</code></pre>

<p>What is <code>Trailing data</code> error ? How do I read it into a data frame ? </p>

<p><strong>EDIT:</strong>
Following some suggestions, here are few lines of the .json file:</p>

<pre><code>{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}
{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\n\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}
{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it's approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}
</code></pre>

<p>This .json file I am using contains one json object in each line as per the specification.</p>

<p>I tried the <a href=""http://jsonlint.com"">jsonlint.com</a> website as suggested and it gives the following error: </p>

<pre><code>Parse error on line 14:
...t7sRT4zwdbzQ8KQmw""}{    ""votes"": {   
----------------------^
Expecting 'EOF', '}', ',', ']'
</code></pre>
";;9;;2015-05-06T21:34:39.387;3.0;30088006;2017-07-12T07:50:03.213;2017-02-23T15:04:32.580;;918.0;;2307804.0;;1;18;<python><json><python-2.7><pandas>;Loading a file with more than one line of JSON into Python's Pandas;10287.0
27712;27712;30304735.0;4.0;"<p>Trying this example from the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_excel.html"">documentation</a> </p>

<pre><code>writer = ExcelWriter('output.xlsx')
df1.to_excel(writer,'Sheet1')
df2.to_excel(writer,'Sheet2')
writer.save()
</code></pre>

<p>I found out that I can not write to an excel file with the error</p>

<pre><code>TypeError: copy() got an unexpected keyword argument 'font'
</code></pre>

<p>I'm using Panda 0.16 on a Mac pro. </p>

<p>EDIT: 
writing to an xls file worked just fine. I do not insist in having an xlsx file, just wonder why it does not work.</p>
";;1;;2015-05-07T13:17:03.510;2.0;30102232;2016-12-21T18:25:26.903;2015-05-08T06:56:45.820;;1213793.0;;1213793.0;;1;11;<python><pandas>;Pandas: can not write to excel file;4532.0
27775;27775;30132313.0;3.0;"<p>I need to make this simple thing:</p>

<pre><code>dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')
dates.str
</code></pre>

<p>But a get an error. How should I transform from datetime to string</p>

<p>Thanks in advance</p>
";;0;;2015-05-08T20:23:11.550;4.0;30132282;2017-01-28T04:42:05.287;2015-05-08T20:26:46.747;;704848.0;;2892281.0;;1;13;<python><datetime><pandas>;datetime to string with series in python pandas;23754.0
27779;27779;30135182.0;1.0;"<p>I have a simple stacked line plot that has exactly the date format I want magically set when using the following code.</p>

<pre><code>df_ts = df.resample(""W"", how='max')
df_ts.plot(figsize=(12,8), stacked=True)
</code></pre>

<p><img src=""https://i.stack.imgur.com/zpCiH.png"" alt=""enter image description here""></p>

<p>However, the dates mysteriously transform themselves to an ugly and unreadable format when plotting the same data as a bar plot.</p>

<pre><code>df_ts = df.resample(""W"", how='max')
df_ts.plot(kind='bar', figsize=(12,8), stacked=True)
</code></pre>

<p><img src=""https://i.stack.imgur.com/8133P.png"" alt=""enter image description here""></p>

<p>The original data was transformed a bit to have the weekly max. Why is this radical change in automatically set dates happening? How can I have the nicely formatted dates as above?</p>

<p>Here is some dummy data</p>

<pre><code>start = pd.to_datetime(""1-1-2012"")
idx = pd.date_range(start, periods= 365).tolist()
df=pd.DataFrame({'A':np.random.random(365), 'B':np.random.random(365)})
df.index = idx
df_ts = df.resample('W', how= 'max')
df_ts.plot(kind='bar', stacked=True)
</code></pre>
";;0;;2015-05-08T21:40:58.213;11.0;30133280;2015-11-11T21:38:09.727;2015-05-08T21:50:00.540;;3707607.0;;3707607.0;;1;20;<pandas><matplotlib><plot>;Pandas bar plot changes date format;5797.0
27980;27980;30214901.0;2.0;"<p>The documentation says: </p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/basics.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/basics.html</a> </p>

<p>""Continuous values can be discretized using the cut (bins based on values) and qcut (bins based on sample quantiles) functions""</p>

<p>Sounds very abstract to me... I can see the differences in the example below but <strong>what does qcut (sample quantile) actually do/mean? When would you use qcut versus cut?</strong></p>

<p>Thanks.</p>

<pre><code>factors = np.random.randn(30)

In [11]:
pd.cut(factors, 5)
Out[11]:
[(-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (0.575, 1.561], ..., (-0.411, 0.575], (-1.397, -0.411], (0.575, 1.561], (-2.388, -1.397], (-0.411, 0.575]]
Length: 30
Categories (5, object): [(-2.388, -1.397] &lt; (-1.397, -0.411] &lt; (-0.411, 0.575] &lt; (0.575, 1.561] &lt; (1.561, 2.547]]

In [14]:
pd.qcut(factors, 5)
Out[14]:
[(-0.348, 0.0899], (-0.348, 0.0899], (0.0899, 1.19], (0.0899, 1.19], (0.0899, 1.19], ..., (0.0899, 1.19], (-1.137, -0.348], (1.19, 2.547], [-2.383, -1.137], (-0.348, 0.0899]]
Length: 30
Categories (5, object): [[-2.383, -1.137] &lt; (-1.137, -0.348] &lt; (-0.348, 0.0899] &lt; (0.0899, 1.19] &lt; (1.19, 2.547]]`
</code></pre>
";;1;;2015-05-13T10:18:25.080;8.0;30211923;2016-12-13T18:56:36.377;;;;;3123992.0;;1;20;<python><pandas>;What is the difference between pandas.qcut and pandas.cut?;7956.0
28025;28025;30222759.0;2.0;"<p>Create a day-of-week column in a Pandas dataframe using Python</p>

<p>Id like to read a csv file into a pandas dataframe, parse a column of dates from string format to a date object, and then generate a new column that indicates the day of the week.</p>

<p>This is what Im trying:</p>

<p>What Id like to do is something like:</p>

<pre><code>import pandas as pd

import csv

df = pd.read_csv('data.csv', parse_dates=['date']))

df['day-of-week'] = df['date'].weekday()


AttributeError: 'Series' object has no attribute 'weekday'
</code></pre>

<hr>

<p>Thank you for your help.
James</p>
";;3;;2015-05-13T18:24:11.783;2.0;30222533;2017-02-10T02:10:30.903;2015-05-13T18:28:01.020;;837534.0;;4458393.0;;1;16;<python><datetime><pandas>;Create a day-of-week column in a Pandas dataframe using Python;14335.0
28245;28245;39830464.0;2.0;"<p>I have tried passing the <code>dtype</code> parameter with <code>read_csv</code> as <code>dtype={n: pandas.Categorical}</code> but this does not work properly (the result is an Object). <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"">The manual is unclear</a>.</p>
";;4;;2015-05-16T05:49:22.903;2.0;30272300;2017-05-06T14:16:30.283;2017-05-06T14:16:30.283;;2901002.0;;832188.0;;1;18;<file><csv><pandas><readfile><categorical-data>;Is it possible to read categorical columns with pandas' read_csv?;2330.0
28374;28374;30319249.0;1.0;"<p>I was wondering if there are classifiers that handle nan/null values in scikit-learn.  I thought random forest regressor handles this but I got an error when I call <code>predict</code>.</p>

<pre><code>X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # Fails!
</code></pre>

<p>Can I not call predict with any scikit-learn algorithm with missing values?</p>

<p><strong>Edit.</strong>
Now that I think about this, it makes sense.  It's not an issue during training but when you predict how do you branch when the variable is null?  maybe you could just split both ways and average the result?  It seems like k-NN should work fine as long as the distance function ignores nulls though.</p>
";;2;;2015-05-19T05:02:35.130;6.0;30317119;2015-06-19T07:37:48.050;2015-06-19T07:37:48.050;;238639.0;;1536499.0;;1;13;<python><pandas><machine-learning><scikit-learn><nan>;classifiers in scikit-learn that handle nan/null;5960.0
28403;28403;30327470.0;2.0;"<p>I have a pandas data frame with 50k rows.  I'm trying to add a new column that is a randomly generated integer from 1 to 5.  </p>

<p>If I want 50k random numbers I'd use:</p>

<pre><code>df1['randNumCol'] = random.sample(xrange(50000), len(df1))
</code></pre>

<p>but for this I'm not sure how to do it.</p>

<p>Side note in R, I'd do:</p>

<pre><code>sample(1:5, 50000, replace = TRUE)
</code></pre>

<p>Any suggestions?</p>
";;1;;2015-05-19T13:41:41.353;2.0;30327417;2017-04-07T09:29:14.753;2017-04-07T09:29:14.753;;202229.0;;914308.0;;1;18;<python><pandas><random><integer><range>;Pandas: create new column in df with random integers from range;6072.0
28414;28414;30328738.0;2.0;"<p>I have a df like this:</p>

<pre><code>cluster  org      time
   1      a       8
   1      a       6
   2      h       34
   1      c       23
   2      d       74
   3      w       6 
</code></pre>

<p>I would like to calculate the average of time per org per cluster.</p>

<p>Expected result:</p>

<pre><code>cluster mean(time)
1       15 ((8+6/2)+23)/2
2       54   (74+34)/2
3       6
</code></pre>

<p>I do not know how to do it in Pandas, can anybody help?</p>
";;2;;2015-05-19T14:33:49.233;3.0;30328646;2017-04-18T12:49:25.980;2015-05-20T09:16:03.110;;2058811.0;;2058811.0;;1;16;<python><pandas><group-by><mean>;Python Pandas : group by in group by and average?;27839.0
28502;28502;30357382.0;3.0;"<p>I would like to fill missing value in one column with the value of another column. </p>

<p>I read that looping through each row would be very bad practice and that it would be better to do everything in one go but I could not find out how to do it with the <code>fillna</code> method.</p>

<p>Data Before</p>

<pre><code>Day  Cat1  Cat2
1    cat   mouse
2    dog   elephant
3    cat   giraf
4    NaN   ant
</code></pre>

<p>Data After</p>

<pre><code>Day  Cat1  Cat2
1    cat   mouse
2    dog   elephant
3    cat   giraf
4    ant   ant
</code></pre>
";;0;;2015-05-20T18:08:33.717;7.0;30357276;2015-05-20T18:16:55.510;;;;;2008527.0;;1;17;<python><pandas>;Pandas - FillNa with another column;11384.0
28740;28740;30470630.0;2.0;"<p>I'm attempting to read a CSV file into a Dataframe in Pandas. When I try to do that, I get the following error:</p>

<blockquote>
  <p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x96 in position 55: invalid start byte</p>
</blockquote>

<p>This is from code:</p>

<pre><code>import pandas as pd

location = r""C:\Users\khtad\Documents\test.csv""

df = pd.read_csv(location, header=0, quotechar='""')
</code></pre>

<p>This is on a Windows 7 Enterprise Service Pack 1 machine and it seems to apply to every CSV file I create. In this particular case the binary from location 55 is 00101001 and location 54 is 01110011, if that matters. </p>

<p>Saving the file as UTF-8 with a text editor doesn't seem to help, either. Similarly, adding the param ""encoding='utf-8' doesn't work, either--it returns the same error.</p>

<p>What is the most likely cause of this error and are there any workarounds other than abandoning the DataFrame construct for the moment and using the csv module to read in the CSV line-by-line?</p>
";;9;;2015-05-26T15:28:16.797;7.0;30462807;2017-04-27T23:40:20.570;2015-05-26T17:17:36.287;;4035775.0;;4035775.0;;1;14;<csv><pandas><utf-8>;Encoding Error in Panda read_csv;17680.0
28903;28903;;1.0;"<p>I'm new to Pandas. I downloaded and installed <a href=""http://continuum.io/downloads"" rel=""nofollow noreferrer"">Anaconda</a>.  Then I tried running the following code via the Spyder app:</p>

<pre><code>import pandas as pd
import numpy as np

train = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')
train
</code></pre>

<p>Although this prints the dataframe as I expected, it also shows these errors</p>

<pre><code>//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1969: RuntimeWarning: invalid value encountered in greater
  has_large_values = (abs_vals &gt; 1e8).any()
//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1970: RuntimeWarning: invalid value encountered in less
  has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp;
//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1971: RuntimeWarning: invalid value encountered in greater
  (abs_vals &gt; 0)).any()
</code></pre>

<p>Why am I getting these errors?</p>

<p>EDIT: I just tested the above code in an <code>IPython</code> notebook and it works without errors.  So, is there something wrong with my <code>Spyder</code> installation?  Any help would be appreciated.</p>

<p>EDIT2: After some testing, I can read the first 5 rows of the CSV without getting the warning.  So, I suspect a <code>NaN</code> in the 6th row for a <code>float64</code> type column is triggering the warning.</p>
";;6;;2015-05-29T00:59:50.200;8.0;30519487;2017-03-03T20:32:59.263;2017-03-03T20:32:59.263;;769871.0;;2146894.0;;1;19;<python><pandas><anaconda><python-3.4><kaggle>;Pandas error - invalid value encountered;6905.0
28923;28923;30522778.0;3.0;"<p>How do I take multiple lists and put them as different columns in a python dataframe? I tried following <a href=""https://stackoverflow.com/questions/29014618/read-lists-into-columns-of-pandas-dataframe"">Read lists into columns of pandas DataFrame</a> but had some trouble.</p>

<p>Attempt 1:</p>

<ul>
<li>Have three lists, and zip them together and use that res = zip(lst1,lst2,lst3)</li>
<li>Yields just one column</li>
</ul>

<p>Attempt 2:</p>

<pre><code>percentile_list = pd.DataFrame({'lst1Tite' : [lst1],
 'lst2Tite' : [lst2],
 'lst3Tite':[lst3]
  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])
</code></pre>

<p>- yields either one row by 3 columns (the way above) or if I transpose it is 3 rows and 1 column</p>

<p>How do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe? </p>
";;0;;2015-05-29T06:37:49.040;10.0;30522724;2017-06-16T09:47:41.917;2017-05-23T12:26:27.013;;-1.0;;3105909.0;;1;30;<python><numpy><pandas>;Take multiple lists into dataframe;38497.0
28925;28925;30525128.0;6.0;"<p>I am doing some exercises with datasets like so:</p>

<p><strong>List with many dictionaries</strong></p>

<pre><code>users = [
    {""id"": 0, ""name"": ""Ashley""},
    {""id"": 1, ""name"": ""Ben""},
    {""id"": 2, ""name"": ""Conrad""},
    {""id"": 3, ""name"": ""Doug""},
    {""id"": 4, ""name"": ""Evin""},
    {""id"": 5, ""name"": ""Florian""},
    {""id"": 6, ""name"": ""Gerald""}
]
</code></pre>

<p><strong>Dictionary with few lists</strong></p>

<pre><code>users2 = {
    ""id"": [0, 1, 2, 3, 4, 5, 6],
    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]
}
</code></pre>

<p><strong>Pandas dataframes</strong></p>

<pre><code>import pandas as pd
pd_users = pd.DataFrame(users)
pd_users2 = pd.DataFrame(users2)
print pd_users == pd_users2
</code></pre>

<p>Questions: </p>

<ol>
<li>Should I structure the datasets like users or like users2?</li>
<li>Are there performance differences?</li>
<li>Is one more readable than the other?</li>
<li>Is there a standard I should follow? </li>
<li>I usually convert these to pandas dataframes. When I do that, both versions are identical... right? </li>
<li>The output is true for each element so it doesn't matter if I work with panda df's right?</li>
</ol>
";;3;;2015-05-29T06:51:57.910;5.0;30522982;2015-11-07T17:21:46.453;2015-11-07T17:21:46.453;;4370109.0;;2469211.0;;1;28;<python><pandas><dataset>;List with many dictionaries VS dictionary with few lists?;1050.0
28957;28957;30531939.0;2.0;"<p>I'm looking for a way to do the equivalent to the sql </p>

<blockquote>
  <p>""SELECT DISTINCT col1, col2 FROM dataframe_table""</p>
</blockquote>

<p>The pandas sql comparison doesn't have anything about ""distinct""</p>

<p>.unique() only works for a single column, so I suppose I could concat the columns, or put them in a list/tuple and compare that way, but this seems like something pandas should do in a more native way.  </p>

<p>Am I missing something obvious, or is there no way to do this?</p>
";;2;;2015-05-29T13:17:32.927;1.0;30530663;2015-05-29T14:18:00.987;;;;;10235.0;;1;24;<python><pandas>;"How to ""select distinct"" across multiple data frame columns in pandas?";25025.0
29269;29269;;4.0;"<p>I need to merge two pandas dataframes on an identifier and a condition where a date in one dataframe is between two dates in the other dataframe.</p>

<p>Dataframe A has a date (""fdate"") and an ID (""cusip""):</p>

<p><img src=""https://i.stack.imgur.com/qQ3UB.png"" alt=""enter image description here""></p>

<p>I need to merge this with this dataframe B:</p>

<p><img src=""https://i.stack.imgur.com/z6ESB.png"" alt=""enter image description here""></p>

<p>on <code>A.cusip==B.ncusip</code> and <code>A.fdate</code> is between <code>B.namedt</code> and <code>B.nameenddt</code>.</p>

<p>In SQL this would be trivial, but the only way I can see how to do this in pandas is to first merge unconditionally on the identifier, and then filter on the date condition:</p>

<pre><code>df = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')
df = df[(df['fdate']&gt;=df['namedt']) &amp; (df['fdate']&lt;=df['nameenddt'])]
</code></pre>

<p>Is this really the best way to do this? It seems that it would be much better if one could filter within the merge so as to avoid having a potentially very large dataframe after the merge but before the filter has completed.</p>
";;8;;2015-06-03T18:33:39.330;5.0;30627968;2017-08-05T22:02:59.550;2017-02-14T07:04:24.257;;368311.0;;368311.0;;1;18;<pandas><join><indexing><timespan><date-range>;Merge pandas dataframes where one value is between two others;3406.0
29285;29285;30653988.0;2.0;"<p>trying to write pandas dataframe to MySQL table using to_sql.  Previously been using flavor='mysql', however it will be depreciated in the future and wanted to start the transition to using SQLAlchemy engine.</p>

<p>sample code:</p>

<pre><code>import pandas as pd
import mysql.connector
from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)
cnx = engine.raw_connection()
data = pd.read_sql('SELECT * FROM sample_table', cnx)
data.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)
</code></pre>

<p>The read works fine but the to_sql has an error:</p>

<p><strong>DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': Wrong number of arguments during string formatting</strong></p>

<p>Why does it look like it is trying to use sqlite? What is the correct use of a sqlalchemy connection with mysql and specifically mysql.connector?</p>

<p>I also tried passing the engine in as the connection as well, and that gave me an error referencing no cursor object.</p>

<pre><code>data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)
&gt;&gt;AttributeError: 'Engine' object has no attribute 'cursor'
</code></pre>
";;7;;2015-06-03T21:45:10.360;13.0;30631325;2017-07-25T01:31:12.583;2015-06-03T22:22:33.367;;2364773.0;;2364773.0;;1;27;<python><mysql><pandas><sqlalchemy><mysql-connector>;Writing to MySQL database with pandas using SQLAlchemy, to_sql;22941.0
29369;29369;30652445.0;1.0;"<p><code>msgpack</code> in Pandas is supposed to be a replacement for <code>pickle</code>.</p>

<p>Per the <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#msgpack-experimental"" rel=""noreferrer"">Pandas docs on msgpack</a>:</p>

<blockquote>
  <p>This is a lightweight portable binary format, similar to binary JSON,
  that is highly space efficient, and provides good performance both on
  the writing (serialization), and reading (deserialization).</p>
</blockquote>

<p>I find, however, that its performance does not appear to stack up against pickle.</p>

<pre><code>df = pd.DataFrame(np.random.randn(10000, 100))

&gt;&gt;&gt; %timeit df.to_pickle('test.p')
10 loops, best of 3: 22.4 ms per loop

&gt;&gt;&gt; %timeit df.to_msgpack('test.msg')
10 loops, best of 3: 36.4 ms per loop

&gt;&gt;&gt; %timeit pd.read_pickle('test.p')
100 loops, best of 3: 10.5 ms per loop

&gt;&gt;&gt; %timeit pd.read_msgpack('test.msg')
10 loops, best of 3: 24.6 ms per loop
</code></pre>

<p><strong>Question:</strong>  Asides from potential security issues with pickle, what are the benefits of msgpack over pickle?  Is pickle still the preferred method of serializing data, or do better alternatives currently exist?</p>
";;1;;2015-06-04T18:43:54.453;2.0;30651724;2015-06-05T04:38:14.777;2015-06-05T04:38:14.777;;2411802.0;;2411802.0;;1;11;<python><pandas><msgpack>;Pandas msgpack vs pickle;3100.0
29378;29378;;1.0;"<p>I am trying to model the score that a post receives, based on both the text of the post, and other features (time of day, length of post, etc.)</p>

<p>I am wondering how to best combine these different types of features into one model. Right now, I have something like the following (stolen from <a href=""https://stackoverflow.com/questions/22687365/concatenate-custom-features-with-countvectorizer"">here</a> and <a href=""https://stackoverflow.com/questions/27993058/pandas-apply-to-dateframe-produces-built-in-method-values-of"">here</a>). </p>

<pre><code>import pandas as pd
...

def features(p):
    terms = vectorizer(p[0])
    d = {'feature_1': p[1], 'feature_2': p[2]}
    for t in terms:
        d[t] = d.get(t, 0) + 1
    return d

posts = pd.read_csv('path/to/csv')

# Create vectorizer for function to use
vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()
y = posts[""score""].values.astype(np.float32) 
vect = DictVectorizer()

# This is the part I want to fix
temp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))
tokenized = map(lambda x: features(x), temp)
X = vect.fit_transform(tokenized)
</code></pre>

<p>It seems very silly to extract all of the features I want out of the pandas dataframe, just to zip them all back together. Is there a better way of doing this step?</p>

<p>The CSV looks something like the following:</p>

<pre><code>ID,message,feature_1,feature_2
1,'This is the text',4,7
2,'This is more text',3,2
...
</code></pre>
";;2;;2015-06-04T20:34:50.810;8.0;30653642;2015-07-11T20:49:05.530;2017-05-23T11:54:39.773;;-1.0;;674301.0;;1;13;<python><pandas><machine-learning><nlp><scikit-learn>;Combining bag of words and other features in one model using sklearn and pandas;2293.0
29697;29697;30781664.0;2.0;"<p>I've read <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers"" rel=""nofollow noreferrer"">the docs about slicers</a> a million times, but have never got my head round it, so I'm still trying to figure out how to use <code>loc</code> to slice a <code>DataFrame</code> with a <code>MultiIndex</code>.</p>

<p>I'll start with the <code>DataFrame</code> from <a href=""https://stackoverflow.com/a/22987532/2071807"">this SO answer</a>:</p>

<pre><code>                           value
first second third fourth       
A0    B0     C1    D0          2
                   D1          3
             C3    D0          6
                   D1          7
      B1     C1    D0         10
                   D1         11
             C3    D0         14
                   D1         15
A1    B0     C1    D0         18
                   D1         19
             C3    D0         22
                   D1         23
      B1     C1    D0         26
                   D1         27
             C3    D0         30
                   D1         31
A2    B0     C1    D0         34
                   D1         35
             C3    D0         38
                   D1         39
      B1     C1    D0         42
                   D1         43
             C3    D0         46
                   D1         47
A3    B0     C1    D0         50
                   D1         51
             C3    D0         54
                   D1         55
      B1     C1    D0         58
                   D1         59
             C3    D0         62
                   D1         63
</code></pre>

<p>To select just <code>A0</code> and <code>C1</code> values, I can do:</p>

<pre><code>In [26]: df.loc['A0', :, 'C1', :]
Out[26]: 
                           value
first second third fourth       
A0    B0     C1    D0          2
                   D1          3
      B1     C1    D0         10
                   D1         11
</code></pre>

<p>Which also works selecting from three levels, and even with tuples:</p>

<pre><code>In [28]: df.loc['A0', :, ('C1', 'C2'), 'D1']
Out[28]: 
                           value
first second third fourth       
A0    B0     C1    D1          3
             C2    D1          5
      B1     C1    D1         11
             C2    D1         13
</code></pre>

<p>So far, intuitive and brilliant.</p>

<p>So why can't I select all values from the first index level?</p>

<pre><code>In [30]: df.loc[:, :, 'C1', :]
---------------------------------------------------------------------------
IndexingError                             Traceback (most recent call last)
&lt;ipython-input-30-57b56108d941&gt; in &lt;module&gt;()
----&gt; 1 df.loc[:, :, 'C1', :]

/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)
   1176     def __getitem__(self, key):
   1177         if type(key) is tuple:
-&gt; 1178             return self._getitem_tuple(key)
   1179         else:
   1180             return self._getitem_axis(key, axis=0)

/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)
    694 
    695         # no multi-index, so validate all of the indexers
--&gt; 696         self._has_valid_tuple(tup)
    697 
    698         # ugly hack for GH #836

/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _has_valid_tuple(self, key)
    125         for i, k in enumerate(key):
    126             if i &gt;= self.obj.ndim:
--&gt; 127                 raise IndexingError('Too many indexers')
    128             if not self._has_valid_type(k, i):
    129                 raise ValueError(""Location based indexing can only have [%s] ""

IndexingError: Too many indexers
</code></pre>

<p>Surely this is not intended behaviour?</p>

<p><em>Note: I know this is possible with <code>df.xs('C1', level='third')</code> but the current <code>.loc</code> behaviour seems inconsistent.</em></p>
";;0;;2015-06-11T12:38:50.753;2.0;30781037;2016-07-01T20:20:06.230;2017-05-23T12:25:13.020;;-1.0;;2071807.0;;1;12;<python><pandas>;"""Too many indexers"" with DataFrame.loc";5468.0
29727;29727;30788360.0;2.0;"<p>Say I have the following DataFrame</p>

<pre>
Letter    Number
A          1
B          2
C          3
D          4
</pre>

<p>Which can be obtained through the following code</p>

<pre><code>import pandas as pd

letters=pd.Series(('A', 'B', 'C', 'D'))
numbers=pd.Series((1, 2, 3, 4))
keys=('Letters', 'Numbers')
df=pd.concat((letters, numbers), axis=1, keys=keys)
</code></pre>

<p>Now I want to get the value C from the column Letters.</p>

<p>The command line</p>

<pre><code>df[df.Letters=='C'].Letters
</code></pre>

<p>will return</p>

<pre>
2    C
Name: Letters, dtype: object
</pre>

<p>My question is: how can I get only the value C and not the whole two line output?</p>

<p>Thank you very much.<br>
Eduardo</p>
";;2;;2015-06-11T17:48:35.223;3.0;30787901;2017-03-06T18:39:09.117;;;;;3951929.0;;1;15;<python><pandas><dataframe>;How to get a value from a Pandas DataFrame and not the index and object type;21224.0
29793;29793;30808571.0;3.0;"<p>I have a dataframe in python pandas. The structure of the dataframe is as the following: </p>

<pre><code>   a    b    c    d1   d2   d3 
   10   14   12   44  45    78
</code></pre>

<p>I would like to select the columns which begin with d. Is there a simple way to achieve this in python . </p>
";;0;;2015-06-12T16:55:19.120;3.0;30808430;2017-05-27T05:03:25.197;2015-06-12T19:25:53.117;;2411802.0;;4867396.0;;1;15;<python><python-2.7><pandas>;How to select columns from dataframe by regex;8035.0
30147;30147;31105951.0;2.0;"<p>What is the recommended way (if any) for doing linear regression using a pandas dataframe? I can do it, but my method seems very elaborate. Am I making things unnecessarily complicated?</p>

<p>The R code, for comparison:</p>

<pre><code>x &lt;- c(1,2,3,4,5)
y &lt;- c(2,1,3,5,4)
M &lt;- lm(y~x)
summary(M)$coefficients
            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)      0.6  1.1489125 0.522233 0.6376181
x                0.8  0.3464102 2.309401 0.1040880
</code></pre>

<p>Now, my python (2.7.10), rpy2 (2.6.0), and pandas (0.16.1)
 version:</p>

<pre><code>import pandas
import pandas.rpy.common as common
from rpy2 import robjects
from rpy2.robjects.packages import importr

base = importr('base')
stats = importr('stats')

dataframe = pandas.DataFrame({'x': [1,2,3,4,5], 
                              'y': [2,1,3,5,4]})

robjects.globalenv['dataframe']\
   = common.convert_to_r_dataframe(dataframe) 

M = stats.lm('y~x', data=base.as_symbol('dataframe'))

print(base.summary(M).rx2('coefficients'))

            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)      0.6  1.1489125 0.522233 0.6376181
x                0.8  0.3464102 2.309401 0.1040880
</code></pre>

<p>By the way, I do get a FutureWarning on the import of <code>pandas.rpy.common</code>. However, when I tried the <code>pandas2ri.py2ri(dataframe)</code> to convert a dataframe from pandas to R (as mentioned <a href=""http://pandas.pydata.org/pandas-docs/stable/r_interface.html"">here</a>), I get </p>

<pre><code>NotImplementedError: Conversion 'py2ri' not defined for objects of type '&lt;class 'pandas.core.series.Series'&gt;'
</code></pre>
";;3;;2015-06-18T17:44:33.677;4.0;30922213;2015-07-03T12:12:58.207;2015-06-20T17:24:08.627;;1009979.0;;1009979.0;;1;12;<r><pandas><rpy2>;Minimal example of rpy2 regression using pandas data frame;3664.0
30163;30163;;3.0;"<p>I have a dataframe that has the columns</p>

<ol>
<li>user_id</li>
<li>item_bought</li>
</ol>

<p>Here user_id is the index of the df. I want to group by both user_id and item_bought and get the item wise count for the user. How do I do that.</p>

<p>Thanks</p>
";;4;;2015-06-18T20:20:21.247;2.0;30925079;2017-05-06T14:19:34.300;;;;;374092.0;;1;18;<pandas>;Group by index + column in pandas;18605.0
30176;30176;30926717.0;3.0;"<p>This may be a stupid question, but how do I add multiple empty columns to a DataFrame from a list?</p>

<p>I can do:</p>

<pre><code>df[""B""] = None
df[""C""] = None
df[""D""] = None
</code></pre>

<p>But I can't do:</p>

<pre><code>df[[""B"", ""C"", ""D""]] = None

KeyError: ""['B' 'C' 'D'] not in index""
</code></pre>
";;0;;2015-06-18T22:09:43.213;4.0;30926670;2017-07-06T14:11:58.887;2016-04-10T06:47:15.760;;4909923.0;;4909923.0;;1;26;<python><pandas>;Pandas: Add multiple empty columns to DataFrame;14668.0
30515;30515;31029857.0;3.0;"<p>I have a data frame with categorical data:</p>

<pre><code>     colour  direction
1    red     up
2    blue    up
3    green   down
4    red     left
5    red     right
6    yellow  down
7    blue    down
</code></pre>

<p>and now I want to generate some graphs, like pie charts and histograns based on the categories. Is it possible without creating dummy numeric variables? Something like</p>

<pre><code>df.plot(kind='hist')
</code></pre>
";;0;;2015-06-24T14:37:16.440;4.0;31029560;2017-08-22T14:52:18.867;;;;;307283.0;;1;25;<python><pandas>;Plotting categorical data with pandas and matplotlib;20647.0
30545;30545;31037360.0;1.0;"<p>I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column <code>weight</code></p>

<pre><code>&gt;&gt;&gt; allDF 
         ID           birthyear  weight
0        619040       1962       0.1231231
1        600161       1963       0.981742
2      25602033       1963       1.3123124     
3        624870       1987       0.94212
</code></pre>

<p>The following returns several values, not one:</p>

<pre><code>allDF[['weight']].mean(axis=1)
</code></pre>

<p>So does this:</p>

<pre><code>allDF.groupby('weight').mean()
</code></pre>
";;2;;2015-06-24T21:22:11.053;4.0;31037298;2016-12-27T13:50:55.173;2016-12-27T13:50:55.173;;1398915.0;;1398915.0;;1;18;<python><pandas>;pandas get column average/mean;49735.0
31134;31134;31247247.0;3.0;"<p>I have pandas DataFrame like this</p>

<pre><code>        X    Y  Z    Value 
0      18   55  1      70   
1      18   55  2      67 
2      18   57  2      75     
3      18   58  1      35  
4      19   54  2      70   
</code></pre>

<p>I want to write this data to a text File in this way,</p>

<pre><code>18 55 1 70   
18 55 2 67 
18 57 2 75     
18 58 1 35  
19 54 2 70 
</code></pre>

<p>I have tried something like </p>

<pre><code>f = open(writePath, 'a')
f.writelines(['\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])
f.close()
</code></pre>

<p>but its not working.
how to do this?      </p>
";;0;;2015-07-06T13:30:05.743;8.0;31247198;2016-06-16T20:07:51.033;;;;;4233113.0;;1;17;<python><file><pandas>;Python, Pandas : write content of DataFrame into text File;34026.0
31451;31451;;2.0;"<p>I installed <code>pandas</code> and <code>matplotlib</code> using <code>pip3 install</code>. I then ran this script:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
data = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")
print (data.sheet_names)
</code></pre>

<p>and received this error:</p>

<pre class=""lang-none prettyprint-override""><code>dhcp-169-233-172-97:Obesity juliushamilton$ python3 ob.py
Traceback (most recent call last):
  File ""ob.py"", line 4, in &lt;module&gt;
    data = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/io/excel.py"", line 169, in __init__
    import xlrd  # throw an ImportError if we need to
ImportError: No module named 'xlrd'
</code></pre>

<p>Why is the necessary <code>xlrd</code> missing?</p>
";;1;;2015-07-09T22:51:02.223;1.0;31329627;2017-01-06T16:51:10.850;2015-07-10T16:23:39.803;;635549.0;;4052200.0;;1;14;<python><pandas><matplotlib><pip>;Why don't I have xlrd?;19895.0
31456;31456;31331449.0;2.0;"<p>I cleaned 400 excel files and read them into python using pandas and appended all the raw data into one big df.</p>

<p>Then when I try to export it to a csv:</p>

<pre><code>df.to_csv(""path"",header=True,index=False)
</code></pre>

<p>I get this error:</p>

<pre><code>UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 20: ordinal not in range(128)
</code></pre>

<p>Can someone suggest a way to fix this and what it means?</p>

<p>Thanks</p>
";;3;;2015-07-10T02:09:04.457;3.0;31331358;2016-03-17T06:12:34.327;;;;;4765705.0;;1;11;<python><pandas><export-to-csv><python-unicode>;Unicode Encode Error when writing pandas df to csv;9938.0
31548;31548;31357733.0;4.0;"<p>I have an existing plot that was created with pandas like this:</p>

<pre><code>df['myvar'].plot(kind='bar')
</code></pre>

<p>The y axis is format as float and I want to change the y axis to percentages.  All of the solutions I found use ax.xyz syntax and <strong>I can only place code below the line above that creates the plot</strong> (I cannot add ax=ax to the line above.) </p>

<p><strong>How can I format the y axis as percentages without changing the line above?</strong></p>

<p>Here is the solution I found <strong>but requires that I redefine the plot</strong>:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as mtick

data = [8,12,15,17,18,18.5]
perc = np.linspace(0,100,len(data))

fig = plt.figure(1, (7,4))
ax = fig.add_subplot(1,1,1)

ax.plot(perc, data)

fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
xticks = mtick.FormatStrFormatter(fmt)
ax.xaxis.set_major_formatter(xticks)

plt.show()
</code></pre>

<p>Link to the above solution: <a href=""https://stackoverflow.com/questions/26294360/pyplot-using-percentage-on-x-axis"">Pyplot: using percentage on x axis</a></p>
";;0;;2015-07-11T13:21:01.793;10.0;31357611;2016-10-26T08:35:36.363;2017-05-23T10:31:16.057;;-1.0;;3971910.0;;1;21;<python><pandas><matplotlib><plot>;Format y axis as percent;18791.0
31567;31567;31364127.0;2.0;"<p>I recently found <a href=""http://dask.pydata.org/en/latest/index.html"">dask</a> module that aims to be an easy-to-use python parallel processing module. Big selling point for me is that it works with pandas.</p>

<p>After reading a bit on its manual page, I can't find a way to do this trivially parallelizable task: </p>

<pre><code>ts.apply(func) # for pandas series
df.apply(func, axis = 1) # for pandas DF row apply
</code></pre>

<p>At the moment, to achieve this in dask, AFAIK,</p>

<pre><code>ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame
</code></pre>

<p>which is ugly syntax and is actually slower than outright</p>

<pre><code>df.apply(func, axis = 1) # for pandas DF row apply
</code></pre>

<p>Any suggestion?</p>

<p>Edit: Thanks @MRocklin for the map function. It seems to be slower than plain pandas apply. Is this related to pandas GIL releasing issue or am I doing it wrong?</p>

<pre><code>import dask.dataframe as dd
s = pd.Series([10000]*120)
ds = dd.from_pandas(s, npartitions = 3)

def slow_func(k):
    A = np.random.normal(size = k) # k = 10000
    s = 0
    for a in A:
        if a &gt; 0:
            s += 1
        else:
            s -= 1
    return s

s.apply(slow_func) # 0.43 sec
ds.map(slow_func).compute() # 2.04 sec
</code></pre>
";;3;;2015-07-11T20:52:46.553;12.0;31361721;2017-06-30T04:30:57.860;2015-07-18T10:05:16.783;;839601.0;;1568919.0;;1;24;<python><pandas><parallel-processing><dask>;python dask DataFrame, support for (trivially parallelizable) row apply?;4089.0
32044;32044;;3.0;"<p>Is it possible to add Tooltips to a Timeseries chart?</p>

<p>In the simplified code example below, I want to see a single column name ('a','b' or 'c') when the mouse hovers over the relevant line.</p>

<p>Instead, a ""???"" is displayed and ALL three lines get a tool tip (rather than just the one im hovering over)</p>

<p><img src=""https://i.stack.imgur.com/Qj925.png"" alt=""enter image description here""></p>

<p>Per the documentation (
<a href=""http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#hovertool"" rel=""noreferrer"">http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#hovertool</a>), field names starting with @ are interpreted as columns on the data source.</p>

<ol>
<li><p>How can I display the 'columns' from a pandas dataframe in the tooltip?</p></li>
<li><p>Or, if the high level TimeSeries interface doesnt support this, any clues for using the lower level interfaces to do the same thing? (line? multi_line?) or convert the DataFrame into a different format (ColumnDataSource?)</p></li>
<li><p>For bonus credit, how should the ""$x"" be formated to display the date as a date?</p></li>
</ol>

<p>thanks in advance</p>

<pre><code>    import pandas as pd
    import numpy as np
    from bokeh.charts import TimeSeries
    from bokeh.models import HoverTool
    from bokeh.plotting import show

    toy_df = pd.DataFrame(data=np.random.rand(5,3), columns = ('a', 'b' ,'c'), index = pd.DatetimeIndex(start='01-01-2015',periods=5, freq='d'))   

    p = TimeSeries(toy_df, tools='hover')  

    hover = p.select(dict(type=HoverTool))
    hover.tooltips = [
        (""Series"", ""@columns""),
        (""Date"", ""$x""),
        (""Value"", ""$y""),
        ]

    show(p)
</code></pre>
";;2;;2015-07-18T23:43:02.157;7.0;31496628;2017-08-09T15:47:36.210;2015-07-20T03:28:27.007;;839957.0;;5028529.0;;1;16;<python-3.x><pandas><tooltip><bokeh><timeserieschart>;In Bokeh, how do I add tooltips to a Timeseries chart (hover tool)?;8590.0
32060;32060;;2.0;"<p>I would like to draw a bootstrap sample of a <code>pandas.DataFrame</code> as efficiently as possible. Using the builtin <code>iloc</code> together with a list of integers seems to be slow:</p>

<pre><code>import pandas
import numpy as np
# Generate some data
n = 5000
values = np.random.uniform(size=(n, 5))
# Construct a pandas.DataFrame
columns = ['a', 'b', 'c', 'd', 'e']
df = pandas.DataFrame(values, columns=columns)
# Bootstrap
%timeit df.iloc[np.random.randint(n, size=n)]
# Out: 1000 loops, best of 3: 1.46 ms per loop
</code></pre>

<p>Indexing the <code>numpy</code> array is of course much faster:</p>

<pre><code>%timeit values[np.random.randint(n, size=n)]
# Out: 10000 loops, best of 3: 159 s per loop
</code></pre>

<p>But even extracting the values, sampling the <code>numpy</code> array, and constructing a new <code>pandas.DataFrame</code> is faster:</p>

<pre><code>%timeit pandas.DataFrame(df.values[np.random.randint(n, size=n)], columns=columns)
# Out: 1000 loops, best of 3: 302 s per loop
</code></pre>

<p>@JohnE suggested <code>sample</code> which is unfortunately even slower:</p>

<pre><code>%timeit df.sample(n, replace=True)
# Out: 100 loops, best of 3: 5.14 ms per loop
</code></pre>

<p>@firelynx suggested <code>merge</code>:</p>

<pre><code>%timeit df.merge(pandas.DataFrame(index=np.random.randint(n, size=n)), left_index=True, right_index=True, how='right')
# Out: 1000 loops, best of 3: 1.23 ms per loop
</code></pre>

<p>Does anyone have an idea why <code>iloc</code> is so slow and/or whether there are better alternatives than extracting the values, sampling and then constructing a new <code>pandas.DataFrame</code>?</p>
";;5;;2015-07-19T15:42:42.930;2.0;31502958;2016-07-26T02:35:44.360;2015-07-21T11:12:34.227;;1150961.0;;1150961.0;;1;15;<numpy><pandas>;Drawing a bootstrap sample from a pandas.DataFrame;3432.0
32076;32076;31512025.0;1.0;"<p>I have a simple DataFrame like the following:</p>

<p><img src=""https://i.stack.imgur.com/KjcSH.png"" alt=""Pandas DataFrame""></p>

<p>I want to select all values from the 'First Season' column and replace those that are over 1990 by 1. In this example, only Baltimore Ravens would have the 1996 replaced by 1 (keeping the rest of the data intact).</p>

<p>I have used the following:</p>

<pre><code>df.loc[(df['First Season'] &gt; 1990)] = 1
</code></pre>

<p>But, it replaces all the values in that row by 1, and not just the values in the 'First Season' column.</p>

<p>How can I replace just the values from that column?</p>
";;0;;2015-07-20T08:35:34.293;9.0;31511997;2015-07-20T08:51:27.270;;;;;4801661.0;;1;19;<python><python-2.7><pandas><dataframe>;Pandas DataFrame: replace all values in a column, based on condition;21833.0
32309;32309;31569866.0;2.0;"<p>given a dataframe that logs uses of some books like this:</p>

<pre><code>Name   Type   ID
Book1  ebook  1
Book2  paper  2
Book3  paper  3
Book1  ebook  1
Book2  paper  2
</code></pre>

<p>I need to get the count of all the books, keeping the other columns and get this:</p>

<pre><code>Name   Type   ID    Count
Book1  ebook  1     2
Book2  paper  2     2
Book3  paper  3     1
</code></pre>

<p>How can this be done?</p>

<p>Thanks!</p>
";;2;;2015-07-22T17:01:02.317;2.0;31569549;2016-06-02T22:06:18.107;2015-07-22T17:16:40.063;;4358680.0;;4256941.0;;1;16;<python><pandas>;How to GroupBy a Dataframe in Pandas and keep Columns;13417.0
32315;32315;31585881.0;1.0;"<p>Can anybody explain why is loc used in python pandas with examples like shown below?</p>

<pre><code>for i in range(0, 2):
for j in range(0, 3):
    df.loc[ (df.Age.isnull()) &amp; (df.Gender == i) &amp; (df.Pclass == j+1),\
            'AgeFill'] = median_ages[i,j]
</code></pre>
";;1;;2015-07-22T18:29:40.237;4.0;31571217;2015-07-23T11:22:05.550;;;;;4105335.0;;1;11;<python><pandas><machine-learning>;loc function in pandas;18712.0
32398;32398;31593712.0;2.0;"<p>Can someone explain how these three methods of slicing are different?<br>
I've seen <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""noreferrer"">the docs</a>, 
and I've seen <a href=""https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat"">these</a> <a href=""https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i"">answers</a>, but I still find myself unable to explain how the three are different.  To me, they seem interchangeable in large part, because they are at the lower levels of slicing.</p>

<p>For example, say we want to get the first five rows of a <code>DataFrame</code>.  How is it that all three of these work?</p>

<pre><code>df.loc[:5]
df.ix[:5]
df.iloc[:5]
</code></pre>

<p>Can someone present three cases where the distinction in uses are clearer?</p>
";;2;;2015-07-23T16:34:10.667;161.0;31593201;2017-05-06T09:25:49.550;2017-05-23T12:02:46.643;;-1.0;;4967110.0;;1;218;<python><pandas><indexing><dataframe>;pandas iloc vs ix vs loc explanation?;118214.0
32433;32433;;1.0;"<p>I installed anaconda to use pandas and scipy. I reading and watching pandas tutorials and they all say to open the ipython notebook using</p>

<pre><code> ipython notebook --pylab==inline
</code></pre>

<p>but when I do that I get a message saying</p>

<pre><code>""Support for specifying --pylab on the command line has been removed. Please use '%pylab = inline' or '%matplotlib =inline' in the notebook itself""
</code></pre>

<p>But that does not work. Then when I try ""plot(arange(10))"" I get a message saying ""name 'plot' is not defined."" I trying plotting data from a .csv file and got</p>

<pre><code>""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".
</code></pre>

<p>What should I do?</p>
";;2;;2015-07-24T11:38:11.643;5.0;31609600;2016-01-22T15:07:57.557;2015-07-24T11:43:27.147;;4248931.0;;5152041.0;;1;30;<python><pandas><ipython>;Jupyter (IPython) notebook not plotting;22330.0
32436;32436;31610890.0;1.0;"<p>In <a href=""https://stackoverflow.com/q/17729853/2071807"">questions</a> and <a href=""https://stackoverflow.com/a/31609618/2071807"">answers</a>, users very often post an example <code>DataFrame</code> which their question/answer works with:</p>

<pre><code>In []: x
Out[]: 
   bar  foo
0    4    1
1    5    2
2    6    3
</code></pre>

<p>It'd be really useful to be able to get this <code>DataFrame</code> into my Python interpreter so I can start debugging the question, or testing the answer.</p>

<p>How can I do this?</p>
";;0;;2015-07-24T12:45:23.867;3.0;31610889;2015-09-03T13:31:32.170;2017-05-23T12:34:33.857;;-1.0;;2071807.0;;1;16;<python><pandas>;How to copy/paste DataFrame from StackOverflow into Python;1675.0
32482;32482;31617974.0;4.0;"<p>I am trying to modify a DataFrame <code>df</code> to only contain rows for which the values in the column <code>closing_price</code> are between 99 and 101 and trying to do this with the code below. </p>

<p>However, I get the error </p>

<blockquote>
  <p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()</p>
</blockquote>

<p>and I am wondering if there is a way to do this without using loops.</p>

<pre><code>df = df[(99 &lt;= df['closing_price'] &lt;= 101)]
</code></pre>
";;1;;2015-07-24T18:56:10.253;2.0;31617845;2017-08-22T16:40:26.133;2015-07-26T14:14:55.337;;4370109.0;;3364569.0;;1;12;<python><pandas>;How to select rows in a DataFrame between two values, in Python Pandas?;12492.0
32590;32590;31679396.0;2.0;"<p>I'm working turning a list of records with two columns (A and B) into a matrix representation. I have been using the pivot function within pandas, but the result ends up being fairly large. Does pandas support pivoting into a sparse format? I know I can pivot it and then turn it into some kind of sparse representation, but isn't as elegant as I would like. My end goal is to use it as the input for a predictive model.</p>

<p>Alternatively, is there some kind of sparse pivot capability outside of pandas?</p>

<p>edit: here is an example of a non-sparse pivot</p>

<pre><code>import pandas as pd
frame=pd.DataFrame()
frame['person']=['me','you','him','you','him','me']
frame['thing']=['a','a','b','c','d','d']
frame['count']=[1,1,1,1,1,1]

frame

  person thing  count
0     me     a      1
1    you     a      1
2    him     b      1
3    you     c      1
4    him     d      1
5     me     d      1

frame.pivot('person','thing')

        count            
thing       a   b   c   d
person                   
him       NaN   1 NaN   1
me          1 NaN NaN   1
you         1 NaN   1 NaN
</code></pre>

<p>This creates a matrix that could contain all possible combinations of persons and things, but it is not sparse.</p>

<p><a href=""http://docs.scipy.org/doc/scipy/reference/sparse.html"">http://docs.scipy.org/doc/scipy/reference/sparse.html</a></p>

<p>Sparse matrices take up less space because they can imply things like NaN or 0. If I have a very large data set, this pivoting function can generate a matrix that should be sparse due to the large number of NaNs or 0s. I was hoping that I could save a lot of space/memory by generating something that was sparse right off the bat rather than creating a dense matrix and then converting it to sparse.</p>
";;5;;2015-07-27T19:26:38.790;9.0;31661604;2016-07-23T21:18:45.883;2015-07-28T12:22:42.570;;2993924.0;;2993924.0;;1;12;<python><pandas><scipy><scikit-learn><sparse-matrix>;Efficiently create sparse pivot tables in pandas?;1860.0
33370;33370;32764796.0;4.0;"<p>I have a Pandas DataFrame that looks similar to this but with 10,000 rows and 500 columns.</p>

<p><a href=""https://i.stack.imgur.com/SVMYI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SVMYI.png"" alt=""My Dataframe""></a></p>

<p>For each row, I would like to find the minimum value between 3 days ago at 15:00 and today at 13:30. </p>

<p>Is there some native numpy way to do this quickly? 
My goal is to be able to get the minimum value for each row by saying something like ""what is the minimum value from 3 days ago ago 15:00 to 0 days ago (aka today) 13:30?""</p>

<p>For this particular example the answers for the last two rows would be:</p>

<pre><code>2011-01-09 2481.22
2011-01-10 2481.22
</code></pre>

<p>My current way is this:</p>

<pre><code>1. Get the earliest row (only the values after the start time)
2. Get the middle rows 
3. Get the last row (only the values before the end time)
4. Concat (1), (2), and (3)
5. Get the minimum of (4)
</code></pre>

<p>But this takes a very long time on a large DataFrame</p>

<hr>

<p>The following code will generate a similar DF:</p>

<pre><code>import numpy
import pandas
import datetime

numpy.random.seed(0)

random_numbers = (numpy.random.rand(10, 8)*100 + 2000)
columns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] 
index          = pandas.date_range('2011/1/1', '2011/1/10')
df             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)

print df
</code></pre>

<hr>

<p>Here is the json version of the dataframe:</p>

<p>'{""13:00:00"":{""1293840000000"":2085,""1293926400000"":2062,""1294012800000"":2035,""1294099200000"":2086,""1294185600000"":2006,""1294272000000"":2097,""1294358400000"":2078,""1294444800000"":2055,""1294531200000"":2023,""1294617600000"":2024},""13:30:00"":{""1293840000000"":2045,""1293926400000"":2039,""1294012800000"":2035,""1294099200000"":2045,""1294185600000"":2025,""1294272000000"":2099,""1294358400000"":2028,""1294444800000"":2028,""1294531200000"":2034,""1294617600000"":2010},""14:00:00"":{""1293840000000"":2095,""1293926400000"":2006,""1294012800000"":2001,""1294099200000"":2032,""1294185600000"":2022,""1294272000000"":2040,""1294358400000"":2024,""1294444800000"":2070,""1294531200000"":2081,""1294617600000"":2095},""14:30:00"":{""1293840000000"":2057,""1293926400000"":2042,""1294012800000"":2018,""1294099200000"":2023,""1294185600000"":2025,""1294272000000"":2016,""1294358400000"":2066,""1294444800000"":2041,""1294531200000"":2098,""1294617600000"":2023},""15:00:00"":{""1293840000000"":2082,""1293926400000"":2025,""1294012800000"":2040,""1294099200000"":2061,""1294185600000"":2013,""1294272000000"":2063,""1294358400000"":2024,""1294444800000"":2036,""1294531200000"":2096,""1294617600000"":2068},""15:30:00"":{""1293840000000"":2090,""1293926400000"":2084,""1294012800000"":2092,""1294099200000"":2003,""1294185600000"":2001,""1294272000000"":2049,""1294358400000"":2066,""1294444800000"":2082,""1294531200000"":2090,""1294617600000"":2005},""16:00:00"":{""1293840000000"":2081,""1293926400000"":2003,""1294012800000"":2009,""1294099200000"":2001,""1294185600000"":2011,""1294272000000"":2098,""1294358400000"":2051,""1294444800000"":2092,""1294531200000"":2029,""1294617600000"":2073},""16:30:00"":{""1293840000000"":2015,""1293926400000"":2095,""1294012800000"":2094,""1294099200000"":2042,""1294185600000"":2061,""1294272000000"":2006,""1294358400000"":2042,""1294444800000"":2004,""1294531200000"":2099,""1294617600000"":2088}}'</p>
";;11;;2015-08-06T22:25:45.097;1.0;31866802;2015-10-01T10:32:51.073;2015-09-24T15:23:32.853;;1367204.0;;1367204.0;;1;14;<python><arrays><numpy><pandas><dataframe>;Pandas DataFrame: How to natively get minimum across range of rows and columns;897.0
33740;33740;;4.0;"<p>Is there any way to do an SQL update-where from a dataframe without iterating through each line? I have a postgresql database and to update a table in the db from a dataframe I would use psycopg2 and do something like:</p>

<pre><code>con = psycopg2.connect(database='mydb', user='abc', password='xyz')
cur = con.cursor()

for index, row in df.iterrows():
    sql = 'update table set column = %s where column = %s'
    cur.execute(sql, (row['whatver'], row['something']))
con.commit()
</code></pre>

<p>But on the other hand if im either reading a table from sql or writing an entire dataframe to sql (with no update-where), then I would just use pandas and sqlalchemy. Something like:</p>

<pre><code>engine = create_engine('postgresql+psycopg2://user:pswd@mydb')
df.to_sql('table', engine, if_exists='append')
</code></pre>

<p>It's great just having a 'one-liner' using to_sql. Isn't there something similar to do an update-where from pandas to postgresql? Or is the only way to do it by iterating through each row like i've done above. Isn't iterating through each row an inefficient way to do it?</p>
";;1;;2015-08-13T12:30:29.197;2.0;31988322;2016-11-22T16:03:53.043;;;;;4169229.0;;1;13;<python><postgresql><pandas>;Pandas update sql;5751.0
33839;33839;32011969.0;4.0;"<p>I have a dataframe with this type of data (too many columns):</p>

<pre><code>col1        int64
col2        int64
col3        category
col4        category
col5        category
</code></pre>

<p>Columns seems like this:</p>

<pre><code>Name: col3, dtype: category
Categories (8, object): [B, C, E, G, H, N, S, W]
</code></pre>

<p>I want to convert all value in columns to integer like this:</p>

<pre><code>[1, 2, 3, 4, 5, 6, 7, 8]
</code></pre>

<p>I solved this for one column by this:</p>

<pre><code>dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes
</code></pre>

<p>Now I have two columns in my dataframe - old 'col3' and new 'c' and need to drop old columns. </p>

<p>That's bad practice. It's work but in my dataframe many columns and I don't want do it manually.  </p>

<p>How do this pythonic and just cleverly?</p>
";;0;;2015-08-14T13:31:50.780;22.0;32011359;2017-07-30T22:47:16.460;2015-08-23T17:44:51.530;;4833034.0;;4833034.0;;1;24;<python><pandas>;Convert categorical data in pandas dataframe;29142.0
33841;33841;32012129.0;3.0;"<p>Given the below pandas DataFrame:</p>

<pre><code>In [115]: times = pd.to_datetime(pd.Series(['2014-08-25 21:00:00','2014-08-25 21:04:00',
                                            '2014-08-25 22:07:00','2014-08-25 22:09:00']))
          locations = ['HK', 'LDN', 'LDN', 'LDN']
          event = ['foo', 'bar', 'baz', 'qux']
          df = pd.DataFrame({'Location': locations,
                             'Event': event}, index=times)
          df
Out[115]:
                               Event Location
          2014-08-25 21:00:00  foo   HK
          2014-08-25 21:04:00  bar   LDN
          2014-08-25 22:07:00  baz   LDN
          2014-08-25 22:09:00  qux   LDN
</code></pre>

<p>I would like resample the data to aggregate it hourly by count while grouping by location to produce a data frame that looks like this:</p>

<pre><code>Out[115]:
                               HK    LDN
          2014-08-25 21:00:00  1     1
          2014-08-25 22:00:00  0     2
</code></pre>

<p>I've tried various combinations of resample() and groupby() but with no luck. How would I go about this?</p>
";;0;;2015-08-14T14:04:02.123;7.0;32012012;2016-08-27T22:34:22.773;;;;;1177720.0;;1;11;<python><pandas><group-by><time-series>;Pandas: resample timeseries with groupby;4612.0
34069;34069;32094352.0;2.0;"<p>I have a dataframe which contains duplicates values according to two columns (A and B):</p>

<pre><code>A B C
1 2 1
1 2 4
2 7 1
3 4 0
3 4 8
</code></pre>

<p>I want to remove duplicates keeping the row with max value in column C. This would lead to: </p>

<pre><code>A B C
1 2 4
2 7 1
3 4 8
</code></pre>

<p>I cannot figure out how to do that. Should I use <code>drop_duplicates()</code>, something else?</p>
";;0;;2015-08-19T11:10:04.797;7.0;32093829;2016-11-07T09:38:42.263;;;;;3218151.0;;1;14;<python><pandas><duplicates>;Python(pandas): removing duplicates based on two columns keeping row with max value in another column;11397.0
34536;34536;32244161.0;2.0;"<p>With the following code:</p>

<pre><code>import matplotlib
matplotlib.style.use('ggplot')
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame({ 'celltype':[""foo"",""bar"",""qux"",""woz""], 's1':[5,9,1,7], 's2':[12,90,13,87]})
df = df[[""celltype"",""s1"",""s2""]]
df.set_index([""celltype""],inplace=True)
df.plot(kind='bar',alpha=0.75)
plt.xlabel("""")
</code></pre>

<p>I made this plot:</p>

<p><a href=""https://i.stack.imgur.com/6pLLq.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6pLLq.jpg"" alt=""enter image description here""></a></p>

<p>How can I rotate the x-axis tick labels to 0 degrees?</p>

<p>I tried adding this but did not work:</p>

<pre><code>plt.set_xticklabels(df.index,rotation=90)
</code></pre>
";;0;;2015-08-27T08:11:34.207;5.0;32244019;2017-01-28T22:06:39.967;2017-01-28T22:06:39.967;;704848.0;;67405.0;;1;22;<python><pandas><matplotlib>;How to rotate x-axis tick labels in Pandas barplot;14916.0
34542;34542;32245026.0;4.0;"<p>I tried the following code (<code>test_seaborn.py</code>):</p>

<pre><code>import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
matplotlib.style.use('ggplot')
import seaborn as sns
sns.set()
df = sns.load_dataset('iris')
sns_plot = sns.pairplot(df, hue='species', size=2.5)
fig = sns_plot.get_figure()
fig.savefig(""output.png"")
#sns.plt.show()
</code></pre>

<p>But I get this error:</p>

<pre><code>  Traceback (most recent call last):
  File ""test_searborn.py"", line 11, in &lt;module&gt;
    fig = sns_plot.get_figure()
AttributeError: 'PairGrid' object has no attribute 'get_figure'
</code></pre>

<p>I expect the final <code>output.png</code> will exist and look like this:</p>

<p><a href=""https://i.stack.imgur.com/n6uXd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/n6uXd.png"" alt=""enter image description here""></a></p>

<p>How can I resolve the problem?</p>
";;0;;2015-08-27T08:51:07.970;10.0;32244753;2017-02-16T21:02:33.310;2015-08-27T08:56:56.763;;67405.0;;67405.0;;1;35;<python><pandas><matplotlib><seaborn>;How to save a Seaborn plot into a file;27148.0
35078;35078;32400969.0;4.0;"<p>I am using Python 3.4 with IPython and have the following code. I'm unable to read a csv-file from the given URL:</p>

<pre><code>import pandas as pd
import requests

url=""https://github.com/cs109/2014_data/blob/master/countries.csv""
s=requests.get(url).content
c=pd.read_csv(s)
</code></pre>

<p>I have the following error</p>

<blockquote>
  <p>""Expected file path name or file-like object, got  type""</p>
</blockquote>

<p>How can I fix this?</p>
";;2;;2015-09-04T14:44:24.740;4.0;32400867;2017-07-27T13:48:11.897;2015-09-04T15:38:50.597;;3991125.0;;5301159.0;;1;22;<python><csv><pandas><request>;Pandas read_csv from url;17280.0
35233;35233;32444187.0;3.0;"<p>I have a list of Pandas dataframes that I would like to combine into one Pandas dataframe.  I am using Python 2.7.10 and Pandas 0.16.2</p>

<p>I created the list of dataframes from:</p>

<pre><code>import pandas as pd
dfs = []
sqlall = ""select * from mytable""

for chunk in pd.read_sql_query(sqlall , cnxn, chunksize=10000):
    dfs.append(chunk)
</code></pre>

<p>This returns a list of dataframes</p>

<pre><code>type(dfs[0])
Out[6]: pandas.core.frame.DataFrame

type(dfs)
Out[7]: list

len(dfs)
Out[8]: 408
</code></pre>

<p>Here is some sample data</p>

<pre><code># sample dataframes
d1 = pd.DataFrame({'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.]})
d2 = pd.DataFrame({'one' : [5., 6., 7., 8.], 'two' : [9., 10., 11., 12.]})
d3 = pd.DataFrame({'one' : [15., 16., 17., 18.], 'two' : [19., 10., 11., 12.]})

# list of dataframes
mydfs = [d1, d2, d3]
</code></pre>

<p>I would like to combine <code>d1</code>, <code>d2</code>, and <code>d3</code> into one pandas dataframe.  Alternatively, a method of reading a large-ish table directly into a dataframe when using the <code>chunksize</code> option would be very helpful.  </p>
";;0;;2015-09-07T18:13:42.283;1.0;32444138;2017-05-02T21:07:51.980;;;;;1683061.0;;1;14;<python><pandas>;Combine a list of pandas dataframes to one pandas dataframe;13235.0
35325;35325;32470490.0;4.0;"<p>I'm looking to turn a pandas cell containing a list into rows for each of those values.</p>

<p>So, take this:</p>

<p><a href=""https://i.stack.imgur.com/j7lFk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j7lFk.png"" alt=""enter image description here""></a></p>

<p>If I'd like to unpack and stack the values in the 'nearest_neighbors"" column so that each value would be a row within each 'opponent' index, how would I best go about this? Are there pandas methods that are meant for operations like this? I'm just not aware.</p>

<p>Thanks in advance, guys.</p>
";;5;;2015-09-08T22:43:05.013;9.0;32468402;2017-06-22T05:45:56.240;2015-09-09T00:22:11.437;;839957.0;;4698759.0;;1;22;<python><pandas><dataframe>;How to explode a list inside a Dataframe cell into separate rows;8761.0
35631;35631;32583988.0;7.0;"<p>I have installed anaconda. Now when i am trying to run </p>

<pre><code>import pandas as pd
</code></pre>

<p>I am getting the following error</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#0&gt;"", line 1, in &lt;module&gt;
import pandasFile
ImportError: No module named pandasFile
</code></pre>

<p>It is my first day to python. I cannot figure out how to fix it. I am hoping that I have to change some path somewhere. I know it can be a silly question to post here. </p>
";;8;;2015-09-14T12:51:02.933;6.0;32565302;2017-02-03T08:16:14.017;;;;;5275540.0;;1;11;<python><pandas><anaconda>;python: after installing anaconda, how to import pandas;39480.0
35753;35753;32591786.0;4.0;"<p>I am importing an excel file into a pandas dataframe with the <code>pandas.read_excel()</code> function.</p>

<p>One of the columns is the primary key of the table: it's all numbers, but it's stored as text (the little green triangle in the top left of the Excel cells confirms this). </p>

<p>However, when I import the file into a pandas dataframe, the column gets imported as a float. This means that, for example, '0614' becomes 614.</p>

<p>Is there a way to specify the datatype when importing a column? I understand this is possible when importing CSV files but couldn't find anything in the syntax of <code>read_excel()</code>. </p>

<p>The only solution I can think of is to add an arbitrary letter at the beginning of the text (converting '0614' into 'A0614') in Excel, to make sure the column is imported as text, and then chopping off the 'A' in python, so I can match it to other tables I am importing from SQL.</p>
";;0;;2015-09-15T16:48:09.733;3.0;32591466;2017-07-22T20:33:43.107;;;;;4045275.0;;1;13;<python><pandas><dataframe>;Python pandas: how to specify data types when reading an Excel file?;15650.0
36247;36247;32751412.0;3.0;"<p>What is the best way to do a groupby on a Pandas dataframe, but exclude some columns from that groupby? E.g. I have the foll. dataframe:</p>

<pre><code>Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963
2   Afghanistan 15          Wheat   5312        Ha      10       20      30
2   Afghanistan 25          Maize   5312        Ha      10       20      30
4   Angola      15          Wheat   7312        Ha      30       40      50
4   Angola      25          Maize   7312        Ha      30       40      50
</code></pre>

<p>I want to groupby the column Country and Item_Code and only compute the sum of the rows falling under the columns Y1961, Y1962 and Y1963. The resulting dataframe should look like this:</p>

<pre><code>Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963
    2   Afghanistan 15      C3      5312        Ha      20       40      60
    4   Angola      25      C4      7312        Ha      60       80      100
</code></pre>

<p>Right now, I am doing this:</p>

<pre><code>df.groupby('Country').sum()
</code></pre>

<p>However, this adds up the values in the Item_Code column as well. Is there any way I can specify which columns to include in the sum() operation and which ones to exclude?</p>
";;0;;2015-09-23T23:45:53.600;3.0;32751229;2016-10-08T22:18:21.840;2015-09-24T00:17:43.617;;202229.0;;308827.0;;1;11;<python><pandas><group-by><aggregate>;Pandas sum by groupby, but exclude certain columns;24183.0
36259;36259;32752318.0;1.0;"<p>I know that if I use<code>randn</code>,</p>

<pre><code>df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD')
</code></pre>

<p>gives me what I am looking for, but with elements from a normal distribution. But what if I just wanted random integers?</p>

<p><code>randint</code> works by providing a range, but not an array like <code>randn</code> does. So how do I do this with random integers between some range?</p>
";;1;;2015-09-24T02:17:00.640;5.0;32752292;2017-04-07T09:19:45.027;2017-04-07T09:19:45.027;;202229.0;;2023745.0;;1;17;<python><pandas><dataframe><size><shape>;Pandas: How to create a data frame of random integers?;16881.0
36279;36279;32761138.0;3.0;"<p>I have a very big <strong>pyspark.sql.dataframe.DataFrame</strong> named df. 
I need some way of enumerating records- thus, being able to access record with certain index. (or select group of records with indexes range)</p>

<p>In pandas, I could make just </p>

<pre><code>indexes=[2,3,6,7] 
df[indexes]
</code></pre>

<p>Here I want something similar, <em>(and without converting dataframe to pandas)</em> </p>

<p>The closest I can get to is:</p>

<ul>
<li><p>Enumerating all the objects in the original dataframe by:</p>

<pre><code>indexes=np.arange(df.count())
df_indexed=df.withColumn('index', indexes)
</code></pre>

<ul>
<li>Searching for  values I need using where() function. </li>
</ul></li>
</ul>

<p>QUESTIONS:</p>

<ol>
<li>Why it doesn't work and how to make it working? How to add a row to a dataframe?</li>
<li><p>Would it work later to make something like:</p>

<pre><code> indexes=[2,3,6,7] 
 df1.where(""index in indexes"").collect()
</code></pre></li>
<li><p>Any faster and simpler way to deal with it?</p></li>
</ol>
";;0;;2015-09-24T12:06:15.197;2.0;32760888;2017-07-11T20:47:30.020;2015-12-07T19:07:07.340;;1560062.0;;5343747.0;;1;12;<python><apache-spark><bigdata><pyspark><rdd>;PySpark DataFrames - way to enumerate without converting to Pandas?;7833.0
36410;36410;32802014.0;3.0;"<p>I am trying to column-bind dataframes and having issue with pandas <code>concat</code>, as <code>ignore_index=True</code> doesn't seem to work:</p>

<pre><code>df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3'],
                    'D': ['D0', 'D1', 'D2', 'D3']},
                    index=[0, 2, 3,4])

df2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],
                    'C': ['C4', 'C5', 'C6', 'C7'],
                    'D2': ['D4', 'D5', 'D6', 'D7']},
                    index=[ 5, 6, 7,3])
df1
#     A   B   D
# 0  A0  B0  D0
# 2  A1  B1  D1
# 3  A2  B2  D2
# 4  A3  B3  D3

df2
#    A1   C  D2
# 5  A4  C4  D4
# 6  A5  C5  D5
# 7  A6  C6  D6
# 3  A7  C7  D7

dfs = [df1,df2]
df = pd.concat( dfs,axis=1,ignore_index=True)     
print df   
</code></pre>

<p>and the result is </p>

<pre><code>     0    1    2    3    4    5    
0   A0   B0   D0  NaN  NaN  NaN  
2   A1   B1   D1  NaN  NaN  NaN    
3   A2   B2   D2   A7   C7   D7   
4   A3   B3   D3  NaN  NaN  NaN  
5  NaN  NaN  NaN   A4   C4   D4  
6  NaN  NaN  NaN   A5   C5   D5  
7  NaN  NaN  NaN   A6   C6   D6           
</code></pre>

<p>Even if I reset index using</p>

<pre><code> df1.reset_index()    
 df2.reset_index() 
</code></pre>

<p>and then try </p>

<pre><code>pd.concat([df1,df2],axis=1) 
</code></pre>

<p>it still produces the same result!</p>
";;5;;2015-09-26T20:38:35.343;4.0;32801806;2017-07-31T04:04:19.023;2017-07-31T04:04:19.023;;4561314.0;;3130926.0;;1;16;<python><pandas><append><concat>;pandas concat ignore_index doesn't work;6496.0
36673;36673;37226672.0;1.0;"<p>I am confused how pandas blew out of bounds for datetime objects with these lines:</p>

<pre><code>import pandas as pd
BOMoffset = pd.tseries.offsets.MonthBegin()
# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters
all_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))
all_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))
</code></pre>

<p>Here <code>all_treatments.iloc[i,micolix]</code> is a datetime set by <code>pd.to_datetime(all_treatments['INDATUMA'], errors='coerce',format='%Y%m%d')</code>, and <code>INDATUMA</code> is date information in the format <code>20070125</code>.</p>

<p>This logic seems to work on mock data (no errors, dates make sense), so at the moment I cannot reproduce while it fails in my entire data with the following error:</p>

<pre><code>pandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-05-01 00:00:00
</code></pre>
";;1;;2015-10-01T12:54:51.037;;32888124;2017-06-20T07:31:40.527;;;;;938408.0;;1;11;<python><datetime><pandas><datetimeoffset>;pandas out of bounds nanosecond timestamp after offset rollforward plus adding a month offset;5520.0
36918;36918;;3.0;"<p>It is quite easy to add many pandas dataframes into excel work book as long as it is different worksheets. But, it is somewhat tricky to get many dataframes into one worksheet if you want to use pandas built-in df.to_excel functionality. </p>

<pre><code># Creating Excel Writer Object from Pandas  
writer = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   
workbook=writer.book
worksheet=workbook.add_worksheet('Validation') 
df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   
another_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) 
</code></pre>

<p>The above code won't work. You will get the error of </p>

<pre><code> Sheetname 'Validation', with case ignored, is already in use.
</code></pre>

<p>Now, I have experimented enough that I found a way to make it work. </p>

<pre><code>writer = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  
workbook=writer.book
df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   
another_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) 
</code></pre>

<p>This will work. So, my purpose of posting this question on stackoverflow is twofold. Firstly, I hope this will help someone if he/she is trying to put many dataframes into a single work sheet at excel. </p>

<p>Secondly,  Can someone help me understand the difference between those two blocks of code? It appears to me that they are pretty much the same except the first block of code created worksheet called ""Validation"" in advance while the second does not. I get that part. </p>

<p>What I don't understand is why should it be any different ? Even if I don't create the worksheet in advance, this line, the line right before the last one, </p>

<pre><code> df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)  
</code></pre>

<p>will create a worksheet anyway. Consequently, by the time we reached the last line of code the worksheet ""Validation"" is already created as well in the second block of code.  So, my question basically, why should the second block of code  work while the first doesn't?  </p>

<p>Please also share if there is another way to put many dataframes into excel using the built-in df.to_excel functionality !! </p>
";;0;;2015-10-05T20:35:03.957;6.0;32957441;2017-04-20T16:22:15.630;2017-04-20T16:22:15.630;;7505321.0;;3817518.0;;1;15;<python><excel><pandas><dataframe><xlsxwriter>;Putting many python pandas dataframes to one excel worksheet;3792.0
37384;37384;33086953.0;3.0;"<p>I have the following problem: I have two pandas data frames of different length containing some rows and columns that have common values and some that are different, like this:</p>

<pre><code>df1:                                 df2:

      Column1  Column2  Column3           ColumnA  ColumnB ColumnC
    0    a        x        x            0    c        y       y
    1    c        x        x            1    e        z       z
    2    e        x        x            2    a        s       s
    3    d        x        x            3    d        f       f
    4    h        x        x
    5    k        x        x            
</code></pre>

<p>What I want to do now is merging the two dataframes so that if ColumnA and Column1 have the same value the rows from df2 are appended to the corresponding row in df1, like this:</p>

<pre><code>df1:
    Column1  Column2  Column3  ColumnB  ColumnC
  0    a        x        x        s        s
  1    c        x        x        y        y
  2    e        x        x        z        z
  3    d        x        x        f        f
  4    h        x        x        NaN      NaN
  5    k        x        x        NaN      NaN
</code></pre>

<p>I know that the merge is doable through <code>df1.merge(df2,left_on='Column1', right_on='ColumnA')</code>, but this command drops all rows that are not the same in Column1 and ColumnA in both files. Instead of that I want to keep these rows in df1 and just assign NaN to them in the columns where other rows have a value from df2, as shown above. Is there a smooth way to do this in pandas?</p>

<p>Thanks in advance!</p>
";;0;;2015-10-12T17:24:16.250;5.0;33086881;2017-07-31T05:00:47.320;;;;;5347728.0;;1;12;<python><pandas><merge><dataframe>;Merge two python pandas data frames of different length but keep all rows in output data frame;10678.0
37395;37395;33088410.0;1.0;"<p>I've got a dataframe <code>df_a</code> with id information:</p>

<pre><code>    unique_id lacet_number 
15    5570613  TLA-0138365 
24    5025490  EMP-0138757 
36    4354431  DXN-0025343 
</code></pre>

<p>and another dataframe <code>df_b</code>, with the same number of rows that I know correspond to the rows in <code>df_a</code>:</p>

<pre><code>     latitude  longitude 
0  -93.193560  31.217029  
1  -93.948082  35.360874  
2 -103.131508  37.787609  
</code></pre>

<p>What I want to do is simply cbind the two and get:</p>

<pre><code>    unique_id lacet_number      latitude  longitude 
0     5570613  TLA-0138365    -93.193560  31.217029  
1     5025490  EMP-0138757    -93.948082  35.360874  
2     4354431  DXN-0025343   -103.131508  37.787609  
</code></pre>

<p>What I have tried:</p>

<pre><code>df_c = pd.concat([df_a, df_b], axis=1)
</code></pre>

<p>which gives me an outer join. </p>

<pre><code>    unique_id lacet_number    latitude  longitude
0         NaN          NaN  -93.193560  31.217029
1         NaN          NaN  -93.948082  35.360874
2         NaN          NaN -103.131508  37.787609
15    5570613  TLA-0138365         NaN        NaN
24    5025490  EMP-0138757         NaN        NaN
36    4354431  DXN-0025343         NaN        NaN
</code></pre>

<p>The problem is that the indices for the two dataframes do not match. I read the documentation for pandas.concat, and saw that there is an option ""ignore_index"". But that only applies to the concatenation axis, in my case the columns and it certainly is not the right choice for me. So my question is: is there a simple way to achieve this?</p>
";;2;;2015-10-12T18:39:53.953;1.0;33088010;2015-10-12T19:37:28.493;2015-10-12T18:47:57.437;;4438271.0;;4438271.0;;1;12;<python><pandas>;Pandas column bind (cbind) two data frames;8286.0
37467;37467;33109272.0;1.0;"<p>I decided to compare skew and kurtosis functions in pandas and scipy.stats, and don't understand why I'm getting different results between libraries.</p>

<p>As far as I can tell from the documentation, both kurtosis functions compute using Fisher's definition, whereas for skew there doesn't seem to be enough of a description to tell if there any major differences with how they are computed.</p>

<pre><code>import pandas as pd
import scipy.stats.stats as st

heights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])

print ""skewness:"", st.skew(heights)
print ""kurtosis:"", st.kurtosis(heights)
</code></pre>

<p>this returns:</p>

<pre><code>skewness: -0.393524456473
kurtosis: -0.330672097724
</code></pre>

<p>whereas if I convert to a pandas dataframe:</p>

<pre><code>heights_df = pd.DataFrame(heights)
print ""skewness:"", heights_df.skew()
print ""kurtosis:"", heights_df.kurtosis() 
</code></pre>

<p>this returns:</p>

<pre><code>skewness: 0   -0.466663
kurtosis: 0    0.379705
</code></pre>

<p>Apologies if I've posted this in the wrong place; not sure if it's a stats or a programming question.</p>
";;0;;2015-10-13T17:37:27.447;3.0;33109107;2015-10-13T18:14:08.780;2015-10-13T18:14:08.780;;4112360.0;;4112360.0;;1;13;<python><numpy><pandas><scipy>;What is the difference between skew and kurtosis functions in pandas vs. scipy?;5608.0
37536;37536;;4.0;"<p>I have this line in my code which converts my data to numeric...</p>

<pre><code>data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)
</code></pre>

<p>The thing is that now the new pandas release (0.17.0) said that this function is deprecated..
This is the error:</p>

<pre><code>FutureWarning: convert_objects is deprecated.  
Use the data-type specific converters pd.to_datetime, 
pd.to_timedelta and pd.to_numeric. 
data[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)
</code></pre>

<p>So, I went to the new documentation and I couldn't find any examples of how to use the new function to convert my data...</p>

<p>It only says this:</p>

<blockquote>
  <p>""DataFrame.convert_objects has been deprecated in favor of type-specific functions pd.to_datetime, pd.to_timestamp and pd.to_numeric (new in 0.17.0) (GH11133).""</p>
</blockquote>

<p>Any help would be nice!</p>
";;6;;2015-10-14T13:20:16.523;1.0;33126477;2017-02-05T01:00:42.223;2016-01-09T17:07:11.223;;249341.0;;4618743.0;;1;13;<python><pandas>;"Pandas "".convert_objects(convert_numeric=True)"" deprecated";6249.0
37676;37676;;1.0;"<p>I installed pandas v0.17.0 directly from the sources on my linux suse 13.2 64 bits. I had previously v0.14.1 installed using yast.
Now</p>

<pre><code>&gt;&gt;&gt; import pandas
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/__init__.py"", line 44, in &lt;module&gt;
    from pandas.core.api import *
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/api.py"", line 9, in &lt;module&gt;
    from pandas.core.groupby import Grouper
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 16, in &lt;module&gt;
    from pandas.core.frame import DataFrame
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 41, in &lt;module&gt;
    from pandas.core.series import Series
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/series.py"", line 2864, in &lt;module&gt;
    import pandas.tools.plotting as _gfx
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 135, in &lt;module&gt;
    if _mpl_ge_1_5_0():
  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 130, in _mpl_ge_1_5_0
    return (matplotlib.__version__  &gt;= LooseVersion('1.5')
  File ""/usr/lib64/python2.7/distutils/version.py"", line 296, in __cmp__
    return cmp(self.version, other.version)
AttributeError: 'unicode' object has no attribute 'version'
</code></pre>

<p>From some posts, I learned that it might be related to the fact that multiple versions are installed. I des-installed the old pandas version using yast and re-installed the newest one, but the problem persists.</p>
";;5;;2015-10-15T22:32:23.533;;33159634;2015-10-16T10:20:10.780;;;;;4966886.0;;1;11;<python><pandas>;pandas v0.17.0: AttributeError: 'unicode' object has no attribute 'version';4192.0
37700;37700;33165860.0;2.0;"<p>Take the following data-frame:</p>

<pre><code>x = np.tile(np.arange(3),3)
y = np.repeat(np.arange(3),3)
df = pd.DataFrame({""x"": x, ""y"": y})
</code></pre>

<pre><code>   x  y
0  0  0
1  1  0
2  2  0
3  0  1
4  1  1
5  2  1
6  0  2
7  1  2
8  2  2
</code></pre>

<p>I need to sort it by <code>x</code> first, and only second by <code>y</code>:</p>

<pre><code>df2 = df.sort([""x"", ""y""])</code></pre>

<pre><code>   x  y
0  0  0
3  0  1
6  0  2
1  1  0
4  1  1
7  1  2
2  2  0
5  2  1
8  2  2
</code></pre>

<p>How can I change the index such that it is ascending again. I.e. how do I get this:</p>

<pre><code>   x  y
0  0  0
1  0  1
2  0  2
3  1  0
4  1  1
5  1  2
6  2  0
7  2  1
8  2  2
</code></pre>

<p>I have tried the following. Unfortunately, it doesn't change the index at all:</p>

<pre><code>df2.reindex(np.arange(len(df2.index)))
</code></pre>
";;0;;2015-10-16T08:24:42.820;3.0;33165734;2017-03-25T20:53:00.583;;;;;841562.0;;1;13;<python><pandas>;Update index after sorting data-frame;6572.0
37770;37770;33259038.0;3.0;"<p>I have a Pandas DataFrame with a column called ""AXLES"", which can take an integer value between 3-12. I am trying to use Seaborn's countplot() option to achieve the following plot:</p>

<ol>
<li>left y axis shows the frequencies of these values occurring in the data. The axis extends are [0%-100%], tick marks at every 10%.</li>
<li>right y axis shows the actual counts, values correspond to tick marks determined by the left y axis (marked at every 10%.)</li>
<li>x axis shows the categories for the bar plots [3, 4, 5, 6, 7, 8, 9, 10, 11, 12].</li>
<li>Annotation on top of the bars show the actual percentage of that category.</li>
</ol>

<p>The following code gives me the plot below, with actual counts, but I could not find a way to convert them into frequencies. I can get the frequencies using <code>df.AXLES.value_counts()/len(df.index)</code> but I am not sure about how to plug this information into Seaborn's <code>countplot()</code>.</p>

<p>I also found a workaround for the annotations, but I am not sure if that is the best implementation.</p>

<p>Any help would be appreciated!</p>

<p>Thanks</p>

<pre><code>plt.figure(figsize=(12,8))
ax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])
plt.title('Distribution of Truck Configurations')
plt.xlabel('Number of Axles')
plt.ylabel('Frequency [%]')

for p in ax.patches:
        ax.annotate('%{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))
</code></pre>

<p><a href=""https://i.stack.imgur.com/4CyuQ.png""><img src=""https://i.stack.imgur.com/4CyuQ.png"" alt=""enter image description here""></a></p>

<h3>EDIT:</h3>

<p>I got closer to what I need with the following code, using Pandas' bar plot, ditching Seaborn. Feels like I'm using so many workarounds, and there has to be an easier way to do it. The issues with this approach:</p>

<ul>
<li>There is no <code>order</code> keyword in Pandas' bar plot function as Seaborn's countplot() has, so I cannot plot all categories from 3-12 as I did in the countplot(). I need to have them shown even if there is no data in that category.</li>
<li><p>The secondary y-axis messes up the bars and the annotation for some reason (see the white gridlines drawn over the text and bars).</p>

<pre><code>plt.figure(figsize=(12,8))
plt.title('Distribution of Truck Configurations')
plt.xlabel('Number of Axles')
plt.ylabel('Frequency [%]')

ax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)
ax.set_yticks(np.arange(0, 110, 10))

ax2 = ax.twinx()
ax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)

for p in ax.patches:
    ax.annotate('{:.2f}%'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
</code></pre></li>
</ul>

<p><a href=""https://i.stack.imgur.com/lqe7r.png""><img src=""https://i.stack.imgur.com/lqe7r.png"" alt=""enter image description here""></a></p>
";;2;;2015-10-16T20:34:12.653;2.0;33179122;2016-07-28T15:56:42.443;2015-10-19T01:05:40.093;;1505259.0;;1505259.0;;1;14;<python><pandas><matplotlib><data-visualization><seaborn>;Seaborn: countplot() with frequencies;9917.0
38020;38020;33247007.0;3.0;"<p>I'm somewhat new to pandas. I have a pandas data frame that is 1 row by 23 columns.</p>

<p>I want to convert this into a series? I'm wondering what the most pythonic way to do this is?</p>

<p>I've tried <code>pd.Series(myResults)</code> but it complains <code>ValueError: cannot copy sequence with size 23 to array axis with dimension 1</code>. It's not smart enough to realize it's still a ""vector"" in math terms. </p>

<p>Thanks!</p>
";;0;;2015-10-20T21:05:48.127;3.0;33246771;2016-10-23T22:20:34.013;;;;;1357015.0;;1;12;<python><pandas><dataframe><series>;Convert pandas data frame to series;30364.0
38119;38119;33271634.0;2.0;"<p>Hello I have the following dataframe.   </p>

<pre><code>    Group           Size

    Short          Small
    Short          Small
    Moderate       Medium
    Moderate       Small
    Tall           Large
</code></pre>

<p>I want to count the frequency of how many time the same row appears in the dataframe.</p>

<pre><code>    Group           Size      Time

    Short          Small        2
    Moderate       Medium       1 
    Moderate       Small        1
    Tall           Large        1
</code></pre>
";;0;;2015-10-21T23:41:23.293;5.0;33271098;2017-05-19T14:30:58.010;2015-10-21T23:52:18.973;;3590067.0;;3590067.0;;1;15;<python><pandas><group-by><dataframe>;Python: get a frequency count based on two columns (variables) in pandas dataframe;8713.0
38289;38289;33324058.0;2.0;"<p>Say I have the following Polygon and Point:</p>

<pre><code>&gt;&gt;&gt; poly = Polygon([(0, 0), (2,8), (14, 10), (6,1)])
&gt;&gt;&gt; point=Point(12,4)
</code></pre>

<p><a href=""https://i.stack.imgur.com/tGtqB.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tGtqB.jpg"" alt=""enter image description here""></a></p>

<p>I can calculate the point's distance to the polygon...</p>

<pre><code>&gt;&gt;&gt; dist=point.distance(poly)
&gt;&gt;&gt; print dist
2.49136439561
</code></pre>

<p>...but I would like to know the coordinate of the point on the polygon border where that shortest distance measures to.</p>

<p>My initial approach is to buffer the point by it's distance to the polygon, and find the point at which that circle is tangent to the polygon:</p>

<pre><code>&gt;&gt;&gt; buff=point.buffer(dist) 
</code></pre>

<p><a href=""https://i.stack.imgur.com/8QTPz.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8QTPz.jpg"" alt=""enter image description here""></a>
However, I'm not sure how to calculate that point. The two polygon's don't intersect so <code>list(poly.intersection(buff))</code> will not give me that point.</p>

<p>Am I on the right track with this? Is there a more straightforward method?</p>
";;3;;2015-10-23T21:24:24.740;2.0;33311616;2017-04-03T11:46:27.027;;;;;3776938.0;;1;12;<python><shapely><geopandas>;Find Coordinate of Closest Point on Polygon Shapely;3717.0
38367;38367;33346694.0;3.0;"<p>That is the difference between <code>groupby(""x"").count</code> and <code>groupby(""x"").size</code> in pandas ?</p>

<p>Does size just exclude nil ?</p>
";;1;;2015-10-26T13:08:36.393;;33346591;2017-03-08T13:34:10.263;2017-03-08T13:34:10.263;;7505321.0;;2432586.0;;1;13;<python><pandas><numpy><difference>;What is the difference between size and count in pandas?;2168.0
38943;38943;;3.0;"<p>I currently have a pandas <code>Series</code> with dtype <code>Timestamp</code>, and I want to group it by date (and have many rows with different times in each group).</p>

<p>The seemingly obvious way of doing this would be something similar to</p>

<pre><code>grouped = s.groupby(lambda x: x.date())
</code></pre>

<p>However, pandas' <code>groupby</code> groups Series by its index. How can I make it group by value instead?</p>
";;2;;2015-11-02T17:43:22.650;3.0;33483670;2017-06-01T09:40:40.770;;;;;305597.0;;1;14;<python><pandas><group-by><series>;How to group a Series by values in pandas?;5479.0
39027;39027;;2.0;"<p>I'm trying to plot a set of histograms for a dataframe  with 25 columns named <code>""Feature_1"",""Feature_2"",....""Feature_25""</code>. When I use <code>df.hist()</code> it sorts individual histograms by their names so they are plotted in the following order: <code>""Feature_1"",""""Feature_10"",""Feature_11""...""Feature_2"",""Feature_20"",...</code>
which is not what I need.</p>

<p>How do I change the sorting order? Passing columns parameter doesn't change anything.</p>

<p>I solved the problem by using matplotlib directly but that's not what I would prefer to do each time I need to plot several histograms.</p>
";;4;;2015-11-03T17:15:48.377;1.0;33505245;2017-05-23T20:33:57.510;2017-03-08T09:43:37.583;;4284627.0;;3052835.0;;1;11;<python><pandas><matplotlib>;How to change order of plots in pandas hist command;447.0
39127;39127;;1.0;"<p>Looking for a way to reliably identify if a numpy object is a view. </p>

<p>Related questions have come up many times before (<a href=""https://stackoverflow.com/questions/11286864/is-there-a-way-to-check-if-numpy-arrays-share-the-same-data"">here</a>, <a href=""https://stackoverflow.com/questions/10747748/how-do-i-check-that-two-slices-of-numpy-arrays-are-the-same-or-overlapping"">here</a>, <a href=""https://stackoverflow.com/questions/28886731/numpy-reshape-on-view"">here</a>), and people have offered some solutions, but all seem to have problems:</p>

<ul>
<li>The test used in <code>pandas</code> now is to call something a view if <code>my_array.base is not None</code>. This seems to always catch views, but also offers lots of false positives (situations where it reports something is a view even if it isn't). </li>
<li><code>numpy.may_share_memory()</code> will check for two specific arrays, but won't answer generically 

<ul>
<li>(@RobertKurn says was best tool as of 2012 -- any changes?)</li>
</ul></li>
<li><code>flags['OWNDATA'])</code> is <a href=""https://stackoverflow.com/questions/11524664/how-can-i-tell-if-numpy-creates-a-view-or-a-copy"">reported (third comment first answer)</a> to fail in some cases. </li>
</ul>

<p>(The reason for my interest is that I'm working on implementing copy-on-write for pandas, and a conservative indicator is leading to over-copying.)</p>
";;6;;2015-11-04T19:48:58.363;3.0;33530753;2016-06-06T22:12:43.067;2017-05-23T11:54:51.030;;-1.0;;2302819.0;;1;45;<python><arrays><numpy><pandas>;numpy: Reliable (non-conservative) indicator if numpy array is view;839.0
40118;40118;33780558.0;4.0;"<p>How do I multiply each element of a given column of my dataframe with a scalar?
(I have tried looking on SO, but cannot seem to find the right solution)</p>

<p>Doing something like: </p>

<pre><code>df['quantity'] *= -1 # trying to multiply each row's quantity column with -1
</code></pre>

<p>gives me a warning: </p>

<pre><code>A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
</code></pre>

<p>Note: If possible, I do not want to be iterating over the dataframe and do something like this...as I think any standard math operation on an entire column should be possible w/o having to write a loop:</p>

<pre><code>for idx, row in df.iterrows():
    df.loc[idx, 'quantity'] *= -1
</code></pre>

<p><strong>EDIT</strong>: </p>

<p>I am running <code>0.16.2</code> of Pandas</p>

<p>full trace:</p>

<pre><code> SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self.obj[item] = s
</code></pre>
";;4;;2015-11-17T22:17:58.327;1.0;33768122;2016-05-24T21:13:01.843;2015-11-17T22:59:25.430;;5133552.0;;5133552.0;;1;13;<python><pandas>;Python: Pandas Dataframe how to multiply entire column with a scalar;19188.0
40283;40283;33814054.0;1.0;"<p>What is the simplest way to read a modest sized Parquet data-set into a Pandas DataFrame? This is only a moderate amount of data that I would like to read into script on a laptop. The data does not reside on HDFS. It is either on the local file system or possibly in S3. I do not want to spin up other services like Hadoop, Hive or Spark.</p>

<p>I thought Blaze/Odo supported this. The Odo documentation mentions Parquet, but the examples seem to be going through Hive. I looking for something lightweight that I can just give the path and get a DataFrame back,
the same way as Odo does for other formats.</p>
";2017-04-13T16:26:16.120;5;;2015-11-19T20:30:59.860;2.0;33813815;2017-05-11T22:04:53.763;;;;;843348.0;;1;11;<python><pandas><parquet><blaze>;Simplest way to read Parquet file into Pandas DataFrame;11061.0
40465;40465;;3.0;"<p>I'm using bokeh with an ipython notebook.</p>

<p>I want to plot a line graph in bokeh using a pandas DataFrame containing datetimes:</p>

<pre><code>import pandas as pd
from datetime import datetime as dt
from bokeh.io import output_notebook
from bokeh.charts import Bar, Line, show

df = pd.DataFrame(data=[1,2,3],
                  index=[dt(2015, 1, 1), dt(2015, 1, 2), dt(2015, 1, 3)],
                  columns=['foo'])

output_notebook()
show(Line(df))
</code></pre>

<p>However, bokeh uses microseconds! Why is this? How do I fix it?</p>

<p><a href=""https://i.stack.imgur.com/NDYJx.png""><img src=""https://i.stack.imgur.com/NDYJx.png"" alt=""bokeh plot of line""></a></p>
";;0;;2015-11-23T10:54:34.543;5.0;33869292;2017-05-01T10:46:49.307;2015-11-23T13:55:22.827;;509706.0;;509706.0;;1;11;<python><pandas><ipython-notebook><bokeh>;How can I set the x-axis as datetimes on a bokeh plot?;6021.0
40751;40751;33952294.0;2.0;"<p>The pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer"">read_csv()</a> method interprets 'NA' as nan (not a number) instead of a valid string.</p>

<p>In the simple case below note that the output in row 1, column 2 (zero based count) is 'nan' instead of 'NA'.</p>

<p><strong>sample.tsv</strong> (tab delimited)</p>

<blockquote>
  <p>PDB   CHAIN   SP_PRIMARY  RES_BEG RES_END PDB_BEG PDB_END SP_BEG  SP_END<br>
  5d8b  N   P60490  1   146 1   146 1   146<br>
  5d8b  NA  P80377  1   126 1   126 1   126<br>
  5d8b  O   P60491  1   118 1   118 1   118<br></p>
</blockquote>

<p><strong>read_sample.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv(
    'sample.tsv',
    sep='\t',
    encoding='utf-8',
)

for df_tuples in df.itertuples(index=True):
    print(df_tuples)
</code></pre>

<p><strong>output</strong></p>

<blockquote>
  <p>(0, u'5d8b', u'N', u'P60490', 1, 146, 1, 146, 1, 146)<br>
  (1, u'5d8b', nan, u'P80377', 1, 126, 1, 126, 1, 126)<br>
  (2, u'5d8b', u'O', u'P60491', 1, 118, 1, 118, 1, 118)<br></p>
</blockquote>

<h1>Additional Information</h1>

<p>Re-writing the file with quotes for data in the 'CHAIN' column and then using the quotechar parameter <code>quotechar='\''</code> has the same result. And passing a dictionary of types via the dtype parameter <code>dtype=dict(valid_cols)</code> does not change the result.</p>

<p>An old answer to <a href=""https://stackoverflow.com/a/12117333/3182836"">Prevent pandas from automatically inferring type in read_csv</a> suggests first using a numpy record array to parse the file, but given the ability to now specify column dtypes, this shouldn't be necessary.</p>

<p>Note that itertuples() is used to preserve dtypes as described in the iterrows documentation: ""To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns tuples of the values and which is generally faster as iterrows.""</p>

<p>Example was tested on Python 2 and 3 with pandas version 0.16.2, 0.17.0, and 0.17.1.</p>

<hr>

<p>Is there a way to capture a valid string 'NA' instead of it being converted to nan?</p>
";;4;;2015-11-27T07:20:04.770;2.0;33952142;2017-05-26T22:07:35.423;2017-05-23T11:54:31.983;;-1.0;;3182836.0;;1;12;<python><pandas>;Prevent pandas from interpreting 'NA' as NaN in a string;2275.0
40760;40760;;4.0;"<p>When I read a csv file to pandas dataframe, each column is cast to its own datatypes. I have a column that was converted to an object. I want to perform string operations for this column such as splitting the values and creating a list. But no such operation is possible because its dtype is object. Can anyone please let me know the way to convert all the items of a column to strings instead of objects?</p>

<p>I tried several ways but nothing worked. I used astype, str(), to_string etc.</p>

<pre><code>a=lambda x: str(x).split(',')
df['column'].apply(a)
</code></pre>

<h1>or</h1>

<pre><code>df['column'].astype(str)
</code></pre>
";;2;;2015-11-27T12:43:50.907;2.0;33957720;2017-08-10T15:09:42.217;2017-07-17T19:36:49.967;;1423492.0;;3546523.0;;1;20;<python><object><pandas>;How to convert column with dtype as object to string in Pandas Dataframe;22670.0
40772;40772;35106735.0;1.0;"<p>Is there anyway to hide <code>E1101</code> errors for objects that are created from a specific library?  Our large repository is littered with <code>#pylint: disable=E1101</code> around various objects created by pandas.</p>

<p>For example, pylint will throw a no member error on the following code:</p>

<pre><code>import pandas.io.data
import pandas as pd
spy = pandas.io.data.DataReader(""SPY"", ""yahoo"")
spy.to_csv(""test.csv"")
spy = pd.read_csv(""test.csv"")
close_px = spy.ix[""2012"":]
</code></pre>

<p>Will have the following errors:</p>

<pre><code>E:  6,11: Instance of 'tuple' has no 'ix' member (no-member)
E:  6,11: Instance of 'TextFileReader' has no 'ix' member (no-member)
</code></pre>
";;5;;2015-11-27T16:48:01.087;3.0;33961756;2016-01-30T20:28:12.070;2015-11-27T22:24:23.797;;1064197.0;;1064197.0;;1;16;<pandas><pylint>;Disabling Pylint no member- E1101 error for specific libraries;3680.0
40919;40919;;4.0;"<p>I am working through the <a href=""https://www.tensorflow.org/tutorials/mnist/pros/index.html"" rel=""noreferrer"">tensor flow tutorial</a>, but am trying to use a numpy or pandas format for the data, so that I can compare it with Scikit-Learn results.</p>

<p>I get the digit recognition data from kaggle - <a href=""https://www.kaggle.com/c/digit-recognizer/data"" rel=""noreferrer"">here</a></p>

<p>The tutorial uses a weird format for uploading the data, where as I'm trying to compare with results from other libraries, so would like to keep it in numpy or pandas format.</p>

<p>Here is the standard tensor flow tutorial code (this all works fine):</p>

<pre><code># Stuff from tensorflow tutorial 
import tensorflow as tf
sess = tf.InteractiveSession()

x = tf.placeholder(""float"", shape=[None, 784])
y_ = tf.placeholder(""float"", shape=[None, 10])

W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x,W) + b)

cross_entropy = -tf.reduce_sum(y_*tf.log(y))

train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code></pre>

<p>Here I read the data, strip out the target variables and split the data into testing and training datasets (this all works fine):</p>

<pre><code># Read dataframe from training data
csvfile='train.csv'
from pandas import DataFrame, read_csv
df = read_csv(csvfile)

# Strip off the target data and make it a separate dataframe.
Target=df.label
del df[""label""]

# Split data into training and testing sets
msk = np.random.rand(len(df)) &lt; 0.8
dfTest = df[~msk]
TargetTest = Target[~msk]
df = df[msk]
Target = Target[msk]

# One hot encode the target
OHTarget=pd.get_dummies(Target)
OHTargetTest=pd.get_dummies(TargetTest)
</code></pre>

<p>Now, when I try to run the training step, I get a FailedPreconditionError:</p>

<pre><code>for i in range(100):
    batch = np.array(df[i*50:i*50+50].values)
    batch = np.multiply(batch, 1.0 / 255.0)
    Target_batch = np.array(OHTarget[i*50:i*50+50].values)
    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
    train_step.run(feed_dict={x: batch, y_: Target_batch})
</code></pre>

<p>Here's the full error:</p>

<pre><code>---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-82-967faab7d494&gt; in &lt;module&gt;()
      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)
      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
----&gt; 6     train_step.run(feed_dict={x: batch, y_: Target_batch})

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)
   1265         none, the default session will be used.
   1266     """"""
-&gt; 1267     _run_using_default_session(self, feed_dict, self.graph, session)
   1268
   1269

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)
   2761                        ""the operation's graph is different from the session's ""
   2762                        ""graph."")
-&gt; 2763   session.run(operation, feed_dict)
   2764
   2765

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)
    343
    344     # Run request and get response.
--&gt; 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
    346
    347     # User may have fetched the same tensor multiple times, but we

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)
    417         # pylint: disable=protected-access
    418         raise errors._make_specific_exception(node_def, op, e.error_message,
--&gt; 419                                               e.code)
    420         # pylint: enable=protected-access
    421       raise e_type, e_value, e_traceback

FailedPreconditionError: Attempting to use uninitialized value Variable_1
     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]
Caused by op u'gradients/add_grad/Shape_1', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ...........

...which was originally created as op u'add', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
[elided 17 identical lines from previous traceback]
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-45-59183d86e462&gt;"", line 1, in &lt;module&gt;
    y = tf.nn.softmax(tf.matmul(x,W) + b)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add
    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op
    op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Any ideas as to how I can fix this?</p>
";;0;;2015-11-30T14:41:20.107;4.0;34001922;2017-07-27T10:06:15.487;2017-05-04T03:43:14.380;;1090562.0;user3654387;3654387.0;;1;21;<python><pandas><classification><tensorflow>;FailedPreconditionError: Attempting to use uninitialized in Tensorflow;20199.0
41308;41308;34092032.0;2.0;"<p>I am reading a csv file into <code>pandas</code>. This csv file constists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following: </p>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t')
Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])
Frame.to_csv(""path/to/file.txt"", sep='\t')
</code></pre>

<p>But when I apply the code, I get the following Error:</p>

<pre><code>ValueError: Shape of passed values is (1, 1), indices imply (4, 1)
</code></pre>

<p>What exactly does the error mean? And what would be a clean way in python to add a header row to my csv file/pandas df?</p>
";;0;;2015-12-04T15:35:59.313;10.0;34091877;2016-06-08T10:09:28.227;2016-06-08T10:09:28.227;;4542359.0;;5347728.0;;1;30;<python><csv><pandas><header>;How to add header row to a pandas DataFrame;55423.0
42569;42569;34402423.0;4.0;"<p>I am running this cell in IPython Notebook:</p>

<pre><code># salaries and teams are Pandas dataframe
salaries.head()
teams.head()
</code></pre>

<p>The result is that I am only getting the output of <code>teams</code> data-frame rather than of both <code>salaries</code> and <code>teams</code>. If I just run <code>salaries.head()</code> I get the result for <code>salaries</code> data-frame but on running both the statement I just see the output of <code>teams.head()</code>. How can I correct this?</p>
";;0;;2015-12-21T14:32:00.480;2.0;34398054;2017-02-27T01:00:07.317;2015-12-21T17:14:57.017;;837534.0;;2623749.0;;1;13;<pandas><ipython><ipython-notebook>;IPython Notebook cell multiple outputs;3664.0
43044;43044;34531543.0;1.0;"<p>I have a <strong>Pandas DF</strong> where I need to <strong>filter</strong> out some rows that contains values == 0 for feature 'a' and feature 'b'.</p>

<p>In order to inspect the values, I run the following:</p>

<pre><code>DF1 = DF[DF['a'] == 0]
</code></pre>

<p>Which returns the right values. Similarly, by doing this:</p>

<pre><code>DF2 = DF[DF['b'] == 0]
</code></pre>

<p>I can see the 0 values for feature 'b'.</p>

<p>However, if I try to combine these 2 in a single line of code using the OR operand:</p>

<pre><code>DF3 = DF[DF['a'] == 0 |  DF['b'] == 0]
</code></pre>

<p>I get this:</p>

<pre><code>TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]
</code></pre>

<p>What's happening here?</p>
";;0;;2015-12-30T14:27:59.077;;34531416;2015-12-30T23:35:12.700;2015-12-30T14:44:23.063;;1290147.0;;5730994.0;;1;11;<python><pandas>;comparing dtyped [float64] array with a scalar of type [bool] in Pandas DataFrame;5334.0
43134;43134;34555201.0;2.0;"<p>I'm trying to extract US states from wiki URL, and for which I'm using Python Pandas. </p>

<pre><code>import pandas as pd
import html5lib
f_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') 
</code></pre>

<p>However, the above code is giving me an error L</p>

<blockquote>
  <p>ImportError                               Traceback (most recent call last)
   in ()
        1 import pandas as pd
  ----> 2 f_states = pd.read_html('<a href=""https://simple.wikipedia.org/wiki/List_of_U.S._states"">https://simple.wikipedia.org/wiki/List_of_U.S._states</a>')</p>
  
  <p>if flavor in ('bs4', 'html5lib'):
      662         if not _HAS_HTML5LIB:
  --> 663             raise ImportError(""html5lib not found, please install it"")
      664         if not _HAS_BS4:
      665             raise ImportError(""BeautifulSoup4 (bs4) not found, please install it"")
  ImportError: html5lib not found, please install it</p>
</blockquote>

<p>I installed html5lib and beautifulsoup4 as well, but it is not working. 
Can someone help pls.</p>
";;0;;2016-01-01T09:55:22.010;1.0;34555135;2017-02-26T21:30:23.937;;;;;4943236.0;;1;12;<python><pandas>;Pandas: read_html;12389.0
43309;43309;;2.0;"<p>I was wondering if it is possible to create a Seaborn count plot, but instead of actual counts on the y-axis, show the relative frequency (percentage) within its group (as specified with the <code>hue</code> parameter).</p>

<p>I sort of fixed this with the following approach, but I can't imagine this is the easiest approach:</p>

<pre><code># Plot percentage of occupation per income class
grouped = df.groupby(['income'], sort=False)
occupation_counts = grouped['occupation'].value_counts(normalize=True, sort=False)

occupation_data = [
    {'occupation': occupation, 'income': income, 'percentage': percentage*100} for 
    (income, occupation), percentage in dict(occupation_counts).items()
]

df_occupation = pd.DataFrame(occupation_data)

p = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)
_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels
</code></pre>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/feVbB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/feVbB.png"" alt=""Percentage plot with seaborn""></a></p>

<p>I'm using the well known adult data set from the <a href=""http://archive.ics.uci.edu/ml/datasets/Adult"" rel=""nofollow noreferrer"">UCI machine learning repository</a>. The pandas dataframe is created like this:</p>

<pre><code># Read the adult dataset
df = pd.read_csv(
    ""data/adult.data"",
    engine='c',
    lineterminator='\n',

    names=['age', 'workclass', 'fnlwgt', 'education', 'education_num',
           'marital_status', 'occupation', 'relationship', 'race', 'sex',
           'capital_gain', 'capital_loss', 'hours_per_week',
           'native_country', 'income'],
    header=None,
    skipinitialspace=True,
    na_values=""?""
)
</code></pre>

<p><a href=""https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies"">This question</a> is sort of related, but does not make use of the <code>hue</code> parameter. And in my case I cannot just change the labels on the y-axis, because the height of the bar must depend on the group.</p>
";;3;;2016-01-05T15:54:55.903;4.0;34615854;2017-03-10T01:15:48.543;2017-05-23T12:34:22.307;;-1.0;;2290197.0;;1;11;<python><pandas><seaborn>;Seaborn countplot with normalized y axis per group;1816.0
43518;43518;34683105.0;1.0;"<p>I have a pandas DataFrame with 4 columns and I want to create a <strong>new</strong> DataFrame that <strong>only</strong> has three of the columns.  This question is similar to: <a href=""https://stackoverflow.com/questions/10085806/extracting-specific-columns-from-a-data-frame"">Extracting specific columns from a data frame</a> but for pandas not R.  The following code does not work, raises an error, and is certainly not the pandasnic way to do it. </p>

<pre><code>import pandas as pd
old = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})
new = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator 
</code></pre>

<p>What is the pandasnic way to do it?  </p>
";;0;;2016-01-08T17:34:45.240;7.0;34682828;2016-12-21T23:39:53.930;2017-05-23T11:55:04.370;;-1.0;;1134881.0;;1;20;<python><pandas>;pandas: Extracting specific selected columns from a DataFrame to new DataFrame;27288.0
43916;43916;34794112.0;3.0;"<p>I'd like to replace bad values in a column of a dataframe by NaN's.</p>

<pre><code>mydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}
df = pd.DataFrame(mydata)

df[df.y == 'N/A']['y'] = np.nan
</code></pre>

<p>Though, the last line fails and throws a warning because it's working on a copy of df. So, what's the correct way to handle this? I've seen many solutions with iloc or ix but here, I need to use a boolean condition.</p>
";;0;;2016-01-14T16:00:09.567;2.0;34794067;2016-01-14T16:21:26.037;;;;;1649095.0;;1;18;<python><pandas><nan>;How to set a cell to NaN in a pandas dataframe;18964.0
44206;44206;34863702.0;1.0;"<p>I thought the third option was supposed to be the fastest way to strip whitespaces? Can someone give me some general rules that I should be applying when working with large data sets? I normally use .astype(str) but clearly that is not worthwhile for columns which I know are objects already.</p>

<pre><code>%%timeit
fcr['id'] = fcr['id'].astype(str).map(str.strip)
10 loops, best of 3: 47.8 ms per loop

%%timeit
fcr['id'] = fcr['id'].map(str.strip)
10 loops, best of 3: 25.2 ms per loop

%%timeit
fcr['id'] = fcr['id'].str.strip(' ')
10 loops, best of 3: 55.5 ms per loop
</code></pre>
";;6;;2016-01-18T19:15:33.287;1.0;34862336;2016-01-18T20:58:58.243;2016-01-18T20:58:58.243;;653364.0;;4895545.0;;1;11;<python-3.x><pandas>;Performance of str.strip for Pandas;360.0
44277;44277;34883876.0;3.0;"<p>I need to process a huge amount of CSV files where the time stamp is always a string representing the unix timestamp in milliseconds. I could not find a method yet to modify these columns efficiently.</p>

<p>This is what I came up with, however this of course duplicates only the column and I have to somehow put it back to the original dataset. I'm sure it can be done when creating the <code>DataFrame</code>?</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3:
    from StringIO import StringIO
else:
    from io import StringIO
import pandas as pd

data = 'RUN,UNIXTIME,VALUE\n1,1447160702320,10\n2,1447160702364,20\n3,1447160722364,42'

df = pd.read_csv(StringIO(data))

convert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)
converted_df = df['UNIXTIME'].apply(convert)
</code></pre>

<p>This will pick the column 'UNIXTIME' and change it from</p>

<pre><code>0    1447160702320
1    1447160702364
2    1447160722364
Name: UNIXTIME, dtype: int64
</code></pre>

<p>into this</p>

<pre><code>0   2015-11-10 14:05:02.320
1   2015-11-10 14:05:02.364
2   2015-11-10 14:05:22.364
Name: UNIXTIME, dtype: datetime64[ns]
</code></pre>

<p>However, I would like to use something like <code>pd.apply()</code> to get the whole dataset returned with the converted column or as I already wrote, simply create datetimes when generating the DataFrame from CSV.</p>
";;0;;2016-01-19T17:20:34.613;2.0;34883101;2017-02-20T09:31:21.083;;;;;531222.0;;1;12;<datetime><pandas>;Pandas converting row with unix timestamp (in milliseconds) to datetime;4306.0
44322;44322;34896835.0;3.0;"<p>I have a CSV that looks like this:</p>

<pre><code>gene,stem1,stem2,stem3,b1,b2,b3,special_col
foo,20,10,11,23,22,79,3
bar,17,13,505,12,13,88,1
qui,17,13,5,12,13,88,3
</code></pre>

<p>And as data frame it looks like this:</p>

<pre><code>In [17]: import pandas as pd
In [20]: df = pd.read_table(""http://dpaste.com/3PQV3FA.txt"",sep="","")
In [21]: df
Out[21]:
  gene  stem1  stem2  stem3  b1  b2  b3  special_col
0  foo     20     10     11  23  22  79            3
1  bar     17     13    505  12  13  88            1
2  qui     17     13      5  12  13  88            3
</code></pre>

<p>What I want to do is to perform pearson correlation from last column (<code>special_col</code>) with every columns between <code>gene</code> column and <code>special column</code>, i.e. <code>colnames[1:number_of_column-1]</code></p>

<p>At the end of the day we will have length 6 data frame.</p>

<pre><code>Coln   PearCorr
stem1  0.5
stem2 -0.5
stem3 -0.9999453506011533
b1    0.5
b2    0.5
b3    -0.5
</code></pre>

<p>The above value is computed manually:</p>

<pre><code>In [27]: import scipy.stats
In [39]: scipy.stats.pearsonr([3, 1, 3], [11,505,5])
Out[39]: (-0.9999453506011533, 0.0066556395400007278)
</code></pre>

<p>How can I do that?</p>
";;3;;2016-01-20T09:42:26.903;6.0;34896455;2016-01-20T23:57:07.383;2016-01-20T23:57:07.383;;67405.0;;67405.0;;1;11;<python><pandas>;How to do Pearson correlation of selected columns of a Pandas data frame;15117.0
44395;44395;34916691.0;3.0;"<p>I have a pandas dataFrame of mixed types, some are strings and some are numbers. I would like to replace the NAN values in string columns by '.', and the NAN values in float columns by 0.</p>

<p>Consider this small fictitious example:</p>

<pre><code>df = pd.DataFrame({'Name':['Jack','Sue',pd.np.nan,'Bob','Alice','John'],
    'A': [1, 2.1, pd.np.nan, 4.7, 5.6, 6.8],
    'B': [.25, pd.np.nan, pd.np.nan, 4, 12.2, 14.4],
    'City':['Seattle','SF','LA','OC',pd.np.nan,pd.np.nan]})
</code></pre>

<p>Now, I can do it in 3 lines:</p>

<pre><code>df['Name'].fillna('.',inplace=True)
df['City'].fillna('.',inplace=True)
df.fillna(0,inplace=True)
</code></pre>

<p>Since this is a small dataframe, 3 lines is probably ok. In my real example (which I cannot share here due to data confidentiality reasons), I have many more string columns and numeric columns. SO I end up writing many lines just for fillna. Is there a concise way of doing this? </p>
";;2;;2016-01-21T01:00:25.927;3.0;34913590;2016-01-21T06:49:08.173;2016-01-21T01:38:24.950;;2838606.0;;5818752.0;;1;11;<python><pandas><dataframe>;Fillna in multiple columns in place in Python Pandas;4398.0
44593;44593;34962199.0;3.0;"<p>I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?</p>
";;1;;2016-01-23T10:04:27.580;6.0;34962104;2017-08-22T16:18:07.417;2017-04-19T12:44:24.760;;2699288.0;;852063.0;;1;32;<python><pandas><dataframe><python-3.5>;Pandas: How can I use the apply() function for a single column?;36873.0
45206;45206;;1.0;"<p>I am trying to add an <code>int</code> to an existing value in a <code>Pandas</code> <code>DataFrame</code> with  </p>

<pre><code>&gt;&gt;&gt; df.ix['index 5','Total Dollars'] += 10
</code></pre>

<p>I get the error: 
<code>ValueError: Must have equal len keys and value when setting with an iterable</code>. </p>

<p>I think the error comes from the <code>datatype</code> as gotten from:</p>

<pre><code>&gt;&gt;&gt; print type(df.ix['index 5','Total Dollars'] 
int64 &lt;class 'pandas.core.series.Series'&gt;
</code></pre>

<p>The dataframe is populated via CSV file. I tried loading the database from another CSV file:</p>

<pre><code>&gt;&gt;&gt; print type(df.ix['index 5','Total Dollars']
int64
</code></pre>

<p>What could be causing this difference in type?</p>
";;2;;2016-01-29T20:45:21.693;;35093729;2016-06-21T14:58:58.067;2016-06-21T14:58:58.067;;1494637.0;;2242044.0;;1;11;<python><csv><numpy><pandas>;Adding data to Pandas Dataframe from a CSV file causing Value Errors;260.0
46110;46110;35318270.0;1.0;"<p>I'm trying to use the pivot_table method of a pandas DataFrame;</p>

<pre><code>mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')
</code></pre>

<p>However, I receive the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-55-cb4d494f2f39&gt; in &lt;module&gt;()
----&gt; 1 mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')

TypeError: pivot_table() got an unexpected keyword argument 'rows'
</code></pre>

<p>The above command was taken from the book '<a href=""http://shop.oreilly.com/product/0636920023784.do"">Python for Data Analysis</a>' by Wes McKinney (the creator of pandas)</p>
";;0;;2016-02-10T14:45:14.693;1.0;35318269;2016-12-23T19:32:12.517;2016-02-10T14:51:39.490;;1033422.0;;1033422.0;;1;12;<pandas>;TypeError: pivot_table() got an unexpected keyword argument 'rows';3901.0
46339;46339;35368792.0;1.0;"<p>I want to change a dataframes' index (rows) from float64 to string or unicode. </p>

<p>I thought this would work apparently not:</p>

<pre><code>#check type
type(df.index)
'pandas.core.index.Float64Index'

#change type to unicode
if not isinstance(df.index, unicode):
    df.index = df.index.astype(unicode)
</code></pre>

<p>error message:</p>

<pre><code>TypeError: Setting &lt;class 'pandas.core.index.Float64Index'&gt; dtype to anything other than float64 or object is not supported
</code></pre>
";;0;;2016-02-12T17:23:26.380;3.0;35368645;2016-12-11T11:53:02.647;2016-02-12T17:30:13.210;;2342399.0;;2342399.0;;1;17;<python><pandas><indexing><dataframe><rows>;pandas - change df.index from float64 to unicode or string;8345.0
46535;46535;35415751.0;2.0;"<p>I would like to run a pivot on a pandas dataframe, with the index being two columns, not one. For example, one field for the year, one for the month, an 'item' field which shows 'item 1' and 'item 2' and a 'value' field with numerical values. I want the index to be year + month.</p>

<p>The only way I managed to get this to work was to combine the two fields into one, then separate them again. is there a better way?</p>

<p>Minimal code copied below. Thanks a lot!</p>

<p>PS Yes, I am aware there are other questions with the keywords 'pivot' and 'multi-index', but I did not understand if/how they can help me with this question.</p>

<pre><code>import pandas as pd
import numpy as np

df= pd.DataFrame()
month = np.arange(1,13)
values1 = np.random.randint(0,100,12)
values2 = np.random.randint(200,300,12)


df['month'] =  np.hstack(( month, month ))
df['year']=2004
df['value'] = np.hstack(( values1, values2 ))
df['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))

# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2
# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )

#This doesn't work, either:
#df.set_index(['year', 'month'], inplace=True)
# ValueError: cannot label index with a null key
#mypiv = df.pivot(columns='item', values='value')

#This below works but is not ideal: I have to first concatenate then separate the fields I need
df['new field']= df['year'] * 100 + df['month']

mypiv = df.pivot('new field', 'item', 'value').reset_index()
mypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  
mypiv['month'] = mypiv['new field'] % 100
</code></pre>
";;0;;2016-02-15T16:43:23.320;2.0;35414625;2016-02-15T18:00:23.873;;;;;4045275.0;;1;12;<python><pandas><pivot><multi-index>;pandas: how to run a pivot with a multi-index?;4754.0
47478;47478;35715029.0;6.0;"<p>I constructed a pandas dataframe of results. This data frame acts as a table. There are MultiIndexed columns and each row represents a name, ie <code>index=['name1','name2',...]</code> when creating the DataFrame. I would like to display this table and save it as a png (or any graphic format really). At the moment, the closest I can get is converting it to html, but I would like a png. It looks like similar questions have been asked such as <a href=""https://stackoverflow.com/questions/19726663/how-to-save-the-pandas-dataframe-series-data-as-a-figure"">How to save the Pandas dataframe/series data as a figure?</a></p>

<p>However, the marked solution converts the dataframe into a line plot (not a table) and the other solution relies on PySide which I would like to stay away simply because I cannot pip install it on linux. I would like this code to be easily portable. I really was expecting table creation to png to be easy with python. All help is appreciated.</p>
";;2;;2016-02-25T17:30:26.963;5.0;35634238;2017-08-06T18:15:03.970;2017-05-23T11:47:04.357;;-1.0;;1308706.0;;1;16;<python><pandas>;How to save a pandas DataFrame table as a png;11155.0
48130;48130;;2.0;"<h3>Problem</h3>

<p>I noticed that memory allocated while iterating through a Pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer"">GroupBy</a> object is not deallocated after iteration. I use <code>resource.getrusage(resource.RUSAGE_SELF).ru_maxrss</code> (<a href=""https://stackoverflow.com/questions/938733/total-memory-used-by-python-process"">second answer in this post for details</a>) to measure the total amount of active memory used by the Python process.</p>

<pre><code>import resource
import gc

import pandas as pd
import numpy as np

i = np.random.choice(list(range(100)), 4000)
cols = list(range(int(2e4)))

df = pd.DataFrame(1, index=i, columns=cols)

gb = df.groupby(level=0)
# gb = list(gb)
for i in range(3):
    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)
    for idx, x in enumerate(gb):
        if idx == 0:
            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)
    # del idx, x
    # gc.collect()
</code></pre>

<p>prints the following total active memory (in gb)</p>

<pre><code>0.671732
1.297424
1.297952
1.923288
1.923288
2.548624
</code></pre>

<h3>Solutions</h3>

<p>Uncommenting <code>del idx, x</code> and <code>gc.collect()</code> fixes the problem. I do however have to <code>del</code> all variables that reference the DataFrames returned by iterating over the groupby (which can be a pain depending on the code in the inner for loop). The new printed memory usages become:</p>

<pre><code>0.671768
1.297412
1.297992
1.297992
1.297992
1.297992
</code></pre>

<p>Alternatively I can uncomment <code>gb = list(gb)</code>. The resulting memory usages are roughly the same as those from the previous solution:</p>

<pre><code>1.32874
1.32874
1.32874
1.32874
1.32874
1.32874
</code></pre>

<h3>Questions</h3>

<ol>
<li>Why is memory for DataFrames resulting from iteration through the groupby not deallocated after iteration is completed?</li>
<li>Is there a better solution than the two above? If not, which of these two solutions is ""better""?</li>
</ol>
";;3;;2016-03-03T21:00:28.060;4.0;35782929;2016-06-13T13:10:42.907;2017-05-23T11:52:52.763;;-1.0;;1953800.0;;1;19;<python><python-3.x><pandas><memory-management>;Pandas GroupBy memory deallocation;862.0
48296;48296;;5.0;"<p>I've been developing a tool that automatically preprocesses data in pandas.DataFrame format. During this preprocessing step, I want to treat continuous and categorical data differently. In particular, I want to be able to apply, e.g., a OneHotEncoder to <em>only</em> the categorical data.</p>

<p>Now, let's assume that we're provided a pandas.DataFrame and have no other information about the data in the DataFrame. What is a good heuristic to use to determine whether a column in the pandas.DataFrame is categorical?</p>

<p>My initial thoughts are:</p>

<p>1) If there are strings in the column (e.g., the column data type is <code>object</code>), then the column very likely contains categorical data</p>

<p>2) If some percentage of the values in the column is unique (e.g., >=20%), then the column very likely contains continuous data</p>

<p>I've found <code>1)</code> to work fine, but <code>2)</code> hasn't panned out very well. I need better heuristics. How would you solve this problem?</p>

<p><strong>Edit:</strong> Someone requested that I explain why <code>2)</code> didn't work well. There were some tests cases where we still had continuous values in a column but there weren't many unique values in the column. The heuristic in <code>2)</code> obviously failed in that case. There were also issues where we had a categorical column that had many, many unique values, e.g., passenger names in the Titanic data set. Same column type misclassification problem there.</p>
";;6;;2016-03-06T12:38:03.793;2.0;35826912;2017-06-03T08:31:31.940;2016-03-06T13:45:23.400;;1383444.0;;1383444.0;;1;11;<python><pandas><scikit-learn>;What is a good heuristic to detect if a column in a pandas.DataFrame is categorical?;1612.0
48996;48996;36001191.0;3.0;"<p>I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.</p>

<p>For some reason, numpy.isnan is failing on this array, however as shown below, each element is a float, numpy.isnan runs correctly on each element, the type of the variable is definitely a numpy array.</p>

<p>What's going on?!</p>

<pre><code>set([type(x) for x in tester])
Out[59]: {float}

tester
Out[60]: 
array([-0.7000000000000001, nan, nan, nan, nan, nan, nan, nan, nan, nan,
   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
   nan, nan], dtype=object)

set([type(x) for x in tester])
Out[61]: {float}

np.isnan(tester)
Traceback (most recent call last):

File ""&lt;ipython-input-62-e3638605b43c&gt;"", line 1, in &lt;module&gt;
np.isnan(tester)

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

set([np.isnan(x) for x in tester])
Out[65]: {False, True}

type(tester)
Out[66]: numpy.ndarray
</code></pre>
";;0;;2016-03-15T01:14:18.530;2.0;36000993;2016-12-19T15:28:46.250;;;;;4294553.0;;1;12;<python><arrays><numpy><pandas>;Numpy isnan() fails on an array of floats (from pandas dataframe apply);9699.0
49165;49165;45523167.0;2.0;"<p>I'm calculating the Autocorrelation Function for a stock's returns. To do so I tested two functions, the <code>autocorr</code> function built into Pandas, and the <code>acf</code> function supplied by <code>statsmodels.tsa</code>. This is done in the following MWE:</p>

<pre><code>import pandas as pd
from pandas_datareader import data
import matplotlib.pyplot as plt
import datetime
from dateutil.relativedelta import relativedelta
from statsmodels.tsa.stattools import acf, pacf

ticker = 'AAPL'
time_ago = datetime.datetime.today().date() - relativedelta(months = 6)

ticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()
ticker_data_len = len(ticker_data)

ticker_data_acf_1 =  acf(ticker_data)[1:32]
ticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]

test_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T
test_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']
test_df.index += 1
test_df.plot(kind='bar')
</code></pre>

<p>What I noticed was the values they predicted weren't identical:</p>

<p><a href=""https://i.stack.imgur.com/TPuER.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TPuER.png"" alt=""enter image description here""></a></p>

<p>What accounts for this difference, and which values should be used?</p>
";;7;;2016-03-16T14:43:08.537;3.0;36038927;2017-08-05T15:40:29.063;;;;;1510846.0;;1;11;<python><pandas><statsmodels>;What's the difference between pandas ACF and statsmodel ACF?;3835.0
49971;49971;36226137.0;1.0;"<p>Given a pandas dataframe containing (maybe) NaN values scattered here and there:</p>

<p><strong>Question:</strong> how do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?</p>

<p>Thank you</p>
";;0;;2016-03-25T18:50:36.653;5.0;36226083;2017-04-27T08:35:28.620;2016-03-25T18:55:47.677;;3462321.0;;3462321.0;;1;20;<python><pandas><dataframe><nan>;How to find which columns contain any NaN value in Pandas dataframe (python);12115.0
51335;51335;36519122.0;1.0;"<p>I have a situation wherein sometimes when I read a <code>csv</code> from <code>df</code> I get an unwanted index-like column named <code>unnamed:0</code>. This is very annoying! I have tried </p>

<pre><code>merge.to_csv('xy.df', mode = 'w', inplace=False)
</code></pre>

<p>which I thought was a solution to this, but I am still getting the <code>unnamed:0</code> column! Does anyone have an idea on this?</p>
";;0;;2016-04-09T15:47:43.570;1.0;36519086;2016-04-09T16:16:29.247;;;;;5211377.0;;1;29;<python><pandas><ipython>;Pandas: how to get rid of `Unnamed:` column in a dataframe;12496.0
51430;51430;36539513.0;6.0;"<p>How can I create a DataFrame from multiple <code>numpy</code> arrays, <code>Pandas</code> Series, or <code>Pandas</code> DataFrame's while preserving the order of the columns?</p>

<p>For example, I have these two <code>numpy</code> arrays and I want to combine them as a <code>Pandas</code> DataFrame.</p>

<pre><code>foo = np.array( [ 1, 2, 3 ] )
bar = np.array( [ 4, 5, 6 ] )
</code></pre>

<p>If I do this, the <code>bar</code> column would come first because <code>dict</code> doesn't preserve order.</p>

<pre><code>pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )

    bar foo
0   4   1
1   5   2
2   6   3
</code></pre>

<p>I can do this, but it gets tedious when I need to combine many variables.</p>

<pre><code>pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )
</code></pre>

<p>EDIT: Is there a way to specify the variables to be joined and to organize the column order in one operation? That is, I don't mind using multiple lines to complete the entire operation, but I'd rather not having to specify the variables to be joined multiple times (since I will be changing the code a lot and this is pretty error prone).</p>

<p>EDIT2: One more point. If I want to add or remove one of the variables to be joined, I only want to add/remove in one place.</p>
";;0;;2016-04-11T03:32:58.757;3.0;36539396;2016-11-30T14:31:32.193;2016-04-11T05:50:08.837;;461389.0;;461389.0;;1;11;<python><pandas>;How to create a DataFrame while preserving order of the columns?;2740.0
51854;51854;38650886.0;1.0;"<p>I'm learning different methods to convert categorical variables to numeric for machine-learning classifiers.  I came across the <code>pd.get_dummies</code> method and <code>sklearn.preprocessing.OneHotEncoder()</code> and I wanted to see how they differed in terms of performance and usage. </p>

<p>I found a tutorial on how to use <code>OneHotEnocder()</code> on <a href=""https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/"">https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/</a> since the <code>sklearn</code> documentation wasn't too helpful on this feature. I have a feeling I'm not doing it correctly...but</p>

<p><strong>Can some explain the pros and cons of using <code>pd.dummies</code> over <code>sklearn.preprocessing.OneHotEncoder()</code> and vice versa?</strong> I know that <code>OneHotEncoder()</code> gives you a sparse matrix but other than that I'm not sure how it is used and what the benefits are over the <code>pandas</code> method.  Am I using it inefficiently? </p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
sns.set()

%matplotlib inline

#Iris Plot
iris = load_iris()
n_samples, m_features = iris.data.shape

#Load Data
X, y = iris.data, iris.target
D_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))

DF_data = pd.DataFrame(X,columns=iris.feature_names)
DF_data[""target""] = pd.Series(y).map(D_target_dummy)
#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \
#0                  5.1               3.5                1.4               0.2   
#1                  4.9               3.0                1.4               0.2   
#2                  4.7               3.2                1.3               0.2   
#3                  4.6               3.1                1.5               0.2   
#4                  5.0               3.6                1.4               0.2   
#5                  5.4               3.9                1.7               0.4   

DF_dummies = pd.get_dummies(DF_data[""target""])
#setosa  versicolor  virginica
#0         1           0          0
#1         1           0          0
#2         1           0          0
#3         1           0          0
#4         1           0          0
#5         1           0          0

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
def f1(DF_data):
    Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()
    DF_data[""Dummies""] = Enc_label.fit_transform(DF_data[""target""])
    DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[""Dummies""]]).todense(), columns = Enc_label.classes_)
    return(DF_dummies2)

%timeit pd.get_dummies(DF_data[""target""])
#1000 loops, best of 3: 777 s per loop

%timeit f1(DF_data)
#100 loops, best of 3: 2.91 ms per loop
</code></pre>
";;0;;2016-04-14T18:28:37.297;5.0;36631163;2016-07-29T05:07:19.750;;;;;678572.0;;1;15;<python><pandas><machine-learning><scikit-learn><dummy-variable>;Panda's get_dummies vs. Sklearn's OneHotEncoder() :: What is more efficient?;2615.0
52077;52077;36685531.0;3.0;"<p>I am kind of getting stuck on extracting value of one variable conditioning on another variable. For example, the following dataframe:</p>

<p><code>A  B
 p1 1
 p1 2
 p3 3
 p2 4</code></p>

<p>How can I get the value of <code>A</code> when <code>B=3</code>? Every time when I extracted the value of A, I got an object, not a string. </p>
";;1;;2016-04-18T01:12:52.610;5.0;36684013;2017-07-14T12:41:53.680;2016-04-18T01:18:04.127;;4962152.0;;4962152.0;;1;14;<python><pandas><dataframe>;extract column value based on another column pandas dataframe;12926.0
52114;52114;36694513.0;3.0;"<p>I'm saving pandas DataFrame to_excel using xlsxwriter. I've managed to format all of my data (set column width, font size etc) except for changing header's font and I can't find the way to do it. Here's my example:</p>

<pre><code>import pandas as pd
data = pd.DataFrame({'test_data': [1,2,3,4,5]})
writer = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')

data.to_excel(writer, sheet_name='test', index=False)

workbook  = writer.book
worksheet = writer.sheets['test']

font_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})
header_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})

worksheet.set_column('A:A', None, font_fmt)
worksheet.set_row(0, None, header_fmt)

writer.save()
</code></pre>

<p>The penultimate line that tries to set format for the header does nothing.</p>
";;0;;2016-04-18T12:40:40.000;7.0;36694313;2017-05-31T15:53:27.647;;;;;3879981.0;;1;12;<python><pandas><xlsxwriter>;pandas xlsxwriter, format header;2410.0
53137;53137;;2.0;"<p>I need to compare two dataframes of different size row-wise and print out non matching rows. Lets take the following two:</p>

<pre><code>df1 = DataFrame({
'Buyer': ['Carl', 'Carl', 'Carl'],
'Quantity': [18, 3, 5, ]})

df2 = DataFrame({
'Buyer': ['Carl', 'Mark', 'Carl', 'Carl'],
'Quantity': [2, 1, 18, 5]})
</code></pre>

<p>What is the most efficient way to row-wise over df2 and print out rows not in df1 e.g:</p>

<pre><code>Buyer     Quantity 
Carl         2
Mark         1
</code></pre>

<p>Important: I do not want to have row: </p>

<pre><code>Buyer     Quantity 
Carl         3
</code></pre>

<p>included in the diff:</p>

<p>I have already tried: 
<a href=""https://stackoverflow.com/questions/32972856/comparing-two-dataframes-of-different-length-row-by-row-and-adding-columns-for-e"">Comparing two dataframes of different length row by row and adding columns for each row with equal value</a>
and <a href=""https://stackoverflow.com/questions/17095101/outputting-difference-in-two-pandas-dataframes-side-by-side-highlighting-the-d"">Outputting difference in two Pandas dataframes side by side - highlighting the difference</a></p>

<p>But these do not match with my problem.</p>

<p>Thank you</p>

<p>Andy</p>
";;0;;2016-04-27T13:55:57.740;1.0;36891977;2016-04-27T15:23:32.190;2017-05-23T12:34:34.957;;-1.0;;2331506.0;;1;12;<python><pandas><dataframe><diff>;Pandas: Diff of two Dataframes;6038.0
53272;53272;36922103.0;3.0;"<p>Having issue filtering my result dataframe with an or condition. I want my result df to extract all column <em>var</em> values that are above 0.25 and below -0.25. This logic below gives me an ambiguous truth value however it work when I split this filtering in two separate operations. What is happening here? not sure where to use the suggested a.empty(), a.bool(), a.item(),a.any() or a.all().</p>

<pre><code> result = result[(result['var']&gt;0.25) or (result['var']&lt;-0.25)]
</code></pre>
";;2;;2016-04-28T17:46:30.520;15.0;36921951;2017-01-19T07:48:25.823;2016-04-28T17:55:44.067;;6267003.0;;6267003.0;;1;26;<python><pandas><dataframe><boolean><filtering>;Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all();44469.0
53795;53795;37012035.0;1.0;"<p>I'm trying to upload a csv file, which is 250MB. Basically 4 million rows and 6 columns of time series data (1min). The usual procedure is:</p>

<pre><code>location = r'C:\Users\Name\Folder_1\Folder_2\file.csv'
df = pd.read_csv(location)
</code></pre>

<p>This procedure takes about 20 minutes !!!. Very preliminary I have explored the following options</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas/14268804#14268804"">Upload in chunks and then put the chunks together.</a></li>
<li><a href=""http://docs.h5py.org/en/latest/"" rel=""nofollow noreferrer"">HDF5</a></li>
<li><a href=""https://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/"" rel=""nofollow noreferrer"">'feather'</a></li>
<li><a href=""https://docs.python.org/2/library/pickle.html"" rel=""nofollow noreferrer"">'pickle'</a></li>
</ul>

<p>I wonder if anybody has compared these options (or more) and there's a clear winner. If nobody answers, In the future I will post my results. I just don't have time right now. </p>
";;9;;2016-05-03T17:06:48.913;6.0;37010212;2016-06-05T22:05:53.170;2017-05-23T12:02:29.630;;-1.0;;3396911.0;;1;15;<python><csv><pandas><dataframe>;What is the fastest way to upload a big csv file in notebook to work with python pandas?;1296.0
54131;54131;;0.0;"<p>Over the last several years there have been several posts related to the <code>parallelization</code> of <code>pandas.apply()</code> or posts that describe problems that could be solved by structuring the data as a dataframe and using <code>pandas.apply()</code> if <code>parallelization</code> was implemented. </p>

<p>My question to the community of experts here - what is the status of this capability as <code>R</code> already has <code>mclapply</code>. </p>

<p>At the moment there is no clean standard solution. It is incredibly tedious to re-code entire functions and scripts to work with the proposed workarounds. </p>

<p><a href=""https://stackoverflow.com/questions/25510482/python-pandas-multiprocessing-apply"">Python Pandas Multiprocessing Apply</a></p>

<p><a href=""https://stackoverflow.com/questions/26187759/parallelize-apply-after-pandas-groupby"">Parallelize apply after pandas groupby</a></p>

<p><a href=""https://stackoverflow.com/questions/17054026/parallel-and-multicore-processing-in-r"">Parallel and Multicore Processing in R</a></p>

<p><a href=""https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments"">Python multiprocessing pool.map for multiple arguments</a></p>

<p><a href=""https://stackoverflow.com/questions/3842237/parallel-processing-in-python"">Parallel Processing in python</a></p>

<p><a href=""https://stackoverflow.com/questions/34031681/passing-kwargs-with-multiprocessing-pool-map"">passing kwargs with multiprocessing.pool.map</a></p>

<p><a href=""https://stackoverflow.com/questions/28943106/passing-arguments-and-manager-dict-to-pool-in-multiprocessing-in-python-2-7/28945479#28945479"">passing arguments and manager.dict to pool in multiprocessing in python 2.7</a></p>

<p><a href=""https://stackoverflow.com/questions/1704401/is-there-a-simple-process-based-parallel-map-for-python"">Is there a simple process-based parallel map for python?</a></p>

<p><a href=""https://stackoverflow.com/questions/29755787/pandas-with-rpy2-and-multiprocessing"">Pandas with rpy2 and multiprocessing</a></p>

<p><a href=""https://stackoverflow.com/questions/32787728/how-to-asynchronously-apply-function-via-spark-to-subsets-of-dataframe?s=5|0.1462"">How to asynchronously apply function via Spark to subsets of dataframe?</a></p>

<p><a href=""https://stackoverflow.com/questions/11728836/efficiently-applying-a-function-to-a-grouped-pandas-dataframe-in-parallel"">Efficiently applying a function to a grouped pandas DataFrame in parallel</a></p>

<p><a href=""https://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply"">python dask DataFrame, support for (trivially parallelizable) row apply?</a></p>

<p><a href=""https://stackoverflow.com/questions/22674950/python-multiprocessing-job-to-celery-task-but-attributeerror"">Python multiprocessing job to Celery task but AttributeError</a></p>

<p><a href=""https://stackoverflow.com/questions/27003430/parallelizing-apply-function-in-pandas-python-worked-on-groupby"">Parallelizing apply function in pandas python. worked on groupby</a></p>
";2016-05-06T23:07:20.653;11;;2016-05-06T18:11:51.047;13.0;37078880;2016-05-06T20:37:46.400;2017-05-23T11:54:50.260;;-1.0;;668624.0;;1;29;<python><r><pandas><parallel-processing><mclapply>;Status of parallelization of pandas.apply();4502.0
55252;55252;;8.0;"<p>I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a classifier without the encoding? </p>

<p>I am trying to do the following for feature selection:</p>

<ol>
<li><p>I read the train file:</p>

<pre><code>num_rows_to_read = 10000
train_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)
</code></pre></li>
<li><p>I change the type of the categorical features to 'category':</p>

<pre><code>non_categorial_features = ['orig_destination_distance',
                          'srch_adults_cnt',
                          'srch_children_cnt',
                          'srch_rm_cnt',
                          'cnt']

for categorical_feature in list(train_small.columns):
    if categorical_feature not in non_categorial_features:
        train_small[categorical_feature] = train_small[categorical_feature].astype('category')
</code></pre></li>
<li><p>I use one hot encoding: </p>

<pre><code>train_small_with_dummies = pd.get_dummies(train_small, sparse=True)
</code></pre></li>
</ol>

<p>The problem is that the 3'rd part often get stuck, although I am using a strong machine.</p>

<p>Thus, without the one hot encoding I can't do any feature selection, for determining the importance of the features.</p>

<p>What do you recommend?</p>
";;0;;2016-05-18T07:26:52.413;9.0;37292872;2017-08-12T05:46:05.100;2016-05-19T07:50:50.923;;562769.0;;6275292.0;;1;12;<python><pandas><machine-learning><anaconda><one-hot-encoding>;How can I one hot encode in Python?;26080.0
55968;55968;37426982.0;5.0;"<p>I have a set of dataframes where one of the columns contains a categorical variable. I'd like to convert it to several dummy variables, in which case I'd normally use <code>get_dummies</code>.</p>

<p>What happens is that <code>get_dummies</code> looks at the data available in each dataframe to find out how many categories there are, and thus create the appropriate number of dummy variables. However, in the problem I'm working right now, I actually know in advance what the possible categories are. But when looking at each dataframe individually, not all categories necessarily appear.</p>

<p>My question is: is there a way to pass to <code>get_dummies</code> (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s?</p>

<p>Something that would make this:</p>

<pre><code>categories = ['a', 'b', 'c']

   cat
1   a
2   b
3   a
</code></pre>

<p>Become this:</p>

<pre><code>  cat_a  cat_b  cat_c
1   1      0      0
2   0      1      0
3   1      0      0
</code></pre>
";;4;;2016-05-25T00:22:39.470;4.0;37425961;2017-07-28T05:07:56.500;2016-06-25T08:18:56.443;;2285236.0;;5940154.0;;1;11;<python><pandas><machine-learning><dummy-variable>;Dummy variables when not all categories are present;1745.0
56754;56754;37562101.0;3.0;"<p>I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column <code>'Test'</code>. However, when I run <code>myFrame['Test'].dtype</code>, I get;</p>

<pre><code>dtype('O')
</code></pre>

<p>What does this mean?</p>
";;1;;2016-06-01T07:22:00.597;1.0;37561991;2017-07-03T13:37:19.443;2017-05-06T14:19:16.923;;2901002.0;;1613983.0;;1;15;<python><pandas><numpy><dataframe><types>;What does a dtype of 'O' mean?;7438.0
59750;59750;38105540.0;5.0;"<p>How do I convert data from a Scikit-learn Bunch object to a Pandas DataFrame?</p>

<pre><code>from sklearn.datasets import load_iris
import pandas as pd
data = load_iris()
print(type(data))
data1 = pd. # Is there a Pandas method to accomplish this?
</code></pre>
";;0;;2016-06-27T07:28:14.210;6.0;38105539;2017-07-20T04:11:50.560;;;;SANBI samples;5440650.0;;1;17;<dataset><scikit-learn><pandas>;How to convert a Scikit-learn dataset to a Pandas dataset?;7406.0
59925;59925;38134049.0;1.0;"<p>I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.</p>

<p>When I do:</p>

<pre><code>import pandas as pd
df = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})
print df
df.fillna(value=0, inplace=True)
print df
</code></pre>

<p>The output:</p>

<pre><code>     a    b    c
0  1.0  4.0  NaN
1  2.0  5.0  NaN
2  3.0  NaN  7.0
3  NaN  6.0  8.0
     a    b    c
0  1.0  4.0  0.0
1  2.0  5.0  0.0
2  3.0  0.0  7.0
3  0.0  6.0  8.0
</code></pre>

<p>It replaces every <code>None</code> with <code>0</code>'s. What I want to do is, only replace <code>None</code>s in columns <code>a</code> and <code>b</code>, but not <code>c</code>.</p>

<p>What is the best way of doing this?</p>
";;0;;2016-06-30T22:06:16.820;4.0;38134012;2016-06-30T22:19:27.140;2016-06-30T22:09:50.527;;547820.0;;547820.0;;1;13;<python><python-2.7><pandas>;Pandas dataframe fillna() only some columns in place;8092.0
60071;60071;;1.0;"<pre><code>import timeit
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.rand(10, 10))

dft = df[[True, False] * 5]
# df = dft
dft2 = dft.copy()

new_data = np.random.rand(5, 10)

print(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))
print(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))
</code></pre>

<p>On my laptop setting values in <code>dft</code> (the original subset) is about 160 times slower than setting values in <code>dft2</code> (a deep copy of <code>dft</code>).</p>

<p><strong>Edit</strong>: Removed speculation about proxy objects.</p>

<p>As c. leather suggests, this is likely because of a different codepath when setting values on a copy (<code>dft</code>) vs an original dataframe (<code>dft2</code>).</p>

<p>Is this correct? Any thoughts or explanations?</p>

<p>Bonus question: removing the reference to the original DataFrame <code>df</code> (by uncommenting the <code>df = dft</code> line), cuts the speed factor to roughly 2 on my laptop. Any idea why this is the case?</p>
";;3;;2016-07-01T23:29:02.527;1.0;38154997;2016-07-14T22:19:50.087;2016-07-14T22:19:50.087;;1953800.0;;1953800.0;;1;11;<python><performance><pandas><dataframe>;Setting values on Pandas DataFrame subset (copy) is slow;596.0
60517;60517;;3.0;"<p>I have data saved in a postgreSQL database. I am querying this data using Python2.7 and turning it into a Pandas DataFrame. However, the last column of this dataframe has a dictionary (or list?) of values within it. The DataFrame looks like this:</p>

<pre><code>[1] df
Station ID     Pollutants
8809           {""a"": ""46"", ""b"": ""3"", ""c"": ""12""}
8810           {""a"": ""36"", ""b"": ""5"", ""c"": ""8""}
8811           {""b"": ""2"", ""c"": ""7""}
8812           {""c"": ""11""}
8813           {""a"": ""82"", ""c"": ""15""}
</code></pre>

<p>I need to split this column into separate columns so that the DataFrame looks like this:</p>

<pre><code>[2] df2
Station ID     a      b       c
8809           46     3       12
8810           36     5       8
8811           NaN    2       7
8812           NaN    NaN     11
8813           82     NaN     15
</code></pre>

<p>The major issue I'm having is that the lists are not the same lengths. But all of the lists only contain up to the same 3 values: a, b, and c. And they always appear in the same order (a first, b second, c third). </p>

<p>The following code USED to work and return exactly what I wanted (df2). </p>

<pre><code>[3] df 
[4] objs = [df, pandas.DataFrame(df['Pollutant Levels'].tolist()).iloc[:, :3]]
[5] df2 = pandas.concat(objs, axis=1).drop('Pollutant Levels', axis=1)
[6] print(df2)
</code></pre>

<p>I was running this code just last week and it was working fine. But now my code is broken and I get this error from line [4]: </p>

<pre><code>IndexError: out-of-bounds on slice (end) 
</code></pre>

<p>I made no changes to the code but am now getting the error. I feel this is due to my method not being robust or proper. </p>

<p>Any suggestions or guidance on how to split this column of lists into separate columns would be super appreciated!</p>

<p>EDIT: I think the .tolist() and .apply methods are not working on my code because it is one unicode string, i.e.:</p>

<pre><code>#My data format 
u{'a': '1', 'b': '2', 'c': '3'}

#and not
{u'a': '1', u'b': '2', u'c': '3'}
</code></pre>

<p>The data is importing from the postgreSQL database in this format. Any help or ideas with this issue? is there a way to convert the unicode? </p>
";;2;;2016-07-06T18:47:56.110;1.0;38231591;2016-07-07T23:33:56.683;2016-07-06T19:26:15.383;;6157698.0;;6157698.0;;1;14;<python><pandas><dictionary><dataframe>;Splitting dictionary/list inside a Pandas Column into Separate Columns;6897.0
60669;60669;38251213.0;3.0;"<p>I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"" rel=""noreferrer"">train_test_split</a> from <code>sklearn.cross_validation</code>, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data. </p>

<p>I know that a workaround would be to use <code>train_test_split</code> two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2?</p>
";;7;;2016-07-07T16:26:26.693;13.0;38250710;2017-06-20T21:01:54.387;2017-06-20T21:01:54.387;;2336654.0;;3450064.0;;1;34;<pandas><numpy><dataframe><machine-learning><scikit-learn>;How to split data into 3 sets (train, validation and test)?;12872.0
60694;60694;38258390.0;2.0;"<p>After experimenting with timing various types of lookups on a Pandas DataFrame I am left with a few questions. </p>

<p>Here is the set up...</p>

<pre><code>import pandas as pd
import numpy as np
import itertools

letters = [chr(x) for x in range(ord('a'), ord('z'))]
letter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]

df1 = pd.DataFrame({
        'value': np.random.normal(size=(1000000)), 
        'letter': np.random.choice(letter_combinations, 1000000)
    })
df2 = df1.sort_values('letter')
df3 = df1.set_index('letter')
df4 = df3.sort_index()
</code></pre>

<p>So df1 looks something like this...</p>

<pre><code>print(df1.head(5))


&gt;&gt;&gt;
  letter     value
0    bdh  0.253778
1    cem -1.915726
2    mru -0.434007
3    lnw -1.286693
4    fjv  0.245523
</code></pre>

<p>Here is the code to test differences in lookup performance...</p>

<pre><code>print('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
%timeit df1[df1.letter == 'ben']
%timeit df1[df1.letter == 'amy']
%timeit df1[df1.letter == 'abe']

print('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
%timeit df2[df2.letter == 'ben']
%timeit df2[df2.letter == 'amy']
%timeit df2[df2.letter == 'abe']

print('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
%timeit df3.loc['ben']
%timeit df3.loc['amy']
%timeit df3.loc['abe']

print('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
%timeit df4.loc['ben']
%timeit df4.loc['amy']
%timeit df4.loc['abe']
</code></pre>

<p>And the results...</p>

<pre><code>~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
10 loops, best of 3: 59.7 ms per loop
10 loops, best of 3: 59.7 ms per loop
10 loops, best of 3: 59.7 ms per loop
~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
10 loops, best of 3: 192 ms per loop
10 loops, best of 3: 192 ms per loop
10 loops, best of 3: 193 ms per loop
~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The slowest run took 4.66 times longer than the fastest. This could mean that an intermediate result is being cached 
10 loops, best of 3: 40.9 ms per loop
10 loops, best of 3: 41 ms per loop
10 loops, best of 3: 40.9 ms per loop
~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The slowest run took 1621.00 times longer than the fastest. This could mean that an intermediate result is being cached 
1 loops, best of 3: 259 s per loop
1000 loops, best of 3: 242 s per loop
1000 loops, best of 3: 243 s per loop
</code></pre>

<p>Questions...</p>

<ol>
<li><p>It's pretty clear why the lookup on the sorted index is so much faster, binary search to get O(log(n)) performance vs O(n) for a full array scan. But, why is the lookup on the sorted non-indexed <code>df2</code> column <em>SLOWER</em> than the lookup on the unsorted non-indexed column <code>df1</code>?</p></li>
<li><p>What is up with the <code>The slowest run took x times longer than the fastest. This could mean that an intermediate result is being cached</code>. Surely, the results aren't being cached. Is it because the created index is lazy and isn't <em>actually</em> reindexed until needed? That would explain why it is only on the first call to <code>.loc[]</code>.</p></li>
<li><p>Why isn't an index sorted by default? The fixed cost of the sort can be too much?</p></li>
</ol>
";;12;;2016-07-07T19:46:24.030;3.0;38254067;2016-07-08T13:04:52.937;2016-07-07T20:34:31.747;;2939850.0;;2939850.0;;1;15;<python><performance><pandas>;Comparison of Pandas lookup times;326.0
61353;61353;;2.0;"<p>The purpose of this question is to further explore <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html"">MultiIndex dataframes</a> and to ask questions of the best approach for various tasks.</p>

<p><strong>Create the DataFrame</strong></p>

<pre><code>import pandas as pd

df = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], 
               'portfolio' : ['A','B','C','D','E'], 
               'reporting_ccy' : ['GBP','GBP','GBP','GBP','GBP'],
               'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],
               'amount' : [100,200,300,400,500],
               'injection' : [1,2,3,4,5],
               'to_usd' : [1.3167,1.3167,1.3167,1.3167,1.3167],
               'to_ccy' : [0.009564,1,1,1.1093,1.1093],
               'm5' : [2,4,6,8,10],
               'm6' : [1,3,5,7,9]}); 
</code></pre>

<p><strong>Pivot the DataFrame</strong></p>

<pre><code>df_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)
</code></pre>

<p><strong>Rename the columns</strong></p>

<pre><code>df_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']
</code></pre>

<p>This yields a pivoted representation of the data such that:</p>

<ol>
<li>a portfolio may have 1 or many measures</li>
<li>shows the portfolio default currency</li>
<li>shows the portfolio reporting currency</li>
<li>a measure may have 1 or many reporting currencies.</li>
</ol>

<p>I terms of 4. what is the best approach for implementation given that we have the xRates for the currencies?</p>

<p>Such that we create a dataframe such as that derived here:</p>

<p><strong>Create DataFrame</strong></p>

<pre><code>df1 = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], 
           'portfolio' : ['A','B','C','D','E'], 
           'reporting_ccy' : ['JPY','USD','USD','EUR','EUR'],
           'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],
           'amount' : [13767.2522, 263.34, 395.01, 474.785901, 593.4823763],
           'injection' : [1,2,3,4,5],
           'to_usd' : [0.009564, 1, 1, 1.1093, 1.1093],
           'to_ccy' : [1.3167, 1.3167, 1.3167, 1.3167, 1.3167],
           'm5' : [2,4,6,8,10],
           'm6' : [1,3,5,7,9]}); 
</code></pre>

<p><strong>Concatenate &amp; Pivot the DataFrames</strong></p>

<pre><code>df_concat = pd.concat([df,df1])
df_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)
df_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']
</code></pre>

<p>This now shows 1 measure having many currencies.</p>

<pre><code>df_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)
</code></pre>

<p><strong>Question</strong></p>

<p>Is there a better way, such as adding data directly to a multiIndexed dataframe at level 3 <code>df_pivot1.columns.get_level_values(3).unique()</code>? </p>

<p>I would like to be able to iterate through each level and add new measures either derived from other measures using <code>df.assign()</code> or other methods.</p>

<p>The use case here is to add other currencies to the measures where applicable.  The concatenation and re-pivot as above does not seem optimal.</p>
";;6;;2016-07-13T13:11:15.637;4.0;38352742;2016-12-15T23:44:44.833;2016-07-15T09:56:58.710;;4843695.0;;4843695.0;;1;14;<python><pandas>;Pandas Design Considerations for MultiIndexed Dataframes;439.0
62122;62122;38470963.0;1.0;"<h3>Setup</h3>

<p>consider the following dataframe (note the strings):</p>

<pre><code>df = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/iEQcv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iEQcv.png"" alt=""enter image description here""></a></p>

<pre><code>df.info()

&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 2 entries, 0 to 1
Data columns (total 2 columns):
A    2 non-null object
B    2 non-null object
dtypes: object(2)
memory usage: 104.0+ bytes
</code></pre>

<h3>Question</h3>

<p>I'm going to sum.  I expect the strings to be concatenated.</p>

<pre><code>df.sum()

A     30.0
B    112.0
dtype: float64
</code></pre>

<p>It looks as though the strings were concatenated then converted to float.  Is there a good reason for this?  Is this a bug?  Anything enlightening will be up voted.</p>
";;1;;2016-07-20T00:27:31.867;1.0;38470550;2016-07-20T01:58:01.537;;;;;2336654.0;;1;14;<python><pandas>;why is a sum of strings converted to floats;430.0
62834;62834;38579700.0;1.0;"<p><code>np.where</code> has the semantics of a vectorized if/else (similar to Apache Spark's <code>when</code>/<code>otherwise</code> DataFrame method). I know that I can use <code>np.where</code> on pandas <code>Series</code>, but <code>pandas</code> often defines its own API to use instead of raw <code>numpy</code> functions, which is usually more convenient with <code>pd.Series</code>/<code>pd.DataFrame</code>.</p>

<p>Sure enough, I found <code>pandas.DataFrame.where</code>. However, at first glance, it has a completely different semantics. I could not find a way to rewrite the most basic example of <code>np.where</code> using pandas <code>where</code>:</p>

<pre><code># df is pd.DataFrame
# how to write this using df.where?
df['C'] = np.where((df['A']&lt;0) | (df['B']&gt;0), df['A']+df['B'], df['A']/df['B'])
</code></pre>

<p>Am I missing something obvious? Or is pandas <code>where</code> intended for a completely different use case, despite same name as <code>np.where</code>? </p>
";;5;;2016-07-26T00:52:45.647;1.0;38579532;2016-07-26T01:33:25.767;2016-07-26T00:59:45.257;;336527.0;;336527.0;;1;12;<python><numpy><pandas>;pandas equivalent of np.where;4669.0
63695;63695;38713387.0;3.0;"<p>I have had to do this several times and I'm always frustrated.  I have a dataframe:</p>

<pre><code>df = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])

print df

   A  B  C  D
a  1  2  3  4
b  5  6  7  8
</code></pre>

<p>I want to turn <code>df</code> into:</p>

<pre><code>pd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])

a    [1, 2, 3, 4]
b    [5, 6, 7, 8]
dtype: object
</code></pre>

<p>I've tried</p>

<pre><code>df.apply(list, axis=1)
</code></pre>

<p>Which just gets me back the same <code>df</code></p>

<p>What is a convenient/effective way to do this?</p>
";;0;;2016-08-02T06:29:48.163;7.0;38713200;2017-05-06T14:12:46.250;2017-05-06T14:12:46.250;;2901002.0;;6284734.0;;1;19;<python><list><pandas><dataframe><series>;How do I turn a dataframe into a series of lists?;936.0
63720;63720;;1.0;"<p>I've just started to learn to use python. I'm using anaconda python <code>3.5</code> and Rodeo to do a simple <code>ggplot</code>.</p>

<pre><code>from ggplot import *
df=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})
p=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill='blue')
p
ggsave(""test.png"",p)
</code></pre>

<p>Everything works fine before the 5th line. I got the plot as I wanted. But I got an error when I tried to save the plot:</p>

<blockquote>
  <p>NameError: name 'ggsave' is not defined</p>
</blockquote>

<p>It seems that there's no ggsave function in ggplot module? The ggplot version is 0.11.1. Am I missing something here?</p>
";;4;;2016-08-02T09:57:09.273;;38717282;2017-04-24T07:59:32.397;2016-08-02T11:16:54.757;;6207849.0;;4329532.0;;1;11;<python><pandas><ggplot2>;Python ggplot- ggsave function not defined;1042.0
63835;63835;;6.0;"<p>Given a matrix from an <code>SFrame</code>:</p>

<pre><code>&gt;&gt;&gt; from sframe import SFrame
&gt;&gt;&gt; sf =SFrame({'x':[1,1,2,5,7], 'y':[2,4,6,8,2], 'z':[2,5,8,6,2]})
&gt;&gt;&gt; sf
Columns:
    x   int
    y   int
    z   int

Rows: 5

Data:
+---+---+---+
| x | y | z |
+---+---+---+
| 1 | 2 | 2 |
| 1 | 4 | 5 |
| 2 | 6 | 8 |
| 5 | 8 | 6 |
| 7 | 2 | 2 |
+---+---+---+
[5 rows x 3 columns]
</code></pre>

<p>I want to get the unique values for the <code>x</code> and <code>y</code> columns and I can do it as such:</p>

<pre><code>&gt;&gt;&gt; sf['x'].unique().append(sf['y'].unique()).unique()
dtype: int
Rows: 7
[2, 8, 5, 4, 1, 7, 6]
</code></pre>

<p>This way I get the unique values of x and unique values of y then append them and get the unique values of the appended list.</p>

<p>I could also do it as such:</p>

<pre><code>&gt;&gt;&gt; sf['x'].append(sf['y']).unique()
dtype: int
Rows: 7
[2, 8, 5, 4, 1, 7, 6]
</code></pre>

<p>But that way, if my x and y columns are huge with lots of duplicates, I would be appending it into a very huge container before getting the unique.</p>

<p><strong>Is there a more efficient way to get the unique values of a combined columns created from 2 or more columns in an SFrame?</strong></p>

<p><strong>What is the equivalence in pandas of the efficent way to get unique values from 2 or more columns in <code>pandas</code>?</strong></p>
";;2;;2016-08-03T03:07:31.460;;38733719;2017-08-18T01:58:18.443;;;;;610569.0;;1;12;<python><csv><pandas><dataframe><sframe>;Efficient way to get the unique values from 2 or more columns in a Dataframe;651.0
64165;64165;38801975.0;3.0;"<p>I have two pandas dataframes and I would like to display them in Jupyter notebook.</p>

<p>Doing something like: </p>

<pre><code>display(df1)
display(df2)
</code></pre>

<p>Shows them one below another:</p>

<p><a href=""https://i.stack.imgur.com/e9jdGm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e9jdGm.png"" alt=""enter image description here""></a></p>

<p>I would like to have a second dataframe on the right of the first one. There is <a href=""https://stackoverflow.com/q/35790922/1090562"">a similar question</a>, but it looks like there a person is satisfied either with merging them in one dataframe of showing the difference between them.</p>

<p>This will not work for me. In my case dataframes can represent completely different (non-comparable elements) and the size of them can be different. Thus my main goal is to save space.</p>
";;0;;2016-08-05T06:58:08.027;5.0;38783027;2017-07-05T10:42:16.080;2017-05-23T12:18:12.490;;-1.0;;1090562.0;;1;16;<pandas><ipython-notebook><jupyter-notebook>;Jupyter notebook display two pandas tables side by side;3149.0
64793;64793;38886211.0;1.0;"<p>Why do we use 'loc' for pandas dataframes? it seems the following code with or without using loc both compile anr run at a simulular speed</p>

<pre><code>%timeit df_user1 = df.loc[df.user_id=='5561']

100 loops, best of 3: 11.9 ms per loop
</code></pre>

<p>or</p>

<pre><code>%timeit df_user1_noloc = df[df.user_id=='5561']

100 loops, best of 3: 12 ms per loop
</code></pre>

<p>So why use loc?</p>

<p><strong>Edit:</strong> This has been flagged as a duplicate question. But although <a href=""https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation/31593712#31593712"">pandas iloc vs ix vs loc explanation?</a> does mention that *</p>

<blockquote>
  <p>you can do column retrieval just by using the data frame's
  <strong>getitem</strong>:</p>
</blockquote>

<p>*</p>

<pre><code>df['time']    # equivalent to df.loc[:, 'time']
</code></pre>

<p>it does not say why we use loc, although it does explain lots of features of loc, my specific question is 'why not just omit loc altogether'? for which i have accepted a very detailed answer below.</p>

<p>Also that other post the answer (which i do not think is an answer) is very hidden in the discussion and any person searching for what i was looking for would find it hard to locate the information and would be much better served by the answer provided to my question.</p>
";;8;;2016-08-11T01:51:08.090;8.0;38886080;2016-08-11T03:18:34.440;2017-05-23T11:55:07.950;;-1.0;;6056160.0;;1;15;<python><pandas><series><loc>;Python: Pandas Series - Why use loc?;1881.0
64903;64903;39045224.0;6.0;"<p>I've a pandas dataframe with a datetime64 object on one of the columns.</p>

<pre><code>    time    volume  complete    closeBid    closeAsk    openBid openAsk highBid highAsk lowBid  lowAsk  closeMid
0   2016-08-07 21:00:00+00:00   9   True    0.84734 0.84842 0.84706 0.84814 0.84734 0.84842 0.84706 0.84814 0.84788
1   2016-08-07 21:05:00+00:00   10  True    0.84735 0.84841 0.84752 0.84832 0.84752 0.84846 0.84712 0.8482  0.84788
2   2016-08-07 21:10:00+00:00   10  True    0.84742 0.84817 0.84739 0.84828 0.84757 0.84831 0.84735 0.84817 0.847795
3   2016-08-07 21:15:00+00:00   18  True    0.84732 0.84811 0.84737 0.84813 0.84737 0.84813 0.84721 0.8479  0.847715
4   2016-08-07 21:20:00+00:00   4   True    0.84755 0.84822 0.84739 0.84812 0.84755 0.84822 0.84739 0.84812 0.847885
5   2016-08-07 21:25:00+00:00   4   True    0.84769 0.84843 0.84758 0.84827 0.84769 0.84843 0.84758 0.84827 0.84806
6   2016-08-07 21:30:00+00:00   5   True    0.84764 0.84851 0.84768 0.84852 0.8478  0.84857 0.84764 0.84851 0.848075
7   2016-08-07 21:35:00+00:00   4   True    0.84755 0.84825 0.84762 0.84844 0.84765 0.84844 0.84755 0.84824 0.8479
8   2016-08-07 21:40:00+00:00   1   True    0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.847855
9   2016-08-07 21:45:00+00:00   3   True    0.84727 0.84817 0.84743 0.8482  0.84743 0.84822 0.84727 0.84817 0.84772
</code></pre>

<p>My application follows the (simplified) structure below:</p>

<pre><code>class Runner():
    def execute_tick(self, clock_tick, previous_tick):
        candles = self.broker.get_new_candles(clock_tick, previous_tick)
        if candles:
            run_calculations(candles)

class Broker():
    def get_new_candles(clock_tick, previous_tick)
        start = previous_tick - timedelta(minutes=1)
        end = clock_tick - timedelta(minutes=3)
        return df[(df.time &gt; start) &amp; (df.time &lt;= end)]
</code></pre>

<p>I noticed when profiling the app, that calling the <code>df[(df.time &gt; start) &amp; (df.time &lt;= end)]</code> causes the highest performance issues and I was wondering if there is a way to speed up these calls?</p>

<p>EDIT: I'm adding some more info about the use-case here (also, source is available at: <a href=""https://github.com/jmelett/pyFxTrader"" rel=""noreferrer"">https://github.com/jmelett/pyFxTrader</a>)</p>

<ul>
<li>The application will accept a list of <a href=""https://en.wikipedia.org/wiki/Foreign_exchange_market#Financial_instruments"" rel=""noreferrer"">instruments</a> (e.g. EUR_USD, USD_JPY, GBP_CHF) and then <a href=""https://github.com/jmelett/pyFxTrader/blob/master/trader/broker/oanda_backtest.py#L68"" rel=""noreferrer"">pre-fetch</a> ticks/<a href=""https://en.wikipedia.org/wiki/Candlestick_chart"" rel=""noreferrer"">candles</a> for each one of them and their timeframes (e.g. 5 minutes, 30 minutes, 1 hour etc.). The initialised data is basically a <code>dict</code> of Instruments, each containing another <code>dict</code> with candle data for M5, M30, H1 timeframes. </li>
<li>Each ""timeframe"" is a pandas dataframe like shown at the top</li>
<li>A <a href=""https://github.com/jmelett/pyFxTrader/blob/master/trader/controller.py#L145"" rel=""noreferrer"">clock simulator</a> is then used to query the individual candles for the specific time (e.g. at 15:30:00, give me the last x ""5-minute-candles"") for EUR_USD</li>
<li>This piece of data is then used to ""<a href=""https://en.wikipedia.org/wiki/Backtesting"" rel=""noreferrer"">simulate</a>"" specific market conditions (e.g. average price over last 1 hour increased by 10%, buy market position)</li>
</ul>
";;13;;2016-08-11T17:01:57.317;5.0;38902239;2016-08-19T18:02:10.673;2016-08-15T22:07:45.867;;123172.0;;123172.0;;1;13;<python><pandas><numpy><dataframe>;Performance issues with pandas and filtering on datetime column;319.0
65097;65097;38935669.0;2.0;"<p>I used to work with R and really love the dplyr package which you can easily group by and summarize. </p>

<p>However, in pandas, I don't see an equivalent of summarize and here is how I achieve it in Python: </p>

<pre><code>import pandas as pd
data = pd.DataFrame(
    {'col1':[1,1,1,1,1,2,2,2,2,2],
    'col2':[1,2,3,4,5,6,7,8,9,0],
     'col3':[-1,-2,-3,-4,-5,-6,-7,-8,-9,0]
    }
)
result = []
for k,v in data.groupby('col1'):
    result.append([k, max(v['col2']), min(v['col3'])])
print pd.DataFrame(result, columns=['col1', 'col2_agg', 'col3_agg'])
</code></pre>

<p>It is not only very verbose, but also might not be the most optimized and efficient. (I used to rewrite a <code>for loop groupby</code> implementation into <code>groupby.agg</code> and the performance enhancement was huge). </p>

<p>In R the code will be </p>

<pre><code>data %&gt;% groupby(col1) %&gt;% summarize(col2_agg=max(col2), col3_agg=min(col3))
</code></pre>

<p>Is there an efficient equivalent in Python or for loop is what I have to work with. </p>

<hr>

<p>also, @ayhan really gave a solution to my answer, this is a follow-up question that I will list here instead of the comment: </p>

<p>what is the equivalent of <code>groupby().summarize(newcolumn=max(col2 * col3))</code></p>
";;1;;2016-08-13T18:03:32.243;3.0;38935541;2017-08-08T11:45:38.830;2016-08-13T18:56:05.703;;1953475.0;;1953475.0;;1;14;<python><r><pandas>;dplyr summarize equivalent in pandas;2356.0
65223;65223;;1.0;"<p>I am using PyCharm 2016.2.1 . When I try to view a Pandas dataframe through the newly added feature 'View as DataFrame' in the debugger, this works as expected for a small (e.g. 4x4) DataFrame. </p>

<p>However when I try to view a DataFrame (generated by custom script) of ~10,000 rows x ~50 columns, I get the message: ""Nothing to show"". </p>

<p>When I run the same script (that generates the DataFrame) in Spyder, I am able to view it, so I am pretty sure it's not an error in my script. </p>

<p>Does anyone know if there is a maximum size to the DataFrames that can be viewed in PyCharm, and if there is a way to change this? </p>

<h1>EDIT:</h1>

<p>It seems that the maximum size allowed is 1000 x 15 , as in some cases it gets truncated to this size (when the number of rows is too large, but when there are too many columns pycharm just says 'nothing to show').</p>

<p>Still, I would like to know if there is a way to increase the maximum allowed rows and columns viewable through the DataFrame viewer.</p>
";;2;;2016-08-15T14:07:12.430;;38956660;2017-03-30T10:38:32.517;2016-08-15T15:24:56.913;;5571377.0;;5571377.0;;1;14;<python><pandas><pycharm>;Dataframe not showing in Pycharm;2214.0
66065;66065;;3.0;"<p>I have a really large csv file that I opened in pandas as follows....</p>

<pre><code>import pandas
df = pandas.read_csv('large_txt_file.txt')
</code></pre>

<p>Once I do this my memory usage increases by 2GB, which is expected because this file contains millions of rows.  My problem comes when I need to release this memory.  I ran....</p>

<pre><code>del df
</code></pre>

<p>However, my memory usage did not drop.  Is this the wrong approach to release memory used by a pandas data frame?  If it is, what is the proper way? </p>
";;4;;2016-08-23T12:17:10.457;9.0;39100971;2017-03-02T14:11:21.400;;;;;789294.0;;1;18;<python><pandas><memory>;How do I release memory used by a pandas dataframe?;11401.0
66279;66279;39132900.0;4.0;"<p>I have the following dataframe:</p>

<pre><code>df = pd.DataFrame([
    (1, 1, 'term1'),
    (1, 2, 'term2'),
    (1, 1, 'term1'),
    (1, 1, 'term2'),
    (2, 2, 'term3'),
    (2, 3, 'term1'),
    (2, 2, 'term1')
], columns=['id', 'group', 'term'])
</code></pre>

<p>I want to group it by <code>id</code> and <code>group</code> and calculate the number of each term for this id, group pair.</p>

<p>So in the end I am going to get something like this:</p>

<p><a href=""https://i.stack.imgur.com/DIwPYm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DIwPYm.png"" alt=""enter image description here""></a></p>

<p>I was able to achieve what I want by looping over all the rows with <code>df.iterrows()</code> and creating a new dataframe, but this is clearly inefficient. (If it helps, I know the list of all terms beforehand and there are ~10 of them).</p>

<p>It looks like I have to group by and then count values, so I tried that with <code>df.groupby(['id', 'group']).value_counts()</code> which does not work because <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.core.groupby.SeriesGroupBy.value_counts.html"" rel=""noreferrer"">value_counts</a> operates on the groupby series and not a dataframe.</p>

<p>Anyway I can achieve this without looping?</p>
";;0;;2016-08-24T20:45:24.230;2.0;39132742;2016-08-25T07:17:10.660;2016-08-24T22:05:16.567;;2901002.0;;1090562.0;;1;11;<python><pandas><dataframe><group-by><crosstab>;Groupby value counts on the dataframe pandas;3883.0
66393;66393;39432898.0;3.0;"<p>I have a df that looks like: </p>

<pre><code>df.head()
Out[1]:
        A   B   C
city0   40  12  73
city1   65  56  10
city2   77  58  71
city3   89  53  49
city4   33  98  90
</code></pre>

<p>An example df can be created by the following code:</p>

<pre><code>df = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))

indx = ['city'+str(x) for x in range(0,1000000)]
df.index = indx
</code></pre>

<p>What I want to do is:</p>

<p>a) determine appropriate histogram bucket lengths for column A and assign each city to a bucket for column A</p>

<p>b) determine appropriate histogram bucket lengths for column B and assign each city to a bucket for column B</p>

<p>Maybe the resulting df looks like (or is there a better built in way in pandas?)</p>

<pre><code>    df.head()
    Out[1]:
            A   B   C  Abkt Bbkt
    city0   40  12  73  2  1
    city1   65  56  10  4  3
    city2   77  58  71  4  3
    city3   89  53  49  5  3
    city4   33  98  90  2  5
</code></pre>

<p>Where Abkt and Bbkt are histogram bucket identifiers:</p>

<pre><code>1-20 = 1
21-40 = 2
41-60 = 3
61-80 = 4
81-100 = 5
</code></pre>

<p>Ultimately, I want to better understand the behavior of each city with respect to columns A, B and C and be able to answer questions like:</p>

<p>a) What does the distribution of Column A (or B) look like - i.e. what buckets are most/least populated.</p>

<p>b) Conditional on a particular slice/bucket of Column A, what does the distribution of Column B look like - i.e. what buckets are most/least populated.</p>

<p>c) Conditional on a particular slice/bucket of Column A and B, what does the behavior of C look like.</p>

<p>Ideally, I want to be able to visualize the data (heat maps, region identifiers etc). I'm a relative pandas/python newbie and don't know what is possible to develop. </p>

<p>If the SO community can kindly provide code examples of how I can do what I want (or a better approach if there are better pandas/numpy/scipy built in methods) I would be grateful. </p>

<p>As well, any pointers to resources that can help me better summarize/slice/dice my data and be able to visualize at intermediate steps as I proceed with my analysis. </p>

<p><strong>UPDATE:</strong></p>

<p>I am following some of the suggestions in the comments. </p>

<p>I tried:</p>

<p>1) <code>df.hist()</code></p>

<pre><code>ValueError: The first argument of bincount must be non-negative
</code></pre>

<p>2) <code>df[['A']].hist(bins=10,range=(0,10))</code></p>

<pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000A2350615C0&gt;]], dtype=object)
</code></pre>

<p>Isn't <code>#2</code> suppose to show a plot? instead of producing an object that is not rendered? I am using <code>jupyter notebook</code>. </p>

<p>Is there something I need to turn-on / enable in <code>Jupyter Notebook</code> to render the histogram objects? </p>

<p><strong>UPDATE2:</strong></p>

<p>I solved the rendering problem by: <a href=""https://stackoverflow.com/questions/10511024/in-ipython-notebook-pandas-is-not-displying-the-graph-i-try-to-plot"">in Ipython notebook, Pandas is not displying the graph I try to plot.</a></p>

<p><strong>UPDATE3:</strong></p>

<p>As per suggestions from the comments, I started looking through <a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html"" rel=""nofollow noreferrer"">pandas visualization</a>, <a href=""http://bokeh.pydata.org/en/latest/"" rel=""nofollow noreferrer"">bokeh</a> and <a href=""http://stanford.edu/~mwaskom/software/seaborn/"" rel=""nofollow noreferrer"">seaborn</a>. However, I'm not sure how I can create linkages between plots. </p>

<p>Lets say I have 10 variables. I want to explore them but since 10 is a large number to explore at once, lets say I want to explore 5 at any given time (r,s,t,u,v). </p>

<p>If I want an interactive hexbin with marginal distributions plot to examine the relationship between r &amp; s, how do I also see the distribution of t, u and v given interactive region selections/slices of r&amp;s (polygons).</p>

<p>I found hexbin with marginal distribution plot here <a href=""http://stanford.edu/~mwaskom/software/seaborn/examples/hexbin_marginals.html"" rel=""nofollow noreferrer"">hexbin plot</a>: </p>

<p><strong>But:</strong></p>

<p>1) How to make this interactive (allow selections of polygons)</p>

<p>2) How to link region selections of r &amp; s to other plots, for example 3 histogram plots of t,u, and v (or any other type of plot).</p>

<p>This way, I can navigate through the data more rigorously and explore the relationships in depth.</p>
";;11;;2016-08-26T00:08:52.177;3.0;39156545;2016-09-12T10:23:56.827;2017-05-23T12:01:36.027;;-1.0;;668624.0;;1;12;<python><pandas><data-visualization><seaborn><bokeh>;interactive conditional histogram bucket slicing data visualization;881.0
67377;67377;;1.0;"<p>It's <a href=""https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas"">well known</a> (and <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">understandable</a>) that pandas behavior is essentially unpredictable when assigning to a slice. But I'm used to being warned about it by <code>SettingWithCopy</code> warning.</p>

<p>Why is the warning not generated in either of the following two code snippets, and what techniques could reduce the chance of writing such code unintentionally?</p>

<pre><code># pandas 0.18.1, python 3.5.1
import pandas as pd
data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
new_data = data[['a', 'b']]
data = data['a']
new_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data

data[0] == 1
True


data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
new_data = data['a']
data = data['a']
new_data.loc[0] = 100 # no warning, propagates to data

data[0] == 100
True
</code></pre>

<p>I thought the explanation was that pandas only produces the warning when the parent DataFrame is still reachable from the current context. (This would be a weakness of the detection algorithm, as my previous examples show.)</p>

<p>In the next snippet, AFAIK the original two-column DataFrame is no longer reachable, and yet pandas warning mechanism manages to trigger (luckily):</p>

<pre><code>data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
new_data = data['a']
data = data[['a']]
new_data.loc[0] = 100 # warning, so we're safe
</code></pre>

<p>Edit:</p>

<p>While investigating this, I found another case of a missing warning:</p>

<pre><code>data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
data = data.groupby('a')
new_data = data.filter(lambda g: len(g)==1)
new_data.loc[0, 'a'] = 100 # no warning, does not propagate to data
assert data.filter(lambda g: True).loc[0, 'a'] == 1
</code></pre>

<p>Even though an almost identical example does trigger a warning:</p>

<pre><code>data = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})
data = data.groupby('a')
new_data = data.filter(lambda g: len(g)==1)
new_data.loc[0, 'a'] = 100 # warning, does not propagate to data
assert data.filter(lambda g: True).loc[0, 'a'] == 1
</code></pre>

<p>Update: I'm responding to the answer by @firelynx here because it's hard to put it in the comment.</p>

<p>In the answer, @firelynx says that the first code snippet results in no warning because I'm taking the entire dataframe. But even if I took part of it, I still don't get a warning:</p>

<pre><code># pandas 0.18.1, python 3.5.1
import pandas as pd
data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})
new_data = data[['a', 'b']]
data = data['a']
new_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data

data[0] == 1
True
</code></pre>
";;8;;2016-09-04T22:07:24.817;3.0;39321914;2017-05-29T20:48:47.617;2017-05-29T20:48:47.617;;336527.0;;336527.0;;1;15;<python><pandas>;Unpredictable pandas slice assignment behavior with no SettingWithCopyWarning;320.0
69455;69455;40005797.0;2.0;"<p>I am trying to implement an <a href=""https://keras.io/layers/recurrent/#lstm"" rel=""nofollow noreferrer"">LSTM with Keras</a>.</p>

<p>I know that LSTM's in Keras require a 3D tensor with shape <code>(nb_samples, timesteps, input_dim)</code> as an input. However, I am not entirely sure how the input should look like in my case, as I have just one sample of <code>T</code> observations for each input, not multiple samples, i.e. <code>(nb_samples=1, timesteps=T, input_dim=N)</code>. Is it better to split each of my inputs into samples of length <code>T/M</code>? <code>T</code> is around a few million observations for me, so how long should each sample in that case be, i.e., how would I choose <code>M</code>?</p>

<p>Also, am I right in that this tensor should look something like:</p>

<p><code>[[[a_11, a_12, ..., a_1M], [a_21, a_22, ..., a_2M], ..., [a_N1, a_N2, ..., a_NM]], [[b_11, b_12, ..., b_1M], [b_21, b_22, ..., b_2M], ..., [b_N1, b_N2, ..., b_NM]], ..., [[x_11, x_12, ..., a_1M], [x_21, x_22, ..., x_2M], ..., [x_N1, x_N2, ..., x_NM]]]</code>, where M and N defined as before and x corresponds to the last sample that I would have obtained from splitting as discussed above? </p>

<p>Finally, given a pandas dataframe with <code>T</code> observations in each column, and <code>N</code> columns, one for each input, how can I create such an input to feed to Keras?</p>
";;4;;2016-09-24T09:21:49.350;10.0;39674713;2017-05-11T07:04:59.527;2017-05-11T07:04:59.527;;1079075.0;;2151205.0;;1;17;<python><pandas><keras><lstm>;Neural Network LSTM input shape from dataframe;6582.0
72225;72225;40144368.0;5.0;"<p>I have a list of names (strings) divided into words. There are 8 million names, each name consists of up to 20 words (tokens). Number of unique tokens is 2.2 million. I need an efficient way to find all names containing at least one word from the query (which may contain also up to 20 words, but usually only a few).</p>

<p>My current approach uses Python Pandas and looks like this (later referred as <code>original</code>):</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([['foo', 'bar', 'joe'], 
                       ['foo'], 
                       ['bar', 'joe'], 
                       ['zoo']], 
                      index=['id1', 'id2', 'id3', 'id4'])
&gt;&gt;&gt; df.index.rename('id', inplace=True)  # btw, is there a way to include this into prev line?
&gt;&gt;&gt; print df

       0     1     2
id                  
id1  foo   bar   joe
id2  foo  None  None
id3  bar   joe  None
id4  zoo  None  None

def filter_by_tokens(df, tokens):
    # search within each column and then concatenate and dedup results    
    results = [df.loc[lambda df: df[i].isin(tokens)] for i in range(df.shape[1])]
    return pd.concat(results).reset_index().drop_duplicates().set_index(df.index.name)

&gt;&gt;&gt; print filter_by_tokens(df, ['foo', 'zoo'])

       0     1     2
id                  
id1  foo   bar   joe
id2  foo  None  None
id4  zoo  None  None    
</code></pre>

<p>Currently such lookup (on the full dataset) takes 5.75s on my (rather powerful) machine. I'd like to speed it up at least, say, 10 times.</p>

<p>I was able to get to 5.29s by squeezing all columns into one and perform a lookup on that (later referred as <code>original, squeezed</code>):</p>

<pre><code>&gt;&gt;&gt; df = pd.Series([{'foo', 'bar', 'joe'}, 
                    {'foo'}, 
                    {'bar', 'joe'}, 
                    {'zoo'}], 
                    index=['id1', 'id2', 'id3', 'id4'])
&gt;&gt;&gt; df.index.rename('id', inplace=True)
&gt;&gt;&gt; print df

id
id1    {foo, bar, joe}
id2              {foo}
id3         {bar, joe}
id4              {zoo}
dtype: object

def filter_by_tokens(df, tokens):
    return df[df.map(lambda x: bool(x &amp; set(tokens)))]

&gt;&gt;&gt; print filter_by_tokens(df, ['foo', 'zoo'])

id
id1    {foo, bar, joe}
id2              {foo}
id4              {zoo}
dtype: object
</code></pre>

<p>But that's still not fast enough. </p>

<p>Another solution which seems to be easy to implement is to use Python multiprocessing (threading shouldn't help here because of GIL and there is no I/O, right?). But the problem with it is that the big dataframe needs to be copied to each process, which takes up all the memory. Another problem is that I need to call <code>filter_by_tokens</code> many times in a loop, so it would copy the dataframe on every call, which is inefficient.</p>

<p>Note that words may occur many times in names (e.g. the most popular word occurs 600k times in names), so a reverse index would be huge.</p>

<p>What is a good way to write this efficiently? Python solution preferred, but I'm also open to other languages and technologies (e.g. databases).</p>

<hr>

<p><strong>UPD:</strong>
I've measured execution time of my two solutions and the 5 solutions suggested by @piRSquared in his <a href=""https://stackoverflow.com/a/40144368/304209"">answer</a>. Here are the results (tl;dr the best is 2x improvement):</p>

<pre><code>+--------------------+----------------+
|       method       | best of 3, sec |
+--------------------+----------------+
| original           | 5.75           |
| original, squeezed | 5.29           |
| zip                | 2.54           |
| merge              | 8.87           |
| mul+any            | MemoryError    |
| isin               | IndexingError  |
| query              | 3.7            |
+--------------------+----------------+
</code></pre>

<p><code>mul+any</code> gives MemoryError on <code>d1 = pd.get_dummies(df.stack()).groupby(level=0).sum()</code> (on a 128Gb RAM machine).</p>

<p><code>isin</code> gives <code>IndexingError: Unalignable boolean Series key provided</code> on <code>s[d1.isin({'zoo', 'foo'}).unstack().any(1)]</code>, apparently because shape of <code>df.stack().isin(set(tokens)).unstack()</code> is slightly less than the shape of the original dataframe (8.39M vs 8.41M rows), not sure why and how to fix that.</p>

<p>Note that the machine I'm using has 12 cores (though I mentioned some problems with parallelization above). All of the solutions utilize a single core.</p>

<p><strong>Conclusion</strong> (as of now): there is 2.1x improvement by <code>zip</code> (2.54s) vs <code>original squeezed</code> solution (5.29s). It's good, though I aimed for at least 10x improvement, if possible. So I'm leaving the (still great) @piRSquared answer unaccepted for now, to welcome more suggestions.</p>
";;5;;2016-10-20T01:09:51.223;3.0;40143902;2016-10-25T17:07:09.160;2017-05-23T12:10:17.807;;-1.0;;304209.0;;1;11;<python><performance><pandas><hashtable><lookup>;Efficient lookup by common words;296.0
72233;72233;40145561.0;5.0;"<p>I have done some searching for the answer to this question, but all I can figure out is this: </p>

<pre><code>df[df.columns[len(df.columns)-1]]
</code></pre>

<p>which to me seems unweildy, and un-pythonic (and slow?). </p>

<p>What is the easiest way to select the data for the last column in a pandas dataframe without specifying the name of the column?</p>
";;0;;2016-10-20T03:11:10.737;2.0;40144769;2017-06-24T08:36:28.593;2016-10-24T16:03:21.353;;4325994.0;;4325994.0;;1;11;<python><pandas>;How to select the last column of dataframe;6781.0
72645;72645;40314236.0;6.0;"<p>consider the <code>df</code></p>

<pre><code>tidx = pd.date_range('2012-12-31', periods=11, freq='D')
df = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)
df
</code></pre>

<p>I want to calculate the sum over a trailing 5 days, every 3 days.</p>

<p>I expect something that looks like this</p>

<p><a href=""https://i.stack.imgur.com/KpXAI.png""><img src=""https://i.stack.imgur.com/KpXAI.png"" alt=""enter image description here""></a></p>

<p><strong><em>this was edited</em></strong><br>
what I had was incorrect.  @ivan_pozdeev and @boud noticed this was a centered window and that was not my intention.  Appologies for the confusion.<br>
everyone's solutions capture much of what I was after.</p>

<hr>

<p><strong><em>criteria</em></strong>  </p>

<ul>
<li><p>I'm looking for smart efficient solutions that can be scaled to large data sets.</p></li>
<li><p>I'll be timing solutions and also considering elegance.</p></li>
<li><p>Solutions should also be generalizable for a variety of sample and look back frequencies.</p></li>
</ul>

<hr>

<p><strong><em>from comments</em></strong>  </p>

<ul>
<li>I want a solution that generalizes to handle a look back of a specified frequency and grab anything that falls within that look back.

<ul>
<li>for the sample above, the look back is <code>5D</code> and there may be 4 or 50 observations that fall within that look back.</li>
</ul></li>
<li>I want the timestamp to be the last observed timestamp within the look back period.</li>
</ul>
";;11;;2016-10-24T01:21:06.510;;40209520;2016-11-02T01:39:09.983;2016-10-30T14:54:55.623;;2336654.0;;2336654.0;;1;11;<python><pandas><numpy>;daily data, resample every 3 days, calculate over trailing 5 days efficiently;667.0
72754;72754;40225796.0;2.0;"<p>let say I have a dataframe that looks like this:</p>

<pre><code>df = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})
 df
Out[92]: 
   A  B
a  0  0
b  1  1
c  2  2
d  3  3
e  4  4
</code></pre>

<p>Asumming that this dataframe already exist, how can I simply add a level 'C' to the column index so I get this:</p>

<pre><code> df
Out[92]: 
   A  B
   C  C
a  0  0
b  1  1
c  2  2
d  3  3
e  4  4
</code></pre>

<p>I saw SO anwser like this <a href=""https://stackoverflow.com/questions/15989281/python-pandas-how-to-combine-two-dataframes-into-one-with-hierarchical-column-i"">python/pandas: how to combine two dataframes into one with hierarchical column index?</a> but this concat different dataframe instead of adding a column level to an already existing dataframe.</p>

<p>-</p>
";;0;;2016-10-24T19:03:52.743;;40225683;2016-11-09T00:44:56.920;2017-05-23T12:02:50.750;;-1.0;;5626112.0;;1;12;<python><pandas><dataframe><multi-level>;How to simply add a column level to a pandas dataframe;1947.0
79243;79243;41191127.0;2.0;"<h2>Example Problem</h2>

<p>As a simple example, consider the numpy array <code>arr</code> as defined below:</p>

<pre><code>import numpy as np
arr = np.array([[5, np.nan, np.nan, 7, 2],
                [3, np.nan, 1, 8, np.nan],
                [4, 9, 6, np.nan, np.nan]])
</code></pre>

<p>where <code>arr</code> looks like this in console output:</p>

<pre><code>array([[  5.,  nan,  nan,   7.,   2.],
       [  3.,  nan,   1.,   8.,  nan],
       [  4.,   9.,   6.,  nan,  nan]])
</code></pre>

<p>I would now like to row-wise 'forward-fill' the <code>nan</code> values in array <code>arr</code>. By that I mean replacing each <code>nan</code> value with the nearest valid value from the left. The desired result would look like this:</p>

<pre><code>array([[  5.,   5.,   5.,  7.,  2.],
       [  3.,   3.,   1.,  8.,  8.],
       [  4.,   9.,   6.,  6.,  6.]])
</code></pre>

<hr>

<h2>Tried thus far</h2>

<p>I've tried using for-loops:</p>

<pre><code>for row_idx in range(arr.shape[0]):
    for col_idx in range(arr.shape[1]):
        if np.isnan(arr[row_idx][col_idx]):
            arr[row_idx][col_idx] = arr[row_idx][col_idx - 1]
</code></pre>

<p>I've also tried using a pandas dataframe as an intermediate step (since pandas dataframes have a very neat built-in method for forward-filling):</p>

<pre><code>import pandas as pd
df = pd.DataFrame(arr)
df.fillna(method='ffill', axis=1, inplace=True)
arr = df.as_matrix()
</code></pre>

<p>Both of the above strategies produce the desired result, but I keep on wondering: wouldn't a strategy that uses only numpy vectorized operations be the most efficient one?</p>

<hr>

<h2>Summary</h2>

<p>Is there another more efficient way to 'forward-fill' <code>nan</code> values in numpy arrays? (e.g. by using numpy vectorized operations)</p>

<hr>

<h1>Update: Solutions Comparison</h1>

<p>I've tried to time all solutions thus far. This was my setup script:</p>

<pre><code>import numba as nb
import numpy as np
import pandas as pd

def random_array():
    choices = [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]
    out = np.random.choice(choices, size=(1000, 10))
    return out

def loops_fill(arr):
    out = arr.copy()
    for row_idx in range(out.shape[0]):
        for col_idx in range(1, out.shape[1]):
            if np.isnan(out[row_idx, col_idx]):
                out[row_idx, col_idx] = out[row_idx, col_idx - 1]
    return out

@nb.jit
def numba_loops_fill(arr):
    '''Numba decorator solution provided by shx2.'''
    out = arr.copy()
    for row_idx in range(out.shape[0]):
        for col_idx in range(1, out.shape[1]):
            if np.isnan(out[row_idx, col_idx]):
                out[row_idx, col_idx] = out[row_idx, col_idx - 1]
    return out

def pandas_fill(arr):
    df = pd.DataFrame(arr)
    df.fillna(method='ffill', axis=1, inplace=True)
    out = df.as_matrix()
    return out

def numpy_fill(arr):
    '''Solution provided by Divakar.'''
    mask = np.isnan(arr)
    idx = np.where(~mask,np.arange(mask.shape[1]),0)
    np.maximum.accumulate(idx,axis=1, out=idx)
    out = arr[np.arange(idx.shape[0])[:,None], idx]
    return out
</code></pre>

<p>followed by this console input:</p>

<pre><code>%timeit -n 1000 loops_fill(random_array())
%timeit -n 1000 numba_loops_fill(random_array())
%timeit -n 1000 pandas_fill(random_array())
%timeit -n 1000 numpy_fill(random_array())
</code></pre>

<p>resulting in this console output:</p>

<pre><code>1000 loops, best of 3: 9.64 ms per loop
1000 loops, best of 3: 377 s per loop
1000 loops, best of 3: 455 s per loop
1000 loops, best of 3: 351 s per loop
</code></pre>
";;9;;2016-12-16T19:02:18.937;1.0;41190852;2016-12-16T22:35:34.723;2016-12-16T22:35:34.723;;7306999.0;;7306999.0;;1;12;<python><arrays><performance><pandas><numpy>;Most efficient way to forward-fill NaN values in numpy array;1088.0
79336;79336;41205215.0;3.0;"<p>Thank you for your time.</p>

<p>I am writing some code that is checking for correlation between multiple sets of data. It works great when I am using the original data (which I am honestly unsure of which format it is in at that point), but after I run the data through some equations using the Decimal module, the data set will not show up when tested for correlation.</p>

<p>I feel really stupid and new lol, I am sure it's a very easy fix.</p>

<p>Here is a small program I wrote to demonstrate what I mean.</p>

<pre><code>from decimal import Decimal
import numpy as np
import pandas as pd

a = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]
b = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]

h = [2.3,1.5,5.7,4.6,5.5,1.5]
j = [2.1,1.2,5.3,4.4,5.3,1.7]

corr_data1 = pd.DataFrame({'A': a, 'B': b}) 

corr_data2 = corr_data1.corr()
print(corr_data2)

corr_data3 = pd.DataFrame({'H': h, 'J': j})

corr_data4 = corr_data3.corr()
print(corr_data4)
</code></pre>

<p>The data for both lists A &amp; B as well as H &amp; F are exactly the same, with the only difference of A &amp; B being decimal formated numbers, where as H &amp; F are not.</p>

<p>When the program is run, A &amp; B returns:</p>

<pre><code>Empty DataFrame
Columns: []
Index: []
</code></pre>

<p>and H &amp; J returns:</p>

<pre><code>          H         J
H  1.000000  0.995657
J  0.995657  1.000000
</code></pre>

<p>How do I make it so I can utilize the data after I've ran it through my equations?</p>

<p>Sorry for the stupid question and thank you for your time. I hope you are all well, happy holidays!</p>
";;3;;2016-12-18T02:43:37.060;;41205001;2016-12-18T17:48:54.130;2016-12-18T17:48:54.130;;240490.0;;7311804.0;;1;11;<python><python-3.x><pandas><numpy><decimal>;How do I check for correlation using Decimal numbers/data with python 3;146.0
81194;81194;41464194.0;1.0;"<p>Consider the sorted array <code>a</code>:</p>

<pre><code>a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])
</code></pre>

<p>If I specified left and right deltas,</p>

<pre><code>delta_left, delta_right = 1, 1
</code></pre>

<p>Then this is how I'd expect the clusters to be assigned:</p>

<pre><code>#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20
#                                         11                         20
#
#                                     [10--|-12]                 [19--|-21]
#           [1--|--3]                 [10--|-12]                 [19--|-21]
#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]
#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|
#              [2--|--4]                       [13--|-15]
#
#         ?    ????????                 ????        ?              ????
#         ?   cluster 2                Cluster 3    ?           Cluster 5
#     Cluster 1                                 Cluster 4
</code></pre>

<p><strong><em>NOTE:</em></strong> Despite the interval <code>[-1, 1]</code> sharing an edge with <code>[1, 3]</code>, neither interval includes an adjacent point and therefore do not constitute joining their respective clusters.</p>

<p>Assuming the cluster assignments were stored in an array named <code>clusters</code>, I'd expect the results to look like this</p>

<pre><code>print(clusters)
[1 2 2 2 2 3 3 3 4 5 5 5]
</code></pre>

<hr>

<p>However, suppose I change the left and right deltas to be different:</p>

<pre><code>delta_left, delta_right = 2, 1
</code></pre>

<p>This means that for a value of <code>x</code> it should be combined with any other point in the interval <code>[x - 2, x + 1]</code></p>

<pre><code>#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20
#                                         11                         20
#
#                                   [9-----|-12]              [18-----|-21]
#        [0-----|--3]               [9-----|-12]              [18-----|-21]
# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]
#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|
#           [1 ----|--4]                    [12-----|-15]
#
#         ?????????????                 ????        ?              ????
#           cluster 1                Cluster 2      ?           Cluster 4
#                                               Cluster 3
</code></pre>

<p><strong><em>NOTE:</em></strong> Despite the interval <code>[9, 12]</code> sharing an edge with <code>[12, 15]</code>, neither interval includes an adjacent point and therefore do not constitute joining their respective clusters.</p>

<p>Assuming the cluster assignments were stored in an array named <code>clusters</code>, I'd expect the results to look like this:</p>

<pre><code>print(clusters)
[1 1 1 1 1 2 2 2 3 4 4 4]
</code></pre>
";;0;;2017-01-04T12:44:56.837;4.0;41464177;2017-01-05T01:37:07.187;2017-01-04T20:28:58.487;;57611.0;;2336654.0;;1;13;<python><pandas><numpy>;Identify clusters linked by delta to the left and different delta to the right;263.0
84205;84205;41890871.0;1.0;"<p>You can calculate skew and kurtosis with the the methods</p>

<ul>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.skew.html""><code>pd.Series.skew</code></a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.kurt.html""><code>pd.Series.kurt</code></a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.skew.html""><code>pd.DataFrame.skew</code></a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.kurt.html""><code>pd.DataFrame.kurt</code></a></li>
</ul>

<p>However, there is no convenient way to calculate the coskew or cokurtosis between variables.  Or even better, the coskew or cokurtosis matrix.</p>

<hr>

<p>Consider the <code>pd.DataFrame</code> <code>df</code></p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed([3,1415])
df = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))

df

          a         b
0  0.444939  0.407554
1  0.460148  0.465239
2  0.462691  0.016545
3  0.850445  0.817744
4  0.777962  0.757983
5  0.934829  0.831104
6  0.879891  0.926879
7  0.721535  0.117642
8  0.145906  0.199844
9  0.437564  0.100702
</code></pre>

<h3>How do I calculate the coskew and cokurtosis of <code>a</code> and <code>b</code>?</h3>
";;0;;2017-01-27T09:41:18.470;2.0;41890870;2017-01-27T09:41:18.470;;;;;2336654.0;;1;12;<python><pandas><numpy>;how to calculate coskew and cokurtosis;312.0
84508;84508;41929958.0;3.0;"<p>I have a dataframe that looks like this:</p>

<pre><code>from    to         datetime              other
-------------------------------------------------
11      1     2016-11-06 22:00:00          -
11      1     2016-11-06 20:00:00          -
11      1     2016-11-06 15:45:00          -
11      12    2016-11-06 15:00:00          -
11      1     2016-11-06 12:00:00          -
11      18    2016-11-05 10:00:00          -
11      12    2016-11-05 10:00:00          -
12      1     2016-10-05 10:00:59          -
12      3     2016-09-06 10:00:34          -
</code></pre>

<p>I want to groupby ""from"" and then ""to"" columns and then sort the ""datetime"" in descending order and then finally want to calculate the time difference within these grouped by objects between the current time and the next time. For eg, in this case,
I would like to have a dataframe like the following:</p>

<pre><code>from    to     timediff in minutes                                          others
11      1            120
11      1            255
11      1            225
11      1            0 (preferrably subtract this date from the epoch)
11      12           300
11      12           0
11      18           0
12      1            25
12      3            0
</code></pre>

<p>I can't get my head around figuring this out!! Is there a way out for this?
Any help will be much much appreciated!!
Thank you so much in advance!</p>
";;1;;2017-01-30T05:54:40.033;1.0;41929772;2017-05-06T14:15:55.200;2017-05-06T14:15:55.200;;2901002.0;;6383910.0;;1;13;<python><pandas><group-by><difference><data-science>;Time difference within group by objects in Python Pandas;820.0
85516;85516;;5.0;"<p>For every pair of <code>src</code> and <code>dest</code> airport cities I want to return a percentile of column <code>a</code> given a value of column <code>b</code>. </p>

<p>I can do this manually as such:</p>

<p>example df with only 2 pairs of src/dest (I have thousands in my actual df):</p>

<pre><code>dt  src dest    a   b
0   2016-01-01  YYZ SFO 548.12  279.28
1   2016-01-01  DFW PDX 111.35  -65.50
2   2016-02-01  YYZ SFO 64.84   342.35
3   2016-02-01  DFW PDX 63.81   61.64
4   2016-03-01  YYZ SFO 614.29  262.83

{'a': {0: 548.12,
  1: 111.34999999999999,
  2: 64.840000000000003,
  3: 63.810000000000002,
  4: 614.28999999999996,
  5: -207.49000000000001,
  6: 151.31999999999999,
  7: -56.43,
  8: 611.37,
  9: -296.62,
  10: 6417.5699999999997,
  11: -376.25999999999999,
  12: 465.12,
  13: -821.73000000000002,
  14: 1270.6700000000001,
  15: -1410.0899999999999,
  16: 1312.6600000000001,
  17: -326.25999999999999,
  18: 1683.3699999999999,
  19: -24.440000000000001,
  20: 583.60000000000002,
  21: -5.2400000000000002,
  22: 1122.74,
  23: 195.21000000000001,
  24: 97.040000000000006,
  25: 133.94},
 'b': {0: 279.27999999999997,
  1: -65.5,
  2: 342.35000000000002,
  3: 61.640000000000001,
  4: 262.82999999999998,
  5: 115.89,
  6: 268.63999999999999,
  7: 2.3500000000000001,
  8: 91.849999999999994,
  9: 62.119999999999997,
  10: 778.33000000000004,
  11: -142.78,
  12: 1675.53,
  13: -214.36000000000001,
  14: 983.80999999999995,
  15: -207.62,
  16: 632.13999999999999,
  17: -132.53,
  18: 422.36000000000001,
  19: 13.470000000000001,
  20: 642.73000000000002,
  21: -144.59999999999999,
  22: 213.15000000000001,
  23: -50.200000000000003,
  24: 338.27999999999997,
  25: -129.69},
 'dest': {0: 'SFO',
  1: 'PDX',
  2: 'SFO',
  3: 'PDX',
  4: 'SFO',
  5: 'PDX',
  6: 'SFO',
  7: 'PDX',
  8: 'SFO',
  9: 'PDX',
  10: 'SFO',
  11: 'PDX',
  12: 'SFO',
  13: 'PDX',
  14: 'SFO',
  15: 'PDX',
  16: 'SFO',
  17: 'PDX',
  18: 'SFO',
  19: 'PDX',
  20: 'SFO',
  21: 'PDX',
  22: 'SFO',
  23: 'PDX',
  24: 'SFO',
  25: 'PDX'},
 'dt': {0: Timestamp('2016-01-01 00:00:00'),
  1: Timestamp('2016-01-01 00:00:00'),
  2: Timestamp('2016-02-01 00:00:00'),
  3: Timestamp('2016-02-01 00:00:00'),
  4: Timestamp('2016-03-01 00:00:00'),
  5: Timestamp('2016-03-01 00:00:00'),
  6: Timestamp('2016-04-01 00:00:00'),
  7: Timestamp('2016-04-01 00:00:00'),
  8: Timestamp('2016-05-01 00:00:00'),
  9: Timestamp('2016-05-01 00:00:00'),
  10: Timestamp('2016-06-01 00:00:00'),
  11: Timestamp('2016-06-01 00:00:00'),
  12: Timestamp('2016-07-01 00:00:00'),
  13: Timestamp('2016-07-01 00:00:00'),
  14: Timestamp('2016-08-01 00:00:00'),
  15: Timestamp('2016-08-01 00:00:00'),
  16: Timestamp('2016-09-01 00:00:00'),
  17: Timestamp('2016-09-01 00:00:00'),
  18: Timestamp('2016-10-01 00:00:00'),
  19: Timestamp('2016-10-01 00:00:00'),
  20: Timestamp('2016-11-01 00:00:00'),
  21: Timestamp('2016-11-01 00:00:00'),
  22: Timestamp('2016-12-01 00:00:00'),
  23: Timestamp('2016-12-01 00:00:00'),
  24: Timestamp('2017-01-01 00:00:00'),
  25: Timestamp('2017-01-01 00:00:00')},
 'src': {0: 'YYZ',
  1: 'DFW',
  2: 'YYZ',
  3: 'DFW',
  4: 'YYZ',
  5: 'DFW',
  6: 'YYZ',
  7: 'DFW',
  8: 'YYZ',
  9: 'DFW',
  10: 'YYZ',
  11: 'DFW',
  12: 'YYZ',
  13: 'DFW',
  14: 'YYZ',
  15: 'DFW',
  16: 'YYZ',
  17: 'DFW',
  18: 'YYZ',
  19: 'DFW',
  20: 'YYZ',
  21: 'DFW',
  22: 'YYZ',
  23: 'DFW',
  24: 'YYZ',
  25: 'DFW'}}
</code></pre>

<p>I want the percentile per group of <code>src</code> and <code>dest</code> pairs. So there should only be 1 percentile value for each pair. I only want to perform the percentile given <code>b</code> where <code>date = 2017-01-01</code> for each <code>src</code> and <code>dest</code> pair over the entire column <code>a</code> for each pair. Make sense?</p>

<p>I can do this manually for example for a specific pair <code>i.e. src=YYZ and dest=SFT</code>:</p>

<pre><code>from scipy import stats
import datetime as dt
import pandas as pd

p0 = dt.datetime(2017,1,1)

# lets slice df for src=YYZ and dest = SFO
x = df[(df.src =='YYZ') &amp;
(df.dest =='SFO') &amp;
(df.dt ==p0)].b.values[0]

# given B, what percentile does it fall in for the entire column A for YYZ, SFO
stats.percentileofscore(df['a'],x)
61.53846153846154
</code></pre>

<p>In the above case, I did this manually for pairs YYZ and SFO. However, I have thousands of pairs in my df.  </p>

<p>How do I <code>vectorize</code> this using <code>pandas features</code> rather than looping through every pair? </p>

<p>There must be a way to use <code>groupby</code> and use <code>apply</code> over a function? </p>

<p>My desired df should look something like:</p>

<pre><code>    src dest  percentile
0   YYZ SFO   61.54
1   DFW PDX   23.07
2   XXX YYY   blahblah1
3   AAA BBB   blahblah2
...
</code></pre>

<p><strong>UPDATE:</strong></p>

<p>I implemented the following:</p>

<pre><code>def b_percentile_a(df,x,y,b):
    z = df[(df['src'] == x ) &amp; (df['dest'] == y)].a
    r = stats.percentileofscore(z,b)
    return r

b_vector_df = df[df.dt == p0]

b_vector_df['p0_a_percentile_b'] = \
    b_vector_df.apply(lambda x: b_percentile_a(df,x.src,x.dest,x.b), axis=1)
</code></pre>

<p>It takes <code>5.16</code> seconds for <code>100</code> pairs. I have <code>55,000</code> pairs. So this will take <code>~50</code> minutes. I need to run this <code>36</code> times so its going to take <code>several days</code> of run time. </p>

<p>There must be a faster approach? </p>
";;8;;2017-02-06T19:52:14.997;;42076126;2017-02-26T12:21:32.513;2017-02-08T00:31:03.670;;668624.0;;668624.0;;1;12;<python><pandas><scipy><percentile>;vectorize percentile value of column B of column A (for groups);374.0
86106;86106;;9.0;"<p>I have a number of text files, say 50, that I need to read into a massive dataframe. At the moment, I am using the following steps.</p>

<ol>
<li>Read every file and check what the labels are. The information I need is often contained in the first few lines. The same labels just repeat for the rest of the file, with different types of data listed against them each time.</li>
<li>Create a dataframe with those labels.</li>
<li>Read the file again and fill the dataframe with values.</li>
<li>Concatenate that dataframe with a master dataframe.</li>
</ol>

<p>This works pretty well for files that are of the 100 KB size - a few minutes, but at 50 MB, it just takes hours, and is not practical. </p>

<p>How can I optimise my code? In particular -</p>

<ol>
<li>How can I identify what functions are taking the most time, which I need to optimise? Is it the reading of the file? Is it the writing to the dataframe? Where is my program spending time?</li>
<li>Should I consider multithreading or multiprocessing?</li>
<li>Can I improve the algorithm? 

<ul>
<li>Perhaps read the entire file in in one go into a list, rather than line by line,</li>
<li>Parse data in chunks/entire file, rather than line by line,</li>
<li>Assign data to the dataframe in chunks/one go, rather than row by row.</li>
</ul></li>
<li>Is there anything else that I can do to make my code execute faster?</li>
</ol>

<p>Here is an example code. My own code is a little more complex, as the text files are more complex such that I have to use about 10 regular expressions and multiple while loops to read the data in and allocate it to the right location in the right array. To keep the MWE simple, I haven't used repeating labels in the input files for the MWE either, so it would like I'm reading the file twice for no reason. I hope that makes sense!</p>

<pre><code>import re
import pandas as pd

df = pd.DataFrame()
paths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]
reg_ex = re.compile('^(.+) (.+)\n')
# read all files to determine what indices are available
for path in paths:
    file_obj = open(path, 'r')
    print file_obj.readlines()

['a 1\n', 'b 2\n', 'end']
['c 3\n', 'd 4\n', 'end']

indices = []
for path in paths:
    index = []
    with open(path, 'r') as file_obj:
        line = True
        while line:
            try:
                line = file_obj.readline()
                match = reg_ex.match(line)
                index += match.group(1)
            except AttributeError:
                pass
    indices.append(index)
# read files again and put data into a master dataframe
for path, index in zip(paths, indices):
    subset_df = pd.DataFrame(index=index, columns=[""Number""])
    with open(path, 'r') as file_obj:
        line = True
        while line:
            try:
                line = file_obj.readline()
                match = reg_ex.match(line)
                subset_df.loc[[match.group(1)]] = match.group(2)
            except AttributeError:
                pass
    df = pd.concat([df, subset_df]).sort_index()
print df

  Number
a      1
b      2
c      3
d      4
</code></pre>

<p><strong>My input files:</strong></p>

<p>test1.txt</p>

<pre><code>a 1
b 2
end
</code></pre>

<p>test2.txt</p>

<pre><code>c 3
d 4
end
</code></pre>
";;3;;2017-02-10T11:10:20.270;10.0;42157944;2017-04-02T08:12:53.363;2017-03-30T18:48:52.797;;3521109.0;;3521109.0;;1;34;<python><regex><performance><parsing><pandas>;How can I speed up reading multiple files and putting the data into a dataframe?;1476.0
87428;87428;42426751.0;1.0;"<p>I know there are several ways to convert a column to a date object, but what I am looking for is a way to do so while simultaneously formatting other columns. Say I have the following data frame:</p>

<pre><code>import pandas as pd

url = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""
df = pd.read_csv(url)
df[""date""] = list(range(42005, 42005+len(df)))
</code></pre>

<p>What I'm trying to achieve is the ability to print these data using some formatting, so I might do something like the following:</p>

<pre><code>print(
  df
  .head(10)
  .to_string(
    formatters={""total_bill"": ""${:,.2f}"".format, 
                ""tip"": ""${:,.2f}"".format
    }
  )
)
</code></pre>

<p>But I also want to format the date in this step as well. I tried looking through <a href=""https://pyformat.info/#simple"" rel=""noreferrer"">here</a> for what I was looking for, but the datetime options didn't seem like they would work in what I'm trying to do, and building a custom option is a bit outside scope for my target audience.</p>

<p>Is it possible to do this in a simple manner?</p>
";;6;;2017-02-20T15:02:57.237;2.0;42347868;2017-03-02T19:35:28.057;2017-03-02T13:16:59.853;;2789863.0;;2789863.0;;1;24;<python>;Convert to date using formatters parameter in pandas to_string;839.0
88646;88646;43054028.0;4.0;"<p>So I would like make a slice of a dataframe and then set the value of the first item in that slice without copying the dataframe. For example:</p>

<pre><code>df = pandas.DataFrame(numpy.random.rand(3,1))
df[df[0]&gt;0][0] = 0
</code></pre>

<p>The slice here is irrelevant and just for the example and will return the whole data frame again. Point being, by doing it like it is in the example you get a setting with copy warning (understandably). I have also tried slicing first and then using ILOC/IX/LOC and using ILOC twice, i.e. something like:</p>

<pre><code>df.iloc[df[0]&gt;0,:][0] = 0
df[df[0]&gt;0,:].iloc[0] = 0
</code></pre>

<p>And neither of these work. Again- I don't want to make a copy of the dataframe even if it id just the sliced version.</p>

<p>EDIT:
It seems there are two ways, using a mask or IdxMax. The IdxMax method seems to work if your index is unique, and the mask method if not. In my case, the index is not unique which I forgot to mention in the initial post.</p>
";;0;;2017-02-28T18:21:36.057;2.0;42516070;2017-05-06T14:14:27.253;2017-05-06T14:14:27.253;;2901002.0;;2356861.0;;1;14;<python><pandas><slice><mask><argmax>;Set value of first item in slice in python pandas;672.0
96006;96006;43559496.0;4.0;"<p>There are countless questions about the dreaded <code>SettingWithCopyWarning</code></p>

<p>I've got a good handle on how it comes about. (Notice I said good, not great)</p>

<p>It happens when a dataframe <code>df</code> is ""attached"" to another dataframe via an attribute stored in <code>is_copy</code>.</p>

<p>Here's an example</p>

<pre><code>df = pd.DataFrame([[1]])

d1 = df[:]

d1.is_copy

&lt;weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0&gt;
</code></pre>

<p>We can either set that attribute to <code>None</code> or</p>

<pre><code>d1 = d1.copy()
</code></pre>

<p>I've seen devs like @Jeff and I can't remember who else, warn about doing that.  Citing that the <code>SettingWithCopyWarning</code> has a purpose.</p>

<p><strong><em>Question</em></strong><br>
Ok, so what is a concrete example that demonstrates why ignoring the warning by assigning a <code>copy</code> back to the original is a bad idea.</p>

<p>I'll define <em>""bad idea""</em> for clarification.</p>

<p><strong><em>Bad Idea</em></strong><br>
It is a <strong><em>bad idea</em></strong> to place code into production that will lead to getting a phone call in the middle of a Saturday night saying your code is broken and needs to be fixed.</p>

<p><strong>Now</strong> how can using <code>df = df.copy()</code> in order to bypass the <code>SettingWithCopyWarning</code> lead to getting that kind of phone call.  I want it spelled out because this is a source of confusion and I'm attempting to find clarity.  I want to see the edge case that blows up!</p>
";;9;;2017-04-15T07:14:37.893;6.0;43423347;2017-04-28T22:27:30.803;2017-04-21T13:57:36.810;;2336654.0;;2336654.0;;1;23;<python><pandas>;why is blindly using df.copy() a bad idea to fix the SettingWithCopyWarning;697.0
97036;97036;43547088.0;2.0;"<p>The following code only shows the main category ['one', 'two', 'three', 'four', 'five', 'six'] as the x axis labels. Is there a way show subcategory ['A', 'B', 'C', 'D'] as secondary x axis labels?
<a href=""https://i.stack.imgur.com/du1my.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/du1my.png"" alt=""enter image description here""></a></p>

<pre><code>df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'], 
                 name='Genus')).round(2)


df.plot(kind='bar',figsize=(10,4))
</code></pre>
";;1;;2017-04-21T14:56:22.830;2.0;43545879;2017-04-21T16:05:07.243;;;;;4642234.0;;1;13;<python><pandas><matplotlib><plot>;Bar Chart with multiple labels;506.0
97324;97324;43578600.0;4.0;"<p>I would like to find out what is the most efficient way to achieve the following in Python:</p>

<p>Suppose we have two lists <code>a</code> and <code>b</code> which are of equal length and contain up to 1e7 elements.
However, for the ease of illustration we may consider the following:</p>

<pre><code>a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]
b = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
</code></pre>

<p>The goal is to create a strictly monotonic list <code>a_new</code> from <code>a</code> whereas only the first sample point of sample points with identical values is used.
The same indices that have to be deleted in <code>a</code> should also be deleted in <code>b</code> such that the final result will be:</p>

<pre><code>a_new = [2,3,4,5,6,7,8,9,10,11]
b_new = [1,4,5,6,8,10,11,12,14,15]
</code></pre>

<p>Of course this can be done using computationally expensive <code>for</code> loops which is however not suitable due to the huge amount of data.</p>

<p>Any suggestions are very appreciated.</p>
";;1;;2017-04-23T23:47:39.390;5.0;43577744;2017-04-24T07:30:30.023;;;;;1971254.0;;1;16;<python><python-2.7><pandas><numpy><scipy>;Fastest way to create strictly increasing lists in Python;1592.0
99152;99152;43932573.0;5.0;"<p>Edited:</p>

<p>I have a financial portfolio in a pandas dataframe df, where the index is the date and I have multiple financial stocks per date.</p>

<p>Eg dataframe:</p>

<pre><code>Date    Stock   Weight  Percentile  Final weight
1/1/2000    Apple   0.010   0.75    0.010
1/1/2000    IBM    0.011    0.4     0
1/1/2000    Google  0.012   0.45    0
1/1/2000    Nokia   0.022   0.81    0.022
2/1/2000    Apple   0.014   0.56    0
2/1/2000    Google  0.015   0.45    0
2/1/2000    Nokia   0.016   0.55    0
3/1/2000    Apple   0.020   0.52    0
3/1/2000    Google  0.030   0.51    0
3/1/2000    Nokia   0.040   0.47    0
</code></pre>

<p>I created <code>Final_weight</code> by doing assigning values of <code>Weight</code> whenever <code>Percentile</code> is greater than <code>0.7</code></p>

<p>Now I want this to be a bit more sophisticated, I still want <code>Weight</code> to be assigned to <code>Final_weight</code> when <code>Percentile is &gt; 0.7</code>, however after this date (at any point in the future), rather than become 0 when a stocks <code>Percentile</code> is not <code>&gt;0.7</code>, we would still get a weight as long as the Stocks <code>Percentile</code> is above <code>0.5</code> (ie holding the position for longer than just one day).</p>

<p>Then if the stock goes below <code>0.5</code> (in the near future) then <code>Final_weight would become 0</code>.</p>

<p>Eg modified dataframe from above:</p>

<pre><code>Date    Stock   Weight  Percentile  Final weight
1/1/2000    Apple   0.010   0.75    0.010
1/1/2000    IBM     0.011   0.4     0
1/1/2000    Google  0.012   0.45    0
1/1/2000    Nokia   0.022   0.81    0.022
2/1/2000    Apple   0.014   0.56    0.014
2/1/2000    Google  0.015   0.45    0
2/1/2000    Nokia   0.016   0.55    0.016
3/1/2000    Apple   0.020   0.52    0.020
3/1/2000    Google  0.030   0.51    0
3/1/2000    Nokia   0.040   0.47    0
</code></pre>

<p>Everyday the portfolios are different not always have the same stock from the day before.</p>
";;2;;2017-05-04T20:08:52.423;3.0;43791970;2017-05-12T16:32:45.280;2017-05-11T11:54:25.377;;7390912.0;;7390912.0;;1;11;<python><pandas><dataframe><finance><portfolio>;Pandas: assigning columns with multiple conditions and date thresholds;821.0
101241;101241;44192797.0;2.0;"<p>I have a ranking function that I apply to a large number of columns of several million rows which takes minutes to run.  By removing all of the logic preparing the data for application of the <code>.rank(</code> method, i.e., by doing this: </p>

<pre><code>ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        
</code></pre>

<p>I managed to get this down to seconds.  However, I need to retain my logic, and am struggling to restructure my code: ultimately, the largest bottleneck is my double use of lambda x:, but clearly other aspects are slowing things down (see below).  I have provided a sample data frame, together with my ranking functions below, i.e. an MCVE.  Broadly, I think that my questions boil down to:</p>

<p>(i)  How can one replace the <code>.apply(lambda x</code> usage in the code with a fast, vectorized equivalent?  (ii)  How can one loop over multi-indexed, grouped, data frames and apply a function? in my case, to each unique combination of the date_id and category columns.<br>
(iii)  What else can I do to speed up my ranking logic? the main overhead seems to be in <code>.value_counts()</code>.  This overlaps with (i) above; perhaps one can do most of this logic on df, perhaps via construction of temporary columns, before sending for ranking.  Similarly, can one rank the sub-dataframe in one call?<br>
(iv)  Why use <code>pd.qcut()</code> rather than <code>df.rank()</code>? the latter is cythonized and seems to have more flexible handling of ties, but I cannot see a comparison between the two, and <code>pd.qcut()</code> seems most widely used.</p>

<p>Sample input data is as follows:</p>

<pre><code>import pandas as pd
import numpy as np
import random

to_rank = ['var_1', 'var_2', 'var_3']
df = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})
df['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])
df['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')
</code></pre>

<p>The two ranking functions are:</p>

<pre><code>def rank_fun(df, to_rank): # calls ranking function f(x) to rank each category at each date
    #extra data tidying logic here beyond scope of question - can remove
    ranked = df[to_rank].apply(lambda x: f(x))
    return ranked


def f(x):
    nans = x[np.isnan(x)] # Remove nans as these will be ranked with 50
    sub_df = x.dropna() # 
    nans_ranked = nans.replace(np.nan, 50) # give nans rank of 50

    if len(sub_df.index) == 0: #check not all nan.  If no non-nan data, then return with rank 50
        return nans_ranked

    if len(sub_df.unique()) == 1: # if all data has same value, return rank 50
        sub_df[:] = 50
        return sub_df

    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.
    max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max
    max_bins = len(sub_df) / max_cluster 

    if max_bins &gt; 100: #if largest cluster &lt;1% of available data, then we can percentile_rank
        max_bins = 100

    if max_bins &lt; 5: #if we don't have the resolution to quintile rank then assume no data.
        sub_df[:] = 50
        return sub_df

    bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)

    sub_df_ranked = pd.qcut(sub_df, bins, labels=False) #currently using pd.qcut.  pd.rank( seems to have extra functionality, but overheads similar in practice
    sub_df_ranked *= (100 / bins) #Since we bin using the resolution specified in bins, to convert back to decile rank, we have to multiply by 100/bins.  E.g. with quintiles, we'll have scores 1 - 5, so have to multiply by 100 / 5 = 20 to convert to percentile ranking
    ranked_df = pd.concat([sub_df_ranked, nans_ranked])
    return ranked_df
</code></pre>

<p>And the code to call my ranking function and recombine with df is:</p>

<pre><code># ensure don't get duplicate columns if ranking already executed
ranked_cols = [col + '_ranked' for col in to_rank]

ranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) 
ranked.columns = ranked_cols        
ranked.reset_index(inplace = True)
ranked.set_index('level_1', inplace = True)    
df = df.join(ranked[ranked_cols])
</code></pre>

<p>I am trying to get this ranking logic as fast as I can, by removing both lambda x calls; I can remove the logic in rank_fun so that only f(x)'s logic is applicable, but I also don't know how to process multi-index dataframes in a vectorized fashion.  An additional question would be on differences between <code>pd.qcut(</code> and <code>df.rank(</code>: it seems that both have different ways of dealing with ties, but the overheads seem similar, despite the fact that .rank( is cythonized; perhaps this is misleading, given the main overheads are due to my usage of lambda x.</p>

<p>I ran <code>%lprun</code> on <code>f(x)</code> which gave me the following results, although the main overhead is the use of <code>.apply(lambda x</code> rather than a vectorized approach:</p>

<h1>Line #      Hits         Time  Per Hit   % Time  Line Contents</h1>

<pre><code> 2                                           def tst_fun(df, field):
 3         1          685    685.0      0.2      x = df[field]
 4         1        20726  20726.0      5.8      nans = x[np.isnan(x)]
 5         1        28448  28448.0      8.0      sub_df = x.dropna()
 6         1          387    387.0      0.1      nans_ranked = nans.replace(np.nan, 50)
 7         1            5      5.0      0.0      if len(sub_df.index) == 0: 
 8                                                   pass #check not empty.  May be empty due to nans for first 5 years e.g. no revenue/operating margin data pre 1990
 9                                                   return nans_ranked
10                                           
11         1        65559  65559.0     18.4      if len(sub_df.unique()) == 1: 
12                                                   sub_df[:] = 50 #e.g. for subranks where all factors had nan so ranked as 50 e.g. in 1990
13                                                   return sub_df
14                                           
15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.
16         1        74610  74610.0     20.9      max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max
17                                               # print(counts)
18         1            9      9.0      0.0      max_bins = len(sub_df) / max_cluster #
19                                           
20         1            3      3.0      0.0      if max_bins &gt; 100: 
21         1            0      0.0      0.0          max_bins = 100 #if largest cluster &lt;1% of available data, then we can percentile_rank
22                                           
23                                           
24         1            0      0.0      0.0      if max_bins &lt; 5: 
25                                                   sub_df[:] = 50 #if we don't have the resolution to quintile rank then assume no data.
26                                           
27                                               #     return sub_df
28                                           
29         1            1      1.0      0.0      bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)
30                                           
31                                               #should track bin resolution for all data.  To add.
32                                           
33                                               #if get here, then neither nans_ranked, nor sub_df are empty
34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)
35         1       160530 160530.0     45.0      sub_df_ranked = (sub_df.rank(ascending = True) - 1)*100/len(x)
36                                           
37         1         5777   5777.0      1.6      ranked_df = pd.concat([sub_df_ranked, nans_ranked])
38                                               
39         1            1      1.0      0.0      return ranked_df
</code></pre>
";;5;;2017-05-17T17:03:25.027;1.0;44030936;2017-05-30T19:52:22.530;2017-05-17T17:31:25.063;;4035478.0;;4035478.0;;1;13;<python><pandas><lambda><vectorization><ranking>;Performance enhancement of ranking function by replacement of lambda x with vectorization;463.0
102081;102081;44123892.0;2.0;"<p>I face some problem here, in my python package I have install numpy, but I still have this error <strong>'DataFrame' object has no attribute 'sort'</strong></p>

<p>Anyone can give me some idea..</p>

<p>This is my code :</p>

<pre><code>final.loc[-1] =['', 'P','Actual']
final.index = final.index + 1  # shifting index
final = final.sort()
final.columns=[final.columns,final.iloc[0]]
final = final.iloc[1:].reset_index(drop=True)
final.columns.names = (None, None)
</code></pre>
";;2;;2017-05-23T00:14:45.457;1.0;44123874;2017-07-17T03:23:08.347;2017-05-23T00:30:39.877;;7892936.0;;7892936.0;;1;12;<python><pandas><numpy><dataframe>;'DataFrame' object has no attribute 'sort';8204.0
102805;102805;44208229.0;6.0;"<p><strong>definition</strong><br>
<em>factorize: Map each unique object into a unique integer. Typically, the range of integers mapped to is from zero to the n - 1 where n is the number of unique objects.  Two variations are typical as well.  Type 1 is where the numbering occurs in the order in which unique objects are identified.  Type 2 is where unique objects are first sorted and then the same process as in Type 1 is applied.</em> </p>

<p><strong>The Setup</strong><br>
Consider the list of tuples <code>tups</code></p>

<pre><code>tups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]
</code></pre>

<p>I want to factorize this into</p>

<pre><code>[0, 1, 2, 3, 4, 1, 2]
</code></pre>

<p>I know there are many ways to do this.  However, I want to do this as efficiently as possible.</p>

<hr>

<p><strong>What I've tried</strong>  </p>

<p><code>pandas.factorize</code> and get an error...</p>

<pre><code>pd.factorize(tups)[0]

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-84-c84947ac948c&gt; in &lt;module&gt;()
----&gt; 1 pd.factorize(tups)[0]

//anaconda/envs/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)
    553     uniques = vec_klass()
    554     check_nulls = not is_integer_dtype(original)
--&gt; 555     labels = table.get_labels(values, uniques, 0, na_sentinel, check_nulls)
    556 
    557     labels = _ensure_platform_int(labels)

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_labels (pandas/_libs/hashtable.c:21804)()

ValueError: Buffer has wrong number of dimensions (expected 1, got 2)
</code></pre>

<hr>

<p>Or <code>numpy.unique</code> and get incorrect result...</p>

<pre><code>np.unique(tups, return_inverse=1)[1]

array([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])
</code></pre>

<hr>

<p>?I could use either of these on the hashes of the tuples</p>

<pre><code>pd.factorize([hash(t) for t in tups])[0]

array([0, 1, 2, 3, 4, 1, 2])
</code></pre>

<hr>

<p>Yay!  That's what I wanted... so what's the problem?  </p>

<p><strong>First Problem</strong><br>
Look at the performance drop from this technique</p>

<pre><code>lst = [10, 7, 4, 33, 1005, 7, 4]

%timeit pd.factorize(lst * 1000)[0]
1000 loops, best of 3: 506 s per loop

%timeit pd.factorize([hash(i) for i in lst * 1000])[0]
1000 loops, best of 3: 937 s per loop
</code></pre>

<p><strong>Second Problem</strong><br>
Hashing is not guaranteed unique!</p>

<hr>

<p><strong>Question</strong><br>
What is a super fast way to factorize a list of tuples?</p>

<hr>

<p><strong>Timing</strong><br>
<em>both axes are in log space</em>  </p>

<p><a href=""https://i.stack.imgur.com/zIlh0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zIlh0.png"" alt=""enter image description here""></a></p>

<p><em><code>code</code></em>  </p>

<pre><code>from itertools import count

def champ(tups):
    d = {}
    c = count()
    return np.array(
        [d[tup] if tup in d else d.setdefault(tup, next(c)) for tup in tups]
    )

def root(tups):
    return pd.Series(tups).factorize()[0]

def iobe(tups):
    return np.unique(tups, return_inverse=True, axis=0)[1]

def get_row_view(a):
    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))
    a = np.ascontiguousarray(a)
    return a.reshape(a.shape[0], -1).view(void_dt).ravel()

def diva(tups):
    return np.unique(get_row_view(np.array(tups)), return_inverse=1)[1]

def gdib(tups):
    return pd.factorize([str(t) for t in tups])[0]

from string import ascii_letters

def tups_creator_1(size, len_of_str=3, num_ints_to_choose_from=1000, seed=None):
    c = len_of_str
    n = num_ints_to_choose_from
    np.random.seed(seed)
    d = pd.DataFrame(np.random.choice(list(ascii_letters), (size, c))).sum(1).tolist()
    i = np.random.randint(n, size=size)
    return list(zip(d, i))

results = pd.DataFrame(
    index=pd.Index([100, 1000, 5000, 10000, 20000, 30000, 40000, 50000], name='Size'),
    columns=pd.Index('champ root iobe diva gdib'.split(), name='Method')
)

for i in results.index:
    tups = tups_creator_1(i, max(1, int(np.log10(i))), max(10, i // 10))
    for j in results.columns:
        stmt = '{}(tups)'.format(j)
        setup = 'from __main__ import {}, tups'.format(j)
        results.set_value(i, j, timeit(stmt, setup, number=100) / 100)

results.plot(title='Avg Seconds', logx=True, logy=True)
</code></pre>
";;6;;2017-05-26T18:20:24.910;1.0;44207926;2017-06-01T00:44:31.533;2017-05-26T19:59:47.333;;2336654.0;;2336654.0;;1;12;<python><pandas><numpy>;How to I factorize a list of tuples?;401.0
104258;104258;44478699.0;1.0;"<p>I got good use out of pandas' <code>MovingOLS</code> class (source <a href=""https://github.com/pandas-dev/pandas/blob/v0.19.2/pandas/stats/ols.py"" rel=""nofollow noreferrer"">here</a>) within the deprecated <code>stats/ols</code> module.  Unfortunately, it was gutted completely with pandas 0.20.</p>

<p>The question of how to run rolling OLS regression in an efficient manner has been asked several times (<a href=""https://stackoverflow.com/questions/37317727/deprecated-rolling-window-option-in-ols-from-pandas-to-statsmodels"">here</a>, for instance), but phrased a little broadly and left without a great answer, in my view.</p>

<p>Here are my questions:  </p>

<ol>
<li><p>How can I best mimic the basic framework of pandas' <code>MovingOLS</code>?  The most attractive feature of this class was the ability to view multiple methods/attributes as separate time series--i.e. coefficients, r-squared, t-statistics, etc without needing to re-run regression.  For example, you could create something like <code>model = pd.MovingOLS(y, x)</code> and then call <code>.t_stat</code>, <code>.rmse</code>, <code>.std_err</code>, and the like.  In the example below, conversely, I don't see a way around being forced to compute each statistic separately.  Is there a method that doesn't involve creating sliding/rolling ""blocks"" (strides) and running regressions/using linear algebra to get model parameters for each?  </p></li>
<li><p>More broadly, what's going on under the hood in pandas that makes <code>rolling.apply</code> not able to take more complex functions?*  When you create a <code>.rolling</code> object, in layman's terms, what's going on internally--is it fundamentally different from looping over each window and creating a higher-dimensional array as I'm doing below?</p></li>
</ol>

<p>*Namely, <code>func</code> passed to <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.window.Rolling.apply.html"" rel=""nofollow noreferrer""><code>.apply</code></a>:</p>

<blockquote>
  <p>Must produce a single value from an ndarray input *args and **kwargs
  are passed to the function</p>
</blockquote>

<p>Here's where I'm currently at with some sample data, regressing percentage changes in the trade weighted dollar on interest rate spreads and the price of copper.  (This doesn't make a ton of sense; just picked these randomly.)  I've taken it out of a class-based implementation and tried to strip it down to a simpler script.</p>

<pre><code>from datetime import date
from pandas_datareader.data import DataReader
import statsmodels.formula.api as smf

syms = {'TWEXBMTH' : 'usd', 
        'T10Y2YM' : 'term_spread', 
        'PCOPPUSDM' : 'copper'
       }

start = date(2000, 1, 1)
data = (DataReader(syms.keys(), 'fred', start)
        .pct_change()
        .dropna())
data = data.rename(columns = syms)
data = data.assign(intercept = 1.) # required by statsmodels OLS

def sliding_windows(x, window):
    """"""Create rolling/sliding windows of length ~window~.

    Given an array of shape (y, z), it will return ""blocks"" of shape
    (x - window + 1, window, z).""""""

    return np.array([x[i:i + window] for i 
                    in range(0, x.shape[0] - window + 1)])

data.head(3)
Out[33]: 
                 usd  term_spread    copper  intercept
DATE                                                  
2000-02-01  0.012573    -1.409091 -0.019972        1.0
2000-03-01 -0.000079     2.000000 -0.037202        1.0
2000-04-01  0.005642     0.518519 -0.033275        1.0

window = 36
wins = sliding_windows(data.values, window=window)
y, x = wins[:, :, 0], wins[:, :, 1:]

coefs = []

for endog, exog in zip(y, x):
    model = smf.OLS(endog, exog).fit()
        # The full set of model attributes gets lost with each loop
    coefs.append(model.params)

df = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,
                  index=data.index[window - 1:])

df.head(3) # rolling 36m coefficients
Out[70]: 
            term_spread    copper  intercept
DATE                                        
2003-01-01    -0.000122 -0.018426   0.001937
2003-02-01     0.000391 -0.015740   0.001597
2003-03-01     0.000655 -0.016811   0.001546
</code></pre>
";;4;;2017-06-06T01:31:19.453;2.0;44380068;2017-06-23T13:02:29.497;2017-06-15T17:41:08.047;;7954504.0;;7954504.0;;1;21;<python><pandas><numpy><linear-regression><statsmodels>;Pandas rolling regression: alternatives to looping;700.0
107241;107241;44910807.0;8.0;"<p>I have three DataFrames that I'm trying to concatenate.</p>

<pre><code>concat_df = pd.concat([df1, df2, df3])
</code></pre>

<p>This results in a MemoryError. How can I resolve this?</p>

<p>Note that most of the existing similar questions are on MemoryErrors occuring when reading large files. I don't have that problem. I have read my files in into DataFrames. I just can't concatenate that data.</p>
";;8;;2017-06-23T07:20:47.357;0.0;44715393;2017-07-05T00:29:04.507;2017-07-05T00:29:04.507;;5314012.0;;3521109.0;;1;13;<python><pandas><memory><memory-management>;How to concatenate multiple pandas.DataFrames without running into MemoryError;433.0
109481;109481;;5.0;"<p>I have a csv file of 8gb and I am not able to run the code as it shows memory error. </p>

<pre><code>file = ""./data.csv""
df = pd.read_csv(file, sep=""/"", header=0, dtype=str)
</code></pre>

<p>I would like to split the files into 8 small files (""sorted by id"") using python. And fianlly,have a loop so that the output file will have the output of all 8 files.</p>

<p>Or I would like to try parallel computing. Main goal is to process 8gb data in python pandas.  Thank you.</p>

<p>My csv file contains numerous data with '/' as the comma separator,</p>

<pre><code>id    venue           time             code    value ......
AAA   Paris      28/05/2016 09:10      PAR      45   ......
111   Budapest   14/08/2016 19:00      BUD      62   ......
AAA   Tokyo      05/11/2016 23:20      TYO      56   ......
111   LA         12/12/2016 05:55      LAX      05   ......
111   New York   08/01/2016 04:25      NYC      14   ......
AAA   Sydney     04/05/2016 21:40      SYD      2    ......
ABX   HongKong   28/03/2016 17:10      HKG      5    ......
ABX   London     25/07/2016 13:02      LON      22   ......
AAA   Dubai      01/04/2016 18:45      DXB      19   ......
.
.
.
.
</code></pre>
";;4;;2017-07-06T10:17:28.343;3.0;44946141;2017-07-13T13:26:31.830;2017-07-10T10:17:01.130;;7779326.0;;7779326.0;;1;12;<python><loops><csv><pandas><parallel-processing>;How to input large data into python pandas using looping or parallel computing?;332.0
109969;109969;44999009.0;8.0;"<p>Now there are a lot of similar questions but most of them answer how to delete the duplicate columns. However, I want to know how can I make a list of tuples where each tuple contains the column names of duplicate columns. I am assuming that each column has a unique name. Just to further illustrate my question:</p>

<pre><code>df = pd.DataFrame({'A': [1, 2, 3, 4, 5],'B': [2, 4, 2, 1, 9],
                   'C': [1, 2, 3, 4, 5],'D': [2, 4, 2, 1, 9],
                   'E': [3, 4, 2, 1, 2],'F': [1, 1, 1, 1, 1]},
                   index = ['a1', 'a2', 'a3', 'a4', 'a5'])
</code></pre>

<p>then I want the output:</p>

<pre><code>[('A', 'C'), ('B', 'D')]
</code></pre>

<p>And if you are feeling great today then also extend the same question to rows. How to get a list of tuples where each tuple contains duplicate rows.</p>
";;4;;2017-07-09T15:50:57.807;4.0;44998223;2017-07-09T22:25:22.733;2017-07-09T19:45:21.817;;3293881.0;;5576491.0;;1;16;<python><pandas><numpy><dataframe><duplicates>;Group duplicate column IDs in pandas dataframe;579.0
111784;111784;45193636.0;3.0;"<p>I have the following (simplified) dataframe:</p>

<pre><code>df = pd.DataFrame({'X': [1, 2, 3, 4, 5,6,7,8,9,10],
'Y': [10,20,30,40,50,-10,-20,-30,-40,-50],
'Z': [20,18,16,14,12,10,8,6,4,2]},index=list('ABCDEFGHIJ'))
</code></pre>

<p>Which gives the following:</p>

<pre><code>    X   Y   Z
A   1  10  20
B   2  20  18
C   3  30  16
D   4  40  14
E   5  50  12
F   6 -10  10
G   7 -20   8
H   8 -30   6
I   9 -40   4
J  10 -50   2
</code></pre>

<p>I want to create a new dataframe that returns the index of the n smallest values, by column.</p>

<p>Desired output (say, 3 smallest values):</p>

<pre><code>   X  Y  Z
0  A  J  J
1  B  I  I
2  C  H  H
</code></pre>

<p>What is the best way to do this?</p>
";;0;;2017-07-19T14:09:55.643;1.0;45193131;2017-07-30T09:16:35.897;2017-07-30T09:16:35.897;;466862.0;;7626510.0;;1;11;<python><pandas>;return n smallest indexes by column using pandas;369.0
112365;112365;45379057.0;1.0;"<p><strong>Edit:</strong> I condensed this question given that it was probably too involved to begin with.  The meat of the question is in bold below.</p>

<p>I'd like to know more about the object that is actually created when using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html#pandas.DataFrame.rolling"" rel=""nofollow noreferrer""><code>DataFrame.rolling</code></a> or <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rolling.html"" rel=""nofollow noreferrer""><code>Series.rolling</code></a>:</p>

<pre><code>print(type(df.rolling))
&lt;class 'pandas.core.window.Rolling'&gt;
</code></pre>

<p>Some background: consider the oft-used alternative with <code>np.as_strided</code>.  This code snippet itself isn't important, but its result is my reference point in asking this question.</p>

<pre><code>def rwindows(a, window):
    if a.ndim == 1:
        a = a.reshape(-1, 1)
    shape = a.shape[0] - window + 1, window, a.shape[-1]
    strides = (a.strides[0],) + a.strides
    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)
    return np.squeeze(windows)
</code></pre>

<p>Here <code>rwindows</code> will take a 1d or 2d <code>ndarray</code> and build rolling ""blocks"" equal to the specified window size (as below).  <strong>How does a <code>.rolling</code> object compare to the <code>ndarray</code> output below?</strong>  Is it an iterator, with certain attributes stored for each block?  Or something else entirely?  I've tried playing around with tab completion on the object with attributes/methods such as <code>__dict__</code> and <code>_get_index()</code> and they're not telling me much.  I've also seen a <a href=""https://github.com/pandas-dev/pandas/blob/v0.20.3/pandas/core/window.py#L132"" rel=""nofollow noreferrer""><code>_create_blocks</code></a> method in pandas--does it at all resemble the <code>strided</code> method?</p>

<pre><code># as_strided version

a = np.arange(5)
print(rwindows(a, 3))           # 1d input
[[0 1 2]
 [1 2 3]
 [2 3 4]]

b = np.arange(10).reshape(5,2)
print(rwindows(b, 4))           # 2d input
[[[0 1]
  [2 3]
  [4 5]
  [6 7]]

 [[2 3]
  [4 5]
  [6 7]
  [8 9]]]
</code></pre>

<h2>Part 2, extra credit</h2>

<p>Using the NumPy approach above (OLS implementation <a href=""https://github.com/bsolomon1/ols/blob/master/ols.py"" rel=""nofollow noreferrer"">here</a>) is necessitated by the fact that <code>func</code> within <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.window.Rolling.apply.html"" rel=""nofollow noreferrer"">pandas.core.window.Rolling.apply</a> must</p>

<blockquote>
  <p>produce a single value from an ndarray input *args and **kwargs are
  passed to the function</p>
</blockquote>

<p>So the argument can't be another rolling object.  I.e.</p>

<pre><code>def prod(a, b):
    return a * b
df.rolling(3).apply(prod, args=((df + 2).rolling(3),))
-----------------------------------------------------------------------
...
TypeError: unsupported operand type(s) for *: 'float' and 'Rolling'
</code></pre>

<p>So this is really from where my question above stems.  Why is it that the passed function must use a NumPy array and produce a single scalar value, and what does this have to do with the layout of a <code>.rolling</code> object?</p>
";;1;;2017-07-22T11:45:29.647;1.0;45254174;2017-08-24T21:00:24.927;2017-07-31T15:05:41.117;;7954504.0;;7954504.0;;1;13;<python><pandas><dataframe>;How do pandas Rolling objects work?;402.0
113170;113170;45333133.0;7.0;"<p>Suppose I have two dataframes <code>d1</code> and <code>d2</code></p>

<pre><code>d1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])
d2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])
</code></pre>

<hr>

<pre><code>d1

   0  1  2
a  1  1  1
b  1  1  1
c  1  1  1
</code></pre>

<hr>

<pre><code>d2

   3  4
a  0  0
b  0  0
c  0  0
</code></pre>

<hr>

<p>What is an easy and generalized way to interweave two dataframes' columns.  We can assume that the number of columns in <code>d2</code> is always one less than the number of columns in <code>d1</code>.  And, the indices are the same.</p>

<p>I want this:</p>

<pre><code>pd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)

   0  3  1  4  2
a  1  0  1  0  1
b  1  0  1  0  1
c  1  0  1  0  1
</code></pre>
";;0;;2017-07-26T16:45:26.617;2.0;45332960;2017-07-29T02:41:20.573;;;;;2336654.0;;1;14;<python><pandas><numpy>;Interweave two dataframes;596.0
114804;114804;;6.0;"<p>I'm trying to slice a dataframe based on list of values, how would I go about this?</p>

<p>Say I have a list, l, and it looks like: <code>[0,1,0,0,1,1,0,0,0,1]</code></p>

<p>I want to return all rows in a dataframe, df, based on if the value in the list is a 1. In this example, I would include rows where index is 1, 4, 5, and 9. Any easy way to do this? Apologize if this is an easy question but I am still getting used to dataframes.</p>
";;1;;2017-08-03T21:16:36.833;3.0;45494649;2017-08-05T20:21:49.757;2017-08-04T10:29:27.080;;254830.0;;7180132.0;;1;13;<python><pandas><dataframe>;Return subset based on a list of boolean values;635.0
117383;117383;45741989.0;1.0;"<p>Given a <a href=""https://stackoverflow.com/questions/17921010/how-to-query-multiindex-index-columns-values-in-pandas"">dataframe like this</a>:</p>

<pre><code>          C
A   B      
1.1 111  20
    222  31
3.3 222  24
    333  65
5.5 333  22
6.6 777  74 
</code></pre>

<p>How do I read it in using <code>pd.read_clipboard</code>? I've tried this:</p>

<pre><code>df = pd.read_clipboard(index_col=[0, 1])
</code></pre>

<p>But it throws an error:</p>

<blockquote>
<pre><code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3
</code></pre>
</blockquote>

<p>How can I fix this?</p>

<hr>

<p>Other <code>pd.read_clipboard</code> questions:</p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/45528198/"">How do you handle column names having spaces in them when using pd.read_clipboard?</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/45725500/how-to-handle-custom-named-index-when-copying-a-dataframe-using-pd-read-clipboar"">How to handle custom named index when copying a dataframe using pd.read_clipboard?</a></p></li>
</ul>
";;11;;2017-08-17T16:29:45.753;6.0;45740537;2017-08-20T19:53:03.803;2017-08-17T17:00:19.850;;4909087.0;;4909087.0;;1;12;<python><pandas><dataframe>;Copying MultiIndex dataframes with pd.read_clipboard?;199.0
118146;118146;;2.0;"<p><strong>Update</strong>: not sure if this is possible without some form of a <a href=""https://stackoverflow.com/questions/38008390/how-to-constuct-a-column-of-data-frame-recursively-with-pandas-python"">loop</a>, but <code>np.where</code> will not work here. If the answer is, ""you can't"", then so be it.  If it can be done, it may use something from <a href=""https://stackoverflow.com/questions/26267809/recursive-definitions-in-pandas""><code>scipy.signal</code></a>.</p>

<hr>

<p>I'd like to vectorize the loop in the code below, but unsure as to how, due to the recursive nature of the output.</p>

<p>Walk-though of my current setup:</p>

<p>Take a starting amount ($1 million) and a quarterly dollar distribution ($5,000):</p>

<pre><code>dist = 5000.
v0 = float(1e6)
</code></pre>

<p>Generate some random security/account returns (decimal form) at monthly freq:</p>

<pre><code>r = pd.Series(np.random.rand(12) * .01,
              index=pd.date_range('2017', freq='M', periods=12))
</code></pre>

<p>Create an empty Series that will hold the monthly account values:</p>

<pre><code>value = pd.Series(np.empty_like(r), index=r.index)
</code></pre>

<p>Add a ""start month"" to <code>value</code>.  This label will contain <code>v0</code>.</p>

<pre><code>from pandas.tseries import offsets
value = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)]))
              .sort_index())
</code></pre>

<p>The loop I'd like to get rid of is here:</p>

<pre><code>for date in value.index[1:]:
    if date.is_quarter_end:
        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \
                        * (1 + r.loc[date]) - dist
    else:
        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \
                        * (1 + r.loc[date]) 
</code></pre>

<p><strong>Combined code:</strong></p>

<pre><code>import pandas as pd
from pandas.tseries import offsets
from pandas import Series
import numpy as np

dist = 5000.
v0 = float(1e6)
r = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))
value = pd.Series(np.empty_like(r), index=r.index)
value = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())
for date in value.index[1:]:
    if date.is_quarter_end:
        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) - dist
    else:
        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) 
</code></pre>

<p>In psuedocode, what is loop is doing is just:</p>

<pre><code>for each date in index of value:
    if the date is not a quarter end:
        multiply previous value by (1 + r) for that month
    if the date is a quarter end:
        multiply previous value by (1 + r) for that month and subtract dist
</code></pre>

<p>The issue is, I don't currently see how vectorization is possible since the successive value depends on whether or not a distribution was taken in the month prior.  I get to the desired result, but pretty inefficiently for higher frequency data or larger time periods.
<a href=""https://i.stack.imgur.com/LWtLc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LWtLc.png"" alt=""enter image description here""></a></p>
";;3;;2017-08-22T14:20:57.070;3.0;45820242;2017-08-25T13:18:24.230;2017-08-25T01:21:06.953;;7954504.0;;7954504.0;;1;12;<python><python-3.x><pandas><recursion><finance>;Recursion: account value with distributions;264.0
28;28;;;"<p>The <code>append</code> function has an optional argument <code>ignore_index</code> which you should use here to join the records together, since the index isn't meaningful for your application.</p>
";;2;;2011-10-15T16:58:28.077;;7779260;2011-10-15T16:58:28.077;;;;;776560.0;7776679.0;2;38;;;
32;32;;;"<p>Pandas is based on NumPy arrays.
The key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.</p>

<p>For example, if <code>close</code> is a 1-d array, and you want the day-over-day percent change,</p>

<pre><code>pct_change = close[1:]/close[:-1]
</code></pre>

<p>This computes the entire array of percent changes as one statement, instead of </p>

<pre><code>pct_change = []
for row in close:
    pct_change.append(...)
</code></pre>

<p>So try to avoid the Python loop <code>for i, row in enumerate(...)</code> entirely, and
think about how to perform your calculations with operations on the entire array (or dataframe) as a whole, rather than row-by-row.</p>
";;6;;2011-10-20T15:02:16.950;;7837947;2011-10-20T15:02:16.950;;;;;190597.0;7837722.0;2;111;;;
34;34;;;"<p>You can loop through the rows by transposing and then calling iteritems:</p>

<pre><code>for date, row in df.T.iteritems():
   # do some logic here
</code></pre>

<p>I am not certain about efficiency in that case. To get the best possible performance in an iterative algorithm, you might want to explore writing it in <a href=""http://cython.org"">Cython</a>, so you could do something like:</p>

<pre><code>def my_algo(ndarray[object] dates, ndarray[float64_t] open,
            ndarray[float64_t] low, ndarray[float64_t] high,
            ndarray[float64_t] close, ndarray[float64_t] volume):
    cdef:
        Py_ssize_t i, n
        float64_t foo
    n = len(dates)

    for i from 0 &lt;= i &lt; n:
        foo = close[i] - open[i] # will be extremely fast
</code></pre>

<p>I would recommend writing the algorithm in pure Python first, make sure it works and see how fast it is-- if it's not fast enough, convert things to Cython like this with minimal work to get something that's about as fast as hand-coded C/C++.</p>
";;4;;2011-10-21T13:04:53.253;;7849789;2011-10-21T13:04:53.253;;;;;776560.0;7837722.0;2;63;;;
50;50;;;"<p>I encourage you to pose these questions on the <a href=""http://groups.google.com/group/pystatsmodels"" rel=""noreferrer"">mailing list</a>, but in any case, it's still a very much low level affair working with the underlying NumPy arrays. For example, to select rows where the value in any column exceed, say, 1.5 in this example:</p>

<pre><code>In [11]: df
Out[11]: 
            A        B        C        D      
2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572
2000-01-04  0.83935  0.15993  0.95911 -1.12959
2000-01-05  2.80215 -0.10858 -1.62114 -0.20170
2000-01-06  0.71670 -0.26707  1.36029  1.74254
2000-01-07 -0.45749  0.22750  0.46291 -0.58431
2000-01-10 -0.78702  0.44006 -0.36881 -0.13884
2000-01-11  0.79577 -0.09198  0.14119  0.02668
2000-01-12 -0.32297  0.62332  1.93595  0.78024
2000-01-13  1.74683 -1.57738 -0.02134  0.11596
2000-01-14 -0.55613  0.92145 -0.22832  1.56631
2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723
2000-01-18  0.73274  0.24387  0.88146 -0.94490
2000-01-19  0.56644 -0.49321  1.17584 -0.17585
2000-01-20  1.56441  0.62331 -0.26904  0.11952
2000-01-21  0.61834  0.17463 -1.62439  0.99103
2000-01-24  0.86378 -0.68111 -0.15788 -0.16670
2000-01-25 -1.12230 -0.16128  1.20401  1.08945
2000-01-26 -0.63115  0.76077 -0.92795 -2.17118
2000-01-27  1.37620 -1.10618 -0.37411  0.73780
2000-01-28 -1.40276  1.98372  1.47096 -1.38043
2000-01-31  0.54769  0.44100 -0.52775  0.84497
2000-02-01  0.12443  0.32880 -0.71361  1.31778
2000-02-02 -0.28986 -0.63931  0.88333 -2.58943
2000-02-03  0.54408  1.17928 -0.26795 -0.51681
2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639
2000-02-07 -0.65483 -0.29584 -0.02722  0.31270
2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239
2000-02-09 -2.28496  0.36352  1.11596  0.02293
2000-02-10  0.51054  0.97249  1.74501  0.20525
2000-02-11  0.10100  0.27722  0.65843  1.73591

In [12]: df[(df.values &gt; 1.5).any(1)]
Out[12]: 
            A       B       C        D     
2000-01-05  2.8021 -0.1086 -1.62114 -0.2017
2000-01-06  0.7167 -0.2671  1.36029  1.7425
2000-01-12 -0.3230  0.6233  1.93595  0.7802
2000-01-13  1.7468 -1.5774 -0.02134  0.1160
2000-01-14 -0.5561  0.9215 -0.22832  1.5663
2000-01-20  1.5644  0.6233 -0.26904  0.1195
2000-01-28 -1.4028  1.9837  1.47096 -1.3804
2000-02-10  0.5105  0.9725  1.74501  0.2052
2000-02-11  0.1010  0.2772  0.65843  1.7359
</code></pre>

<p>Multiple conditions have to be combined using <code>&amp;</code> or <code>|</code> (and parentheses!):</p>

<pre><code>In [13]: df[(df['A'] &gt; 1) | (df['B'] &lt; -1)]
Out[13]: 
            A        B       C        D     
2000-01-05  2.80215 -0.1086 -1.62114 -0.2017
2000-01-13  1.74683 -1.5774 -0.02134  0.1160
2000-01-20  1.56441  0.6233 -0.26904  0.1195
2000-01-27  1.37620 -1.1062 -0.37411  0.7378
2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564
</code></pre>

<p>I'd be very interested to have some kind of query API to make these kinds of things easier</p>
";;2;;2012-01-18T20:12:06.520;;8916746;2012-01-18T20:12:06.520;;;;;776560.0;8916302.0;2;34;;;
55;55;;;"<p>The reason pandas is faster is because I came up with a better algorithm, which is implemented very carefully using <a href=""https://github.com/attractivechaos/klib"" rel=""noreferrer"">a fast hash table implementation - klib</a> and in C/<a href=""http://cython.org/"" rel=""noreferrer"">Cython</a> to avoid the Python interpreter overhead for the non-vectorizable parts. The algorithm is described in some detail in my presentation: <a href=""http://wesmckinney.com/blog/nycpython-1102012-a-look-inside-pandas-design-and-development/"" rel=""noreferrer""><em>A look inside pandas design and development</em></a>.</p>

<p>The comparison with <code>data.table</code> is actually a bit interesting because the whole point of R's <code>data.table</code> is that it contains <em>pre-computed indexes</em> for various columns to accelerate operations like data selection and merges. In this case (database joins) pandas' DataFrame contains <em>no pre-computed information</em> that is being used for the merge, so to speak it's a ""cold"" merge. If I had stored the factorized versions of the join keys, the join would be significantly faster - as factorizing is the biggest bottleneck for this algorithm.</p>

<p>I should also add that the internal design of pandas' DataFrame is much more amenable to these kinds of operations than R's data.frame (which is just a list of arrays internally).</p>
";;8;;2012-01-24T19:17:34.530;;8992714;2016-03-24T11:30:10.683;2016-03-24T11:30:10.683;;3923281.0;;776560.0;8991709.0;2;185;;;
56;56;;;"<p>It looks like Wes may have discovered a known issue in <code>data.table</code> when the number of unique strings (<em>levels</em>) is large: 10,000.</p>

<p>Does <code>Rprof()</code> reveal most of the time spent in the call <code>sortedmatch(levels(i[[lc]]), levels(x[[rc]])</code>?  This isn't really the join itself (the algorithm), but a preliminary step.</p>

<p>Recent efforts have gone into allowing character columns in keys, which should resolve that issue by integrating more closely with R's own global string hash table. Some benchmark results are already reported by <code>test.data.table()</code> but that code isn't hooked up yet to replace the levels to levels match.</p>

<p>Are pandas merges faster than <code>data.table</code> for regular integer columns?  That should be a way to isolate the algorithm itself vs factor issues.</p>

<p>Also, <code>data.table</code> has <em>time series merge</em> in mind. Two aspects to that: i) multi column <em>ordered</em> keys such as (id,datetime) ii) fast prevailing join (<code>roll=TRUE</code>) a.k.a. last observation carried forward.</p>

<p>I'll need some time to confirm as it's the first I've seen of the comparison to <code>data.table</code> as presented.</p>

<hr>

<p><strong>UPDATE from data.table v1.8.0 released July 2012</strong></p>

<ul>
<li>Internal function sortedmatch() removed and replaced with chmatch()
       when matching i levels to x levels for columns of type 'factor'. This
       preliminary step was causing a (known) significant slowdown when the number
       of levels of a factor column was large (e.g. >10,000). Exacerbated in
       tests of joining four such columns, as demonstrated by Wes McKinney
       (author of Python package Pandas). Matching 1 million strings of which
       of which 600,000 are unique is now reduced from 16s to 0.5s, for example.</li>
</ul>

<p>also in that release was :</p>

<ul>
<li><p>character columns are now allowed in keys and are preferred to
factor. data.table() and setkey() no longer coerce character to
factor. Factors are still supported. Implements FR#1493, FR#1224
and (partially) FR#951.</p></li>
<li><p>New functions chmatch() and %chin%, faster versions of match()
and %in% for character vectors. R's internal string cache is
utilised (no hash table is built). They are about 4 times faster
than match() on the example in ?chmatch.</p></li>
</ul>

<p>As of Sep 2013 data.table is v1.8.10 on CRAN and we're working on v1.9.0. <strong><a href=""https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&amp;root=datatable"" rel=""noreferrer"">NEWS</a></strong> is updated live.</p>

<hr>

<p>But as I wrote originally, above :</p>

<blockquote>
  <p><code>data.table</code> has <em>time series merge</em> in mind. Two aspects to that: i)
  multi column <em>ordered</em> keys such as (id,datetime) ii) fast prevailing
  join (<code>roll=TRUE</code>) a.k.a. last observation carried forward.</p>
</blockquote>

<p>So the Pandas equi join of two character columns is probably still faster than data.table. Since it sounds like it hashes the combined two columns. data.table doesn't hash the key because it has prevailing ordered joins in mind. A ""key"" in data.table is literally just the sort order (similar to a clustered index in SQL; i.e., that's how the data is ordered in RAM). On the list is to add secondary keys, for example.</p>

<p>In summary, the glaring speed difference highlighted by this particular two-character-column test with over 10,000 unique strings shouldn't be as bad now, since the known problem has been fixed.</p>
";;6;;2012-01-25T04:42:42.983;;8997908;2013-09-26T09:46:45.943;2013-09-26T09:46:45.943;;403310.0;;403310.0;8991709.0;2;93;;;
74;74;;;"<p>If you are one Windows, I can advise <a href=""http://code.google.com/p/pythonxy/"">pythonxy</a> for an easy and painless installation of Python and the core scientific libraries. </p>

<p>It is quite large and contains a lot of packages, which you maybe do not need, but at the installation, you can opt to choose which libraries to install.</p>
";;2;;2012-03-04T14:42:12.120;;9555766;2012-03-04T14:52:37.847;2012-03-04T14:52:37.847;;653364.0;;653364.0;9555635.0;2;14;;;
77;77;;;"<p>On MacOSX, there is <a href=""http://fonnesbeck.github.com/ScipySuperpack/"">ScipySuperpack</a>.</p>

<p>On Linux, there are... Linux distributions :) If you want recent builds on Debian and Ubuntu I recommend: <a href=""http://neuro.debian.net/"">http://neuro.debian.net/</a></p>
";;0;;2012-03-04T17:47:48.320;;9557319;2012-03-04T17:47:48.320;;;;;163740.0;9555635.0;2;8;;;
78;78;;;"<p><a href=""http://www.sagemath.org/"" rel=""noreferrer"">Sage</a>. It doesn't have the GUI tools of Enthought but otherwise contains a full scientific python stack. </p>
";;0;;2012-03-04T21:11:31.877;;9558852;2012-03-04T21:19:01.887;2012-03-04T21:19:01.887;;238882.0;;238882.0;9555635.0;2;6;;;
79;79;;;"<p>Have you seen <a href=""http://www.enthought.com/products/epd_free.php"" rel=""noreferrer"">EPD free</a>?</p>

<p>From the enthought website:</p>

<blockquote>
  <p>Our new lightweight distribution of scientific Python essentials:
  SciPy, NumPy, IPython, matplotlib, Traits, &amp; Chaco</p>
</blockquote>

<p>it might be enough to get you started.</p>
";;1;;2012-03-06T02:58:07.067;;9577305;2012-03-06T02:58:07.067;;;;;1004096.0;9555635.0;2;13;;;
83;83;;;"<p>Assuming that you have a file called 2010.csv with contents</p>

<pre><code>category,value
AB,100.00
AB,200.00
AC,150.00
AD,500.00
</code></pre>

<p>Then, using the ability to apply <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once"" rel=""noreferrer"">multiple aggregation functions following a groupby</a>, you can say:</p>

<pre><code>import pandas
data_2010 = pandas.read_csv(""/path/to/2010.csv"")
data_2010.groupby(""category"").agg([len, sum])
</code></pre>

<p>You should get a result that looks something like</p>

<pre><code>          value     
            len  sum
category            
AB            2  300
AC            1  150
AD            1  500
</code></pre>

<p>Note that Wes will likely come by to point out that sum is optimized and that you should probably use np.sum.</p>
";;1;;2012-03-08T16:08:29.813;;9620832;2012-03-08T16:08:29.813;;;;;171965.0;9588331.0;2;12;;;
85;85;;;"<p>Thanks, Jeff. It is possible to do this using <code>pivot_table</code> for those interested:</p>

<pre><code>In [8]: df
Out[8]: 
  category  value
0       AB    100
1       AB    200
2       AC    150
3       AD    500

In [9]: df.pivot_table(rows='category', aggfunc=[len, np.sum])
Out[9]: 
            len    sum
          value  value
category              
AB            2    300
AC            1    150
AD            1    500
</code></pre>

<p>Note that the result's columns are hierarchically indexed. If you had multiple data columns, you would get a result like this:</p>

<pre><code>In [12]: df
Out[12]: 
  category  value  value2
0       AB    100       5
1       AB    200       5
2       AC    150       5
3       AD    500       5

In [13]: df.pivot_table(rows='category', aggfunc=[len, np.sum])
Out[13]: 
            len            sum        
          value  value2  value  value2
category                              
AB            2       2    300      10
AC            1       1    150       5
AD            1       1    500       5
</code></pre>

<p>The main reason to use <code>__builtin__.sum</code> vs. <code>np.sum</code> is that you get NA-handling from the latter. Probably could intercept the Python built-in, will make a note about that now.</p>
";;0;;2012-03-08T19:49:23.657;;9623878;2012-03-08T19:49:23.657;;;;;776560.0;9588331.0;2;14;;;
92;92;;;"<p>The documentation lists a <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.from_csv.html"" rel=""noreferrer"">.from_csv</a> function that appears to do what you want:</p>

<pre><code>DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\t')
</code></pre>

<p>If you have a header, you can pass <code>header=0</code>.</p>

<pre><code>DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\t', header=0)
</code></pre>
";;6;;2012-03-11T06:06:56.443;;9652858;2016-09-29T05:30:07.943;2016-09-29T05:30:07.943;;4216625.0;;1256624.0;9652832.0;2;49;;;
93;93;;;"<p>Use <code>read_table(filepath)</code>. The default separator is tab</p>
";;1;;2012-03-11T15:34:23.943;;9656288;2012-03-11T15:34:23.943;;;;;776560.0;9652832.0;2;35;;;
94;94;;;"<p>You might at first exhale ""<strong><em>what is he smoking?</em></strong>"" to my answer, but here it comes as an echo to ogrisel's answer:</p>

<p><strong>The best Python distribution is Debian GNU/Linux</strong> -- it comes with multiple versions of Python supported, hundreds (if not thousands) of Python modules and extensions packaged so their installation is guaranteed to be flawless (in 99% of the cases) regardless how complex underlying software/extension is, majority of them are unit-tested against supported versions and 3rd party modules at package build-time guaranteeing lack of head-ache later on.</p>

<p>Besides Python itself you can also choose there among a dozen of available Python IDEs (e.g. spyder, Eric, PIDA, and others), Python-aware editors (vim, emacs etc), alternative Python implementations (pypy), compilers (Cython, nuitka), etc.  Debug build of Python (python-dbg) in tandem with gdb allow you right away debug your extensions while inspecting Python stack etc.  And all of those Python-specific tools are available within the same software management framework as the rest of the system which carries thousands of generic and specialized software tools and resources.
Depending on your demand you can choose between <strong>stable</strong>, <strong>testing</strong> and <strong>unstable</strong> Debian ""suites"".</p>

<p>Now ""<strong>how</strong>"": virtualization if you cannot or just prefer not to dual-boot.  In a matter of minutes you can have a full blown system work on your Windows or OS X box without any major performance hit (unless you need heavy 3D graphics), and only need sufficient amount of RAM to share with your host OS needs -- you can easily access your host drive space within a virtual machine, see e.g. <a href=""http://neuro.debian.net/vm.html"">http://neuro.debian.net/vm.html</a>  for an easy starting point.</p>

<p>Ah right -- <em>pandas</em>, we provide backport builds from NeuroDebian repository, so you could easily use stable Debian and bleeding-edge pandas. </p>
";;0;;2012-03-13T02:45:55.417;;9677723;2012-03-13T02:45:55.417;;;;;1265472.0;9555635.0;2;13;;;
104;104;;;"<p>How about:</p>

<pre><code>subset = data_set[['data_date', 'data_1', 'data_2']]
tuples = [tuple(x) for x in subset.values]
</code></pre>
";;1;;2012-03-18T20:39:19.787;;9762084;2012-03-18T20:39:19.787;;;;;776560.0;9758450.0;2;71;;;
108;108;;;"<p>Do <code>df.join(df2)</code>:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index</a></p>
";;1;;2012-03-19T14:30:06.803;;9772031;2012-03-19T14:30:06.803;;;;;776560.0;9762935.0;2;18;;;
114;114;;;"<p>How about: <code>df2.combine_first(df1)</code>? </p>

<pre><code>In [33]: df2
Out[33]: 
                   A         B         C         D
2000-01-03  0.638998  1.277361  0.193649  0.345063
2000-01-04 -0.816756 -1.711666 -1.155077 -0.678726
2000-01-05  0.435507 -0.025162 -1.112890  0.324111
2000-01-06 -0.210756 -1.027164  0.036664  0.884715
2000-01-07 -0.821631 -0.700394 -0.706505  1.193341
2000-01-10  1.015447 -0.909930  0.027548  0.258471
2000-01-11 -0.497239 -0.979071 -0.461560  0.447598

In [34]: df1
Out[34]: 
                   A         B         C
2000-01-03  2.288863  0.188175 -0.040928
2000-01-04  0.159107 -0.666861 -0.551628
2000-01-05 -0.356838 -0.231036 -1.211446
2000-01-06 -0.866475  1.113018 -0.001483
2000-01-07  0.303269  0.021034  0.471715
2000-01-10  1.149815  0.686696 -1.230991
2000-01-11 -1.296118 -0.172950 -0.603887
2000-01-12 -1.034574 -0.523238  0.626968
2000-01-13 -0.193280  1.857499 -0.046383
2000-01-14 -1.043492 -0.820525  0.868685

In [35]: df2.comb
df2.combine        df2.combineAdd     df2.combine_first  df2.combineMult    

In [35]: df2.combine_first(df1)
Out[35]: 
                   A         B         C         D
2000-01-03  0.638998  1.277361  0.193649  0.345063
2000-01-04 -0.816756 -1.711666 -1.155077 -0.678726
2000-01-05  0.435507 -0.025162 -1.112890  0.324111
2000-01-06 -0.210756 -1.027164  0.036664  0.884715
2000-01-07 -0.821631 -0.700394 -0.706505  1.193341
2000-01-10  1.015447 -0.909930  0.027548  0.258471
2000-01-11 -0.497239 -0.979071 -0.461560  0.447598
2000-01-12 -1.034574 -0.523238  0.626968       NaN
2000-01-13 -0.193280  1.857499 -0.046383       NaN
2000-01-14 -1.043492 -0.820525  0.868685       NaN
</code></pre>

<p>Note that it takes the values from <code>df1</code> for indices that do not overlap with <code>df2</code>. If this doesn't do exactly what you want I would be willing to improve this function / add options to it.</p>
";;2;;2012-03-20T21:02:32.567;;9794891;2012-03-20T21:02:32.567;;;;;776560.0;9787853.0;2;17;;;
162;162;;;"<p>Try using <code>merge</code> (http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging):</p>

<pre><code>In [14]: right
Out[14]: 
    ST_NAME  value2
0    Oregon   6.218
1  Nebraska   0.001

In [15]: merge(left, right)
Out[15]: 
    ST_NAME  value  value2
0  Nebraska  2.491   0.001
1    Oregon  4.685   6.218

In [18]: merge(left, right, on='ST_NAME', sort=False)
Out[18]: 
    ST_NAME  value  value2
0    Oregon  4.685   6.218
1  Nebraska  2.491   0.001
</code></pre>

<p><code>DataFrame.join</code> is a bit of legacy method and apparently doesn't do column-on-column joins (originally it did index on column using the on parameter, hence the ""legacy"" designation).</p>
";;2;;2012-04-11T22:06:26.303;;10114652;2012-04-11T22:06:26.303;;;;;776560.0;10114399.0;2;11;;;
176;176;;;"<p>The problem lies in my assumption(incorrect) that I was in the dev version while in reality I had 1.6.1, one can check the current installed version with:</p>

<pre><code>import pandas
print pandas.__version__
</code></pre>

<p>in the current version <code>df.xs()</code> with the level parameter works ok.</p>
";;0;;2012-04-16T21:47:06.370;;10182172;2012-04-16T21:47:06.370;;;;;1330293.0;10175068.0;2;6;;;
184;184;;;"<p>You just need the <code>argmax()</code> (<em>now called <code>idxmax</code></em>) function. It's straightforward:</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])
&gt;&gt;&gt; df
          A         B         C
0  1.232853 -1.979459 -0.573626
1  0.140767  0.394940  1.068890
2  0.742023  1.343977 -0.579745
3  2.125299 -0.649328 -0.211692
4 -0.187253  1.908618 -1.862934
&gt;&gt;&gt; df['A'].argmax()
3
&gt;&gt;&gt; df['B'].argmax()
4
&gt;&gt;&gt; df['C'].argmax()
1
</code></pre>

<p>This function was updated to the name <code>idxmax</code> in the Pandas API, though as of Pandas 0.16, <code>argmax</code> still exists and performs the same function (though appears to run more slowly than <code>idxmax</code>).</p>

<p>You can also just use <code>numpy.argmax</code>, such as <code>numpy.argmax(df['A'])</code> -- it provides the same thing as either of the two <code>pandas</code> functions, and appears at least as fast as <code>idxmax</code> in cursory observations.</p>

<p>Previously (as noted in the comments) it appeared that <code>argmax</code> would exist as a separate function which provided the <em>integer position</em> within the index of the row location of the maximum element. For example, if you have string values as your index labels, like rows 'a' through 'e', you might want to know that the max occurs in row 4 (not row 'd'). However, in pandas 0.16, all of the listed methods above only provide the <em>label</em> from the <code>Index</code> for the row in question, and if you want the position integer of that label within the <code>Index</code> you have to get it manually (which can be tricky now that duplicate row labels are allowed).</p>

<p>In general, I think the move to <code>idxmax</code>-like behavior for all three of the approaches (<code>argmax</code>, which still exists, <code>idxmax</code>, and <code>numpy.argmax</code>) is a bad thing, since it is very common to require the positional integer location of a maximum, perhaps even more common than desiring the <em>label</em> of that positional location within some index, especially in applications where duplicate row labels are common.</p>

<p>For example, consider this toy <code>DataFrame</code> with a duplicate row label:</p>

<pre><code>In [19]: dfrm
Out[19]: 
          A         B         C
a  0.143693  0.653810  0.586007
b  0.623582  0.312903  0.919076
c  0.165438  0.889809  0.000967
d  0.308245  0.787776  0.571195
e  0.870068  0.935626  0.606911
f  0.037602  0.855193  0.728495
g  0.605366  0.338105  0.696460
h  0.000000  0.090814  0.963927
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260

In [20]: dfrm['A'].idxmax()
Out[20]: 'i'

In [21]: dfrm.ix[dfrm['A'].idxmax()]
Out[21]: 
          A         B         C
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260
</code></pre>

<p>So here a naive use of <code>idxmax</code> is not sufficient, whereas the old form of <code>argmax</code> would correctly provide the <em>positional</em> location of the max row (in this case, position 9).</p>

<p>This is exactly one of those nasty kinds of bug-prone behaviors in dynamically typed languages that makes this sort of thing so unfortunate, and worth beating a dead horse over. If you are writing systems code and your system suddenly gets used on some data sets that are not cleaned properly before being joined, it's very easy to end up with duplicate row labels, especially string labels like a CUSIP or SEDOL identifier for financial assets. You can't easily use the type system to help you out, and you may not be able to enforce uniqueness on the index without running into unexpectedly missing data. </p>

<p>So you're left with hoping that your unit tests covered everything (they didn't, or more likely no one wrote any tests) -- otherwise (most likely) you're just left waiting to see if you happen to smack into this error at runtime, in which case you probably have to go drop many hours worth of work from the database you were outputting results to, bang your head against the wall in IPython trying to manually reproduce the problem, finally figuring out that it's because <code>idxmax</code> can <em>only</em> report the <em>label</em> of the max row, and then being disappointed that no standard function automatically gets the <em>positions</em> of the max row for you, writing a buggy implementation yourself, editing the code, and praying you don't run into the problem again.</p>
";;3;;2012-04-18T04:31:30.417;;10202789;2015-05-11T02:49:20.243;2015-05-11T02:49:20.243;;567620.0;;567620.0;10202570.0;2;97;;;
185;185;;;"<p>You might also try <code>idxmax</code>:</p>

<pre><code>In [5]: df = pandas.DataFrame(np.random.randn(10,3),columns=['A','B','C'])

In [6]: df
Out[6]: 
          A         B         C
0  2.001289  0.482561  1.579985
1 -0.991646 -0.387835  1.320236
2  0.143826 -1.096889  1.486508
3 -0.193056 -0.499020  1.536540
4 -2.083647 -3.074591  0.175772
5 -0.186138 -1.949731  0.287432
6 -0.480790 -1.771560 -0.930234
7  0.227383 -0.278253  2.102004
8 -0.002592  1.434192 -1.624915
9  0.404911 -2.167599 -0.452900

In [7]: df.idxmax()
Out[7]: 
A    0
B    8
C    7
</code></pre>

<p>e.g.</p>

<pre><code>In [8]: df.ix[df['A'].idxmax()]
Out[8]: 
A    2.001289
B    0.482561
C    1.579985
</code></pre>
";;3;;2012-04-18T15:51:32.550;;10213167;2012-04-18T15:51:32.550;;;;;776560.0;10202570.0;2;52;;;
195;195;;;"<p><code>g1</code> here <em>is</em> a DataFrame. It has a hierarchical index, though:</p>

<pre><code>In [19]: type(g1)
Out[19]: pandas.core.frame.DataFrame

In [20]: g1.index
Out[20]: 
MultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),
       ('Mallory', 'Seattle')], dtype=object)
</code></pre>

<p>Perhaps you want something like this?</p>

<pre><code>In [21]: g1.add_suffix('_Count').reset_index()
Out[21]: 
      Name      City  City_Count  Name_Count
0    Alice   Seattle           1           1
1      Bob   Seattle           2           2
2  Mallory  Portland           2           2
3  Mallory   Seattle           1           1
</code></pre>

<p>Or something like:</p>

<pre><code>In [36]: DataFrame({'count' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()
Out[36]: 
      Name      City  count
0    Alice   Seattle      1
1      Bob   Seattle      2
2  Mallory  Portland      2
3  Mallory   Seattle      1
</code></pre>
";;4;;2012-04-29T17:50:33.950;;10374456;2012-04-29T17:50:33.950;;;;;776560.0;10373660.0;2;241;;;
197;197;;;"<p>argh.  you've got two pythons in your path that are the same version?  don't do that.</p>

<p>pip, easy-install, etc are associated with a particular python install and will use that python by default.  so if you have a system-provided python and a system-provided easy_install (or installed easy_install yourself using the system python) then easy_install will, by default, install packages for the system python.</p>

<p>the best way to avoid this mess, imho, is to use use system python for that version (2.7 probably) and, for other versions, use <code>make alt-install</code> when installing, which will give you executables like <code>python3.1</code> and the like.  if you really need to replace the version provided by the system, uninstall it.</p>

<p>once you've done that. each python will have a distinct name (ending in the version) and <code>python</code> will remain the system one.</p>

<p>next, when you install easy_install, you'll notice that there are version-specific versions (<code>easy_install-2.7</code> for example).  use those.  if one is missing, then install distutils with the appropriate python (eg use <code>python3.1</code> and you'll get an <code>easy_install-3.1</code>).  unfortunately, each time you do this (iirc) you overwrite the un-versioned <code>easy_install</code>, so <em>never</em> use that - always use the versioned one.</p>

<p>alternatively, you could not install easy_install or pip for anything other than the system version, then <em>always use virtualenv</em>.  virtualenv will let you specify a python version (so you can use the system virtualenv for all pythons installed) and then installs easy_install/pip for the python you use.  so once you're inside the virtual environment, everything just works.</p>

<p>and i just realised i haven't much experience with pip, so i can't actually help with that (except to note that virtualenv does provide it) (about which is preferable: it used to be that pip was better maintained; i think these days the latest distutils/easy_install is as good as pip, but pip has a few more features that i have never used).</p>

<p>disclaimer: the above is from experience gained developing lepl, which runs on 2.6 to 3.2, so i need to test it on all those.  as far as i know, what i describe above works for me, but i have no deep knowledge of python/easy_install/pip so i may have some mistakes in rationalising/describing things (in other words, i'm writing all this in case it helps, but i'm a bit worried i have an error - please, someone correct me if so).</p>
";;0;;2012-04-30T02:30:34.447;;10377863;2012-04-30T15:59:40.643;2012-04-30T15:59:40.643;;181772.0;;181772.0;10376647.0;2;14;;;
207;207;;;"<p>Why don't you simply use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#add-an-index-using-dataframe-columns"" rel=""noreferrer""><code>set_index</code></a> method?</p>

<pre><code>In : col = ['a','b','c']

In : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)

In : data
Out:
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22

In : data2 = data.set_index('a')

In : data2
Out:
     b   c
a
1    2   3
10  11  12
20  21  22
</code></pre>
";;1;;2012-05-05T02:44:12.800;;10458386;2012-05-05T02:44:12.800;;;;;843822.0;10457584.0;2;168;;;
213;213;;;"<p>You can use <code>DataFrame.apply</code> with <code>Series.interpolate</code> to get a linear interpolation. </p>

<pre><code>In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])

In : df
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
g -1.632493  0.938456  0.492695

In : df2 = df.reindex(['a','b','c','d','e','f','g'])

In : df2
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
b       NaN       NaN       NaN
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
f       NaN       NaN       NaN
g -1.632493  0.938456  0.492695

In : df2.apply(pandas.Series.interpolate)
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
b  0.052363 -1.729055  0.114652
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
f -1.330113  1.134579  0.000958
g -1.632493  0.938456  0.492695
</code></pre>

<p>For anything more complex, you need to roll-out your own function that will deal with a <code>Series</code> object and fill <code>NaN</code> values as you like and return another <code>Series</code> object.</p>
";;5;;2012-05-05T19:16:19.813;;10465162;2012-05-05T19:16:19.813;;;;;843822.0;10464738.0;2;30;;;
223;223;;;"<p>With your <code>import matplotlib.pyplot as plt</code> just add</p>

<pre><code>plt.show()
</code></pre>

<p>and it will show all stored plots.</p>
";;1;;2012-05-09T07:00:40.560;;10511230;2012-05-09T07:00:40.560;;;;;449449.0;10511024.0;2;19;;;
224;224;;;"<p>Ok, It seems the answer is to start ipython notebook with --pylab=inline. 
so ipython notebook --pylab=inline
This has it do what I saw earlier and what I wanted it to do. 
Sorry about the vague original question. </p>
";;2;;2012-05-09T07:24:51.110;;10511545;2012-05-09T07:24:51.110;;;;;442158.0;10511024.0;2;49;;;
235;235;;;"<p>Here's an example that does what you want:</p>

<pre><code>In [32]: from datetime import datetime as dt

In [33]: dr = p.DateRange(dt(2009,1,1),dt(2010,12,31), offset=p.datetools.Hour())

In [34]: hr = dr.map(lambda x: x.hour)

In [35]: dt = p.DataFrame(rand(len(dr),2), dr)

In [36]: dt 

Out[36]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DateRange: 17497 entries, 2009-01-01 00:00:00 to 2010-12-31 00:00:00
offset: &lt;1 Hour&gt;
Data columns:
0    17497  non-null values
1    17497  non-null values
dtypes: float64(2)

In [37]: dt[(hr &gt;= 10) &amp; (hr &lt;=16)]

Out[37]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 5103 entries, 2009-01-01 10:00:00 to 2010-12-30 16:00:00
Data columns:
0    5103  non-null values
1    5103  non-null values
dtypes: float64(2)
</code></pre>
";;2;;2012-05-12T17:03:18.753;;10565742;2012-05-12T17:56:29.413;2012-05-12T17:56:29.413;;1209307.0;;1209307.0;10565282.0;2;7;;;
236;236;;;"<p>In upcoming pandas 0.8.0, you'll be able to write</p>

<pre><code>hour = ts.index.hour
selector = ((10 &lt;= hour) &amp; (hour &lt;= 13)) | ((20 &lt;= hour) &amp; (hour &lt;= 23))
data = ts[selector]
</code></pre>
";;0;;2012-05-12T20:46:40.990;;10567298;2012-05-12T20:46:40.990;;;;;776560.0;10565282.0;2;22;;;
252;252;;;"<p>You can slice along the columns of a <code>DataFrame</code> by referring to the names of each column in a list, like so:</p>

<pre><code>data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))
data_ab = data[list('ab')]
data_cde = data[list('cde')]
</code></pre>
";;4;;2012-05-19T15:02:43.627;;10666301;2012-05-19T15:02:43.627;;;;;126751.0;10665889.0;2;23;;;
255;255;;;"<p>The DataFrame.ix index is what you want to be accessing. It's a little confusing (I agree that Pandas indexing is perplexing at times!), but the following seems to do what you want:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(np.random.rand(4,5), columns = list('abcde'))
&gt;&gt;&gt; df.ix[:,'b':]
      b         c         d         e
0  0.418762  0.042369  0.869203  0.972314
1  0.991058  0.510228  0.594784  0.534366
2  0.407472  0.259811  0.396664  0.894202
3  0.726168  0.139531  0.324932  0.906575
</code></pre>

<p>where .ix[row slice, column slice] is what is being interpreted. More on Pandas indexing here: <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced</a></p>
";;5;;2012-05-20T22:51:30.290;;10677896;2012-05-20T22:51:30.290;;;;;1284636.0;10665889.0;2;137;;;
264;264;;;"<p>You could use <code>pandas.concat()</code> or <code>DataFrame.append()</code>. For details and examples, see <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"" rel=""noreferrer"">Merge, join, and concatenate</a>.</p>
";;8;;2012-05-23T08:14:43.650;;10716007;2012-10-17T19:38:13.543;2012-10-17T19:38:13.543;;1064197.0;;367273.0;10715965.0;2;185;;;
270;270;;;"<p>You probably want to do </p>

<pre><code>df['Normalized'] = np.where(df['Currency'] == '$', df['Budget'] * 0.78125, df['Budget'])
</code></pre>
";;1;;2012-05-23T19:05:57.160;;10726275;2012-05-23T19:05:57.160;;;;;776560.0;10715519.0;2;38;;;
277;277;;;"<p>You should use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html""><code>df.iterrows()</code></a>. Though iterating row-by-row is not especially efficient since Series objects have to be created.</p>
";;4;;2012-05-24T14:24:52.837;;10739432;2015-10-25T12:06:59.040;2015-10-25T12:06:59.040;;1916449.0;;776560.0;10729210.0;2;167;;;
282;282;;;"<p>Returning a Series, rather than tuple, should produce a new multi-column DataFrame. For example,</p>

<pre><code>return pandas.Series({'pvalue': pvalue, 'mean_ratio': mean_ratio})
</code></pre>
";;5;;2012-05-25T23:48:46.777;;10762516;2012-05-25T23:48:46.777;;;;;243434.0;10751127.0;2;55;;;
287;287;;;"<p>It looks like you can't avoid pandas from trying to convert numeric/boolean values in the CSV file. Take a look at the source code of pandas for the IO parsers, in particular functions <code>_convert_to_ndarrays</code>, and <code>_convert_types</code>.
<a href=""https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py"" rel=""noreferrer"">https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py</a></p>

<p>You can always assign the type you want <em>after</em> you have read the file:</p>

<pre><code>df.phone = df.phone.astype(str)
</code></pre>
";;1;;2012-05-28T08:16:28.780;;10781413;2012-05-28T08:16:28.780;;;;;1063605.0;10591000.0;2;12;;;
305;305;;;"<p>Yes, <code>dropna</code>. See <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html"">http://pandas.pydata.org/pandas-docs/stable/missing_data.html</a> and the <code>DataFrame.dropna</code> docstring:</p>

<pre><code>Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)
Docstring:
Return object with labels on given axis omitted where alternately any
or all of the data are missing

Parameters
----------
axis : {0, 1}
how : {'any', 'all'}
    any : if any NA values are present, drop that label
    all : if all values are NA, drop that label
thresh : int, default None
    int value : require that many non-NA values
subset : array-like
    Labels along other axis to consider, e.g. if you are dropping rows
    these would be a list of columns to include

Returns
-------
dropped : DataFrame
</code></pre>

<p>The specific command to run would be:</p>

<pre><code>df=df.dropna(axis=1,how='all')
</code></pre>
";;2;;2012-06-02T04:52:48.933;;10859883;2012-07-24T14:30:10.463;2012-07-24T14:30:10.463;;905596.0;;776560.0;10857924.0;2;73;;;
317;317;;;"<p>It sounds like you don't want reindex.  Somewhat confusingly <code>reindex</code> is not for defining a new index, exactly; rather, it looks for rows that have the specified indices.  So if you have a DataFrame with index <code>[0, 1, 2]</code>, then doing a <code>reindex([2, 1, 0])</code> will return the rows in reverse order.  Doing something like <code>reindex([8, 9, 10])</code> does not make a new index for the rows; rather, it will return a DataFrame with <code>NaN</code> values, since there are no rows with indices 8, 9, or 10.</p>

<p>It seems like what you want is to just keep the same rows, but make a totally new index for them.  For that you can just assign to the index directly.  So try doing <code>df.index = df['dtstamp']</code>.</p>
";;2;;2012-06-08T05:34:25.010;;10943545;2012-06-08T05:34:25.010;;;;;1427416.0;10943478.0;2;21;;;
322;322;;;"<p>Yes; use the <code>.apply(...)</code> function, which will be called on each sub-<code>DataFrame</code>. For example:</p>

<pre><code>grouped = df.groupby(keys)

def wavg(group):
    d = group['data']
    w = group['weights']
    return (d * w).sum() / w.sum()

grouped.apply(wavg)
</code></pre>
";;4;;2012-06-09T22:07:32.317;;10964938;2017-01-04T23:16:48.747;2017-01-04T23:16:48.747;;2336654.0;;776560.0;10951341.0;2;63;;;
325;325;;;"<p>Try this:</p>

<pre><code>pandas.concat([df['foo'].dropna(), df['bar'].dropna()]).reindex_like(df)
</code></pre>

<p>If you want that data to become the new column <code>bar</code>, just assign the result to <code>df['bar']</code>.</p>
";;3;;2012-06-10T21:38:40.860;;10972557;2012-06-10T21:38:40.860;;;;;1427416.0;10972410.0;2;20;;;
329;329;;;"<pre><code>In [18]: a
Out[18]: 
   x1  x2
0   0   5
1   1   6
2   2   7
3   3   8
4   4   9

In [19]: a.x2 = a.x2.shift(1)

In [20]: a
Out[20]: 
   x1  x2
0   0 NaN
1   1   5
2   2   6
3   3   7
4   4   8
</code></pre>
";;1;;2012-06-11T14:35:02.033;;10982198;2012-06-11T14:35:02.033;;;;;449449.0;10982089.0;2;70;;;
336;336;;;"<p>I added a ticket to add an option of some sort here:</p>

<p><a href=""https://github.com/pydata/pandas/issues/1450"">https://github.com/pydata/pandas/issues/1450</a></p>

<p>In the meantime, <code>result.fillna('')</code> should do what you want</p>

<p>EDIT: in the development version (to be 0.8.0 final) if you specify an empty list of <code>na_values</code>, empty strings will stay empty strings in the result</p>
";;4;;2012-06-12T21:33:20.550;;11005208;2012-06-25T22:35:25.977;2012-06-25T22:35:25.977;;776560.0;;776560.0;10867028.0;2;23;;;
355;355;;;"<pre><code>df.reindex_axis(sorted(df.columns), axis=1)
</code></pre>

<p>This assumes that sorting the column names will give the order you want.  If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas.</p>
";;1;;2012-06-16T21:12:21.780;;11067072;2012-06-16T21:12:21.780;;;;;1427416.0;11067027.0;2;138;;;
358;358;;;"<p>You can group on any array/Series of the same length as your DataFrame --- even a computed factor that's not actually a column of the DataFrame.  So to group by minute you can do:</p>

<pre><code>df.groupby(df.index.map(lambda t: t.minute))
</code></pre>

<p>If you want to group by minute and something else, just mix the above with the column you want to use:</p>

<pre><code>df.groupby([df.index.map(lambda t: t.minute), 'Source'])
</code></pre>

<p>Personally I find it useful to just add columns to the DataFrame to store some of these computed things (e.g., a ""Minute"" column) if I want to group by them often, since it makes the grouping code less verbose.</p>

<p>Or you could try something like this:</p>

<pre><code>df.groupby([df['Source'],pd.TimeGrouper(freq='Min')])
</code></pre>
";;3;;2012-06-17T18:56:19.437;;11073962;2015-09-16T13:15:45.697;2015-09-16T13:15:45.697;;2444213.0;;1427416.0;11073609.0;2;33;;;
360;360;;;"<p>Numpy is required by pandas (and by virtually all numerical tools for Python).  Scipy is not strictly required for pandas but is listed as an ""optional dependency"".  I wouldn't say that pandas is an alternative to Numpy and/or Scipy.  Rather, it's an extra tool that provides a more streamlined way of working with numerical and tabular data in Python.  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them.</p>
";;0;;2012-06-18T04:51:10.193;;11077060;2012-06-18T04:51:10.193;;;;;1427416.0;11077023.0;2;36;;;
361;361;;;"<p>Indeed, pandas provides high level data manipulation tools built on top of NumPy. NumPy by itself is a fairly low-level tool, and will be very much similar to using MATLAB. pandas on the other hand provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has become very popular in recent years in financial applications. I will have a chapter dedicated to financial data analysis using pandas in my upcoming book. </p>
";;7;;2012-06-18T05:11:17.927;;11077215;2012-06-18T19:32:13.647;2012-06-18T19:32:13.647;;776560.0;;776560.0;11077023.0;2;160;;;
365;365;;;"<p>If I understand you correctly, you want something like:</p>

<pre><code>(x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)
</code></pre>

<p>This will give the sum of the two dataframes.  If a value is in one dataframe and not the other, the result at that position will be that existing value.  If a value is missing in both dataframes, the result at that position will be zero.</p>

<pre><code>&gt;&gt;&gt; x
   A   B   C
0  1   2 NaN
1  3 NaN   4
&gt;&gt;&gt; y
    A   B   C
0   8 NaN  88
1   2 NaN   5
2  10  11  12
&gt;&gt;&gt; (x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)
    A   B   C
0   9   2  88
1   5   0   9
2  10  11  12
</code></pre>
";;2;;2012-06-19T19:02:15.203;;11107627;2012-06-19T19:02:15.203;;;;;1427416.0;11106823.0;2;6;;;
366;366;;;"<p>How about <code>x.add(y, fill_value=0)</code>?</p>

<pre><code>import pandas as pd

df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])
Out: 
   a  b
0  1  2
1  3  4
2  5  6

df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])
Out: 
     a    b
0  100  200
1  300  400
2  500  600

df_add = df1.add(df2, fill_value=0)
Out: 
     a    b
0  101  202
1  303  404
2  505  606
</code></pre>
";;1;;2012-06-20T03:28:12.140;;11112419;2014-03-11T08:55:02.963;2014-03-11T08:55:02.963;;125617.0;;776560.0;11106823.0;2;45;;;
374;374;;;"<p>As Wes says, io/sql's read_sql will do it, once you've gotten a database connection using a DBI compatible library.  We can look at two short examples using the <code>MySQLdb</code> and <code>cx_Oracle</code> libraries to connect to Oracle and MySQL and query their data dictionaries. Here is the example for <code>cx_Oracle</code>:</p>

<pre><code>import pandas as pd
import cx_Oracle

ora_conn = cx_Oracle.connect('your_connection_string')
df_ora = pd.read_sql('select * from user_objects', con=ora_conn)    
print 'loaded dataframe from Oracle. # Records: ', len(df_ora)
ora_conn.close()
</code></pre>

<p>And here is the equivalent example for <code>MySQLdb</code>:</p>

<pre><code>import MySQLdb
mysql_cn= MySQLdb.connect(host='myhost', 
                port=3306,user='myusername', passwd='mypassword', 
                db='information_schema')
df_mysql = pd.read_sql('select * from VIEWS;', con=mysql_cn)    
print 'loaded dataframe from MySQL. records:', len(df_mysql)
mysql_cn.close()
</code></pre>
";;0;;2012-06-21T12:34:29.960;;11138275;2016-12-29T02:34:16.233;2016-12-29T02:34:16.233;;1676152.0;;1472080.0;10065051.0;2;76;;;
414;414;;;"<p>The column names (which are strings) cannot be sliced in the manner you tried.</p>

<p>Here you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the <code>__getitem__</code> syntax (the []'s).</p>

<pre><code>df1 = df[['a','b']]
</code></pre>

<p>Alternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:</p>

<pre><code>df1 = df.ix[:,0:2] # Remember that Python does not slice inclusive of the ending index.
</code></pre>

<p>Additionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices). </p>

<p>Sometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the <code>copy()</code> function to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.</p>

<pre><code>df1 = df.ix[0,0:2].copy() # To avoid the case where changing df1 also changes df
</code></pre>
";;12;;2012-07-02T02:43:02.740;;11287278;2012-07-02T02:48:05.720;2012-07-02T02:48:05.720;;567620.0;;567620.0;11285613.0;2;490;;;
434;434;;;"<p>Just assign it to the <code>.columns</code> attribute:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})
&gt;&gt;&gt; df.columns = ['a', 'b']
&gt;&gt;&gt; df
   a   b
0  1  10
1  2  20
</code></pre>
";;9;;2012-07-05T14:23:27.910;;11346337;2012-07-05T14:23:27.910;;;;;449449.0;11346283.0;2;768;;;
439;439;;;"<p>Use the rename function and refer the columns to be renamed. Not all the columns have to be renamed:</p>

<pre><code>df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})
# OR
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html</a></p>
";;6;;2012-07-06T01:48:15.147;;11354850;2016-12-09T23:26:45.830;2016-12-09T23:26:45.830;;138163.0;;1505540.0;11346283.0;2;1304;;;
442;442;;;"<p>There is too much data to be displayed on the screen, therefore a summary is displayed instead.</p>

<p>If you want to output the data anyway (it won't probably fit on a screen and does not look very well):</p>

<pre><code>print paramdata.values
</code></pre>

<p>converts the dataframe to its numpy-array matrix representation.</p>

<pre><code>paramdata.columns
</code></pre>

<p>stores the respective column names and</p>

<pre><code>paramdata.index
</code></pre>

<p>stores the respective index (row names).</p>
";;1;;2012-07-06T12:18:26.667;;11362056;2012-07-06T12:18:26.667;;;;;449449.0;11361985.0;2;36;;;
446;446;;;"<p>you can also use <code>DataFrame.head(x)</code> / <code>.tail(x)</code> to display the first / last x rows of the DataFrame.</p>
";;0;;2012-07-06T16:50:17.943;;11366429;2012-07-22T15:53:26.650;2012-07-22T15:53:26.650;;1301710.0;;1342094.0;11361985.0;2;9;;;
447;447;;;"<p>Here's what I ended up doing for partial string matches.  If anyone has a more efficient way of doing this please let me know.</p>

<pre><code>def stringSearchColumn_DataFrame(df, colName, regex):
    newdf = DataFrame()
    for idx, record in df[colName].iteritems():

        if re.search(regex, record):
            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)

    return newdf
</code></pre>
";;1;;2012-07-06T17:08:46.797;;11366706;2012-07-06T17:08:46.797;;;;;1170342.0;11350770.0;2;7;;;
449;449;;;"<p>If you want to add the legend manually, you have to ask the subplot for the elements of the bar plot:</p>

<pre><code>In [17]: ax = x.plot(kind='bar', legend=False)

In [18]: patches, labels = ax.get_legend_handles_labels()

In [19]: ax.legend(patches, labels, loc='best')
Out[19]: &lt;matplotlib.legend.Legend at 0x10b292ad0&gt;
</code></pre>

<p>Also, <code>plt.legend(loc='best')</code> or <code>ax.legend(loc='best')</code> should ""just work"", because there are already ""links"" to the bar plot patches set up when the plot is made, so you don't have to pass a list of axis labels.</p>

<p>I'm not sure if the version of pandas you're using returns a handle to the subplot (<code>ax = ...</code>) but I'm fairly certain that 0.7.3 does. You can always get a reference to it with <code>plt.gca()</code>.</p>
";;2;;2012-07-08T16:16:39.380;;11384667;2012-07-08T16:16:39.380;;;;;776560.0;11348183.0;2;20;;;
454;454;;;"<pre><code>In [39]: df
Out[39]: 
   index  a  b  c
0      1  2  3  4
1      2  3  4  5

In [40]: df1 = df[['b', 'c']]

In [41]: df1
Out[41]: 
   b  c
0  3  4
1  4  5
</code></pre>
";;2;;2012-07-08T17:55:12.493;;11385335;2012-07-08T17:55:12.493;;;;;776560.0;11285613.0;2;35;;;
455;455;;;"<p>You can also do more succinctly:</p>

<p><code>df.sort_index(axis=1)</code></p>

<p><em>Edit</em>:</p>

<p>Make sure you hold the value </p>

<p><code>df = df.sort_index(axis=1)</code></p>

<p>Or do it in place</p>

<p><code>df.sort_index(axis=1, inplace=True)</code></p>
";;3;;2012-07-08T18:56:47.510;;11385780;2017-03-30T16:00:05.367;2017-03-30T16:00:05.367;;1019952.0;;776560.0;11067027.0;2;147;;;
457;457;;;"<p>This should work:</p>

<pre><code>data.groupby(lambda x: data['date'][x].year)
</code></pre>
";;0;;2012-07-09T12:39:29.280;;11395193;2012-07-09T12:39:29.280;;;;;567292.0;11391969.0;2;10;;;
458;458;;;"<p>ecatmur's solution will work fine. This will be better performance on large datasets, though:</p>

<pre><code>data.groupby(data['date'].map(lambda x: x.year))
</code></pre>
";;2;;2012-07-09T14:25:27.513;;11397052;2012-07-09T14:25:27.513;;;;;776560.0;11391969.0;2;57;;;
469;469;;;"<p>If my comment answered your question, my answer does not have to comment on it any more ;-)</p>

<pre><code>pandas.DataFrame(initialload, columns=list_of_column_names)
</code></pre>
";;0;;2012-07-10T14:44:48.687;;11415882;2012-07-10T14:44:48.687;;;;;449449.0;11415701.0;2;17;;;
475;475;;;"<p>Suppose I had a DataFrame as follows:</p>

<pre><code>In [39]: df
Out[39]: 
      mass1     mass2  velocity
0  1.461711 -0.404452  0.722502
1 -2.169377  1.131037  0.232047
2  0.009450 -0.868753  0.598470
3  0.602463  0.299249  0.474564
4 -0.675339 -0.816702  0.799289
</code></pre>

<p>I can use sin and DataFrame.prod to create a boolean mask:</p>

<pre><code>In [40]: mask = (np.sin(df.velocity) / df.ix[:, 0:2].prod(axis=1)) &gt; 0

In [41]: mask
Out[41]: 
0    False
1    False
2    False
3     True
4     True
</code></pre>

<p>Then use the mask to select from the DataFrame:</p>

<pre><code>In [42]: df[mask]
Out[42]: 
      mass1     mass2  velocity
3  0.602463  0.299249  0.474564
4 -0.675339 -0.816702  0.799289
</code></pre>
";;1;;2012-07-10T19:35:29.833;;11420594;2012-07-10T19:35:29.833;;;;;1306530.0;11418192.0;2;9;;;
488;488;;;"<p>You can do this using <code>DataFrame.apply</code>, which applies a function along a given axis,</p>

<pre><code>In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])

In [4]: df
Out[4]: 
          a         b         c
0 -0.001968 -1.877945 -1.515674
1 -0.540628  0.793913 -0.983315
2 -1.313574  1.946410  0.826350
3  0.015763 -0.267860 -2.228350
4  0.563111  1.195459  0.343168

In [6]: df[df.apply(lambda x: x['b'] &gt; x['c'], axis=1)]
Out[6]: 
          a         b         c
1 -0.540628  0.793913 -0.983315
2 -1.313574  1.946410  0.826350
3  0.015763 -0.267860 -2.228350
4  0.563111  1.195459  0.343168
</code></pre>
";;0;;2012-07-13T17:33:12.873;;11475486;2012-07-13T17:33:12.873;;;;;128580.0;11418192.0;2;56;;;
493;493;;;"<p>Looks like Python does not add an intercept by default to your expression, whereas R does when you use the formula interface..</p>

<p>This means you did fit two different models. Try</p>

<pre><code>lm( y ~ x - 1, data)
</code></pre>

<p>in R to exclude the intercept, or in your case and with somewhat more standard notation</p>

<pre><code>lm(num_rx ~ ridageyr - 1, data=demoq)
</code></pre>
";;3;;2012-07-15T19:49:30.727;;11495086;2012-07-15T21:56:15.517;2012-07-15T21:56:15.517;;143305.0;;143305.0;11495051.0;2;15;;;
499;499;;;"<p>Based on github issue <a href=""https://github.com/pydata/pandas/issues/620"" rel=""noreferrer"">#620</a>, it looks like you'll soon be able to do the following:</p>

<pre><code>df[df['A'].str.contains(""hello"")]
</code></pre>

<p>Update: <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html#text-string-methods"" rel=""noreferrer"">vectorized string methods (i.e., Series.str)</a> are available in pandas 0.8.1 and up. </p>
";;5;;2012-07-17T21:52:18.983;;11531402;2015-10-07T19:08:04.023;2015-10-07T19:08:04.023;;2014591.0;;243434.0;11350770.0;2;267;;;
505;505;;;"<p><code>NaN</code> can't be stored in an integer array. This is a known limitation of pandas at the moment; I have been waiting for progress to be made with NA values in NumPy (similar to NAs in R), but it will be at least 6 months to a year before NumPy gets these features, it seems:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na</a></p>
";;7;;2012-07-18T18:43:27.740;;11548224;2012-07-18T18:43:27.740;;;;;776560.0;11548005.0;2;49;;;
514;514;;;"<p>It's hard to infer what you're looking for from the question, but my best guess is as follows.</p>

<p>If we assume you have a DataFrame where some column is 'Category' and contains integers (or otherwise unique identifiers) for categories, then we can do the following.</p>

<p>Call the DataFrame <code>dfrm</code>, and assume that for each row, <code>dfrm['Category']</code> is some value in the set of integers from 1 to N. Then,</p>

<pre><code>for elem in dfrm['Category'].unique():
    dfrm[str(elem)] = dfrm['Category'] == elem
</code></pre>

<p>Now there will be a new indicator column for each category that is True/False depending on whether the data in that row are in that category.</p>

<p>If you want to control the category names, you could make a dictionary, such as</p>

<pre><code>cat_names = {1:'Some_Treatment', 2:'Full_Treatment', 3:'Control'}
for elem in dfrm['Category'].unique():
    dfrm[cat_names[elem]] = dfrm['Category'] == elem
</code></pre>

<p>to result in having columns with specified names, rather than just string conversion of the category values. In fact, for some types, <code>str()</code> may not produce anything useful for you.</p>
";;1;;2012-07-21T02:29:57.203;;11589000;2012-07-21T02:29:57.203;;;;;567620.0;11587782.0;2;12;;;
515;515;;;"<p>With pandas 0.18 the resample API changed (see the <a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/whatsnew.html#resample-api"">docs</a>). 
So for pandas >= 0.18 the answer is:</p>

<pre><code>In [31]: frame.resample('1H').agg({'radiation': np.sum, 'tamb': np.mean})
Out[31]: 
                         tamb   radiation
2012-04-05 08:00:00  5.161235  279.507182
2012-04-05 09:00:00  4.968145  290.941073
2012-04-05 10:00:00  4.478531  317.678285
2012-04-05 11:00:00  4.706206  335.258633
2012-04-05 12:00:00  2.457873    8.655838
</code></pre>

<hr>

<p>Old Answer:</p>

<p>I am answering my question to reflect the time series related changes in <code>pandas &gt;= 0.8</code> (all other answers are outdated).</p>

<p>Using pandas >= 0.8 the answer is:</p>

<pre><code>In [30]: frame.resample('1H', how={'radiation': np.sum, 'tamb': np.mean})
Out[30]: 
                         tamb   radiation
2012-04-05 08:00:00  5.161235  279.507182
2012-04-05 09:00:00  4.968145  290.941073
2012-04-05 10:00:00  4.478531  317.678285
2012-04-05 11:00:00  4.706206  335.258633
2012-04-05 12:00:00  2.457873    8.655838
</code></pre>
";;5;;2012-07-22T19:13:55.277;;11603242;2016-04-08T12:36:22.763;2016-04-08T12:36:22.763;;1301710.0;;1301710.0;10020591.0;2;35;;;
521;521;;;"<p>The newest versions of pandas now include a built-in function for iterating over rows. </p>

<pre><code>for index, row in df.iterrows():

    # do some logic here
</code></pre>

<p>Or, if you want it faster use <code>itertuples()</code></p>

<p>But, unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code. </p>
";;4;;2012-07-23T17:09:39.743;;11617194;2015-07-30T13:27:46.793;2015-07-30T13:27:46.793;;475872.0;;475872.0;7837722.0;2;209;;;
522;522;;;"<p>If you pass a list to <code>index_col</code>, it means you want to create a hierarchical index out of the columns in the list.</p>

<p>In addition, the <code>parse_dates</code> keyword can be set to either True or a list/dict. If True, then it tries to parse individual columns as dates, otherwise it combines columns to parse a single date column.</p>

<p>In summary, what you want to do is:</p>

<pre><code>from datetime import datetime
import pandas as pd
parse = lambda x: datetime.strptime(x, '%Y%m%d %H')
pd.read_csv(""..\\file.csv"",  parse_dates = [['YYYYMMDD', 'HH']], 
            index_col = 0, 
            date_parser=parse)
</code></pre>
";;3;;2012-07-23T17:45:03.773;;11617682;2013-04-03T03:59:26.717;2013-04-03T03:59:26.717;;1307905.0;;1306530.0;11615504.0;2;27;;;
530;530;;;"<p>In principle it shouldn't run out of memory, but there are currently memory problems with <code>read_csv</code> on large files caused by some complex Python internal issues (this is vague but it's been known for a long time: <a href=""http://github.com/pydata/pandas/issues/407"">http://github.com/pydata/pandas/issues/407</a>). </p>

<p>At the moment there isn't a perfect solution (here's a tedious one: you could transcribe the file row-by-row into a pre-allocated NumPy array or memory-mapped file--<code>np.mmap</code>), but it's one I'll be working on in the near future. Another solution is to read the file in smaller pieces (use <code>iterator=True, chunksize=1000</code>) then concatenate then with <code>pd.concat</code>. The problem comes in when you pull the entire text file into memory in one big slurp.</p>
";;3;;2012-07-24T01:10:50.813;;11622769;2012-09-09T02:46:50.043;2012-09-09T02:46:50.043;;776560.0;;776560.0;11622652.0;2;64;;;
534;534;;;"<p>It sounds like maybe you want <code>pandas.concat</code>?  <code>merge</code> and <code>join</code> do, well, joins, which means they will give you something based around the Cartesian product of the two inputs, but it sounds like you just want to paste them together into one big table.</p>

<p>Edit: did you try concat with <code>axis=1</code>?  It seems to do what you're asking for:</p>

<pre><code>&gt;&gt;&gt; print x
          A         B         C
0  0.155614 -0.252148  0.861163
1  0.973517  1.156465 -0.458846
2  2.504356 -0.356371 -0.737842
3  0.012994  1.785123  0.161667
4  0.574578  0.123689  0.017598
&gt;&gt;&gt; print y
         A2        B2        C2
0 -0.280993  1.278750 -0.704449
1  0.140282  1.955322 -0.953826
2  0.581997 -0.239829  2.227069
3 -0.876146 -1.955199 -0.155030
4 -0.518593 -2.630978  0.333264
&gt;&gt;&gt; print pandas.concat([x, y], axis=1)
          A         B         C        A2        B2        C2
0  0.155614 -0.252148  0.861163 -0.280993  1.278750 -0.704449
1  0.973517  1.156465 -0.458846  0.140282  1.955322 -0.953826
2  2.504356 -0.356371 -0.737842  0.581997 -0.239829  2.227069
3  0.012994  1.785123  0.161667 -0.876146 -1.955199 -0.155030
4  0.574578  0.123689  0.017598 -0.518593 -2.630978  0.333264
</code></pre>
";;7;;2012-07-24T18:54:43.090;;11637456;2012-07-24T19:15:57.220;2012-07-24T19:15:57.220;;1427416.0;;1427416.0;11637384.0;2;14;;;
535;535;;;"<p>Does your index have duplicates <code>x.index.is_unique</code>? If so would explain the behavior you're seeing:</p>

<pre><code>In [16]: left
Out[16]: 
            a
2000-01-01  1
2000-01-01  1
2000-01-01  1
2000-01-02  2
2000-01-02  2
2000-01-02  2

In [17]: right
Out[17]: 
            b
2000-01-01  3
2000-01-01  3
2000-01-01  3
2000-01-02  4
2000-01-02  4
2000-01-02  4

In [18]: left.join(right)
Out[18]: 
            a  b
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
</code></pre>
";;1;;2012-07-24T21:10:36.830;;11639358;2012-07-24T21:10:36.830;;;;;776560.0;11637384.0;2;19;;;
537;537;;;"<p>I think this might work:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
from pandas import DataFrame
df = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])

fig, ax = plt.subplots()
ax2, ax3 = ax.twinx(), ax.twinx()
rspine = ax3.spines['right']
rspine.set_position(('axes', 1.25))
ax3.set_frame_on(True)
ax3.patch.set_visible(False)
fig.subplots_adjust(right=0.75)

df.A.plot(ax=ax, style='b-')
df.B.plot(ax=ax2, style='r-', secondary_y=True)
df.C.plot(ax=ax3, style='g-')
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/XEkyp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XEkyp.png"" alt=""Output""></a></p>
";;2;;2012-07-25T06:28:37.937;;11643893;2016-06-08T15:27:46.130;2016-06-08T15:27:46.130;;1424758.0;;1306530.0;11640243.0;2;51;;;
558;558;;;"<p>I checked out <code>iterrows</code> after noticing <a href=""https://stackoverflow.com/users/475872/nick-crawford"">Nick Crawford's</a> answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the <code>itertuples</code> method for my problem, which yields (index, row_value1...) tuples.</p>

<p>There's also <code>iterkv</code>, which iterates through (column, series) tuples.</p>
";;4;;2012-07-29T04:53:26.230;;11706782;2012-07-29T04:53:26.230;2017-05-23T12:02:47.340;;-1.0;;386279.0;7837722.0;2;20;;;
560;560;;;"<p>You can use <code>print df.describe().to_string()</code> to force it to show the whole table.  (You can use <code>to_string()</code> like this for any DataFrame.  The result of <code>describe</code> is just a DataFrame itself.)</p>

<p>The 8 is the number of rows in the DataFrame holding the ""description"" (because <code>describe</code> computes 8 statistics, min, max, mean, etc.).</p>
";;1;;2012-07-29T08:03:35.063;;11707706;2012-07-29T08:03:35.063;;;;;1427416.0;11707586.0;2;15;;;
562;562;;;"<p>Note: when you load the data with pandas it will create a <code>DataFrame</code> object where each column has an homogeneous datatype for all the rows but 2 columns can have distinct datatypes (e.g. integer, dates, strings).</p>

<p>When you pass a <code>DataFrame</code> instance to a scikit-learn model it will first allocate a homogeneous 2D numpy array with dtype np.float32 or np.float64 (depending on the implementation of the models). At this point you will have 2 copies of your dataset in memory.</p>

<p>To avoid this you could write / reuse a CSV parser that directly allocates the data in the internal format / dtype expected by the scikit-learn model. You can try <code>numpy.loadtxt</code> for instance (have a look at the docstring for the parameters).</p>

<p>Also if you data is very sparse (many zero values) it will be better to use a scipy.sparse datastructure and a scikit-learn model that can deal with such an input format (check the docstrings to know). However the CSV format itself is not very well suited for sparse data and I am not sure there exist a direct CSV-to-<code>scipy.sparse</code> parser.</p>

<p><strong>Edit:</strong> for reference KNearestNeighborsClassifer allocate temporary distances array with shape <code>(n_samples_predict, n_samples_train)</code> which is very wasteful when only <code>(n_samples_predict, n_neighbors)</code> is needed instead. This issue can be tracked here:</p>

<p><a href=""https://github.com/scikit-learn/scikit-learn/issues/325"" rel=""noreferrer"">https://github.com/scikit-learn/scikit-learn/issues/325</a></p>
";;3;;2012-07-29T10:47:08.103;;11708610;2012-07-30T12:59:14.030;2012-07-30T12:59:14.030;;163740.0;;163740.0;11707805.0;2;10;;;
563;563;;;"<p>You can adjust pandas print options with <code>set_printoptions</code>.</p>

<pre><code>In [3]: df.describe()
Out[3]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 8 entries, count to max
Data columns:
x1    8  non-null values
x2    8  non-null values
x3    8  non-null values
x4    8  non-null values
x5    8  non-null values
x6    8  non-null values
x7    8  non-null values
dtypes: float64(7)

In [4]: pd.set_printoptions(precision=2)

In [5]: df.describe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      8.0      8.0      8.0      8.0      8.0      8.0      8.0
mean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5
std       17.1     17.1     17.1     17.1     17.1     17.1     17.1
min    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0
25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2
50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5
75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8
max    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0
</code></pre>

<p>However this will not work in all cases as pandas detects your console width and it will only use <code>to_string</code> if the output fits in the console (see the docstring of <code>set_printoptions</code>). 
In this case you can explicitly call <code>to_string</code> as answered by <a href=""https://stackoverflow.com/a/11707706/1301710"">BrenBarn</a>.</p>

<p><strong>Update</strong></p>

<p>With version 0.10 the way wide dataframes are printed <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#wide-dataframe-print"" rel=""nofollow noreferrer"">changed</a>:</p>

<pre><code>In [3]: df.describe()
Out[3]: 
                 x1            x2            x3            x4            x5  \
count      8.000000      8.000000      8.000000      8.000000      8.000000   
mean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   
std    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   
min    31906.695474   1648.359160     56.378115  16278.322271     43.745574   
25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   
50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   
75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   
max    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   

                 x6            x7  
count      8.000000      8.000000  
mean   41863.000717  33950.235126  
std    38709.468281  29075.745673  
min     3590.990740   1833.464154  
25%    15145.759625   6879.523949  
50%    22139.243042  33706.029946  
75%    72038.983496  51449.893980  
max    98601.190488  83309.051963  
</code></pre>

<p>Further more the API for setting pandas options changed:</p>

<pre><code>In [4]: pd.set_option('display.precision', 2)

In [5]: df.describe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      8.0      8.0      8.0      8.0      8.0      8.0      8.0
mean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2
std    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7
min    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5
25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5
50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0
75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9
max    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1
</code></pre>
";;2;;2012-07-29T10:56:01.317;;11708664;2013-01-10T10:06:59.090;2017-05-23T11:55:19.683;;-1.0;;1301710.0;11707586.0;2;21;;;
564;564;;;"<p>As @bmu <a href=""https://stackoverflow.com/a/11708664/623735"">mentioned</a>, pandas auto detects (by default) the size of the display area, a summary view will be used when an object repr does not fit on the display. You mentioned resizing the IDLE window, to no effect. If you do <code>print df.describe().to_string()</code> does it fit on the IDLE window?</p>

<p>The terminal size is determined by <code>pandas.util.terminal.get_terminal_size()</code>, this returns a tuple containing the <code>(width, height)</code> of the display. Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs).</p>

<p>Note that it is possible to bypass the autodetect, <code>pandas.set_printoptions(max_rows=200, max_columns=10)</code> will never switch to summary view if number of rows, columns does not exceed the given limits.</p>

<hr>

<p><strong>Update: Pandas 0.11.0 onwards</strong></p>

<p><code>pandas.set_printoptions(...)</code> is depracted. Instead, use <code>pandas.set_option</code>. Like:</p>

<pre><code>import pandas as pd
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
</code></pre>

<p>Here is the help:</p>

<pre>
set_option(pat,value) - Sets the value of the specified option

Available options:
display.[chop_threshold, colheader_justify, column_space, date_dayfirst,
         date_yearfirst, encoding, expand_frame_repr, float_format, height,
         line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,
         max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,
         pprint_nest_depth, precision, width]
mode.[sim_interactive, use_inf_as_null]

Parameters
----------
pat - str/regexp which should match a single option.

Note: partial matches are supported for convenience, but unless you use the
full option name (e.g. x.y.z.option_name), your code may break in future
versions if new options with similar names are introduced.

value - new value of option.

Returns
-------
None

Raises
------
KeyError if no such option exists

display.chop_threshold: [default: None] [currently: None]
: float or None
        if set to a float value, all float values smaller then the given threshold
        will be displayed as exactly 0 by repr and friends.
display.colheader_justify: [default: right] [currently: right]
: 'left'/'right'
        Controls the justification of column headers. used by DataFrameFormatter.
display.column_space: [default: 12] [currently: 12]No description available.

display.date_dayfirst: [default: False] [currently: False]
: boolean
        When True, prints and parses dates with the day first, eg 20/01/2005
display.date_yearfirst: [default: False] [currently: False]
: boolean
        When True, prints and parses dates with the year first, eg 2005/01/20
display.encoding: [default: UTF-8] [currently: UTF-8]
: str/unicode
        Defaults to the detected encoding of the console.
        Specifies the encoding to be used for strings returned by to_string,
        these are generally strings meant to be displayed on the console.
display.expand_frame_repr: [default: True] [currently: True]
: boolean
        Whether to print out the full DataFrame repr for wide DataFrames
        across multiple lines, `max_columns` is still respected, but the output will
        wrap-around across multiple ""pages"" if it's width exceeds `display.width`.
display.float_format: [default: None] [currently: None]
: callable
        The callable should accept a floating point number and return
        a string with the desired format of the number. This is used
        in some places like SeriesFormatter.
        See core.format.EngFormatter for an example.
display.height: [default: 60] [currently: 1000]
: int
        Deprecated.
        (Deprecated, use `display.height` instead.)

display.line_width: [default: 80] [currently: 1000]
: int
        Deprecated.
        (Deprecated, use `display.width` instead.)

display.max_columns: [default: 20] [currently: 500]
: int
        max_rows and max_columns are used in __repr__() methods to decide if
        to_string() or info() is used to render an object to a string.  In case
        python/IPython is running in a terminal this can be set to 0 and pandas
        will correctly auto-detect the width the terminal and swap to a smaller
        format in case all columns would not fit vertically. The IPython notebook,
        IPython qtconsole, or IDLE do not run in a terminal and hence it is not
        possible to do correct auto-detection.
        'None' value means unlimited.
display.max_colwidth: [default: 50] [currently: 50]
: int
        The maximum width in characters of a column in the repr of
        a pandas data structure. When the column overflows, a ""...""
        placeholder is embedded in the output.
display.max_info_columns: [default: 100] [currently: 100]
: int
        max_info_columns is used in DataFrame.info method to decide if
        per column information will be printed.
display.max_info_rows: [default: 1690785] [currently: 1690785]
: int or None
        max_info_rows is the maximum number of rows for which a frame will
        perform a null check on its columns when repr'ing To a console.
        The default is 1,000,000 rows. So, if a DataFrame has more
        1,000,000 rows there will be no null check performed on the
        columns and thus the representation will take much less time to
        display in an interactive session. A value of None means always
        perform a null check when repr'ing.
display.max_rows: [default: 60] [currently: 500]
: int
        This sets the maximum number of rows pandas should output when printing
        out various output. For example, this value determines whether the repr()
        for a dataframe prints out fully or just a summary repr.
        'None' value means unlimited.
display.max_seq_items: [default: None] [currently: None]
: int or None

        when pretty-printing a long sequence, no more then `max_seq_items`
        will be printed. If items are ommitted, they will be denoted by the addition
        of ""..."" to the resulting string.

        If set to None, the number of items to be printed is unlimited.
display.mpl_style: [default: None] [currently: None]
: bool

        Setting this to 'default' will modify the rcParams used by matplotlib
        to give plots a more pleasing visual style by default.
        Setting this to None/False restores the values to their initial value.
display.multi_sparse: [default: True] [currently: True]
: boolean
        ""sparsify"" MultiIndex display (don't display repeated
        elements in outer levels within groups)
display.notebook_repr_html: [default: True] [currently: True]
: boolean
        When True, IPython notebook will use html representation for
        pandas objects (if it is available).
display.pprint_nest_depth: [default: 3] [currently: 3]
: int
        Controls the number of nested levels to process when pretty-printing
display.precision: [default: 7] [currently: 7]
: int
        Floating point output precision (number of significant digits). This is
        only a suggestion
display.width: [default: 80] [currently: 1000]
: int
        Width of the display in characters. In case python/IPython is running in
        a terminal this can be set to None and pandas will correctly auto-detect the
        width.
        Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a
        terminal and hence it is not possible to correctly detect the width.
mode.sim_interactive: [default: False] [currently: False]
: boolean
        Whether to simulate interactive mode for purposes of testing
mode.use_inf_as_null: [default: False] [currently: False]
: boolean
        True means treat None, NaN, INF, -INF as null (old way),
        False means None and NaN are null, but INF, -INF are not null
        (new way).
Call def:   pd.set_option(self, *args, **kwds)
</pre>
";;8;;2012-07-29T18:02:02.687;;11711637;2015-07-14T19:24:41.040;2017-05-23T11:33:24.947;;-1.0;;1548051.0;11707586.0;2;228;;;
590;590;;;"<p>Simplest way is probably <code>list(dt.T.itertuples())</code> (where <code>dt</code> is your dataframe).  This generates a list of tuples.</p>
";;3;;2012-08-04T19:31:03.777;;11811425;2012-08-04T19:31:03.777;;;;;1427416.0;11811392.0;2;34;;;
605;605;;;"<p>We added an <code>append</code> option to <code>set_index</code>. Try that.</p>

<p>The command is:</p>

<pre><code>df.set_index(['d'], append=True)
</code></pre>

<p>(we don't need to specify ['a', 'b'], as they already are in the index and we're appending to them)</p>
";;0;;2012-08-08T02:50:06.007;;11856979;2014-05-14T08:01:27.177;2014-05-14T08:01:27.177;;544059.0;;776560.0;11040626.0;2;22;;;
607;607;;;"<p><code>df['bar'] = df.bar.map(str) + "" is "" + df.foo</code>.</p>
";;0;;2012-08-08T06:03:51.577;;11858532;2012-08-08T06:03:51.577;;;;;1427416.0;11858472.0;2;74;;;
611;611;;;"<p>I'm not entirely sure what you want, and your last line of code does not help either, but anyway:</p>

<p>""Chained"" filtering is done by ""chaining"" the criteria in the boolean index.</p>

<pre><code>In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(df.A == 1) &amp; (df.D == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6
</code></pre>

<p>If you want to chain methods, you can add your own mask method and use that one.</p>

<pre><code>In [90]: def mask(df, key, value):
   ....:     return df[df[key] == value]
   ....:

In [92]: pandas.DataFrame.mask = mask

In [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))

In [95]: df.ix['d','A'] = df.ix['a', 'A']

In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [97]: df.mask('A', 1)
Out[97]:
   A  B  C  D
a  1  4  9  1
d  1  3  9  6

In [98]: df.mask('A', 1).mask('D', 6)
Out[98]:
   A  B  C  D
d  1  3  9  6
</code></pre>
";;3;;2012-08-08T20:10:43.290;;11872393;2016-12-20T15:23:26.917;2016-12-20T15:23:26.917;;918.0;;1548051.0;11869910.0;2;186;;;
612;612;;;"<p>The problem in your code is that you want to apply the operation on every row. The way you've written it though takes the whole 'bar' and 'foo' columns, converts them to strings and gives you back one big string. You can write it like:</p>

<pre><code>df.apply(lambda x:'%s is %s' % (x['bar'],x['foo']),axis=1)
</code></pre>

<p>It's longer than the other answer but is more generic (can be used with values that are not strings).</p>
";;0;;2012-08-08T23:15:47.073;;11874590;2014-05-16T22:58:47.560;2014-05-16T22:58:47.560;;243238.0;;243238.0;11858472.0;2;30;;;
615;615;;;"<pre><code>In [36]: df
Out[36]:
   A  B  C  D
a  0  2  6  0
b  6  1  5  2
c  0  2  6  0
d  9  3  2  2

In [37]: rows
Out[37]: ['a', 'c']

In [38]: df.drop(rows)
Out[38]:
   A  B  C  D
b  6  1  5  2
d  9  3  2  2

In [39]: df[~((df.A == 0) &amp; (df.B == 2) &amp; (df.C == 6) &amp; (df.D == 0))]
Out[39]:
   A  B  C  D
b  6  1  5  2
d  9  3  2  2

In [40]: df.ix[rows]
Out[40]:
   A  B  C  D
a  0  2  6  0
c  0  2  6  0

In [41]: df[((df.A == 0) &amp; (df.B == 2) &amp; (df.C == 6) &amp; (df.D == 0))]
Out[41]:
   A  B  C  D
a  0  2  6  0
c  0  2  6  0
</code></pre>
";;2;;2012-08-09T11:26:08.873;;11882354;2012-08-09T11:26:08.873;;;;;1548051.0;11881165.0;2;30;;;
626;626;;;"<p>The answer from @lodagro is great. I would extend it by generalizing the mask function as:</p>

<pre><code>def mask(df, f):
  return df[f(df)]
</code></pre>

<p>Then you can do stuff like:</p>

<pre><code>df.mask(lambda x: x[0] &lt; 0).mask(lambda x: x[1] &gt; 0)
</code></pre>
";;2;;2012-08-09T23:20:59.093;;11893375;2012-08-09T23:20:59.093;;;;;243238.0;11869910.0;2;49;;;
635;635;;;"<p>You can specify the <code>color</code> option as a list directly to the <code>plot</code> function.</p>

<pre><code>from matplotlib import pyplot as plt
from itertools import cycle, islice
import pandas, numpy as np  # I find np.random.randint to be better

# Make the data
x = [{i:np.random.randint(1,5)} for i in range(10)]
df = pandas.DataFrame(x)

# Make a list by cycling through the colors you care about
# to match the length of your data.
my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))

# Specify this list of colors as the `color` option to `plot`.
df.plot(kind='bar', stacked=True, color=my_colors)
</code></pre>

<p>To define your own custom list, you can do a few of the following, or just look up the Matplotlib techniques for defining a color item by its RGB values, etc. You can get as complicated as you want with this.</p>

<pre><code>my_colors = ['g', 'b']*5 # &lt;-- this concatenates the list to itself 5 times.
my_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # &lt;-- make two custom RGBs and repeat/alternate them over all the bar elements.
my_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # &lt;-- Quick gradient example along the Red/Green dimensions.
</code></pre>

<p>The last example yields the follow simple gradient of colors for me:</p>

<p><img src=""https://i.stack.imgur.com/D0hRr.png"" alt=""enter image description here""></p>

<p>I didn't play with it long enough to figure out how to force the legend to pick up the defined colors, but I'm sure you can do it.</p>

<p>In general, though, a big piece of advice is to just use the functions from Matplotlib directly. Calling them from Pandas is OK, but I find you get better options and performance calling them straight from Matplotlib.</p>
";;8;;2012-08-13T03:39:34.913;;11927922;2012-12-20T13:17:50.547;2012-12-20T13:17:50.547;;567620.0;;567620.0;11927715.0;2;61;;;
639;639;;;"<p>Syntax like the following will work:</p>

<pre><code>df.ix['a']
df.ix['a'].ix['c']
</code></pre>

<p>since <code>group1</code> and <code>group2</code> are indices. Please forgive my previous attempt!</p>

<p>To get at the second index only, I think you have to swap indices:</p>

<pre><code>df.swaplevel(0,1).ix['c']
</code></pre>

<p>But I'm sure Wes will correct me if I'm wrong.</p>
";;10;;2012-08-13T20:26:38.163;;11941772;2012-08-13T20:49:22.077;2012-08-13T20:49:22.077;;429726.0;;429726.0;11941492.0;2;9;;;
640;640;;;"<p>Try using <code>xs</code> to be very precise:</p>

<pre><code>In [5]: df.xs('a', level=0)
Out[5]: 
        value1  value2
group2                
c          1.1     7.1
c          2.0     8.0
d          3.0     9.0

In [6]: df.xs('c', level='group2')
Out[6]: 
        value1  value2
group1                
a          1.1     7.1
a          2.0     8.0
</code></pre>
";;9;;2012-08-13T21:37:08.807;;11942697;2012-08-13T21:37:08.807;;;;;776560.0;11941492.0;2;47;;;
658;658;;;"<pre><code>In [5]: a.reset_index().merge(b, how=""left"").set_index('index')
Out[5]:
       col1  to_merge_on  col2
index
a         1            1     1
b         2            3     2
c         3            4   NaN
</code></pre>
";;5;;2012-08-16T07:53:01.620;;11982843;2012-08-16T07:53:01.620;;;;;1548051.0;11976503.0;2;50;;;
671;671;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head</a></p>

<pre><code>df2 = df.head(10)
</code></pre>

<p>should do the trick</p>
";;0;;2012-08-18T20:27:58.567;;12022003;2012-08-18T20:27:58.567;;;;;521586.0;12021754.0;2;27;;;
672;672;;;"<p>I think there's just a missing <code>\</code> in the docs (maybe because it was interpreted as an escape marker at some point?)  It's a regexp, after all:</p>

<pre><code>In [68]: data = read_table('sample.txt', skiprows=3, header=None, sep=r""\s*"")

In [69]: data
Out[69]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 7 entries, 0 to 6
Data columns:
X.1     7  non-null values
X.2     7  non-null values
X.3     7  non-null values
X.4     7  non-null values
X.5     7  non-null values
X.6     7  non-null values
[...]
X.23    7  non-null values
X.24    7  non-null values
X.25    5  non-null values
X.26    3  non-null values
dtypes: float64(8), int64(10), object(8)
</code></pre>

<p>Because of the delimiter problem noted by @MRAB, it has some trouble with the last few columns:</p>

<pre><code>In [73]: data.ix[:,20:]
Out[73]: 
   X.21  X.22           X.23                   X.24            X.25    X.26
0   315  0.95            ABC            transporter   transmembrane  region
1   527  0.93            ABC            transporter            None    None
2   408  0.86  RecF/RecN/SMC                      N        terminal  domain
3   575  0.85  RecF/RecN/SMC                      N        terminal  domain
4   556  0.72            AAA                 ATPase          domain    None
5   275  0.85      YceG-like                 family            None    None
6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None
</code></pre>

<p>but that can be patched up at the end.</p>
";;0;;2012-08-18T20:35:00.870;;12022047;2012-08-18T20:35:00.870;;;;;487339.0;12021730.0;2;18;;;
675;675;;;"<p><code>df.ix[10,:]</code> gives you all the columns from the 10th row. In your case you want everything up to the 10th row which is <code>df.ix[:9,:]</code>. Note that the right end of the slice range is inclusive: <a href=""http://pandas.sourceforge.net/gotchas.html#endpoints-are-inclusive"" rel=""noreferrer"">http://pandas.sourceforge.net/gotchas.html#endpoints-are-inclusive</a></p>
";;0;;2012-08-19T09:02:03.070;;12025395;2012-08-19T09:02:03.070;;;;;243238.0;12021754.0;2;9;;;
682;682;;;"<p>I use <code>QTableWidget</code> from PyQt to display a <code>DataFrame</code>. I create a <code>QTableWidgetObject</code> and then populate with <code>QTableWidgetItems</code> created with <code>DataFrame</code> values.
Following is the snippet of code that reads a CSV file ,create a <code>DataFrame</code>, then display in a GUI:</p>

<pre><code>df  = read_csv(filename, index_col = 0,header = 0)
self.datatable = QtGui.QTableWidget(parent=self)
self.datatable.setColumnCount(len(df.columns))
self.datatable.setRowCount(len(df.index))
for i in range(len(df.index)):
    for j in range(len(df.columns)):
        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))
</code></pre>
";;0;;2012-08-20T11:30:18.340;;12036847;2012-08-20T13:28:26.187;2012-08-20T13:28:26.187;;789649.0;;1319128.0;10636024.0;2;11;;;
691;691;;;"<h3>Edit 2014-09-30:</h3>

<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>

<h3>Original answer:</h3>

<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>

<pre><code>import decimal

import pydobc
import numpy as np
import pandas

cnn, cur = myConnectToDBfunction()
cmd = ""SELECT * FROM myTable""
cur.execute(cmd)
dataframe = __processCursor(cur, dataframe=True)

def __processCursor(cur, dataframe=False, index=None):
    '''
    Processes a database cursor with data on it into either
    a structured numpy array or a pandas dataframe.

    input:
    cur - a pyodbc cursor that has just received data
    dataframe - bool. if false, a numpy record array is returned
                if true, return a pandas dataframe
    index - list of column(s) to use as index in a pandas dataframe
    '''
    datatypes = []
    colinfo = cur.description
    for col in colinfo:
        if col[1] == unicode:
            datatypes.append((col[0], 'U%d' % col[3]))
        elif col[1] == str:
            datatypes.append((col[0], 'S%d' % col[3]))
        elif col[1] in [float, decimal.Decimal]:
            datatypes.append((col[0], 'f4'))
        elif col[1] == datetime.datetime:
            datatypes.append((col[0], 'O4'))
        elif col[1] == int:
            datatypes.append((col[0], 'i4'))

    data = []
    for row in cur:
        data.append(tuple(row))

    array = np.array(data, dtype=datatypes)
    if dataframe:
        output = pandas.DataFrame.from_records(array)

        if index is not None:
            output = output.set_index(index)

    else:
        output = array

    return output
</code></pre>
";;3;;2012-08-21T14:20:22.593;;12056933;2015-09-28T21:00:38.807;2015-09-28T21:00:38.807;;1552748.0;;1552748.0;12047193.0;2;16;;;
693;693;;;"<p>Here's the shortest code that will do the job:</p>

<pre><code>from pandas import DataFrame
df = DataFrame(resoverall.fetchall())
df.columns = resoverall.keys()
</code></pre>

<p>You can go fancier and parse the types as in Paul's answer.</p>
";;1;;2012-08-21T18:28:30.690;;12060886;2012-08-21T18:28:30.690;;;;;243238.0;12047193.0;2;46;;;
696;696;;;"<p>Use the <code>isin</code> method.  <code>rpt[rpt['STK_ID'].isin(stk_list)]</code>.</p>
";;12;;2012-08-22T03:21:12.067;;12065904;2012-08-22T03:21:12.067;;;;;1427416.0;12065885.0;2;326;;;
701;701;;;"<p>I'm currently in the middle of transition to Pandas DataFrames from the various Numpy arrays. This has been relatively painless since Pandas, AFAIK, if built largely on top of Numpy. What I mean by that is that <code>.mean()</code>, <code>.sum()</code> etc all work as you would hope. On top of that, the ability to add a hierarchical index and use the <code>.ix[]</code> (index) attribute and <code>.xs()</code> (cross-section) method to pull out arbitray pieces of the data has greatly improved the readability and performance of my code (mainly by reducing the number of round-trips to my database).</p>

<p>One thing I haven't fully investigated yet is Pandas compatibility with the more advanced functionality of Scipy and Matplotlib. However, in case of any issues, it's easy enough to pull out a single column that behaves enough like an array for those libraries to work, or even convert to an array on the fly. A DataFrame's plotting methods, for instance, rely on matplotlib and take care of any conversion for you.</p>

<p>Also, if you're like me and your main use of Scipy is the statistics module, pystatsmodels is quickly maturing and relies heavily on pandas. </p>

<p>That's my two cents' worth</p>
";;0;;2012-08-22T16:48:03.987;;12077782;2013-07-09T18:14:44.640;2013-07-09T18:14:44.640;;1552748.0;;1552748.0;12052067.0;2;6;;;
710;710;;;"<p>This is indeed a duplicate of <a href=""https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in"">how to filter the dataframe rows of pandas by ""within""/""in""?</a>, translating the response to your example gives:</p>

<pre><code>In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})

In [6]: df
Out[6]:
   A  B
0  5  1
1  6  2
2  3  3
3  4  5

In [7]: df[df['A'].isin([3, 6])]
Out[7]:
   A  B
1  6  2
2  3  3
</code></pre>
";;6;;2012-08-23T19:20:12.243;;12098586;2012-08-23T19:20:12.243;2017-05-23T12:10:43.057;;-1.0;;1548051.0;12096252.0;2;363;;;
713;713;;;"<p>I'm not sure I follow you, but do you use <code>DataFrame.ix</code> to select/set individual elements:</p>

<pre><code>In [79]: M
Out[79]: 
        one       two     three      four
a -0.277981  1.500188 -0.876751 -0.389292
b -0.705835  0.108890 -1.502786 -0.302773
c  0.880042 -0.056620 -0.550164 -0.409458
d  0.704202  0.619031  0.274018 -1.755726

In [75]: M.ix[0]
Out[75]: 
one     -0.277981
two      1.500188
three   -0.876751
four    -0.389292
Name: a

In [78]: M.ix[0,0]
Out[78]: -0.27798082190723405

In [81]: M.ix[0,0] = 1.0

In [82]: M
Out[82]: 
        one       two     three      four
a  1.000000  1.500188 -0.876751 -0.389292
b -0.705835  0.108890 -1.502786 -0.302773
c  0.880042 -0.056620 -0.550164 -0.409458
d  0.704202  0.619031  0.274018 -1.755726

In [84]: M.ix[(0,1),(0,1)] = 1

In [85]: M
Out[85]: 
        one       two     three      four
a  1.000000  1.000000 -0.876751 -0.389292
b  1.000000  1.000000 -1.502786 -0.302773
c  0.880042 -0.056620 -0.550164 -0.409458
d  0.704202  0.619031  0.274018 -1.755726
</code></pre>

<p>You can also slice by indices:</p>

<pre><code>In [98]: M.ix[""a"":""c"",""one""] = 2.0

In [99]: M
Out[99]: 
        one       two     three      four
a  2.000000  1.000000 -0.876751 -0.389292
b  2.000000  1.000000 -1.502786 -0.302773
c  2.000000 -0.056620 -0.550164 -0.409458
d  0.704202  0.619031  0.274018 -1.755726
</code></pre>
";;4;;2012-08-23T21:54:44.830;;12100543;2012-12-30T18:18:41.947;2012-12-30T18:18:41.947;;1240268.0;;1443118.0;12100497.0;2;17;;;
719;719;;;"<p>I think your best bet is to read the data in as a record array first using numpy.</p>

<pre><code># what you described:
In [15]: import numpy as np
In [16]: import pandas
In [17]: x = pandas.read_csv('weird.csv')

In [19]: x.dtypes
Out[19]: 
int_field            int64
floatlike_field    float64  # what you don't want?
str_field           object

In [20]: datatypes = [('int_field','i4'),('floatlike','S10'),('strfield','S10')]

In [21]: y_np = np.loadtxt('weird.csv', dtype=datatypes, delimiter=',', skiprows=1)

In [22]: y_np
Out[22]: 
array([(1, '2.31', 'one'), (2, '3.12', 'two'), (3, '1.32', 'three ')], 
      dtype=[('int_field', '&lt;i4'), ('floatlike', '|S10'), ('strfield', '|S10')])

In [23]: y_pandas = pandas.DataFrame.from_records(y_np)

In [25]: y_pandas.dtypes
Out[25]: 
int_field     int64
floatlike    object  # better?
strfield     object
</code></pre>
";;1;;2012-08-24T22:15:41.020;;12117333;2012-08-24T22:15:41.020;;;;;1552748.0;12101113.0;2;7;;;
722;722;;;"<p>Try using <code>order</code>, i.e. <code>means = means.order()</code></p>
";;4;;2012-08-26T19:31:37.803;;12133235;2012-08-26T19:31:37.803;;;;;776560.0;12133075.0;2;27;;;
731;731;;;"<p><code>pandas</code> has a <code>replace</code> method too:</p>

<pre><code>In [25]: df = DataFrame({1: [2,3,4], 2: [3,4,5]})

In [26]: df
Out[26]: 
   1  2
0  2  3
1  3  4
2  4  5

In [27]: df[2]
Out[27]: 
0    3
1    4
2    5
Name: 2

In [28]: df[2].replace(4, 17)
Out[28]: 
0     3
1    17
2     5
Name: 2

In [29]: df[2].replace(4, 17, inplace=True)
Out[29]: 
0     3
1    17
2     5
Name: 2

In [30]: df
Out[30]: 
   1   2
0  2   3
1  3  17
2  4   5
</code></pre>

<p>or you could use <code>numpy</code>-style advanced indexing:</p>

<pre><code>In [47]: df[1]
Out[47]: 
0    2
1    3
2    4
Name: 1

In [48]: df[1] == 4
Out[48]: 
0    False
1    False
2     True
Name: 1

In [49]: df[1][df[1] == 4]
Out[49]: 
2    4
Name: 1

In [50]: df[1][df[1] == 4] = 19

In [51]: df
Out[51]: 
    1   2
0   2   3
1   3  17
2  19   5
</code></pre>
";;5;;2012-08-28T04:25:10.727;;12152759;2012-08-28T04:45:23.173;2012-08-28T04:45:23.173;;487339.0;;487339.0;12152716.0;2;27;;;
738;738;;;"<p>You can get the maximum like this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({""A"": [1,2,3], ""B"": [-2, 8, 1]})
&gt;&gt;&gt; df
   A  B
0  1 -2
1  2  8
2  3  1
&gt;&gt;&gt; df[[""A"", ""B""]]
   A  B
0  1 -2
1  2  8
2  3  1
&gt;&gt;&gt; df[[""A"", ""B""]].max(axis=1)
0    1
1    8
2    3
</code></pre>

<p>and so:</p>

<pre><code>&gt;&gt;&gt; df[""C""] = df[[""A"", ""B""]].max(axis=1)
&gt;&gt;&gt; df
   A  B  C
0  1 -2  1
1  2  8  8
2  3  1  3
</code></pre>

<p>If you know that ""A"" and ""B"" are the only columns, you could even get away with</p>

<pre><code>&gt;&gt;&gt; df[""C""] = df.max(axis=1)
</code></pre>

<p>And you could use <code>.apply(max, axis=1)</code> too, I guess.</p>
";;0;;2012-08-29T00:27:33.543;;12169357;2016-12-04T19:06:43.250;2016-12-04T19:06:43.250;;138163.0;;487339.0;12169170.0;2;32;;;
739;739;;;"<p>How about:</p>

<pre><code>df['new_col'] = range(1, len(df) + 1)
</code></pre>

<p>Alternatively if you want the index to be the ranks and store the original index as a column:</p>

<pre><code>df = df.reset_index()
</code></pre>
";;1;;2012-08-29T03:13:24.517;;12170403;2012-08-29T03:13:24.517;;;;;1306530.0;12168648.0;2;36;;;
745;745;;;"<p>The documentation explains this clearly. The apply method accepts a python function which should have a single parameter. If you want to pass more parameters you should use <code>functools.partial</code> as suggested by Joel Cornett in his comment.</p>

<p>An example:</p>

<pre><code>&gt;&gt;&gt; import functools
&gt;&gt;&gt; import operator
&gt;&gt;&gt; add_3 = functools.partial(operator.add,3)
&gt;&gt;&gt; add_3(2)
5
&gt;&gt;&gt; add_3(7)
10
</code></pre>

<p>You can also pass keyword arguments using <code>partial</code>.</p>

<p>Another way would be to create a lambda:</p>

<pre><code>my_series.apply((lambda x: your_func(a,b,c,d,...,x)))
</code></pre>

<p>But I think using <code>partial</code> is better.</p>

<hr>

<p>Note that newer versions of pandas <em>do</em> allow you to pass extra arguments (see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html#pandas.Series.apply"" rel=""noreferrer"">new documentation</a>). So now you can do:</p>

<pre><code>my_series.apply(your_function, args=(2,3,4), extra_kw=1)
</code></pre>

<p>The positional arguments are added <em>after</em> the element of the series.</p>
";;3;;2012-08-29T17:36:10.973;;12183507;2016-02-12T15:56:15.783;2016-02-12T15:56:15.783;;487890.0;;510937.0;12182744.0;2;65;;;
749;749;;;"<p>pandas's DataFrame is a high level tool while structured arrays are a very low-level tool, enabling you to interpret a binary blob of data as a table-like structure. One thing that is hard to do in pandas is nested data types with the same semantics as structured arrays, though this can be imitated with hierarchical indexing (structured arrays can't do most things you can do with hierarchical indexing).</p>

<p>Structured arrays are also amenable to working with massive tabular data sets loaded via memory maps (<code>np.memmap</code>). This is a limitation that will be addressed in pandas eventually, though.</p>
";;0;;2012-08-29T18:53:08.047;;12184679;2012-08-29T18:53:08.047;;;;;776560.0;12052067.0;2;11;;;
758;758;;;"<p>What version of pandas are you using? For me your code works fine (i`m on git master).</p>

<p>Another approach could be:</p>

<pre><code>In [117]: import pandas

In [118]: import random

In [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))

In [120]: rows = random.sample(df.index, 10)

In [121]: df_10 = df.ix[rows]

In [122]: df_90 = df.drop(rows)
</code></pre>

<p>Newer version (from 0.16.1 on) supports this directly: 
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html</a></p>
";;5;;2012-08-30T07:36:18.530;;12192021;2015-06-19T06:17:16.833;2015-06-19T06:17:16.833;;1548051.0;;1548051.0;12190874.0;2;74;;;
762;762;;;"<p>Wes is of course right! I'm just chiming in to provide a little more complete example code. I had the same issue with a 129 Mb file, which was solved by:</p>

<pre><code>from pandas import *

tp = read_csv('large_dataset.csv', iterator=True, chunksize=1000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.
df = concat(tp, ignore_index=True)  # df is DataFrame. If errors, do `list(tp)` instead of `tp`
</code></pre>
";;11;;2012-08-30T08:57:26.267;;12193309;2016-05-02T17:25:58.293;2016-05-02T17:25:58.293;;5076471.0;;709479.0;11622652.0;2;66;;;
768;768;;;"<pre><code>In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})

In [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')
Out[98]:
           A         B  month       A_r
0  -0.040710  0.182269      0 -0.331816
1  -0.004867  0.642243      1  2.448232
2  -0.162191  0.442338      4  2.045909
3  -0.979875  1.367018      5 -2.736399
4  -1.126198  0.338946      5 -2.736399
5  -0.992209 -1.343258      1  2.448232
6  -1.450310  0.021290      0 -0.331816
7  -0.675345 -1.359915      9  2.722156
</code></pre>
";;7;;2012-08-30T16:47:17.243;;12201723;2012-09-08T20:26:16.633;2012-09-08T20:26:16.633;;1548051.0;;1548051.0;12200693.0;2;30;;;
770;770;;;"<p>Inspecting frame.py, it looks like pandas tries to insert a column 'index' or 'level_0'.  If either/both(??) of them are already taken, then it throws the error.</p>

<p>Fortunately, there's a ""drop"" option.  AFAICT, this drops an existing index with the same name and replaces it with the new, reset index.  This might get you in trouble if you have a column named ""index,"" but I think otherwise you're okay.</p>

<p>""Fixed"" code:</p>

<pre><code>import pandas
A = pandas.DataFrame({
    'val' :  ['aaaaa', 'acaca', 'ddddd', 'zzzzz'],
    'extra' : range(10,14),
})
A = A.reset_index(drop=True)
A = A.reset_index(drop=True)
A = A.reset_index(drop=True)
</code></pre>
";;1;;2012-08-30T19:52:44.037;;12204428;2012-08-30T19:52:44.037;;;;;660664.0;12203901.0;2;21;;;
772;772;;;"<p>Maybe <code>.value_counts()</code>?</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; my_series = pandas.Series([1,2,2,3,3,3, ""fred"", 1.8, 1.8])
&gt;&gt;&gt; my_series
0       1
1       2
2       2
3       3
4       3
5       3
6    fred
7     1.8
8     1.8
&gt;&gt;&gt; counts = my_series.value_counts()
&gt;&gt;&gt; counts
3       3
2       2
1.8     2
fred    1
1       1
&gt;&gt;&gt; len(counts)
5
&gt;&gt;&gt; sum(counts)
9
&gt;&gt;&gt; counts[""fred""]
1
&gt;&gt;&gt; dict(counts)
{1.8: 2, 2: 2, 3: 3, 1: 1, 'fred': 1}
</code></pre>
";;10;;2012-08-31T00:14:46.937;;12207352;2012-09-26T20:23:30.963;2012-09-26T20:23:30.963;;487339.0;;487339.0;12207326.0;2;102;;;
789;789;;;"<p>you can use the <a href=""http://pypi.python.org/pypi/xlrd"">xlrd</a> library and open the workbook with the ""on_demand=True"" flag, so that the sheets won't be loaded automaticaly.</p>

<p>Than you can retrieve the sheet names in a similar way to pandas:</p>

<pre><code>import xlrd
xls = xlrd.open_workbook(r'&lt;path_to_your_excel_file&gt;', on_demand=True)
print xls.sheet_names() # &lt;- remeber: xlrd sheet_names is a function, not a property
</code></pre>
";;1;;2012-09-03T15:12:35.257;;12250416;2014-11-20T13:35:35.267;2014-11-20T13:35:35.267;;2327328.0;;1643773.0;12250024.0;2;25;;;
811;811;;;"<p>You want <code>matplotlib.pcolor</code>:</p>

<pre><code>import numpy as np 
from pandas import DataFrame
import matplotlib.pyplot as plt

Index= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']
Cols = ['A', 'B', 'C', 'D']
df = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)

plt.pcolor(df)
plt.yticks(np.arange(0.5, len(df.index), 1), df.index)
plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)
plt.show()
</code></pre>
";;5;;2012-09-05T17:42:37.583;;12286958;2014-05-29T13:12:24.473;2014-05-29T13:12:24.473;;1648033.0;;1648033.0;12286607.0;2;37;;;
820;820;;;"<p>Try this:</p>

<pre><code>df.ix[df.A==0, 'B'] = np.nan
</code></pre>

<p>the <code>df.A==0</code> expression creates a boolean series that indexes the rows, <code>'B'</code> selects the column. You can also use this to transform a subset of a column, e.g.:</p>

<pre><code>df.ix[df.A==0, 'B'] = df.ix[df.A==0, 'B'] / 2
</code></pre>
";;11;;2012-09-06T19:37:18.910;;12307162;2015-10-20T22:53:01.400;2015-10-20T22:53:01.400;;210945.0;;1427416.0;12307099.0;2;148;;;
825;825;;;"<p>Figured out one way to do it by reading the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#splitting-an-object-into-groups"" rel=""noreferrer"">split-apply-combine</a> documentation examples. </p>

<pre><code>df = pandas.DataFrame({'b':[2,2,4,5], 'c': [3,3,0,9]}, index=[1,1,3,7])
df_unique = df.groupby(level=0).first()

df
   b  c
1  2  3
1  2  3
3  4  0
7  5  9

df_unique
   b  c
1  2  3
3  4  0
7  5  9
</code></pre>
";;3;;2012-09-07T17:38:49.223;;12322877;2012-09-07T20:17:54.793;2012-09-07T20:17:54.793;;656188.0;;656188.0;12322779.0;2;9;;;
826;826;;;"<pre><code>In [29]: df.drop_duplicates()
Out[29]: 
   b  c
1  2  3
3  4  0
7  5  9
</code></pre>
";;4;;2012-09-07T18:37:39.740;;12323599;2012-09-07T18:37:39.740;;;;;1548051.0;12322779.0;2;48;;;
831;831;;;"<p>As you mentioned in the question, looping through each column should work for you:</p>

<pre><code>df1.apply(lambda x: x.asof(df2.index))
</code></pre>

<p>We could potentially create a faster NaN-naive version of DataFrame.asof to do all the columns in one shot. But for now, I think this is the most straightforward way.</p>
";;2;;2012-09-07T22:16:09.623;;12326113;2012-09-07T22:16:09.623;;;;;1306530.0;12322289.0;2;8;;;
835;835;;;"<p>There may be an elegant built-in function (but I haven't found it yet). You could write one:</p>

<pre><code># reorder columns
def set_column_sequence(dataframe, seq, front=True):
    '''Takes a dataframe and a subsequence of its columns,
       returns dataframe with seq as first columns if ""front"" is True,
       and seq as last columns if ""front"" is False.
    '''
    cols = seq[:] # copy so we don't mutate seq
    for x in dataframe.columns:
        if x not in cols:
            if front: #we want ""seq"" to be in the front
                #so append current column to the end of the list
                cols.append(x)
            else:
                #we want ""seq"" to be last, so insert this
                #column in the front of the new column list
                #""cols"" we are building:
                cols.insert(0, x)
return dataframe[cols]
</code></pre>

<p><em>For your example: <code>set_column_sequence(df, ['x','y'])</code> would return the desired output.</em></p>

<p>If you want the seq at the <em>end</em> of the DataFrame instead simply pass in ""front=False"".</p>
";;5;;2012-09-08T10:41:23.073;;12329993;2015-11-03T17:53:25.870;2015-11-03T17:53:25.870;;3462321.0;;1240268.0;12329853.0;2;7;;;
836;836;;;"<p>While I'm still exploring all of the incredibly smart ways that <code>apply</code> concatenates the pieces it's given, here's another way to add a new column in the parent after a groupby operation.</p>

<pre><code>In [236]: df
Out[236]: 
  yearmonth    return
0    201202  0.922132
1    201202  0.220270
2    201202  0.228856
3    201203  0.277170
4    201203  0.747347

In [237]: def add_mkt_return(grp):
   .....:     grp['mkt_return'] = grp['return'].sum()
   .....:     return grp
   .....: 

In [238]: df.groupby('yearmonth').apply(add_mkt_return)
Out[238]: 
  yearmonth    return  mkt_return
0    201202  0.922132    1.371258
1    201202  0.220270    1.371258
2    201202  0.228856    1.371258
3    201203  0.277170    1.024516
4    201203  0.747347    1.024516
</code></pre>
";;0;;2012-09-08T17:33:00.997;;12332974;2012-09-08T17:33:00.997;;;;;243434.0;12200693.0;2;20;;;
838;838;;;"<p>May I suggest the <code>transform</code> method (instead of aggregate)? If you use it in your original example it should do what you want (the broadcasting).</p>
";;4;;2012-09-08T22:35:52.577;;12335016;2012-09-08T22:35:52.577;;;;;776560.0;12200693.0;2;8;;;
846;846;;;"<p>I wrote an under-advertised <code>ordered_merge</code> function some time ago:</p>

<pre><code>In [27]: quotes
Out[27]: 
                        time    bid    ask  bsize  asize
0 2012-09-06 09:30:00.026000  13.34  13.44      3     16
1 2012-09-06 09:30:00.043000  13.34  13.44      3     17
2 2012-09-06 09:30:00.121000  13.36  13.65      1     10
3 2012-09-06 09:30:00.386000  13.36  13.52     21      1
4 2012-09-06 09:30:00.440000  13.40  13.44     15     17

In [28]: trades
Out[28]: 
                        time  price   size
0 2012-09-06 09:30:00.439000  13.42  60511
1 2012-09-06 09:30:00.439000  13.42  60511
2 2012-09-06 09:30:02.332000  13.42    100
3 2012-09-06 09:30:02.332000  13.42    100
4 2012-09-06 09:30:02.333000  13.41    100

In [29]: ordered_merge(quotes, trades)
Out[29]: 
                        time    bid    ask  bsize  asize  price   size
0 2012-09-06 09:30:00.026000  13.34  13.44      3     16    NaN    NaN
1 2012-09-06 09:30:00.043000  13.34  13.44      3     17    NaN    NaN
2 2012-09-06 09:30:00.121000  13.36  13.65      1     10    NaN    NaN
3 2012-09-06 09:30:00.386000  13.36  13.52     21      1    NaN    NaN
4 2012-09-06 09:30:00.439000    NaN    NaN    NaN    NaN  13.42  60511
5 2012-09-06 09:30:00.439000    NaN    NaN    NaN    NaN  13.42  60511
6 2012-09-06 09:30:00.440000  13.40  13.44     15     17    NaN    NaN
7 2012-09-06 09:30:02.332000    NaN    NaN    NaN    NaN  13.42    100
8 2012-09-06 09:30:02.332000    NaN    NaN    NaN    NaN  13.42    100
9 2012-09-06 09:30:02.333000    NaN    NaN    NaN    NaN  13.41    100

In [32]: ordered_merge(quotes, trades, fill_method='ffill')
Out[32]: 
                        time    bid    ask  bsize  asize  price   size
0 2012-09-06 09:30:00.026000  13.34  13.44      3     16    NaN    NaN
1 2012-09-06 09:30:00.043000  13.34  13.44      3     17    NaN    NaN
2 2012-09-06 09:30:00.121000  13.36  13.65      1     10    NaN    NaN
3 2012-09-06 09:30:00.386000  13.36  13.52     21      1    NaN    NaN
4 2012-09-06 09:30:00.439000  13.36  13.52     21      1  13.42  60511
5 2012-09-06 09:30:00.439000  13.36  13.52     21      1  13.42  60511
6 2012-09-06 09:30:00.440000  13.40  13.44     15     17  13.42  60511
7 2012-09-06 09:30:02.332000  13.40  13.44     15     17  13.42    100
8 2012-09-06 09:30:02.332000  13.40  13.44     15     17  13.42    100
9 2012-09-06 09:30:02.333000  13.40  13.44     15     17  13.41    100
</code></pre>

<p>It could be easily (well, for someone who is familiar with the code) extended to be a ""left join"" mimicking KDB. I realize in this case that forward-filling the trade data is not appropriate; just illustrating the function.</p>
";;2;;2012-09-09T02:22:04.847;;12336039;2012-09-09T02:22:04.847;;;;;776560.0;12322289.0;2;13;;;
847;847;;;"<p>You can also do as a convenience:</p>

<p><code>df[:10]</code></p>
";;0;;2012-09-09T19:21:23.347;;12342180;2012-09-09T19:21:23.347;;;;;776560.0;12021754.0;2;21;;;
853;853;;;"<p>I'd just use <code>zip</code>:</p>

<pre><code>In [1]: from pandas import *

In [2]: def calculate(x):
   ...:     return x*2, x*3
   ...: 

In [3]: df = DataFrame({'a': [1,2,3], 'b': [2,3,4]})

In [4]: df
Out[4]: 
   a  b
0  1  2
1  2  3
2  3  4

In [5]: df[""A1""], df[""A2""] = zip(*df[""a""].map(calculate))

In [6]: df
Out[6]: 
   a  b  A1  A2
0  1  2   2   3
1  2  3   4   6
2  3  4   6   9
</code></pre>
";;3;;2012-09-10T17:20:49.987;;12356541;2012-09-10T17:20:49.987;;;;;487339.0;12356501.0;2;92;;;
855;855;;;"<p>[updated to simplify]</p>

<p>tl;dr:</p>

<pre><code>In [29]: new_columns = df.columns[df.ix[df.last_valid_index()].argsort()]

In [30]: df[new_columns]
Out[30]: 
        aaa       ppp       fff       ddd
0  0.328281  0.375458  1.188905  0.503059
1  0.305457  0.186163  0.077681 -0.543215
2  0.684265  0.681724  0.210636 -0.532685
3 -1.134292  1.832272  0.067946  0.250131
4 -0.834393  0.010211  0.649963 -0.551448
5 -1.032405 -0.749949  0.442398  1.274599
</code></pre>

<hr>

<p>Some explanation follows.  First, build the <code>DataFrame</code>:</p>

<pre><code>In [24]: df = pd.DataFrame(np.random.randn(6, 4), columns=['ddd', 'fff', 'aaa', 'ppp'])

In [25]: df
Out[25]: 
        ddd       fff       aaa       ppp
0  0.503059  1.188905  0.328281  0.375458
1 -0.543215  0.077681  0.305457  0.186163
2 -0.532685  0.210636  0.684265  0.681724
3  0.250131  0.067946 -1.134292  1.832272
4 -0.551448  0.649963 -0.834393  0.010211
5  1.274599  0.442398 -1.032405 -0.749949
</code></pre>

<p>Get the last row:</p>

<pre><code>In [26]: last_row = df.ix[df.last_valid_index()]
</code></pre>

<p>Get the indices that would sort it:</p>

<pre><code>In [27]: last_row.argsort()
Out[27]: 
ddd    2
fff    3
aaa    1
ppp    0
Name: 5, Dtype: int32
</code></pre>

<p>Use this to index <code>df</code>:</p>

<pre><code>In [28]: df[last_row.argsort()]
Out[28]: 
        aaa       ppp       fff       ddd
0  0.328281  0.375458  1.188905  0.503059
1  0.305457  0.186163  0.077681 -0.543215
2  0.684265  0.681724  0.210636 -0.532685
3 -1.134292  1.832272  0.067946  0.250131
4 -0.834393  0.010211  0.649963 -0.551448
5 -1.032405 -0.749949  0.442398  1.274599
</code></pre>

<p>Profit!</p>
";;0;;2012-09-10T20:00:09.180;;12358601;2013-03-20T16:48:53.027;2013-03-20T16:48:53.027;;487339.0;;487339.0;12358360.0;2;19;;;
870;870;;;"<p>You could have <code>is_hammer</code> in terms of <code>row[""Open""]</code> etc. as follows</p>

<pre><code>def is_hammer(rOpen,rLow,rClose,rHigh):
    return lower_wick_at_least_twice_real_body(rOpen,rLow,rClose) \
       and closed_in_top_half_of_range(rHigh,rLow,rClose)
</code></pre>

<p>Then you can use map:</p>

<pre><code>df[""isHammer""] = map(is_hammer, df[""Open""], df[""Low""], df[""Close""], df[""High""])
</code></pre>
";;1;;2012-09-11T20:04:39.830;;12377080;2012-09-11T20:04:39.830;;;;;1240268.0;12376863.0;2;27;;;
871;871;;;"<p>The exact code will vary for each of the columns you want to do, but it's likely you'll want to use the <code>map</code> and <code>apply</code> functions.  In some cases you can just compute using the existing columns directly, since the columns are Pandas Series objects, which also work as Numpy arrays, which automatically work element-wise for usual mathematical operations.</p>

<pre><code>&gt;&gt;&gt; d
    A   B  C
0  11  13  5
1   6   7  4
2   8   3  6
3   4   8  7
4   0   1  7
&gt;&gt;&gt; (d.A + d.B) / d.C
0    4.800000
1    3.250000
2    1.833333
3    1.714286
4    0.142857
&gt;&gt;&gt; d.A &gt; d.C
0     True
1     True
2     True
3    False
4    False
</code></pre>

<p>If you need to use operations like max and min within a row, you can use <code>apply</code> with <code>axis=1</code> to apply any function you like to each row.  Here's an example that computes <code>min(A, B)-C</code>, which seems to be like your ""lower wick"":</p>

<pre><code>&gt;&gt;&gt; d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)
0    6
1    2
2   -3
3   -3
4   -7
</code></pre>

<p>Hopefully that gives you some idea of how to proceed.</p>

<p>Edit: to compare rows against neighboring rows, the simplest approach is to slice the columns you want to compare, leaving off the beginning/end, and then compare the resulting slices.  For instance, this will tell you for which rows the element in column A is less than the next row's element in column C:</p>

<pre><code>d['A'][:-1] &lt; d['C'][1:]
</code></pre>

<p>and this does it the other way, telling you which rows have A less than the preceding row's C:</p>

<pre><code>d['A'][1:] &lt; d['C'][:-1]
</code></pre>

<p>Doing <code>['A""][:-1]</code> slices off the last element of column A, and doing <code>['C'][1:]</code> slices off the first element of column C, so when you line these two up and compare them, you're comparing each element in A with the C from the following row.</p>
";;3;;2012-09-11T20:04:56.837;;12377083;2012-09-11T20:56:40.427;2012-09-11T20:56:40.427;;1427416.0;;1427416.0;12376863.0;2;51;;;
886;886;;;"<p>What problems are you running into with <code>apply</code>? It works for this toy example here and the group lengths are different:</p>

<pre><code>In [82]: df
Out[82]: 
   X         Y
0  0 -0.631214
1  0  0.783142
2  0  0.526045
3  1 -1.750058
4  1  1.163868
5  1  1.625538
6  1  0.076105
7  2  0.183492
8  2  0.541400
9  2 -0.672809

In [83]: def func(x):
   ....:     x['NewCol'] = np.nan
   ....:     return x
   ....: 

In [84]: df.groupby('X').apply(func)
Out[84]: 
   X         Y  NewCol
0  0 -0.631214     NaN
1  0  0.783142     NaN
2  0  0.526045     NaN
3  1 -1.750058     NaN
4  1  1.163868     NaN
5  1  1.625538     NaN
6  1  0.076105     NaN
7  2  0.183492     NaN
8  2  0.541400     NaN
9  2 -0.672809     NaN
</code></pre>
";;5;;2012-09-12T18:19:35.553;;12394122;2012-09-12T18:19:35.553;;;;;1306530.0;12389898.0;2;30;;;
889;889;;;"<p>instead of call plot(test[""x""][5:10]), you can call the plot method of Series object:</p>

<pre><code>test[""x""][5:10].plot()
</code></pre>

<p>The reason: test[""x""][5:10] is a Series object with integer index from 5 to 10. plot() try to get index 0 of it, that will cause error.</p>
";;2;;2012-09-13T13:43:36.910;;12407691;2012-09-13T13:43:36.910;;;;;772649.0;12406162.0;2;8;;;
911;911;;;"<p>Short answer: Yes. Use Python's <a href=""http://docs.python.org/library/urllib.html"">urllib</a> to pull the historical data pages for the stocks you want. Go with Yahoo! Finance; Google is both less reliable, has less data coverage, and is more restrictive in how you can use it once you have it. Also, I believe Google specifically prohibits you from scraping the data in their ToS.</p>

<p>Longer answer: This is the script I use to pull all the historical data on a particular company. It pulls the historical data page for a particular ticker symbol, then saves it to a csv file named by that symbol. You'll have to provide your own list of ticker symbols that you want to pull.</p>

<pre><code>import urllib

base_url = ""http://ichart.finance.yahoo.com/table.csv?s=""
def make_url(ticker_symbol):
    return base_url + ticker_symbol

output_path = ""C:/path/to/output/directory""
def make_filename(ticker_symbol, directory=""S&amp;P""):
    return output_path + ""/"" + directory + ""/"" + ticker_symbol + "".csv""

def pull_historical_data(ticker_symbol, directory=""S&amp;P""):
    try:
        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))
    except urllib.ContentTooShortError as e:
        outfile = open(make_filename(ticker_symbol, directory), ""w"")
        outfile.write(e.content)
        outfile.close()
</code></pre>
";;0;;2012-09-14T23:29:54.900;;12433236;2012-09-14T23:29:54.900;;;;;999724.0;12433076.0;2;32;;;
917;917;;;"<p>HYRY explained why you get the KeyError.
To plot with slices using matplotlib you can do:</p>

<pre><code>In [157]: plot(test['x'][5:10].values)
Out[157]: [&lt;matplotlib.lines.Line2D at 0xc38348c&gt;]

In [158]: plot(test['x'][5:10].reset_index(drop=True))
Out[158]: [&lt;matplotlib.lines.Line2D at 0xc37e3cc&gt;]
</code></pre>

<p>x, y plotting in one go with 0.7.3</p>

<pre><code>In [161]: test[5:10].set_index('x')['y'].plot()
Out[161]: &lt;matplotlib.axes.AxesSubplot at 0xc48b1cc&gt;
</code></pre>
";;5;;2012-09-16T19:03:26.833;;12449785;2012-09-16T19:03:26.833;;;;;1548051.0;12406162.0;2;16;;;
932;932;;;"<p>Try this:</p>

<pre><code>df.groupby(['A']).max()
</code></pre>
";;3;;2012-09-19T15:10:56.213;;12497577;2012-09-19T15:10:56.213;;;;;449449.0;12497402.0;2;7;;;
936;936;;;"<p>You could use <code>fill_value</code>:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])
&gt;&gt;&gt; Sr2 = pd.Series([5,6], index = ['A', 'C'])
&gt;&gt;&gt; Sr1+Sr2
A     6
B   NaN
C     9
D   NaN
&gt;&gt;&gt; Sr1.add(Sr2, fill_value=0)
A    6
B    2
C    9
D    4
</code></pre>
";;0;;2012-09-20T00:09:18.543;;12504527;2012-09-20T00:09:18.543;;;;;487339.0;12504493.0;2;29;;;
939;939;;;"<pre><code>In [23]: %logstart /tmp/session.log
Activating auto-logging. Current session state plus future input saved.
Filename       : /tmp/session.log
Mode           : backup
Output logging : False
Raw input log  : False
Timestamping   : False
State          : active

In [24]: x = 1

In [25]: %logstop

In [26]: quit()
Do you really want to exit ([y]/n)? y
</code></pre>

<p>Then we can restore the session with:</p>

<pre><code>% ipython -log /tmp/session.log 
Activating auto-logging. Current session state plus future input saved.
Filename       : ipython_log.py
...

In [1]: x
Out[1]: 1
</code></pre>

<p>For more on ""Session logging and restoring"" see <a href=""http://ipython.org/ipython-doc/stable/interactive/reference.html#session-logging-and-restoring"" rel=""noreferrer"">the docs</a>.</p>
";;4;;2012-09-20T01:34:06.203;;12505031;2012-09-20T01:34:06.203;;;;;190597.0;12504951.0;2;9;;;
940;940;;;"<p>You could use the <code>tolist</code> method as an intermediary:</p>

<pre><code>In [99]: import pandas as pd

In [100]: d1 = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})

In [101]: d1.ticker.str.split().tolist()
Out[101]: 
[['spx', '5/25/2001', 'p500'],
 ['spx', '5/25/2001', 'p600'],
 ['spx', '5/25/2001', 'p700']]
</code></pre>

<p>From which you could make a new DataFrame:</p>

<pre><code>In [102]: d2 = pd.DataFrame(d1.ticker.str.split().tolist(), 
   .....:                   columns=""symbol date price"".split())

In [103]: d2
Out[103]: 
  symbol       date price
0    spx  5/25/2001  p500
1    spx  5/25/2001  p600
2    spx  5/25/2001  p700
</code></pre>

<p>For good measure, you could fix the price:</p>

<pre><code>In [104]: d2[""price""] = d2[""price""].str.replace(""p"","""").astype(float)

In [105]: d2
Out[105]: 
  symbol       date  price
0    spx  5/25/2001    500
1    spx  5/25/2001    600
2    spx  5/25/2001    700
</code></pre>

<p>PS: but if you <em>really</em> just want the last column, <code>apply</code> would suffice:</p>

<pre><code>In [113]: temp2.apply(lambda x: x[2])
Out[113]: 
0    p500
1    p600
2    p700
Name: ticker
</code></pre>
";;1;;2012-09-20T01:43:43.317;;12505089;2012-09-20T01:43:43.317;;;;;487339.0;12504976.0;2;28;;;
941;941;;;"<p>When you're going to work with such time series in Python, <code>pandas</code> is indispensable. And here's the good news: it comes with a historical data downloader for Yahoo: <code>pandas.io.data.DataReader</code>.</p>

<pre><code>from pandas.io.data import DataReader
from datetime import datetime

ibm = DataReader('IBM',  'yahoo', datetime(2000, 1, 1), datetime(2012, 1, 1))
print(ibm['Adj Close'])
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#yahoo-finance"">Here's an example from the <code>pandas</code> documentation.</a></p>

<p><strong>Update for pandas >= 0.19:</strong></p>

<p>The <code>pandas.io.data</code> module has been removed from <code>pandas&gt;=0.19</code> onwards. Instead, you should use the separate <a href=""https://github.com/pydata/pandas-datareader""><code>pandas-datareader</code> package</a>. Install with:</p>

<pre><code>pip install pandas-datareader
</code></pre>

<p>And then you can do this in Python:</p>

<pre><code>import pandas_datareader as pdr
from datetime import datetime

ibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1))
print(ibm['Adj Close'])
</code></pre>

<p><a href=""http://pandas-datareader.readthedocs.io/en/latest/remote_data.html#google-finance"">Downloading from Google Finance is also supported.</a></p>

<p><a href=""http://pandas-datareader.readthedocs.io"">There's more in the documentation of <code>pandas-datareader</code>.</a></p>
";;3;;2012-09-20T10:05:41.657;;12510334;2016-09-15T02:33:14.270;2016-09-15T02:33:14.270;;1571826.0;;1571826.0;12433076.0;2;85;;;
956;956;;;"<pre><code>In [92]: df
Out[92]:
           a         b          c         d
A  -0.488816  0.863769   4.325608 -4.721202
B -11.937097  2.993993 -12.916784 -1.086236
C  -5.569493  4.672679  -2.168464 -9.315900
D   8.892368  0.932785   4.535396  0.598124

In [93]: df_norm = (df - df.mean()) / (df.max() - df.min())

In [94]: df_norm
Out[94]:
          a         b         c         d
A  0.085789 -0.394348  0.337016 -0.109935
B -0.463830  0.164926 -0.650963  0.256714
C -0.158129  0.605652 -0.035090 -0.573389
D  0.536170 -0.376229  0.349037  0.426611

In [95]: df_norm.mean()
Out[95]:
a   -2.081668e-17
b    4.857226e-17
c    1.734723e-17
d   -1.040834e-17

In [96]: df_norm.max() - df_norm.min()
Out[96]:
a    1
b    1
c    1
d    1
</code></pre>
";;7;;2012-09-21T07:14:15.117;;12525836;2012-09-21T07:14:15.117;;;;;1548051.0;12525722.0;2;124;;;
965;965;;;"<p>Doing this directly via <a href=""http://en.wikipedia.org/wiki/NumPy"">NumPy</a> will be the most efficient:</p>

<pre><code>df1['e'] = np.random.randn(sLength)
</code></pre>

<hr>

<p>Note my original (very old) suggestion was to use <code>map</code> (which is much slower):</p>

<pre><code>df1['e'] = df1['a'].map(lambda x: np.random.random())
</code></pre>
";;2;;2012-09-23T19:22:27.897;;12555491;2015-10-20T13:05:01.403;2015-10-20T13:05:01.403;;63550.0;;1240268.0;12555323.0;2;29;;;
966;966;;;"<p>Use the original df1 indexes to create the series:</p>

<pre><code>df1['e'] = Series(np.random.randn(sLength), index=df1.index)
</code></pre>

<hr>

<hr>

<p><strong>Edit 2015</strong><br>
Some reported to get the <code>SettingWithCopyWarning</code> with this code.<br>
However, the code still runs perfect with the current pandas version 0.16.1.</p>

<pre><code>&gt;&gt;&gt; sLength = len(df1['a'])
&gt;&gt;&gt; df1
          a         b         c         d
6 -0.269221 -0.026476  0.997517  1.294385
8  0.917438  0.847941  0.034235 -0.448948

&gt;&gt;&gt; df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e
6 -0.269221 -0.026476  0.997517  1.294385  1.757167
8  0.917438  0.847941  0.034235 -0.448948  2.228131

&gt;&gt;&gt; p.version.short_version
'0.16.1'
</code></pre>

<p>The <code>SettingWithCopyWarning</code> aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn't necessarily say you did it wrong (it can trigger false positives) but from 0.13.0 it let you know there are more adequate methods for the same purpose. Then, if you get the warning, just follow its advise: <em>Try using .loc[row_index,col_indexer] = value instead</em></p>

<pre><code>&gt;&gt;&gt; df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e         f
6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927
8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109
&gt;&gt;&gt; 
</code></pre>

<p>In fact, this is currently the more efficient method as <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"">described in pandas docs</a></p>

<hr>

<hr>

<p><strong>Edit 2017</strong></p>

<p>As indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using <code>assign</code>:</p>

<pre><code>df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)
</code></pre>
";;22;;2012-09-23T19:24:45.883;;12555510;2017-01-07T12:07:59.963;2017-01-07T12:07:59.963;;308903.0;;308903.0;12555323.0;2;411;;;
968;968;;;"<p>I found the answer.</p>

<p>Using the data, dataframe from the question:</p>

<pre><code>from pandas import *

P1Channels = data.filter(regex=""P1"")
P1Sum = P1Channels.sum(axis=1)
</code></pre>
";;0;;2012-09-24T18:05:26.103;;12570410;2012-09-24T18:05:26.103;;;;;865662.0;12569730.0;2;43;;;
987;987;;;"<p>As @DSM points out, you can do this more directly using the <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"">vectorised string methods</a>:</p>

<pre><code>df['Date'].str[-4:].astype(int)
</code></pre>

<p>Or using extract (assuming there is only one set of digits of length 4 somewhere in each string):</p>

<pre><code>df['Date'].str.extract('(?P&lt;year&gt;\d{4})').astype(int)
</code></pre>

<p>An alternative slightly more flexible way, might be to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html""><code>apply</code></a> (or equivalently <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html""><code>map</code></a>) to do this:</p>

<pre><code>df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))
             #  converts the last 4 characters of the string to an integer
</code></pre>

<p>The lambda function, is taking the input from the <code>Date</code> and converting it to a year.<br>
You could (and perhaps should) write this more verbosely as:</p>

<pre><code>def convert_to_year(date_in_some_format);
    date_as_string = str(date_in_some_format)
    year_as_string = date_in_some_format[-4:] # last four characters
    return int(year_as_string)

df['Date'] = df['Date'].apply(convert_to_year)
</code></pre>

<p><em>Perhaps 'Year' is a better name for this column...</em></p>
";;11;;2012-09-26T15:20:01.467;;12605055;2014-01-16T00:38:57.740;2014-01-16T00:38:57.740;;1240268.0;;1240268.0;12604909.0;2;54;;;
988;988;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"" rel=""noreferrer"">Here</a> is from pandas docs on advanced indexing: </p>

<p>The section after 'Assignment / setting values is possible when using ix:' will explain exactly what you need! Turns out <code>df.ix</code> can be used for cool slicing/dicing of a dataframe. And. It can also be used to set things. </p>

<pre><code>df.ix[selection criteria, columns I want] = value
</code></pre>

<p>So Bren's answer is saying 'find me all the places where <code>df.A == 0</code>, select column <code>B</code> and set it to <code>np.nan</code>'</p>
";;3;;2012-09-26T17:14:39.427;;12607018;2015-12-23T10:52:56.170;2015-12-23T10:52:56.170;;2901002.0;;684543.0;12307099.0;2;55;;;
997;997;;;"<p>Have a look at df['column_label].str
Below example will drop all rows where column A holds 'a' character and 'B' equals 20.</p>

<pre><code>In [46]: df
Out[46]:
     A   B
0  foo  10
1  bar  20
2  baz  30

In [47]: cond = df['A'].str.contains('a') &amp; (df['B'] == 20)

In [48]: df.drop(df[cond].index.values)
Out[48]:
     A   B
0  foo  10
2  baz  30
</code></pre>
";;0;;2012-09-27T18:03:33.140;;12627465;2012-09-27T18:03:33.140;;;;;1548051.0;12625650.0;2;15;;;
1018;1018;;;"<p>How about something like this:</p>

<pre><code>In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              
                    for _, row in a.iterrows()]).reset_index()
Out[55]: 
  index  0
0     a  1
1     b  1
2     c  1
3     d  2
4     e  2
5     f  2
</code></pre>

<p>Then you just have to rename the columns</p>
";;8;;2012-10-01T21:15:03.957;;12681217;2012-10-01T21:15:03.957;;;;;1306530.0;12680754.0;2;22;;;
1027;1027;;;"<p>You can get the columns from the cursor description:</p>

<p><code>columns = [column[0] for column in cursor.description]</code></p>
";;0;;2012-10-03T11:23:18.300;;12707465;2012-10-03T11:23:18.300;;;;;1452002.0;12704305.0;2;39;;;
1038;1038;;;"<p>It`s a private method, but it will do the trick: source._get_numeric_data()</p>

<pre><code>In [2]: import pandas as pd

In [3]: source = pd.DataFrame({'A': ['foo', 'bar'], 'B': [1, 2], 'C': [(1,2), (3,4)]})

In [4]: source
Out[4]:
     A  B       C
0  foo  1  (1, 2)
1  bar  2  (3, 4)

In [5]: source._get_numeric_data()
Out[5]:
   B
0  1
1  2
</code></pre>
";;3;;2012-10-04T11:41:00.773;;12726468;2012-10-04T11:41:00.773;;;;;1548051.0;12725417.0;2;25;;;
1039;1039;;;"<p>Pandas has exponentially weighted moving moment functions</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/computation.html?highlight=exponential#exponentially-weighted-moment-functions"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/computation.html?highlight=exponential#exponentially-weighted-moment-functions</a></p>

<p>By the way, there shouldn't be any functionality leftover in the scikits.timeseries package that is not also in pandas.</p>

<p><strong>Edit</strong>: Since this is still a popular question, there is now a work in progress pull request to add more fully featured exponential smoothing to statsmodels <a href=""https://github.com/statsmodels/statsmodels/pull/1489"" rel=""noreferrer"">here</a></p>
";;1;;2012-10-04T13:04:01.953;;12727919;2014-05-08T19:26:48.127;2014-05-08T19:26:48.127;;535665.0;;535665.0;12726432.0;2;22;;;
1043;1043;;;"<p>If I understand you right, you're looking for the <code>applymap</code> method.</p>

<pre><code>&gt;&gt;&gt; print df
   A  B  C
0 -1  0  0
1 -4  3 -1
2 -1  0  2
3  0  3  2
4  1 -1  0
&gt;&gt;&gt; print df.applymap(lambda x: x&gt;1)
       A      B      C
0  False  False  False
1  False   True  False
2  False  False   True
3  False   True   True
4  False  False  False
</code></pre>
";;1;;2012-10-05T07:01:18.150;;12741168;2012-10-05T07:01:18.150;;;;;1427416.0;12741092.0;2;33;;;
1060;1060;;;"<p>The same syntax works for Ms SQL server using podbc also. </p>

<pre><code>import pyodbc
import pandas.io.sql as psql

cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password') 
cursor = cnxn.cursor()
sql = (""""""select * from mytable"""""")

df = psql.frame_query(sql, cnxn)
cnxn.close()
</code></pre>
";;0;;2012-10-11T07:18:29.813;;12834193;2012-10-11T07:18:29.813;;;;;573491.0;10065051.0;2;8;;;
1070;1070;;;"<p>How are you generating your data?</p>

<p>See how the output shows that your data is of 'object' type? the groupby operations specifically check whether each column is a numeric dtype first.</p>

<pre><code>In [31]: data
Out[31]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00
Freq: &lt;1 DateOffset&gt;
Columns: 360 entries, -89.75 to 89.75
dtypes: object(360)
</code></pre>

<p><strong>look ?</strong></p>

<p><br/>
Did you initialize an empty DataFrame first and then filled it? If so that's probably why it changed with the new version as before 0.9 empty DataFrames were initialized to float type but now they are of object type. If so you can change the initialization to <code>DataFrame(dtype=float)</code>.</p>

<p>You can also call <code>frame.astype(float)</code></p>
";;1;;2012-10-11T18:25:07.097;;12846154;2012-10-11T18:25:07.097;;;;;1306530.0;12844529.0;2;16;;;
1073;1073;;;"<p>I believe you can use the <code>append</code> method</p>

<pre><code>bigdata = data1.append(data2, ignore_index=True)
</code></pre>

<p>to keep their indexes just dont use the <code>ignore_index</code> keyword ...</p>
";;1;;2012-10-12T00:07:38.193;;12850453;2016-10-27T13:16:43.383;2016-10-27T13:16:43.383;;2825570.0;;541038.0;12850345.0;2;35;;;
1075;1075;;;"<p>Do you mean something like this?</p>

<pre><code>In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', 
                         aggfunc=lambda x: len(x.unique()))
Out[39]: 
Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1
</code></pre>

<p>Note that using <code>len</code> assumes you don't have <code>NA</code>s in your DataFrame. You can do <code>x.value_counts().count()</code> or <code>len(x.dropna().unique())</code> otherwise.</p>
";;2;;2012-10-12T15:19:00.237;;12862196;2014-02-17T16:29:03.533;2014-02-17T16:29:03.533;;1226722.0;;1306530.0;12860421.0;2;37;;;
1081;1081;;;"<p>I'd do like Vishal but instead of using sum() using size() to get a count of the number of rows allocated to each group of 'start_station_id'. So:</p>

<pre><code>df = male_trips.groupby('start_station_id').size()
</code></pre>
";;1;;2012-10-13T14:29:37.487;;12874054;2012-10-13T22:45:36.843;2012-10-13T22:45:36.843;;1743507.0;;1743507.0;12867178.0;2;46;;;
1082;1082;;;"<p>My answer below works in Pandas 0.7.3. Not sure about the new releases.</p>

<p>This is what the <code>pandas.Series.value_counts</code> method is for:</p>

<pre><code>count_series = male_trips.start_station_id.value_counts()
</code></pre>

<p>It should be straight-forward to then inspect <code>count_series</code> based on the values in <code>stations['id']</code>. However, if you insist on <em>only</em> considering those values, you could do the following:</p>

<pre><code>count_series = (
                male_trips[male_trips.start_station_id.isin(stations.id.values)]
                    .start_station_id
                    .value_counts()
               )
</code></pre>

<p>and this will only give counts for station IDs actually found in <code>stations.id</code>.</p>
";;0;;2012-10-13T14:39:56.103;;12874135;2012-10-13T14:39:56.103;;;;;567620.0;12867178.0;2;18;;;
1084;1084;;;"<p>As mentioned in the comments, it is a general floating point problem.</p>

<p>However you can use the <code>float_format</code> key word of <code>to_csv</code> to hide it:</p>

<pre><code>df.to_csv('pandasfile.csv', float_format='%.3f')
</code></pre>

<p>or, if you don't want 0.0001 to be rounded to zero:</p>

<pre><code>df.to_csv('pandasfile.csv', float_format='%g')
</code></pre>

<p>will give you:</p>

<pre><code>Bob,0.085
Alice,0.005
</code></pre>

<p>in your output file.</p>

<p>For an explanation of <code>%g</code>, see <a href=""https://docs.python.org/library/string.html#format-specification-mini-language"" rel=""nofollow noreferrer"">Format Specification Mini-Language</a>.</p>
";;0;;2012-10-14T12:58:04.093;;12882439;2017-07-14T19:21:43.753;2017-07-14T19:21:43.753;;832230.0;;1301710.0;12877189.0;2;47;;;
1109;1109;;;"<h2>Quick Answer</h2>

<p>Use <strong>index_col=False</strong> instead of index_col=None when you have delimiters at the end of each line to turn off index column inference and discard the last column.</p>

<h2>More Detail</h2>

<p>After looking at the data, there is a comma at the end of each line. And this quote: </p>

<blockquote>
  <p>index_col: column number, column name, or list of column numbers/names, to use as the index (row labels) of the resulting DataFrame. By default, it will number the rows without using any column, unless there is one more data column than there are headers, in which case the first column is taken as the index.</p>
</blockquote>

<p>from <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html"" rel=""nofollow noreferrer"">the documentation</a> shows that pandas believes you have n headers and n+1 data columns and is treating the first column as the index.</p>

<hr>

<p>EDIT 10/20/2014 - More information</p>

<p>I found <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#index-columns-and-trailing-delimiters"" rel=""nofollow noreferrer"">another valuable entry</a> that is specifically about trailing limiters and how to simply ignore them:</p>

<blockquote>
  <p>If a file has one more column of data than the number of column names, the first column will be used as the DataFrames row names: ... </p>
  
  <p>Ordinarily, you can achieve this behavior using the index_col option. </p>
  
  <p>There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False: ...</p>
</blockquote>
";;5;;2012-10-18T18:19:30.690;;12961158;2017-07-26T15:29:18.997;2017-07-26T15:29:18.997;;1341437.0;;1341437.0;12960574.0;2;24;;;
1119;1119;;;"<p>Don't know if you solved the problem but if anyone has this problem in future.</p>

<pre><code>$python
&gt;&gt;import numpy
&gt;&gt;print(numpy)
</code></pre>

<p>Go to the location printed and delete the <code>numpy</code> installation found there. You can then use <code>pip</code> or <code>easy_install</code></p>
";;3;;2012-10-19T13:39:55.917;;12975518;2012-10-19T13:39:55.917;;;;;1448346.0;12436979.0;2;36;;;
1132;1132;;;"<p>I'm planning to add explicit column dtypes in the upcoming file parser engine overhaul in pandas 0.10. Can't commit myself 100% to it but it should be pretty simple with the new infrastructure coming together (http://wesmckinney.com/blog/?p=543).</p>
";;2;;2012-10-20T20:12:00.853;;12992260;2012-10-20T20:12:00.853;;;;;776560.0;12101113.0;2;10;;;
1142;1142;;;"<p>We can potentially make this easier (I created a <a href=""https://github.com/pydata/pandas/issues/2099"" rel=""noreferrer"">github issue</a>), but for now you can select out the columns you want to plot:</p>

<pre><code>df.ix[:, df.columns - to_excl].hist()
</code></pre>
";;2;;2012-10-22T00:15:38.187;;13003524;2012-10-22T00:15:38.187;;;;;1306530.0;13003051.0;2;8;;;
1153;1153;;;"<p>Use <code>BDay()</code> to get the business days in range. </p>

<pre><code>from pandas.tseries.offsets import *

In [185]: s
Out[185]: 
2011-01-01   -0.011629
2011-01-02   -0.089666
2011-01-03   -1.314430
2011-01-04   -1.867307
2011-01-05    0.779609
2011-01-06    0.588950
2011-01-07   -2.505803
2011-01-08    0.800262
2011-01-09    0.376406
2011-01-10   -0.469988
Freq: D

In [186]: s.asfreq(BDay())
Out[186]: 
2011-01-03   -1.314430
2011-01-04   -1.867307
2011-01-05    0.779609
2011-01-06    0.588950
2011-01-07   -2.505803
2011-01-10   -0.469988
Freq: B
</code></pre>

<p>With slicing:</p>

<pre><code>In [187]: x=datetime(2011, 1, 5)

In [188]: y=datetime(2011, 1, 9)

In [189]: s.ix[x:y]
Out[189]: 
2011-01-05    0.779609
2011-01-06    0.588950
2011-01-07   -2.505803
2011-01-08    0.800262
2011-01-09    0.376406
Freq: D

In [190]: s.ix[x:y].asfreq(BDay())
Out[190]: 
2011-01-05    0.779609
2011-01-06    0.588950
2011-01-07   -2.505803
Freq: B
</code></pre>

<p>and <code>count()</code></p>

<pre><code>In [191]: s.ix[x:y].asfreq(BDay()).count()
Out[191]: 3
</code></pre>
";;1;;2012-10-22T21:16:33.590;;13020027;2012-10-22T21:25:24.947;2012-10-22T21:25:24.947;;1199589.0;;1199589.0;13019719.0;2;9;;;
1156;1156;;;"<p>Sure, you can use <code>.get_loc()</code>:</p>

<pre><code>In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})

In [46]: df.columns
Out[46]: Index([apple, orange, pear], dtype=object)

In [47]: df.columns.get_loc(""pear"")
Out[47]: 2
</code></pre>

<p>although to be honest I don't often need this myself.  Usually access by name does what I want it to (<code>df[""pear""]</code>, <code>df[[""apple"", ""orange""]]</code>, or maybe <code>df.columns.isin([""orange"", ""pear""])</code>), although I can definitely see cases where you'd want the index number. </p>
";;0;;2012-10-23T00:06:36.177;;13021797;2012-10-23T00:06:36.177;;;;;487339.0;13021654.0;2;82;;;
1181;1181;;;"<p>DSM's solution works, but if you wanted a direct equivalent to <code>which</code> you could do <code>(df.columns == name).nonzero()</code></p>
";;1;;2012-10-23T18:27:34.033;;13036844;2012-10-23T18:27:34.033;;;;;776560.0;13021654.0;2;7;;;
1182;1182;;;"<p>Oh my. This is actually so simple!</p>

<pre><code>grouped = df3.groupby(level=0)
df4 = grouped.last()
df4
                      A   B  rownum

2001-01-01 00:00:00   0   0       6
2001-01-01 01:00:00   1   1       7
2001-01-01 02:00:00   2   2       8
2001-01-01 03:00:00   3   3       3
2001-01-01 04:00:00   4   4       4
2001-01-01 05:00:00   5   5       5
</code></pre>

<p><strong>Follow up edit 2013-10-29</strong>
In the case where I have a fairly complex <code>MultiIndex</code>, I think I prefer the <code>groupby</code> approach. Here's simple example for posterity:</p>

<pre><code>import numpy as np
import pandas

# fake index
idx = pandas.MultiIndex.from_tuples([('a', letter) for letter in list('abcde')])

# random data + naming the index levels
df1 = pandas.DataFrame(np.random.normal(size=(5,2)), index=idx, columns=['colA', 'colB'])
df1.index.names = ['iA', 'iB']

# artificially append some duplicate data
df1 = df1.append(df1.select(lambda idx: idx[1] in ['c', 'e']))
df1
#           colA      colB
#iA iB                    
#a  a  -1.297535  0.691787
#   b  -1.688411  0.404430
#   c   0.275806 -0.078871
#   d  -0.509815 -0.220326
#   e  -0.066680  0.607233
#   c   0.275806 -0.078871  # &lt;--- dup 1
#   e  -0.066680  0.607233  # &lt;--- dup 2
</code></pre>

<p><strong>and here's the important part</strong></p>

<pre><code># group the data, using df1.index.names tells pandas to look at the entire index
groups = df1.groupby(level=df1.index.names)  
groups.last() # or .first()
#           colA      colB
#iA iB                    
#a  a  -1.297535  0.691787
#   b  -1.688411  0.404430
#   c   0.275806 -0.078871
#   d  -0.509815 -0.220326
#   e  -0.066680  0.607233
</code></pre>
";;2;;2012-10-23T18:27:46.077;;13036848;2015-03-24T16:51:39.500;2015-03-24T16:51:39.500;;1552748.0;;1552748.0;13035764.0;2;53;;;
1190;1190;;;"<p>You could use groupby:</p>

<pre><code>def f(group):
    row = group.irow(0)
    return DataFrame({'class': [row['class']] * row['count']})
df.groupby('class', group_keys=False).apply(f)
</code></pre>

<p>so you get</p>

<pre><code>In [25]: df.groupby('class', group_keys=False).apply(f)
Out[25]: 
  class
0     A
0     C
1     C
</code></pre>

<p>You can fix the index of the result however you like</p>
";;2;;2012-10-24T15:25:40.213;;13052373;2012-10-24T15:25:40.213;;;;;776560.0;13050003.0;2;17;;;
1192;1192;;;"<p>Do this:</p>

<pre><code>In [43]: temp2.str[-1]
Out[43]: 
0    p500
1    p600
2    p700
Name: ticker
</code></pre>
";;6;;2012-10-24T16:13:48.523;;13053267;2012-10-24T16:13:48.523;;;;;776560.0;12504976.0;2;41;;;
1194;1194;;;"<p>You can set the ticks to where you want just like you set the xticks. </p>

<pre><code>import numpy as np
ax0.yaxis.set_ticks(np.arange(70000,80000,2500))
</code></pre>

<p>This will create four ticks evenly spaced for your ax0 subplot. You can do something similar for your other subplots. </p>
";;0;;2012-10-24T16:22:22.293;;13053381;2012-10-24T16:22:22.293;;;;;484596.0;13052844.0;2;12;;;
1196;1196;;;"<p>An improvement over the approach suggestion by <a href=""https://stackoverflow.com/users/484596/aman"">Aman</a> is the following:</p>

<pre><code>import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

# ... plot some things ...

# Find at most 101 ticks on the y-axis at 'nice' locations
max_yticks = 100
yloc = plt.MaxNLocator(max_yticks)
ax.yaxis.set_major_locator(yloc)

plt.show()
</code></pre>

<p>Hope this helps.</p>
";;2;;2012-10-24T16:55:56.903;;13053967;2013-06-05T20:27:06.377;2017-05-23T12:34:22.307;;-1.0;;1712956.0;13052844.0;2;29;;;
1197;1197;;;"<p>This takes the last. Not the maximum though:</p>

<pre><code>In [10]: df.drop_duplicates(subset='A', keep=""last"")
Out[10]: 
   A   B
1  1  20
3  2  40
4  3  10
</code></pre>

<p>You can do also something like:</p>

<pre><code>In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.ix[x.B.idxmax()])
Out[12]: 
   A   B
A       
1  1  20
2  2  40
3  3  10
</code></pre>
";;6;;2012-10-25T00:10:02.923;;13059751;2017-08-05T18:43:57.133;2017-08-05T18:43:57.133;;428862.0;;776560.0;12497402.0;2;64;;;
1208;1208;;;"<p>Somehow some questions got merged or deleted, so I'll post my answer here.</p>

<p>Exp smoothing in Python natively.</p>

<pre><code>'''
simple exponential smoothing
go back to last N values
y_t = a * y_t + a * (1-a)^1 * y_t-1 + a * (1-a)^2 * y_t-2 + ... + a*(1-a)^n * y_t-n
'''
from random import random,randint

def gen_weights(a,N):
    ws = list()
    for i in range(N):
        w = a * ((1-a)**i)
        ws.append(w)
    return ws

def weighted(data,ws):
    wt = list()
    for i,x in enumerate(data):
        wt.append(x*ws[i])
    return wt

N = 10
a = 0.5
ws = gen_weights(a,N)
data = [randint(0,100) for r in xrange(N)]
weighted_data = weighted(data,ws)
print 'data: ',data
print 'weights: ',ws
print 'weighted data: ',weighted_data
print 'weighted avg: ',sum(weighted_data)
</code></pre>
";;0;;2012-10-25T14:05:14.860;;13070405;2012-10-25T14:05:14.860;;;;;1460614.0;12726432.0;2;7;;;
1223;1223;;;"<p><code>df.head(n)</code> returns a <code>DataFrame</code> holding the first n rows of df.
Now to display a <code>DataFrame</code> pandas checks by default the width of the terminal, if this is too small to display the <code>DataFrame</code> a summary view will be shown. Which is what you get in the second case.</p>

<p>Could you increase the size of your terminal, or disable autodetect on the columns by <code>pd.set_printoptions(max_columns=10)</code>?</p>
";;8;;2012-10-26T11:43:09.147;;13086305;2016-03-07T12:18:21.693;2016-03-07T12:18:21.693;;2535611.0;;1548051.0;13085709.0;2;15;;;
1236;1236;;;"<p>I think you want to do something like this:</p>

<pre><code>In [26]: data
Out[26]: 
           Date   Close  Adj Close
251  2011-01-03  147.48     143.25
250  2011-01-04  147.64     143.41
249  2011-01-05  147.05     142.83
248  2011-01-06  148.66     144.40
247  2011-01-07  147.93     143.69

In [27]: data.set_index('Date').diff()
Out[27]: 
            Close  Adj Close
Date                        
2011-01-03    NaN        NaN
2011-01-04   0.16       0.16
2011-01-05  -0.59      -0.58
2011-01-06   1.61       1.57
2011-01-07  -0.73      -0.71
</code></pre>
";;4;;2012-10-29T03:17:30.307;;13115473;2012-10-29T03:17:30.307;;;;;1306530.0;13114512.0;2;50;;;
1243;1243;;;"<p>You just need to use the histogram function of numpy:</p>

<pre><code>import numpy as np
count, division = np.histogram(series)
</code></pre>

<p>where division is the automatically calculated border for your bins and count is the population inside each bin.</p>

<p>If you need to fix a certain number of bins, you can use the argument bins and specify a number of bins, or give it directly the boundaries between each bin.</p>

<pre><code>count, division = np.histogram(series, bins = [-201,-149,949,1001])
</code></pre>

<p>to plot the results you can use the matplotlib function hist, but if you are working in pandas each Series has its own handle to the hist function, and you can give it the chosen binning:</p>

<pre><code>series.hist(bins=division)
</code></pre>
";;1;;2012-10-29T22:07:25.197;;13130357;2017-02-07T09:45:19.850;2017-02-07T09:45:19.850;;1555275.0;;1784138.0;13129618.0;2;45;;;
1248;1248;;;"<p>One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. </p>

<p>This is what you have now: </p>

<pre><code>In [6]: df
Out[6]:
          0         1         2         3         4      mean
0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543
1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208
2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596
3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653
4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371
5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165
6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529
7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149
8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195
9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593

In [7]: cols = df.columns.tolist()

In [8]: cols
Out[8]: [0L, 1L, 2L, 3L, 4L, 'mean']
</code></pre>

<p>Rearrange <code>cols</code> in any way you want. This is how I moved the last element to the first position: </p>

<pre><code>In [12]: cols = cols[-1:] + cols[:-1]

In [13]: cols
Out[13]: ['mean', 0L, 1L, 2L, 3L, 4L]
</code></pre>

<p>Then reorder the dataframe like this: </p>

<pre><code>In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]

In [17]: df
Out[17]:
       mean         0         1         2         3         4
0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616
1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551
2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694
3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019
4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485
5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447
6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473
7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914
8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561
9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399
</code></pre>
";;6;;2012-10-30T22:38:49.910;;13148611;2012-10-31T15:51:22.220;2012-10-31T15:51:22.220;;484596.0;;484596.0;13148429.0;2;374;;;
1256;1256;;;"<p>Assuming your column names (<code>df.columns</code>) are <code>['index','a','b','c']</code>, then the data you want is in the 
3rd &amp; 4th columns. If you don't know their names when your script runs, you can do this</p>

<pre><code>newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.
</code></pre>

<p>As EMS points out in <a href=""https://stackoverflow.com/a/11287278/623735"">his answer</a>, <code>df.ix</code> slices columns a bit more concisely, but the <code>.columns</code> slicing interface might be more natural because it uses the vanilla 1-D python list indexing/slicing syntax.</p>

<p>WARN: <code>'index'</code> is a bad name for a <code>DataFrame</code> column. That same label is also used for the real <code>df.index</code> attribute, a <code>Index</code> array. So your column is returned by <code>df['index']</code> and the real DataFrame index is returned by <code>df.index</code>. An <code>Index</code> is a special kind of <code>Series</code> optimized for lookup of it's elements' values. For df.index it's for looking up rows by their label. That <code>df.columns</code> attribute is also a <code>pd.Index</code> array, for looking up columns by their labels.</p>
";;6;;2012-10-31T18:57:33.980;;13165753;2017-03-29T17:26:41.727;2017-05-23T12:10:48.913;;-1.0;;623735.0;11285613.0;2;44;;;
1265;1265;;;"<p>If you still need a workaround:</p>

<pre><code>In [49]: pd.concat([group for _, group in grouped if len(group) &gt; 1])
Out[49]: 
     A  B
0  foo  0
2  foo  2
3  foo  3
</code></pre>
";;2;;2012-11-01T17:00:54.720;;13181960;2012-11-01T17:00:54.720;;;;;1306530.0;13167391.0;2;6;;;
1276;1276;;;"<p>You can use the <code>to_records</code> method, but have to play around a bit with the dtypes if they are not what you want from the get go. In my case, having copied your DF from a string, the index type is string (represented by an <code>object</code> dtype in pandas):</p>

<pre><code>In [102]: df
Out[102]: 
label    A    B    C
ID                  
1      NaN  0.2  NaN
2      NaN  NaN  0.5
3      NaN  0.2  0.5
4      0.1  0.2  NaN
5      0.1  0.2  0.5
6      0.1  NaN  0.5
7      0.1  NaN  NaN

In [103]: df.index.dtype
Out[103]: dtype('object')
In [104]: df.to_records()
Out[104]: 
rec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),
       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),
       (7, 0.1, nan, nan)], 
      dtype=[('index', '|O8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
In [106]: df.to_records().dtype
Out[106]: dtype([('index', '|O8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>

<p>Converting the recarray dtype does not work for me, but one can do this in Pandas already:</p>

<pre><code>In [109]: df.index = df.index.astype('i8')
In [111]: df.to_records().view([('ID', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
Out[111]:
rec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),
       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),
       (7, 0.1, nan, nan)], 
      dtype=[('ID', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>

<p>Note that Pandas does not set the name of the index properly (to <code>ID</code>) in the exported record array (a bug?), so we profit from the type conversion to also correct for that. </p>

<p>At the moment Pandas has only 8-byte integers, <code>i8</code>, and floats, <code>f8</code> (see this <a href=""https://github.com/pydata/pandas/issues/622"">issue</a>).</p>
";;3;;2012-11-02T10:16:00.113;;13193256;2012-11-02T10:16:00.113;;;;;54567.0;13187778.0;2;19;;;
1286;1286;;;"<p>This can be accomplished quite simply with the DataFrame method <code>apply</code>.</p>

<pre><code>In[1]: import pandas as pd; import numpy as np

In[2]: df = pd.DataFrame(np.arange(40.).reshape((8, 5)), columns=list('abcde')); df
Out[2]: 
        a   b   c   d   e
    0   0   1   2   3   4
    1   5   6   7   8   9
    2  10  11  12  13  14
    3  15  16  17  18  19
    4  20  21  22  23  24
    5  25  26  27  28  29
    6  30  31  32  33  34
    7  35  36  37  38  39

In[3]: ser = pd.Series(np.arange(8) * 10); ser
Out[3]: 
    0     0
    1    10
    2    20
    3    30
    4    40
    5    50
    6    60
    7    70
</code></pre>

<p>Now that we have our <code>DataFrame</code> and <code>Series</code> we need a function to pass to <code>apply</code>.</p>

<pre><code>In[4]: func = lambda x: np.asarray(x) * np.asarray(ser)
</code></pre>

<p>We can pass this to <code>df.apply</code> and we are good to go </p>

<pre><code>In[5]: df.apply(func)
Out[5]:
          a     b     c     d     e
    0     0     0     0     0     0
    1    50    60    70    80    90
    2   200   220   240   260   280
    3   450   480   510   540   570
    4   800   840   880   920   960
    5  1250  1300  1350  1400  1450
    6  1800  1860  1920  1980  2040
    7  2450  2520  2590  2660  2730
</code></pre>

<p><code>df.apply</code> acts column-wise by default, but it can can also act row-wise by passing <code>axis=1</code> as an argument to <code>apply</code>.</p>

<pre><code>In[6]: ser2 = pd.Series(np.arange(5) *5); ser2
Out[6]: 
    0     0
    1     5
    2    10
    3    15
    4    20

In[7]: func2 = lambda x: np.asarray(x) * np.asarray(ser2)

In[8]: df.apply(func2, axis=1)
Out[8]: 
       a    b    c    d    e
    0  0    5   20   45   80
    1  0   30   70  120  180
    2  0   55  120  195  280
    3  0   80  170  270  380
    4  0  105  220  345  480
    5  0  130  270  420  580
    6  0  155  320  495  680
    7  0  180  370  570  780
</code></pre>

<p>This could be done more concisely by defining the anonymous function inside <code>apply</code></p>

<pre><code>In[9]: df.apply(lambda x: np.asarray(x) * np.asarray(ser))
Out[9]: 
          a     b     c     d     e
    0     0     0     0     0     0
    1    50    60    70    80    90
    2   200   220   240   260   280
    3   450   480   510   540   570
    4   800   840   880   920   960
    5  1250  1300  1350  1400  1450
    6  1800  1860  1920  1980  2040
    7  2450  2520  2590  2660  2730

In[10]: df.apply(lambda x: np.asarray(x) * np.asarray(ser2), axis=1)
Out[10]:
       a    b    c    d    e
    0  0    5   20   45   80
    1  0   30   70  120  180
    2  0   55  120  195  280
    3  0   80  170  270  380
    4  0  105  220  345  480
    5  0  130  270  420  580
    6  0  155  320  495  680
    7  0  180  370  570  780
</code></pre>
";;0;;2012-11-04T06:52:40.570;;13216688;2012-11-04T06:52:40.570;;;;;1742701.0;13166842.0;2;7;;;
1293;1293;;;"<p>Hierarchical indexing (also referred to as multi-level indexing) was introduced in the pandas 0.4 release. </p>

<p>This opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure (DataFrame), for example. </p>

<p>Imagine constructing a dataframe using <code>MultiIndex</code> like this:-</p>

<pre><code>import pandas as pd
import numpy as np

np.arrays = [['one','one','one','two','two','two'],[1,2,3,1,2,3]]

df = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=['A','B'])

df  # This is the dataframe we have generated

          A         B
one 1 -0.732470 -0.313871
    2 -0.031109 -2.068794
    3  1.520652  0.471764
two 1 -0.101713 -1.204458
    2  0.958008 -0.455419
    3 -0.191702 -0.915983
</code></pre>

<p>This <code>df</code> is simply a data structure of two dimensions </p>

<pre><code>df.ndim

2
</code></pre>

<p>But we can imagine it, looking at the output, as a 3 dimensional data structure.</p>

<ul>
<li><code>one</code> with <code>1</code> with data <code>-0.732470 -0.313871</code>. </li>
<li><code>one</code> with <code>2</code> with data <code>-0.031109 -2.068794</code>. </li>
<li><code>one</code> with <code>3</code> with data <code>1.520652  0.471764</code>.</li>
</ul>

<p>A.k.a.: ""effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure""</p>

<p>This is not just a ""pretty display"". It has the benefit of easy retrieval of data since we now have a hierarchal index.</p>

<p>For example.</p>

<pre><code>In [44]: df.ix[""one""]
Out[44]: 
          A         B
1 -0.732470 -0.313871
2 -0.031109 -2.068794
3  1.520652  0.471764
</code></pre>

<p>will give us a new data frame only for the group of data belonging to ""one"".</p>

<p>And we can narrow down our data selection further by doing this:-</p>

<pre><code>In [45]: df.ix[""one""].ix[1]
Out[45]: 
A   -0.732470
B   -0.313871
Name: 1
</code></pre>

<p>And of course, if we want a specific value, here's an example:-</p>

<pre><code>In [46]: df.ix[""one""].ix[1][""A""]
Out[46]: -0.73247029752040727
</code></pre>

<p>So if we have even more indexes (besides the 2 indexes shown in the example above), we can essentially drill down and select the data set we are really interested in without a need for <code>groupby</code>.</p>

<p>We can even grab a cross-section (either rows or columns) from our dataframe...</p>

<p>By rows:-</p>

<pre><code>In [47]: df.xs('one')
Out[47]: 
          A         B
1 -0.732470 -0.313871
2 -0.031109 -2.068794
3  1.520652  0.471764
</code></pre>

<p>By columns:-</p>

<pre><code>In [48]: df.xs('B', axis=1)
Out[48]: 
one  1   -0.313871
     2   -2.068794
     3    0.471764
two  1   -1.204458
     2   -0.455419
     3   -0.915983
Name: B
</code></pre>
";;2;;2012-11-05T05:22:05.390;;13226352;2016-10-20T17:52:16.210;2016-10-20T17:52:16.210;;2786884.0;;482506.0;13226029.0;2;54;;;
1303;1303;;;"<p>Use:</p>

<pre><code>pandas.set_option('display.max_columns', 7)
</code></pre>

<p>This will force Pandas to display the 7 columns you have. Or more generally:</p>

<pre><code>pandas.set_option('display.max_columns', None)
</code></pre>

<p>which will force it to display any number of columns.</p>

<p>Explanation: the default for <code>max_columns</code> is <code>0</code>, which tells Pandas to display the table only if all the columns can be squeezed into the width of your console.</p>
";;2;;2012-11-05T18:13:42.867;;13237914;2013-11-28T22:39:42.647;2013-11-28T22:39:42.647;;1579844.0;;1579844.0;11361985.0;2;125;;;
1315;1315;;;"<p>You need to install XCode AND you need to make sure you install the <strong>command line tools</strong> for XCode so you can get gcc.</p>
";;3;;2012-11-06T14:09:43.830;;13252767;2012-11-06T14:09:43.830;;;;;1306530.0;13249135.0;2;8;;;
1319;1319;;;"<pre><code>In [20]: df = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})

In [21]: df
Out[21]:
   A  B  values
0  1  1      10
1  1  2      15
2  2  1      20
3  2  2      25

In [22]: df['sum_values_A'] = df.groupby('A')['values'].transform(np.sum)

In [23]: df
Out[23]:
   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45
</code></pre>
";;1;;2012-11-06T19:07:38.437;;13257677;2012-11-06T19:07:38.437;;;;;1548051.0;13256917.0;2;19;;;
1332;1332;;;"<p>If you have a key that is repeated for each row, then you can produce a cartesian product using merge (like you would in SQL).</p>

<pre><code>from pandas import DataFrame, merge
df1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})
df2 = DataFrame({'key':[1,1], 'col3':[5,6]})

merge(df1, df2,on='key')[['col1', 'col2', 'col3']]
</code></pre>

<p>See here for the documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra</a></p>
";;1;;2012-11-07T12:47:36.283;;13270110;2012-11-07T12:47:36.283;;;;;1452002.0;13269890.0;2;31;;;
1343;1343;;;"<p>I believe <code>DataFrame.fillna()</code> will do this for you.</p>

<p>Link to Docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer"">a dataframe</a> and for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html"" rel=""noreferrer"">a Series</a>. </p>

<p>Example: </p>

<pre><code>In [7]: df
Out[7]: 
          0         1
0       NaN       NaN
1 -0.494375  0.570994
2       NaN       NaN
3  1.876360 -0.229738
4       NaN       NaN

In [8]: df.fillna(0)
Out[8]: 
          0         1
0  0.000000  0.000000
1 -0.494375  0.570994
2  0.000000  0.000000
3  1.876360 -0.229738
4  0.000000  0.000000
</code></pre>

<p>To fill the NaNs in only one column, select just that column. in this case I'm using inplace=True to actually change the contents of df. </p>

<pre><code>In [12]: df[1].fillna(0, inplace=True)
Out[12]: 
0    0.000000
1    0.570994
2    0.000000
3   -0.229738
4    0.000000
Name: 1

In [13]: df
Out[13]: 
          0         1
0       NaN  0.000000
1 -0.494375  0.570994
2       NaN  0.000000
3  1.876360 -0.229738
4       NaN  0.000000
</code></pre>
";;7;;2012-11-08T18:54:27.467;;13295801;2016-06-23T17:29:23.563;2016-06-23T17:29:23.563;;3760920.0;;484596.0;13295735.0;2;265;;;
1351;1351;;;"<p>What's wrong with</p>

<pre><code>result = dataframe.mul(series, axis=0)
</code></pre>

<p>?</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#flexible-binary-operations"">http://pandas.pydata.org/pandas-docs/stable/basics.html#flexible-binary-operations</a></p>
";;2;;2012-11-09T21:01:11.110;;13315962;2012-11-09T21:01:11.110;;;;;776560.0;13166842.0;2;31;;;
1352;1352;;;"<p>How about:</p>

<pre><code>df.insert(0, 'mean', df.mean(1))
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion</a></p>
";;1;;2012-11-09T21:04:03.803;;13316001;2012-11-09T21:04:03.803;;;;;776560.0;13148429.0;2;58;;;
1362;1362;;;"<p>How to add single item. This is not very effective but follows what you are asking for:</p>

<pre><code>x = p.Series()
N = 4
for i in xrange(N):
   x = x.set_value(i, i**2)
</code></pre>

<p>produces x:</p>

<pre><code>0    0
1    1
2    4
3    9
</code></pre>

<p>Obviously there are better ways to generate this series in only one shot.  </p>

<p>For your second question check answer and references of SO question <a href=""https://stackoverflow.com/questions/10715965/add-one-row-in-a-pandas-dataframe"">add one row in a pandas.DataFrame</a>. </p>
";;4;;2012-11-11T15:42:23.360;;13332682;2012-11-11T15:42:23.360;2017-05-23T12:34:38.620;;-1.0;;308903.0;13331518.0;2;27;;;
1364;1364;;;"<p>Here's an example using <code>apply</code> on the dataframe, which I am calling with <code>axis = 1</code>. </p>

<p>Note the difference is that instead of trying to pass two values to the function <code>f</code>, rewrite the function to accept a pandas Series object, and then index the Series to get the values needed. </p>

<pre><code>In [49]: df
Out[49]: 
          0         1
0  1.000000  0.000000
1 -0.494375  0.570994
2  1.000000  0.000000
3  1.876360 -0.229738
4  1.000000  0.000000

In [50]: def f(x):    
   ....:  return x[0] + x[1]  
   ....:  

In [51]: df.apply(f, axis=1) #passes a Series object, row-wise
Out[51]: 
0    1.000000
1    0.076619
2    1.000000
3    1.646622
4    1.000000
</code></pre>

<p>Depending on your use case, it is sometimes helpful to create a pandas <code>group</code> object, and then use <code>apply</code> on the group. </p>
";;11;;2012-11-12T01:39:09.783;;13337376;2012-11-12T14:57:18.917;2012-11-12T14:57:18.917;;484596.0;;484596.0;13331698.0;2;143;;;
1384;1384;;;"<p>You can just use reindex on a time series using your date range. Also it looks like you would be better off using a TimeSeries instead of a DataFrame (see <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html"" rel=""noreferrer"">documentation</a>), although reindexing is also the correct method for adding missing index values to DataFrames as well.</p>

<p>For example, starting with:</p>

<pre><code>date_index = pd.DatetimeIndex([pd.datetime(2003,6,24), pd.datetime(2003,8,13),
        pd.datetime(2003,8,19), pd.datetime(2003,8,22), pd.datetime(2003,8,24)])

ts = pd.Series([2,1,2,1,5], index=date_index)
</code></pre>

<p>Gives you a time series like your example dataframe's head:</p>

<pre><code>2003-06-24    2
2003-08-13    1
2003-08-19    2
2003-08-22    1
2003-08-24    5
</code></pre>

<p>Simply doing </p>

<pre><code>ts.reindex(pd.date_range(min(date_index), max(date_index)))
</code></pre>

<p>then gives you a complete index, with NaNs for your missing values (you can use <code>fillna</code> if you want to fill the missing values with some other values - see <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html#missing-data-fillna"" rel=""noreferrer"">here</a>):</p>

<pre><code>2003-06-24     2
2003-06-25   NaN
2003-06-26   NaN
2003-06-27   NaN
2003-06-28   NaN
2003-06-29   NaN
2003-06-30   NaN
2003-07-01   NaN
2003-07-02   NaN
2003-07-03   NaN
2003-07-04   NaN
2003-07-05   NaN
2003-07-06   NaN
2003-07-07   NaN
2003-07-08   NaN
2003-07-09   NaN
2003-07-10   NaN
2003-07-11   NaN
2003-07-12   NaN
2003-07-13   NaN
2003-07-14   NaN
2003-07-15   NaN
2003-07-16   NaN
2003-07-17   NaN
2003-07-18   NaN
2003-07-19   NaN
2003-07-20   NaN
2003-07-21   NaN
2003-07-22   NaN
2003-07-23   NaN
2003-07-24   NaN
2003-07-25   NaN
2003-07-26   NaN
2003-07-27   NaN
2003-07-28   NaN
2003-07-29   NaN
2003-07-30   NaN
2003-07-31   NaN
2003-08-01   NaN
2003-08-02   NaN
2003-08-03   NaN
2003-08-04   NaN
2003-08-05   NaN
2003-08-06   NaN
2003-08-07   NaN
2003-08-08   NaN
2003-08-09   NaN
2003-08-10   NaN
2003-08-11   NaN
2003-08-12   NaN
2003-08-13     1
2003-08-14   NaN
2003-08-15   NaN
2003-08-16   NaN
2003-08-17   NaN
2003-08-18   NaN
2003-08-19     2
2003-08-20   NaN
2003-08-21   NaN
2003-08-22     1
2003-08-23   NaN
2003-08-24     5
Freq: D, Length: 62
</code></pre>
";;1;;2012-11-14T00:10:23.060;;13371090;2012-11-14T00:22:51.240;2012-11-14T00:22:51.240;;1452002.0;;1452002.0;13370525.0;2;18;;;
1387;1387;;;"<p>Just want to reiterate this will work in pandas >= 0.9.1:</p>

<pre><code>In [2]: read_csv('sample.csv', dtype={'ID': object})
Out[2]: 
                           ID
0  00013007854817840016671868
1  00013007854817840016749251
2  00013007854817840016754630
3  00013007854817840016781876
4  00013007854817840017028824
5  00013007854817840017963235
6  00013007854817840018860166
</code></pre>

<p>I'm creating an issue about detecting integer overflows also.</p>

<p>EDIT: See resolution here: <a href=""https://github.com/pydata/pandas/issues/2247"">https://github.com/pydata/pandas/issues/2247</a></p>
";;2;;2012-11-14T17:58:57.017;;13384494;2012-12-10T15:59:06.817;2012-12-10T15:59:06.817;;776560.0;;776560.0;13293810.0;2;51;;;
1391;1391;;;"<p>Well, the whitespace is in your data, so you can't read in the data without reading in the whitespace.  However, after you've read it in, you could strip out the whitespace by doing, e.g., <code>df[""Make""] = df[""Make""].map(str.strip)</code> (where <code>df</code> is your dataframe).</p>
";;0;;2012-11-14T19:29:04.327;;13385921;2012-11-14T19:29:04.327;;;;;1427416.0;13385860.0;2;19;;;
1392;1392;;;"<p>You could use converters:</p>

<pre><code>import pandas as pd

def strip(text):
    try:
        return text.strip()
    except AttributeError:
        return text

def make_int(text):
    return int(text.strip('"" '))

table = pd.read_table(""data.csv"", sep=r',',
                      names=[""Year"", ""Make"", ""Model"", ""Description""],
                      converters = {'Description' : strip,
                                    'Model' : strip,
                                    'Make' : strip,
                                    'Year' : make_int})
print(table)
</code></pre>

<p>yields</p>

<pre><code>   Year     Make   Model              Description
0  1997     Ford    E350                     None
1  1997     Ford    E350                     None
2  1997     Ford    E350   Super, luxurious truck
3  1997     Ford    E350  Super ""luxurious"" truck
4  1997     Ford    E350    Super luxurious truck
5  1997     Ford    E350                     None
6  1997     Ford    E350                     None
7  2000  Mercury  Cougar                     None
</code></pre>
";;0;;2012-11-14T19:35:40.043;;13386025;2012-11-14T19:35:40.043;;;;;190597.0;13385860.0;2;30;;;
1399;1399;;;"<p>As <a href=""https://stackoverflow.com/a/31490891/1240268"">Robbie-Clarken answers</a>, since 0.14 you can pass a <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers"" rel=""nofollow noreferrer"">slice in the tuple you pass to loc</a>:</p>

<pre><code>In [11]: s.loc[('b', slice(2, 10))]
Out[11]:
b  2   -0.65394
   4    0.08227
dtype: float64
</code></pre>

<p>Indeed, you can pass a slice for each level:</p>

<pre><code>In [12]: s.loc[(slice('a', 'b'), slice(2, 10))]
Out[12]:
a  5    0.27919
b  2   -0.65394
   4    0.08227
dtype: float64
</code></pre>

<p><em>Note: the slice is inclusive.</em></p>

<hr>

<h3>Old answer:</h3>

<p>You can also do this using:</p>

<pre><code>s.ix[1:10, ""b""]
</code></pre>

<p>(It's good practice to do in a single ix/loc/iloc since this version allows assignment.)</p>

<p>This answer was written prior to the <a href=""https://github.com/pydata/pandas/pull/2922"" rel=""nofollow noreferrer"">introduction of iloc</a> in early 2013, i.e. position/integer location - which may be preferred in this case. The reason it was created was to remove the ambiguity from integer-indexed pandas objects, and be more descriptive: ""I'm slicing on position"".</p>

<pre><code>s[""b""].iloc[1:10]
</code></pre>

<p>That said, I kinda disagree with the docs that ix is:</p>

<blockquote>
  <p>most robust and consistent way</p>
</blockquote>

<p>it's not, the most consistent way is to describe what you're doing:</p>

<ul>
<li>use loc for labels</li>
<li>use iloc for position</li>
<li>use ix for both (if you really have to)</li>
</ul>

<p>Remember the <a href=""https://www.python.org/dev/peps/pep-0020/"" rel=""nofollow noreferrer"">zen of python</a>:</p>

<blockquote>
  <p>explicit is better than implicit</p>
</blockquote>
";;3;;2012-11-15T00:30:57.783;;13389808;2016-01-28T00:12:42.207;2017-05-23T11:59:59.107;;-1.0;;1240268.0;13389203.0;2;13;;;
1412;1412;;;"<p>it depends what sort of t-test you want to do (one sided or two sided dependent or independent) but it should be as simple as:</p>

<pre><code>from scipy.stats import ttest_ind

cat1 = my_data[my_data['Category']=='cat1']
cat2 = my_data[my_data['Category']=='cat2']

ttest_ind(cat1['values'], cat2['values'])
&gt;&gt;&gt; (1.4927289925706944, 0.16970867501294376)
</code></pre>

<p>it returns a tuple with the t-statistic &amp; the p-value</p>

<p>see here for other t-tests <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html"" rel=""noreferrer"">http://docs.scipy.org/doc/scipy/reference/stats.html</a></p>
";;0;;2012-11-16T09:34:21.770;;13413842;2012-11-16T09:34:21.770;;;;;240068.0;13404468.0;2;43;;;
1413;1413;;;"<p>Don't <code>drop</code>. Just take rows where <code>EPS</code> is <strong>finite</strong>:</p>

<pre><code>df = df[np.isfinite(df['EPS'])]
</code></pre>
";;4;;2012-11-16T09:34:38.297;;13413845;2012-11-16T09:34:38.297;;;;;449449.0;13413590.0;2;235;;;
1414;1414;;;"<p>It's good practice to always use the <code>[]</code> notation, one reason is that attribute notation (<code>df.column_name</code>) does not work for numbered indices:</p>

<pre><code>In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])

In [2]: df[1]
Out[2]: 
0    2
1    5
Name: 1

In [3]: df.1
  File ""&lt;ipython-input-3-e4803c0d1066&gt;"", line 1
    df.1
       ^
SyntaxError: invalid syntax
</code></pre>
";;0;;2012-11-16T11:33:47.677;;13415772;2012-11-16T11:33:47.677;;;;;1240268.0;13411544.0;2;23;;;
1427;1427;;;"<p>This question is already resolved, but... </p>

<p>...also consider the solution suggested by Wouter in <a href=""https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-of-certain-column-is-nan/13434501#comment18328797_13413590"">his original comment</a>. The ability to handle missing data, including <code>dropna()</code>, is built into pandas explicitly. Aside from potentially improved performance over doing it manually, these functions also come with a variety of options which may be useful. </p>

<pre><code>In [24]: df = pd.DataFrame(np.random.randn(10,3))

In [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;

In [26]: df
Out[26]:
          0         1         2
0       NaN       NaN       NaN
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
4       NaN       NaN  0.050742
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
8       NaN       NaN  0.637482
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [27]: df.dropna()     #drop all rows that have any NaN values
Out[27]:
          0         1         2
1  2.677677 -1.466923 -0.750366
5 -1.250970  0.030561 -2.678622
7  0.049896 -0.308003  0.823295
</code></pre>

<hr>

<pre><code>In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN
Out[28]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
4       NaN       NaN  0.050742
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
8       NaN       NaN  0.637482
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN
Out[29]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
5 -1.250970  0.030561 -2.678622
7  0.049896 -0.308003  0.823295
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)
Out[30]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
9 -0.310130  0.078891       NaN
</code></pre>

<p>There are also other options (See docs at <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html</a>), including dropping columns instead of rows. </p>

<p>Pretty handy! </p>
";;2;;2012-11-17T20:27:33.253;;13434501;2017-08-14T00:04:56.077;2017-08-14T00:04:56.077;;2285236.0;;484596.0;13413590.0;2;410;;;
1433;1433;;;"<p>How about:</p>

<pre><code>d = d.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)
</code></pre>

<p>The <code>applymap</code> function applies a function to every cell of the dataframe.</p>
";;4;;2012-11-18T23:15:17.407;;13445630;2012-11-18T23:15:17.407;;;;;1427416.0;13445241.0;2;22;;;
1435;1435;;;"<p><code>freq='M'</code> is for month-end frequencies (see <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"" rel=""noreferrer"">here</a>). But you can use <code>.shift</code> to shift it by any number of days (or any frequency for that matter):</p>

<pre><code>pd.date_range(start, end, freq='M').shift(15, freq=pd.datetools.day)
</code></pre>
";;2;;2012-11-19T00:47:38.977;;13446268;2012-11-19T00:47:38.977;;;;;1452002.0;13445174.0;2;19;;;
1441;1441;;;"<p>Edit: Thanks to @WesMcKinney for showing this much more direct way:</p>

<pre><code>data[data.groupby('tag').pid.transform(len) &gt; 1]
</code></pre>

<hr>

<pre><code>import pandas
import numpy as np
data = pandas.DataFrame(
    {'pid' : [1,1,1,2,2,3,3,3],
     'tag' : [23,45,62,24,45,34,25,62],
     })

bytag = data.groupby('tag').aggregate(np.count_nonzero)
tags = bytag[bytag.pid &gt;= 2].index
print(data[data['tag'].isin(tags)])
</code></pre>

<p>yields</p>

<pre><code>   pid  tag
1    1   45
2    1   62
4    2   45
7    3   62
</code></pre>
";;2;;2012-11-19T03:17:23.713;;13447176;2012-11-25T12:31:33.617;2012-11-25T12:31:33.617;;190597.0;;190597.0;13446480.0;2;17;;;
1448;1448;;;"<p>You can use the append function to add another element to it. Only, make a series of the new element, before you append it:</p>

<pre><code>test.append(pd.Series(200, index=[101]))
</code></pre>
";;1;;2012-11-19T15:03:18.223;;13456432;2012-11-19T15:03:18.223;;;;;170005.0;13331518.0;2;12;;;
1464;1464;;;"<p>It's difficult to make <code>del df.column_name</code> work simply as the result of syntactic limitations in Python. <code>del df[name]</code> gets translated to <code>df.__delitem__(name)</code> under the covers by Python</p>
";;5;;2012-11-21T03:12:31.480;;13485766;2012-11-21T03:12:31.480;;;;;776560.0;13411544.0;2;374;;;
1506;1506;;;"<p>A pandas MultiIndex consists of a list of tuples. So the most natural approach would be to reshape your input dict so that its keys are tuples corresponding to the multi-index values you require. Then you can just construct your dataframe using <code>pd.DataFrame.from_dict</code>, using the option <code>orient='index'</code>: </p>

<pre><code>user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},
                  'Category 2': {'att_1': 23, 'att_2': 'another'}},
             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},
                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}

pd.DataFrame.from_dict({(i,j): user_dict[i][j] 
                           for i in user_dict.keys() 
                           for j in user_dict[i].keys()},
                       orient='index')


               att_1     att_2
12 Category 1      1  whatever
   Category 2     23   another
15 Category 1     10       foo
   Category 2     30       bar
</code></pre>

<p>An alternative approach would be to build your dataframe up by concatenating the component dataframes:</p>

<pre><code>user_ids = []
frames = []

for user_id, d in user_dict.iteritems():
    user_ids.append(user_id)
    frames.append(pd.DataFrame.from_dict(d, orient='index'))

pd.concat(frames, keys=user_ids)

               att_1     att_2
12 Category 1      1  whatever
   Category 2     23   another
15 Category 1     10       foo
   Category 2     30       bar
</code></pre>
";;3;;2012-11-27T10:05:17.983;;13581730;2017-01-09T23:36:38.933;2017-01-09T23:36:38.933;;839957.0;;1548051.0;13575090.0;2;42;;;
1512;1512;;;"<p>The <strong>result</strong> of <code>kl.aggregate(np.sum)</code> is a normal DataFrame, you just have to assign it to a variable to further use it. With some random data:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
&gt;&gt;&gt;                         'foo', 'bar', 'foo', 'foo'],
...                  'B' : ['one', 'one', 'two', 'three',
...                         'two', 'two', 'one', 'three'],
...                  'C' : randn(8), 'D' : randn(8)})
&gt;&gt;&gt; grouped = df.groupby('A')
&gt;&gt;&gt; grouped
&lt;pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630&gt;
&gt;&gt;&gt; test = grouped.aggregate(np.sum)
&gt;&gt;&gt; test
            C         D
A                      
bar -1.852376  2.204224
foo -3.398196 -0.045082
</code></pre>
";;2;;2012-11-27T11:16:51.217;;13583024;2012-11-27T11:16:51.217;;;;;653364.0;13582449.0;2;12;;;
1517;1517;;;"<p>You can simply pass the functions as a list:</p>

<pre><code>In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})
Out[20]: 
        returns          
            sum      mean

dummy                    
1      0.285833  0.028583
</code></pre>

<p>or as a dictionary:</p>

<pre><code>In [21]: df.groupby('dummy').agg({'returns':
                                  {'Mean': np.mean, 'Sum': np.sum}})
Out[21]: 
        returns          
            Sum      Mean
dummy                    
1      0.285833  0.028583
</code></pre>
";;6;;2012-11-27T20:57:33.097;;13592901;2016-04-08T12:20:20.700;2016-04-08T12:20:20.700;;1301710.0;;1301710.0;12589481.0;2;32;;;
1518;1518;;;"<p>It is possible to read the data with pandas and to concatenate it.</p>

<p>First import the data</p>

<pre><code>In [449]: import pandas.io.data as web

In [450]: nab = web.get_data_yahoo('NAB.AX', start='2009-05-25',
                                   end='2009-06-05')[['Close', 'Volume']]

In [451]: cba = web.get_data_yahoo('CBA.AX', start='2009-05-26',
                                   end='2009-06-08')[['Close', 'Volume']]

In [453]: nab
Out[453]: 
            Close    Volume
Date                       
2009-05-25  21.15   9685100
2009-05-26  21.64   8541900
2009-05-27  21.74   9042900
2009-05-28  21.63   9701000
2009-05-29  22.02  14665700
2009-06-01  22.52   6782000
2009-06-02  22.80  10473400
2009-06-03  23.11   9931400
2009-06-04  22.21  17869000
2009-06-05  21.95   8214300

In [454]: cba
Out[454]: 
            Close    Volume
Date                       
2009-05-26  35.45   4529600
2009-05-27  35.13   4521500
2009-05-28  33.95   7945400
2009-05-29  35.14  12548500
2009-06-01  36.16   4509400
2009-06-02  36.33   4304900
2009-06-03  36.80   4845400
2009-06-04  36.79   4592300
2009-06-05  36.51   4417500
2009-06-08  36.51         0
</code></pre>

<p>Than concatenate it:</p>

<pre><code>In [455]: keys = ['CBA.AX','NAB.AX']

In [456]: pd.concat([cba, nab], axis=1, keys=keys)
Out[456]: 
            CBA.AX            NAB.AX          
             Close    Volume   Close    Volume
Date                                          
2009-05-25     NaN       NaN   21.15   9685100
2009-05-26   35.45   4529600   21.64   8541900
2009-05-27   35.13   4521500   21.74   9042900
2009-05-28   33.95   7945400   21.63   9701000
2009-05-29   35.14  12548500   22.02  14665700
2009-06-01   36.16   4509400   22.52   6782000
2009-06-02   36.33   4304900   22.80  10473400
2009-06-03   36.80   4845400   23.11   9931400
2009-06-04   36.79   4592300   22.21  17869000
2009-06-05   36.51   4417500   21.95   8214300
2009-06-08   36.51         0     NaN       NaN
</code></pre>
";;0;;2012-11-27T21:57:44.917;;13593882;2012-11-27T21:57:44.917;;;;;1301710.0;11714768.0;2;6;;;
1533;1533;;;"<p>Pandas (and numpy) allow for <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#boolean-indexing"" rel=""noreferrer"">boolean indexing</a>, which will be much more efficient:</p>

<pre><code>In [11]: df.loc[df['col1'] &gt;= 1, 'col1']
Out[11]: 
1    1
2    2
Name: col1

In [12]: df[df['col1'] &gt;= 1]
Out[12]: 
   col1  col2
1     1    11
2     2    12

In [13]: df[(df['col1'] &gt;= 1) &amp; (df['col1'] &lt;=1 )]
Out[13]: 
   col1  col2
1     1    11
</code></pre>

<p>If you want to write helper functions for this, consider something along these lines:</p>

<pre><code>In [14]: def b(x, col, op, n): 
             return op(x[col],n)

In [15]: def f(x, *b):
             return x[(np.logical_and(*b))]

In [16]: b1 = b(df, 'col1', ge, 1)

In [17]: b2 = b(df, 'col1', le, 1)

In [18]: f(df, b1, b2)
Out[18]: 
   col1  col2
1     1    11
</code></pre>

<p>Update: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">pandas 0.13 has a query method</a> for these kind of use cases, assuming column names are valid identifiers the following works (and can be more efficient for large frames as it uses <a href=""https://github.com/pydata/numexpr"" rel=""noreferrer"">numexpr</a> behind the scenes):</p>

<pre><code>In [21]: df.query('col1 &lt;= 1 &amp; 1 &lt;= col1')
Out[21]:
   col1  col2
1     1    11

In [22]: df.query(""col1 &lt;= 1 and 1 &lt;= df['col1']"") # use df[] syntax if not a valid identifier
Out[22]:
   col1  col2
1     1    11
</code></pre>
";;9;;2012-11-28T23:38:41.263;;13616382;2014-04-03T18:05:16.710;2014-04-03T18:05:16.710;;1240268.0;;1240268.0;13611065.0;2;86;;;
1554;1554;;;"<p>There isn't an option to filter the rows before the CSV file is loaded into a pandas object. </p>

<p>You can either load the file and then filter using <code>df[df['field'] &gt; constant]</code>, or if you have a very large file and you are worried about memory running out, then use an iterator and apply the filter as you concatenate chunks of your file e.g.:</p>

<pre><code>iter_csv = pandas.read_csv('file.csv', iterator=True, chunksize=1000)
df = pd.concat([chunk[chunk['field'] &gt; constant] for chunk in iter_csv])
</code></pre>

<p>You can vary the <code>chunksize</code> to suit your available memory. See <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk"" rel=""noreferrer"">here</a> for more details.</p>
";;1;;2012-11-30T21:31:28.067;;13653490;2015-06-27T13:45:21.277;2015-06-27T13:45:21.277;;1452002.0;;1452002.0;13651117.0;2;69;;;
1556;1556;;;"<p>You could use <code>pd.to_datetime</code>:</p>

<pre><code>In [1]: import pandas as pd

In [2]: pd.to_datetime('2008-02-27')
Out[2]: datetime.datetime(2008, 2, 27, 0, 0)
</code></pre>

<p>This allows you to ""clean"" the index (or similarly a column) by applying it to the Series:</p>

<pre><code>df.index = pd.to_datetime(df.index)
</code></pre>

<p>or</p>

<pre><code>df['date_col'] = df['date_col'].apply(pd.to_datetime)
</code></pre>
";;2;;2012-12-01T00:51:12.523;;13655271;2012-12-01T00:51:12.523;;;;;1240268.0;13654699.0;2;41;;;
1558;1558;;;"<p>If you use groupby, you will get what you want. </p>

<pre><code>d.groupby(['ip', 'useragent']).count()
</code></pre>

<p>produces:</p>

<pre><code>ip          useragent               
192.168.0.1 a           2
            b           1
192.168.0.2 b           1
</code></pre>
";;6;;2012-12-01T13:34:16.033;;13659944;2015-07-02T12:36:05.437;2015-07-02T12:36:05.437;;125617.0;;1452002.0;13659881.0;2;31;;;
1560;1560;;;"<p>I had this exact problem. </p>

<p>The issue is that there is an old version of numpy in the default mac install, and that <code>pip install pandas</code> sees that one first and fails -- not going on to see that there is a newer version that <code>pip</code> herself has installed.  </p>

<p>If you're on a default mac install, and you've done <code>pip install numpy --upgrade</code> to be sure you're up to date, but <code>pip install pandas</code> still fails due to an old <code>numpy</code>, try the following: </p>

<pre><code>$ cd /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/
$ sudo rm -r numpy
$ pip install pandas
</code></pre>

<p>This should now install / build pandas.</p>

<p>To check it out what we've done, do the following: start python, and <code>import numpy</code> and <code>import pandas</code>. With any luck, <code>numpy.__version__</code> will be 1.6.2 (or greater), and <code>pandas.__version__</code> will be 0.9.1 (or greater).  </p>

<p>If you'd like to see where pip has put (found!) them, just <code>print(numpy)</code> and <code>print(pandas)</code>. </p>
";;5;;2012-12-02T00:07:29.573;;13665099;2013-01-02T22:41:52.107;2013-01-02T22:41:52.107;;1453172.0;;1453172.0;12436979.0;2;8;;;
1561;1561;;;"<p>Both <code>pandas</code> and <code>matplotlib.dates</code> use <code>matplotlib.units</code> for locating the ticks. </p>

<p>But while <code>matplotlib.dates</code> has convenient ways to set the ticks manually, pandas seems to have the focus on auto formatting so far (you can have a look at the <a href=""https://github.com/pydata/pandas/blob/master/pandas/tseries/converter.py"" rel=""noreferrer"">code</a> for date conversion and formatting in pandas).</p>

<p>So for the moment it seems more reasonable to use <code>matplotlib.dates</code> (as mentioned by @BrenBarn in his comment).</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import matplotlib.dates as dates

idx = pd.date_range('2011-05-01', '2011-07-01')
s = pd.Series(np.random.randn(len(idx)), index=idx)

fig, ax = plt.subplots()
ax.plot_date(idx.to_pydatetime(), s, 'v-')
ax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),
                                                interval=1))
ax.xaxis.set_minor_formatter(dates.DateFormatter('%d\n%a'))
ax.xaxis.grid(True, which=""minor"")
ax.yaxis.grid()
ax.xaxis.set_major_locator(dates.MonthLocator())
ax.xaxis.set_major_formatter(dates.DateFormatter('\n\n\n%b\n%Y'))
plt.tight_layout()
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/gSOoY.png"" alt=""pandas_like_date_fomatting""></p>

<p>(my locale is German, so that Tuesday [Tue] becomes Dienstag [Di])  </p>
";;1;;2012-12-02T21:53:14.187;;13674286;2012-12-02T21:53:14.187;;;;;1301710.0;12945971.0;2;62;;;
1563;1563;;;"<p>Similar to @locojay suggestion, you can apply <a href=""http://docs.python.org/2/library/difflib.html""><code>difflib</code></a>'s <a href=""http://docs.python.org/2/library/difflib.html#difflib.get_close_matches""><code>get_closest_matches</code></a> to <code>df2</code>'s index and then apply a <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#joining-on-index""><code>join</code></a>:</p>

<pre><code>In [23]: import difflib 

In [24]: difflib.get_close_matches
Out[24]: &lt;function difflib.get_close_matches&gt;

In [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])

In [26]: df2
Out[26]: 
      letter
one        a
two        b
three      c
four       d
five       e

In [31]: df1.join(df2)
Out[31]: 
       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e
</code></pre>

<p>.</p>

<p>If these were columns, in the same vein you could apply to the column then <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#database-style-dataframe-joining-merging""><code>merge</code></a>:</p>

<pre><code>df1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])
df2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])

df2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])
df1.merge(df2)
</code></pre>
";;4;;2012-12-03T10:06:04.680;;13680953;2012-12-03T10:23:45.310;2012-12-03T10:23:45.310;;1240268.0;;1240268.0;13636848.0;2;31;;;
1565;1565;;;"<pre><code>data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))
</code></pre>
";;4;;2012-12-03T11:33:51.673;;13682381;2012-12-03T11:33:51.673;;;;;449449.0;13682044.0;2;64;;;
1568;1568;;;"<p>There's a bug here: currently cannot pass arguments to <code>str.lstrip</code> and <code>str.rstrip</code>:</p>

<p><a href=""http://github.com/pydata/pandas/issues/2411"" rel=""noreferrer"">http://github.com/pydata/pandas/issues/2411</a></p>

<p>EDIT: 2012-12-07 this works now on the dev branch:</p>

<pre><code>In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')
Out[8]: 
1     52
2     62
3     44
4     30
5    110
Name: result
</code></pre>
";;1;;2012-12-03T17:00:37.013;;13688105;2012-12-07T16:31:06.253;2012-12-07T16:31:06.253;;776560.0;;776560.0;13682044.0;2;10;;;
1583;1583;;;"<p>One option is to use <code>str</code>, and then <code>to_datetime</code> (or similar):</p>

<pre><code>In [11]: str(dt64)
Out[11]: '2012-05-01T01:00:00.000000+0100'

In [12]: pd.to_datetime(str(dt64))
Out[12]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))
</code></pre>

<p><em>Note: it is not equal to <code>dt</code> because it's become <a href=""https://stackoverflow.com/questions/796008/python-cant-subtract-offset-naive-and-offset-aware-datetimes"">""offset-aware""</a>:</em></p>

<pre><code>In [13]: pd.to_datetime(str(dt64)).replace(tzinfo=None)
Out[13]: datetime.datetime(2012, 5, 1, 1, 0)
</code></pre>

<p>This seems inelegant.</p>

<p>.</p>

<p>Update: this can deal with the ""nasty example"":</p>

<pre><code>In [21]: dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')

In [22]: pd.to_datetime(str(dt64)).replace(tzinfo=None)
Out[22]: datetime.datetime(2002, 6, 28, 1, 0)
</code></pre>
";;2;;2012-12-04T13:08:29.863;;13703721;2013-06-05T09:30:44.263;2017-05-23T12:02:48.590;;-1.0;;1240268.0;13703720.0;2;7;;;
1584;1584;;;"<pre><code>&gt;&gt;&gt; dt64.tolist()
datetime.datetime(2012, 5, 1, 0, 0)
</code></pre>

<p>For <code>DatetimeIndex</code>, the <code>tolist</code> returns a list of <code>datetime</code> objects. For a single <code>datetime64</code> object it returns a single <code>datetime</code> object.</p>
";;5;;2012-12-04T13:22:10.337;;13703930;2012-12-04T13:22:10.337;;;;;449449.0;13703720.0;2;21;;;
1585;1585;;;"<p>To convert <code>numpy.datetime64</code> to datetime object that represents time in UTC on <code>numpy-1.8</code>:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; dt = datetime.utcnow()
&gt;&gt;&gt; dt
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)
&gt;&gt;&gt; dt64 = np.datetime64(dt)
&gt;&gt;&gt; ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
&gt;&gt;&gt; ts
1354650685.3624549
&gt;&gt;&gt; datetime.utcfromtimestamp(ts)
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)
&gt;&gt;&gt; np.__version__
'1.8.0.dev-7b75899'
</code></pre>

<p>The above example assumes that a naive datetime object is interpreted by <code>np.datetime64</code> as time in UTC.</p>

<hr>

<p>To convert datetime to np.datetime64 and back (<code>numpy-1.6</code>):</p>

<pre><code>&gt;&gt;&gt; np.datetime64(datetime.utcnow()).astype(datetime)
datetime.datetime(2012, 12, 4, 13, 34, 52, 827542)
</code></pre>

<p>It works both on a single np.datetime64 object and a numpy array of np.datetime64.</p>

<p>Think of np.datetime64 the same way you would about np.int8, np.int16, etc and apply the same methods to convert beetween Python objects such as int, datetime and corresponding numpy objects.</p>

<p>Your ""nasty example"" works correctly:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy 
&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)
datetime.datetime(2002, 6, 28, 0, 0)
&gt;&gt;&gt; numpy.__version__
'1.6.2' # current version available via pip install numpy
</code></pre>

<p>I can reproduce the <code>long</code> value on <code>numpy-1.8.0</code> installed as:</p>

<pre><code>pip install git+https://github.com/numpy/numpy.git#egg=numpy-dev
</code></pre>

<p>The same example:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)
1025222400000000000L
&gt;&gt;&gt; numpy.__version__
'1.8.0.dev-7b75899'
</code></pre>

<p>It returns <code>long</code> because for <code>numpy.datetime64</code> type <code>.astype(datetime)</code> is equivalent to <code>.astype(object)</code> that returns Python integer (<code>long</code>) on <code>numpy-1.8</code>. </p>

<p>To get datetime object you could:</p>

<pre><code>&gt;&gt;&gt; dt64.dtype
dtype('&lt;M8[ns]')
&gt;&gt;&gt; ns = 1e-9 # number of seconds in a nanosecond
&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int) * ns)
datetime.datetime(2002, 6, 28, 0, 0)
</code></pre>

<p>To get datetime64 that uses seconds directly:</p>

<pre><code>&gt;&gt;&gt; dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')
&gt;&gt;&gt; dt64.dtype
dtype('&lt;M8[s]')
&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int))
datetime.datetime(2002, 6, 28, 0, 0)
</code></pre>

<p>The <a href=""http://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html"">numpy docs</a> say that the datetime API is experimental and may change in future numpy versions.</p>
";;9;;2012-12-04T13:42:08.067;;13704307;2012-12-04T20:30:33.787;2012-12-04T20:30:33.787;;4279.0;;4279.0;13703720.0;2;72;;;
1587;1587;;;"<p>Also check out <a href=""https://store.continuum.io/cshop/anaconda"" rel=""noreferrer"">Anaconda</a> by Continuum Analytics. It includes numpy, scipy, pandas, and ""all the other goodness"". Available for Linux, Windows, and MacOS. Anaconda Community Edition is free and active. </p>

<p>Here's a complete list of packages it includes: 
  <a href=""http://docs.continuum.io/anaconda/pkgs.html"" rel=""noreferrer"">http://docs.continuum.io/anaconda/pkgs.html</a></p>
";;0;;2012-12-04T22:57:19.633;;13713475;2013-09-27T21:16:08.877;2013-09-27T21:16:08.877;;344821.0;;484596.0;9555635.0;2;13;;;
1596;1596;;;"<p>A generic way:</p>

<pre><code>[tuple(x) for x in data_set.to_records(index=False)]
</code></pre>
";;0;;2012-12-05T19:42:50.703;;13731128;2012-12-05T19:42:50.703;;;;;1876739.0;9758450.0;2;27;;;
1601;1601;;;"<p>your call to the function ""weekday"" does not work as it operates on the index of data.my_dt, which is an int64 array (this is where the error message comes from)</p>

<p>you could create a new column in data containing the weekdays using something like:</p>

<pre><code>data['weekday'] = data['my_dt'].apply(lambda x: x.weekday())
</code></pre>

<p>then you can filter for weekdays with:</p>

<pre><code>weekdays_only = data[data['weekday'] &lt; 5 ]
</code></pre>

<p>I hope this helps</p>
";;2;;2012-12-06T10:19:32.793;;13741439;2012-12-06T10:19:32.793;;;;;1768121.0;13740672.0;2;22;;;
1608;1608;;;"<p>Welcome to hell.</p>

<p>You can just pass a datetime64 object to <code>pandas.Timestamp</code>:</p>

<pre><code>In [16]: Timestamp(numpy.datetime64('2012-05-01T01:00:00.000000'))
Out[16]: &lt;Timestamp: 2012-05-01 01:00:00&gt;
</code></pre>

<p>I noticed that this doesn't work right though in NumPy 1.6.1:</p>

<pre><code>numpy.datetime64('2012-05-01T01:00:00.000000+0100')
</code></pre>

<p>Also, <code>pandas.to_datetime</code> can be used (this is off of the dev version, haven't checked v0.9.1):</p>

<pre><code>In [24]: pandas.to_datetime('2012-05-01T01:00:00.000000+0100')
Out[24]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))
</code></pre>
";;2;;2012-12-06T22:40:22.060;;13753918;2012-12-06T22:40:22.060;;;;;776560.0;13703720.0;2;82;;;
1612;1612;;;"<p>I think <em>the</em> default way is to use:</p>

<pre><code>df1 = pandas.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
             'dat a1': range(7)})

df1['dat a1']
</code></pre>

<p>The other methods, like exposing it as an attribute are more for convenience.</p>
";;2;;2012-12-07T07:36:50.637;;13758846;2012-12-07T07:36:50.637;;;;;1755432.0;13757090.0;2;13;;;
1634;1634;;;"<p>Here's a couple of suggestions:</p>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html"" rel=""noreferrer""><code>date_range</code></a> for the index:</p>

<pre><code>import datetime
import pandas as pd
import numpy as np

todays_date = datetime.datetime.now().date()
index = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')

columns = ['A','B', 'C']
</code></pre>

<p><em>Note: we could create an empty DataFrame (with <code>NaN</code>s) simply by writing:</em></p>

<pre><code>df_ = pd.DataFrame(index=index, columns=columns)
df_ = df_.fillna(0) # with 0s rather than NaNs
</code></pre>

<p>To do these type of calculations for the data, use a numpy array:</p>

<pre><code>data = np.array([np.arange(10)]*3).T
</code></pre>

<p>Hence we can create the DataFrame:</p>

<pre><code>In [10]: df = pd.DataFrame(data, index=index, columns=columns)

In [11]: df
Out[11]: 
            A  B  C
2012-11-29  0  0  0
2012-11-30  1  1  1
2012-12-01  2  2  2
2012-12-02  3  3  3
2012-12-03  4  4  4
2012-12-04  5  5  5
2012-12-05  6  6  6
2012-12-06  7  7  7
2012-12-07  8  8  8
2012-12-08  9  9  9
</code></pre>
";;7;;2012-12-09T09:40:46.303;;13786327;2017-01-31T14:26:04.293;2017-01-31T14:26:04.293;;604687.0;;1240268.0;13784192.0;2;118;;;
1637;1637;;;"<p>Update: if you're doing this to a DatetimeIndex / datetime64 column a better way is to use <code>np.round</code> directly rather than via an apply/map:</p>

<pre><code>np.round(dtindex_or_datetime_col.astype(np.int64), -9).astype('datetime64[ns]')
</code></pre>

<p>Old answer (with some more explanation):</p>

<p>Whilst @Matti's answer is clearly the correct way to deal with your situation, I thought I would add an answer how you might round a Timestamp to the nearest second:</p>

<pre><code>from pandas.lib import Timestamp

t1 = Timestamp('2012-1-1 00:00:00')
t2 = Timestamp('2012-1-1 00:00:00.000333')

In [4]: t1
Out[4]: &lt;Timestamp: 2012-01-01 00:00:00&gt;

In [5]: t2
Out[5]: &lt;Timestamp: 2012-01-01 00:00:00.000333&gt;

In [6]: t2.microsecond
Out[6]: 333

In [7]: t1.value
Out[7]: 1325376000000000000L

In [8]: t2.value
Out[8]: 1325376000000333000L

# Alternatively: t2.value - t2.value % 1000000000
In [9]: long(round(t2.value, -9)) # round milli-, micro- and nano-seconds
Out[9]: 1325376000000000000L

In [10]: Timestamp(long(round(t2.value, -9)))
Out[10]: &lt;Timestamp: 2012-01-01 00:00:00&gt;
</code></pre>

<p>Hence you can apply this to the entire index:</p>

<pre><code>def to_the_second(ts):
    return Timestamp(long(round(ts.value, -9)))

dtindex.map(to_the_second)
</code></pre>
";;4;;2012-12-09T14:28:02.343;;13788301;2014-03-15T06:32:02.193;2014-03-15T06:32:02.193;;1240268.0;;1240268.0;13785932.0;2;15;;;
1660;1660;;;"<p>Try this,</p>

<pre><code>df.drop_duplicates(['foo','bar'])
df.pivot('foo','bar','baz')
</code></pre>
";;1;;2012-12-12T05:14:29.717;;13833239;2012-12-12T05:32:09.713;2012-12-12T05:32:09.713;;603744.0;;1896737.0;11232275.0;2;12;;;
1665;1665;;;"<p>Pandas 0.15 introduced <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"">Categorical Series</a>, which allows a much clearer way to do this:</p>

<p>First make the month column a categorical and specify the ordering to use.</p>

<pre><code>In [21]: df['m'] = pd.Categorical(df['m'], [""March"", ""April"", ""Dec""])

In [22]: df  # looks the same!
Out[22]:
   a  b      m
0  1  2  March
1  5  6    Dec
2  3  4  April
</code></pre>

<p>Now, when you sort the month column it will sort with respect to that list:</p>

<pre><code>In [23]: df.sort(""m"")
Out[23]:
   a  b      m
0  1  2  March
2  3  4  April
1  5  6    Dec
</code></pre>

<p><em>Note: if a value is not in the list it will be converted to NaN.</em></p>

<hr>

<p>An older answer for those interested...</p>

<p>You could create an intermediary series, and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.set_index.html""><code>set_index</code></a> on that:</p>

<pre><code>df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])
s = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])
s.sort()

In [4]: df.set_index(s.index).sort()
Out[4]: 
   a  b      m
0  1  2  March
1  3  4  April
2  5  6    Dec
</code></pre>

<hr>

<p>As commented, in newer pandas, Series has a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html""><code>replace</code></a> method to do this more elegantly:</p>

<pre><code>s = df['m'].replace({'March':0, 'April':1, 'Dec':3})
</code></pre>

<p><em>The slight difference is that this won't raise if there is a value outside of the dictionary (it'll just stay the same).</em></p>
";;8;;2012-12-12T11:44:10.410;;13839029;2015-02-03T04:54:11.257;2015-02-03T04:54:11.257;;1240268.0;;1240268.0;13838405.0;2;33;;;
1672;1672;;;"<p><a href=""https://stackoverflow.com/a/24517695/190597"">RukTech's answer</a>, <code>df.set_value('C', 'x', 10)</code>, is far and away faster than the options I've suggested below.</p>

<hr>

<p><strong>Warning</strong>: It is sometimes difficult to predict if an operation returns a copy or a view. For this reason the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">docs recommend avoiding using ""chained indexing""</a>. </p>

<hr>

<p><strong>Why <code>df.xs('C')['x']=10</code> does not work:</strong></p>

<p><code>df.xs('C')</code> by default, returns a new dataframe <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html#pandas.DataFrame.xsY"" rel=""noreferrer"">with a copy</a> of the data, so </p>

<pre><code>df.xs('C')['x']=10
</code></pre>

<p>modifies this new dataframe only.</p>

<p><code>df['x']</code> returns a view of the <code>df</code> dataframe, so </p>

<pre><code>df['x']['C'] = 10
</code></pre>

<p>modifies <code>df</code> itself.</p>

<hr>

<p>Alternatively,</p>

<pre><code>df.xs('C', copy = False)['x']=10
</code></pre>

<p><em>does</em> modify <code>df</code>.</p>

<hr>

<p><code>df.set_value('C', 'x', 10)</code> is the fastest:</p>

<pre><code>In [18]: %timeit df.set_value('C', 'x', 10)
100000 loops, best of 3: 2.9 s per loop

In [20]: %timeit df['x']['C'] = 10
100000 loops, best of 3: 6.31 s per loop

In [19]: %timeit df.ix['C','x'] = 10
10000 loops, best of 3: 104 s per loop

In [32]: %timeit df.xs('C', copy=False)['x'] = 10
10000 loops, best of 3: 89.2 s per loop
</code></pre>
";;8;;2012-12-12T14:51:02.073;;13842286;2016-04-07T00:57:45.973;2017-05-23T11:47:29.467;;-1.0;;190597.0;13842088.0;2;168;;;
1674;1674;;;"<p>This is the simple way of adding a new column: <code>df['e'] = e</code></p>
";;3;;2012-12-12T16:04:31.217;;13843741;2016-12-10T06:53:58.073;2016-12-10T06:53:58.073;;1645853.0;;1645853.0;12555323.0;2;99;;;
1677;1677;;;"<p>When you do <code>len(df['column name'])</code> you are just getting one number, namely the number of rows in the DataFrame (i.e., the length of the column itself).  If you want to apply <code>len</code> to each element in the column, use <code>df['column name'].map(len)</code>.  So try</p>

<pre><code>df[df['column name'].map(len) &lt; 2]
</code></pre>
";;1;;2012-12-13T01:37:01.637;;13851602;2012-12-13T01:37:01.637;;;;;1427416.0;13851535.0;2;42;;;
1683;1683;;;"<p><code>transform</code> is not that well documented, but it seems that the way it works is that what the transform function is passed is not the entire group as a dataframe, but a single column of a single group.  I don't think it's really meant for what you're trying to do, and your solution with <code>apply</code> is fine.</p>

<p>So suppose <code>tips.groupby('smoker').transform(func)</code>.  There will be two groups, call them group1 and group2.  The transform does not call <code>func(group1)</code> and <code>func(group2)</code>.  Instead, it calls <code>func(group1['total_bill'])</code>, then <code>func(group1['tip'])</code>, etc., and then <code>func(group2['total_bill'])</code>, <code>func(group2['total_bill'])</code>.  Here's an example:</p>

<pre><code>&gt;&gt;&gt; print d
   A  B  C
0 -2  5  4
1  1 -1  2
2  0  2  1
3 -3  1  2
4  5  0  2
&gt;&gt;&gt; def foo(df):
...     print ""&gt;&gt;&gt;""
...     print df
...     print ""&lt;&lt;&lt;""
...     return df
&gt;&gt;&gt; print d.groupby('C').transform(foo)
&gt;&gt;&gt;
2    0
Name: A
&lt;&lt;&lt;
&gt;&gt;&gt;
2    2
Name: B
&lt;&lt;&lt;
&gt;&gt;&gt;
1    1
3   -3
4    5
Name: A
&lt;&lt;&lt;
&gt;&gt;&gt;
1   -1
3    1
4    0
Name: B
# etc.
</code></pre>

<p>You can see that <code>foo</code> is first called with just the A column of the C=1 group of the original data frame, then the B column of that group, then the A column of the C=2 group, etc.</p>

<p>This makes sense if you think about what transform is for.  It's meant for applying transform functions on the groups.  But in general, these functions won't make sense when applied to the entire group, only to a given column.  For instance, the example in the pandas docs is about z-standardizing using <code>transform</code>.  If you have a DataFrame with columns for age and weight, it wouldn't make sense to z-standardize with respect to the overall mean of both these variables.  It doesn't even mean anything to take the overall mean of a bunch of numbers, some of which are ages and some of which are weights.  You have to z-standardize the age with respect to the mean age and the weight with respect to the mean weight, which means you want to transform separately for each column.</p>

<p>So basically, you don't need to use transform here.  <code>apply</code> is the appropriate function here, because <code>apply</code> really does operate on each group as a single DataFrame, while <code>transform</code> operates on each column of each group.</p>
";;2;;2012-12-13T07:19:15.917;;13854901;2012-12-13T07:19:15.917;;;;;1427416.0;13854476.0;2;31;;;
1689;1689;;;"<p>On pandas 0.9.1 and higher this should work (this is with 0.10.0b1):</p>

<p>(<strong>Edit:</strong> As of Pandas 0.19, method <code>sort_index</code> is deprecated. Prefer <a href=""http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.DataFrame.sort_values.html?highlight=sort_values#pandas.DataFrame.sort_values"" rel=""nofollow noreferrer"" title=""sort_values""><code>sort_values</code></a>)</p>

<pre><code>In [23]: songs.sort_index(by=['Peak', 'Weeks'], ascending=[True, False])
Out[23]: 
                                      Song  Peak  Weeks
10                           She Loves You     1     36
118                               Hey Jude     1     27
20                I Want To Hold Your Hand     1     24
22                       Can't Buy Me Love     1     17
56                                   Help!     1     17
76                        Paperback Writer     1     16
109                   All You Need Is Love     1     16
45                             I Feel Fine     1     15
29                      A Hard Day's Night     1     14
48                          Ticket To Ride     1     14
85                           Eleanor Rigby     1     14
87                        Yellow Submarine     1     14
173            The Ballad Of John And Yoko     1     13
60                             Day Tripper     1     12
61                      We Can Work It Out     1     12
117                           Lady Madonna     1      9
8                           From Me To You     1      7
115                          Hello Goodbye     1      7
155                               Get Back     1      6
2                         Please Please Me     2     20
107                   Magical Mystery Tour     2     16
176                              Let It Be     2     14
93                              Penny Lane     2     13
92               Strawberry Fields Forever     2     12
0                               Love Me Do     4     26
166                          Come Together     4     10
157                              Something     4      9
58                               Yesterday     8     21
135                   Back In The U.S.S.R.    19      3
164                     Here Comes The Sun    58     19
96   Sgt. Pepper's Lonely Hearts Club Band    63     12
105     With A Little Help From My Friends    63      7
</code></pre>
";;3;;2012-12-13T18:24:51.697;;13866073;2017-04-28T11:57:26.910;2017-04-28T11:57:26.910;;15649.0;;776560.0;13636592.0;2;30;;;
1699;1699;;;"<p>Try:</p>

<pre><code>ax = df1.plot()
df2.plot(ax=ax)
</code></pre>
";;3;;2012-12-14T05:09:24.720;;13873014;2012-12-14T05:09:24.720;;;;;1306530.0;13872533.0;2;144;;;
1701;1701;;;"<p>Although Chang's answer explains how to plot multiple times on the same figure, in this case you might be better off in this case using a <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""noreferrer""><code>groupby</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack"" rel=""noreferrer""><code>unstack</code></a>ing:</p>

<p><em>(Assuming you have this in dataframe, with datetime index already)</em></p>

<pre><code>In [1]: df
Out[1]:
            value  
datetime                         
2010-01-01      1  
2010-02-01      1  
2009-01-01      1  

# create additional month and year columns for convenience
df['Month'] = map(lambda x: x.month, df.index)
df['Year'] = map(lambda x: x.year, df.index)    

In [5]: df.groupby(['Month','Year']).mean().unstack()
Out[5]:
       value      
Year    2009  2010
Month             
1          1     1
2        NaN     1
</code></pre>

<p>Now it's easy to plot (each year as a separate line):</p>

<pre><code>df.groupby(['Month','Year']).mean().unstack().plot()
</code></pre>
";;0;;2012-12-14T10:24:59.393;;13876784;2012-12-14T10:24:59.393;;;;;1240268.0;13872533.0;2;15;;;
1710;1710;;;"<p>One way is to use <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#some-gory-internal-details"" rel=""noreferrer""><code>index.levels</code></a>:</p>

<pre><code>In [11]: df
Out[11]: 
       C
A B     
0 one  3
1 one  2
2 two  1

In [12]: df.index.levels[1]
Out[12]: Index([one, two], dtype=object)
</code></pre>
";;4;;2012-12-15T01:46:22.580;;13888546;2012-12-15T02:21:36.730;2012-12-15T02:21:36.730;;1240268.0;;1240268.0;13888468.0;2;24;;;
1715;1715;;;"<p>Use our friend <code>lookup</code>, designed precisely for this purpose:</p>

<pre><code>In [17]: prices
Out[17]: 
              AAPL    GOOG     IBM    XOM
2011-01-10  339.44  614.21  142.78  71.57
2011-01-13  342.64  616.69  143.92  73.08
2011-01-26  340.82  616.50  155.74  75.89
2011-02-02  341.29  612.00  157.93  79.46
2011-02-10  351.42  616.44  159.32  79.68
2011-03-03  356.40  609.56  158.73  82.19
2011-05-03  345.14  533.89  167.84  82.00
2011-06-03  340.42  523.08  160.97  78.19
2011-06-10  323.03  509.51  159.14  76.84
2011-08-01  393.26  606.77  176.28  76.67
2011-12-20  392.46  630.37  184.14  79.97

In [18]: orders
Out[18]: 
                  Date direction  size ticker  prices
0  2011-01-10 00:00:00       Buy  1500   AAPL  339.44
1  2011-01-13 00:00:00      Sell  1500   AAPL  342.64
2  2011-01-13 00:00:00       Buy  4000    IBM  143.92
3  2011-01-26 00:00:00       Buy  1000   GOOG  616.50
4  2011-02-02 00:00:00      Sell  4000    XOM   79.46
5  2011-02-10 00:00:00       Buy  4000    XOM   79.68
6  2011-03-03 00:00:00      Sell  1000   GOOG  609.56
7  2011-03-03 00:00:00      Sell  2200    IBM  158.73
8  2011-06-03 00:00:00      Sell  3300    IBM  160.97
9  2011-05-03 00:00:00       Buy  1500    IBM  167.84
10 2011-06-10 00:00:00       Buy  1200   AAPL  323.03
11 2011-08-01 00:00:00       Buy    55   GOOG  606.77
12 2011-08-01 00:00:00      Sell    55   GOOG  606.77
13 2011-12-20 00:00:00      Sell  1200   AAPL  392.46

In [19]: prices.lookup(orders.Date, orders.ticker)
Out[19]: 
array([ 339.44,  342.64,  143.92,  616.5 ,   79.46,   79.68,  609.56,
        158.73,  160.97,  167.84,  323.03,  606.77,  606.77,  392.46])
</code></pre>
";;3;;2012-12-15T15:47:22.063;;13893632;2012-12-15T15:47:22.063;;;;;776560.0;13893227.0;2;39;;;
1725;1725;;;"<p><code>df.shape</code>, where <code>df</code> is your DataFrame.</p>
";;0;;2012-12-17T20:29:56.957;;13921674;2012-12-17T20:29:56.957;;;;;1427416.0;13921647.0;2;74;;;
1741;1741;;;"<p>You nearly had it, but you have to use the <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.bitwise_or.html#numpy.bitwise_or"">""bitwise or""</a> operator:</p>

<pre><code>In [6]: df[(df.one == 1) | (df.two == 7)]
Out[6]: 
   one  three  two
0    1      9    5
2    3     17    7

In [7]: df[(df.one.isin(checkList)) | (df.two.isin(checkList))]
Out[7]: 
   one  three  two
0    1      9    5
2    3     17    7
</code></pre>
";;0;;2012-12-18T16:18:48.200;;13937141;2012-12-18T16:34:54.167;2012-12-18T16:34:54.167;;1240268.0;;1240268.0;13937022.0;2;22;;;
1744;1744;;;"<p>Something like this should work:</p>

<pre><code>&gt;&gt;&gt; from pandas import DataFrame
&gt;&gt;&gt; 
&gt;&gt;&gt; df = DataFrame({""A"": [8,9,5,4], ""B"": [3,4,4,8], ""C"": [5,0,3,5], ""D"": [8,4,8,1]})
&gt;&gt;&gt; df.max()
A    9
B    8
C    5
D    8
&gt;&gt;&gt; (df * 1.0)/df.max()
          A      B    C      D
0  0.888889  0.375  1.0  1.000
1  1.000000  0.500  0.0  0.500
2  0.555556  0.500  0.6  1.000
3  0.444444  1.000  1.0  0.125
</code></pre>

<p>Note that I multiplied <code>df</code> by <code>1.0</code> so that it didn't consists of <code>int</code>s anymore (<code>.astype(float)</code> would have worked too) to avoid integer division and a resulting DataFrame full of 0s and 1s.</p>
";;0;;2012-12-18T18:03:13.457;;13938831;2012-12-18T18:03:13.457;;;;;487339.0;13938704.0;2;13;;;
1766;1766;;;"<p>You can store the dataframe with an index of the columns as follows:</p>

<pre><code>import pandas as pd
import numpy as np
from pandas.io.pytables import Term

index = pd.date_range('1/1/2000', periods=8)
df = pd.DataFrame( np.random.randn(8,3), index=index, columns=list('ABC'))  

store = pd.HDFStore('mydata.h5')
store.append('df_cols', df, axes='columns')
</code></pre>

<p>and then select as you might hope:</p>

<pre><code>In [8]: store.select('df_cols', [Term('columns', '=', 'A')])
Out[8]: 
2000-01-01    0.347644
2000-01-02    0.477167
2000-01-03    1.419741
2000-01-04    0.641400
2000-01-05   -1.313405
2000-01-06   -0.137357
2000-01-07   -1.208429
2000-01-08   -0.539854
</code></pre>

<p>Where:</p>

<pre><code>In [9]: df
Out[9]: 
                   A         B         C
2000-01-01  0.347644  0.895084 -1.457772
2000-01-02  0.477167  0.464013 -1.974695
2000-01-03  1.419741  0.470735 -0.309796
2000-01-04  0.641400  0.838864 -0.112582
2000-01-05 -1.313405 -0.678250 -0.306318
2000-01-06 -0.137357 -0.723145  0.982987
2000-01-07 -1.208429 -0.672240  1.331291
2000-01-08 -0.539854 -0.184864 -1.056217
</code></pre>

<p>.</p>

<p>To me this isn't an ideal solution, as we can only indexing the DataFrame by one thing! Worryingly <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#hdf5-pytables"" rel=""noreferrer"">the docs</a> seem to suggest you <em>can</em> only index a DataFrame by one thing, at least using <code>axes</code>:</p>

<blockquote>
  <p>Pass the axes keyword with a list of dimension (currently must by exactly 1 less than the total dimensions of the object).</p>
</blockquote>

<p>I may be reading this incorrectly, in which case <strong>hopefully someone can prove me wrong!</strong></p>

<p>.</p>

<p>Note: One way I have found to index a DataFrame by two things (index and columns), is to convert it to a Panel, which can then retrieve using two indices. However then we have to convert to the selected subpanel to a DataFrame each time items are retrieved... again, not ideal.</p>
";;4;;2012-12-20T17:16:15.857;;13977244;2012-12-20T17:16:15.857;;;;;1240268.0;13926089.0;2;11;;;
1794;1794;;;"<pre><code>In [16]: df.groupby('id')['x'].apply(pd.rolling_mean, 2, min_periods=1)
Out[16]: 
0    0.0
1    0.5
2    1.5
3    3.0
4    3.5
5    4.5

In [17]: df.groupby('id')['x'].cumsum()
Out[17]: 
0     0
1     1
2     3
3     3
4     7
5    12
</code></pre>
";;1;;2012-12-21T23:41:42.507;;13998600;2012-12-21T23:41:42.507;;;;;243434.0;13996302.0;2;18;;;
1795;1795;;;"<p>The way HDFStore records tables, the columns are stored by type as single numpy arrays. You always get back all of the columns, you can filter on them, so you will be returned for what you ask. In 0.10.0 you can pass a Term that involves columns.</p>

<pre><code>store.select('df', [ Term('index', '&gt;', Timestamp('20010105')), 
                     Term('columns', '=', ['A','B']) ])
</code></pre>

<p>or you can reindex afterwards</p>

<pre><code>df = store.select('df', [ Term('index', '&gt;', Timestamp('20010105') ])
df.reindex(columns = ['A','B'])
</code></pre>

<p>The <code>axes</code> is not really the solution here (what you actually created was in effect storing a transposed frame). This parameter allows you to re-order the storage of axes to enable data alignment in different ways. For a dataframe it really doesn't mean much; for 3d or 4d structures, on-disk data alignment is crucial for really fast queries. </p>

<p>0.10.1 will allow a more elegant solution, namely data columns, that is, you can elect certain columns to be represented as there own columns in the table store, so you really can select just them. Here is a taste what is coming.</p>

<pre><code> store.append('df', columns = ['A','B','C'])
 store.select('df', [ 'A &gt; 0', Term('index', '&gt;', Timestamp(2000105)) ])
</code></pre>

<p>Another way to do go about this is to store separate tables in different nodes of the file, then you can select only what you need.</p>

<p>In general, I recommend again really wide tables. hayden offers up the Panel solution, which might be a benefit for you now, as the actual data arangement should reflect how you want to query the data.</p>
";;3;;2012-12-22T01:27:49.173;;13999234;2012-12-22T01:27:49.173;;;;;644898.0;13926089.0;2;10;;;
1797;1797;;;"<p>You could use <a href=""http://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior""><code>strftime</code></a> to save these as separate columns:</p>

<pre><code>df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))
df['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))
</code></pre>

<p>and then be specific about which columns to export to csv:</p>

<pre><code>df[['date', 'time', ... ]].to_csv('df.csv')
</code></pre>
";;0;;2012-12-22T05:46:53.990;;14000420;2012-12-22T06:10:38.707;2012-12-22T06:10:38.707;;1240268.0;;1240268.0;13999850.0;2;27;;;
1804;1804;;;"<p>For DataFrame <code>df</code>:</p>

<pre><code>import numpy as np
index = df['b'].index[df['b'].apply(np.isnan)]
</code></pre>

<p>will give you back the <code>MultiIndex</code> that you can use to index back into <code>df</code>, e.g.:</p>

<pre><code>df['a'].ix[index[0]]
&gt;&gt;&gt; 1.452354
</code></pre>

<p>For the integer index:</p>

<pre><code>df_index = df.index.values.tolist()
[df_index.index(i) for i in index]
&gt;&gt;&gt; [3, 6]
</code></pre>
";;0;;2012-12-24T03:02:12.320;;14016590;2012-12-24T03:02:12.320;;;;;696023.0;14016247.0;2;24;;;
1816;1816;;;"<p>Here is a simpler solution:</p>

<p><code>inds = pd.isnull(df).any(1).nonzero()[0]</code></p>

<pre><code>In [9]: df
Out[9]: 
          0         1
0  0.450319  0.062595
1 -0.673058  0.156073
2 -0.871179 -0.118575
3  0.594188       NaN
4 -1.017903 -0.484744
5  0.860375  0.239265
6 -0.640070       NaN
7 -0.535802  1.632932
8  0.876523 -0.153634
9 -0.686914  0.131185

In [10]: pd.isnull(df).any(1).nonzero()[0]
Out[10]: array([3, 6])
</code></pre>
";;2;;2012-12-25T18:41:23.723;;14033137;2012-12-25T18:41:23.723;;;;;776560.0;14016247.0;2;95;;;
1833;1833;;;"<p>You can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_not.html#numpy.logical_not"" rel=""noreferrer""><code>numpy.logical_not</code></a> to invert the boolean array returned by <code>isin</code>:</p>

<pre><code>In [63]: s = pd.Series(np.arange(10.0))

In [64]: x = range(4, 8)

In [65]: mask = np.logical_not(s.isin(x))

In [66]: s[mask]
Out[66]: 
0    0
1    1
2    2
3    3
8    8
9    9
</code></pre>

<p>As given in the comment by Wes McKinney you can also use</p>

<pre><code>s[-s.isin(x)]
</code></pre>
";;3;;2012-12-27T17:46:47.653;;14058892;2013-02-09T20:25:21.680;2013-02-09T20:25:21.680;;1301710.0;;1301710.0;14057007.0;2;39;;;
1835;1835;;;"<p>You can use the DataFrame <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html#pandas.DataFrame.apply"" rel=""noreferrer""><code>apply</code></a> method:</p>

<pre><code>order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']
                                               if row['Action']=='Sell'
                                               else -row['Prices']*row['Amount']),
                                   axis=1)
</code></pre>

<p><em>It is usually faster to use these methods rather than over for loops.</em></p>
";;1;;2012-12-27T18:59:12.093;;14059783;2012-12-27T18:59:12.093;;;;;1240268.0;14059094.0;2;15;;;
1837;1837;;;"<p>I pushed a fix for this yesterday. Here's the new behavior on github master:</p>

<pre><code>In [1]: paste
from pandas import DataFrame
f = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})
f.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]
f

## -- End pasted text --
Out[1]: 
  level1 item1 level1 item2
               level2 item2
  level3 item1 level3 item2
0            1            2
1            2            3
2            3            4

In [2]: f['level1 item1']
Out[2]: 
  level3 item1
0            1
1            2
2            3
</code></pre>
";;0;;2012-12-27T19:46:05.930;;14060360;2012-12-27T19:46:05.930;;;;;1306530.0;14025879.0;2;8;;;
1840;1840;;;"<p>If we're willing to sacrifice the succinctness of Hayden's solution, one could also do something like this:  </p>

<pre><code>In [22]: orders_df['C'] = orders_df.Action.apply(
               lambda x: (1 if x == 'Sell' else -1))

In [23]: orders_df   # New column C represents the sign of the transaction
Out[23]:
   Prices  Amount Action  C
0       3      57   Sell  1
1      89      42   Sell  1
2      45      70    Buy -1
3       6      43   Sell  1
4      60      47   Sell  1
5      19      16    Buy -1
6      56      89   Sell  1
7       3      28    Buy -1
8      56      69   Sell  1
9      90      49    Buy -1
</code></pre>

<p>Now we have eliminated the need for the <code>if</code> statement. Using <code>DataFrame.apply()</code>, we also do away with the <code>for</code> loop. As Hayden noted, vectorized operations are always faster. </p>

<pre><code>In [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C

In [25]: orders_df   # The resulting dataframe
Out[25]:
   Prices  Amount Action  C  Value
0       3      57   Sell  1    171
1      89      42   Sell  1   3738
2      45      70    Buy -1  -3150
3       6      43   Sell  1    258
4      60      47   Sell  1   2820
5      19      16    Buy -1   -304
6      56      89   Sell  1   4984
7       3      28    Buy -1    -84
8      56      69   Sell  1   3864
9      90      49    Buy -1  -4410
</code></pre>

<p>This solution takes two lines of code instead of one, but is a bit easier to read. I suspect that the computational costs are similar as well. </p>
";;0;;2012-12-27T20:05:45.073;;14060625;2012-12-27T20:05:45.073;;;;;484596.0;14059094.0;2;7;;;
1841;1841;;;"<p>As it looks messy in my comment above, I decided to provide another answer which is a syntax update for pandas 0.10.0 on Marc's answer, combined with Wes' hint:</p>

<pre><code>import pandas as pd
from datetime import datetime

dr = pd.date_range(datetime(2009,1,1),datetime(2010,12,31),freq='H')
dt = pd.DataFrame(rand(len(dr),2),dr)
hour = dt.index.hour
selector = ((10 &lt;= hour) &amp; (hour &lt;= 13)) | ((20&lt;=hour) &amp; (hour&lt;=23))
data = dt[selector]
</code></pre>
";;1;;2012-12-28T00:16:44.100;;14063022;2014-12-29T23:26:31.057;2014-12-29T23:26:31.057;;680232.0;;680232.0;10565282.0;2;6;;;
1843;1843;;;"<p>I think an elegant solution is to use the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-where-method-and-masking""><code>where</code></a> method (also see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html""><code>API docs</code></a>):</p>

<pre><code>In [37]: values = df.Prices * df.Amount

In [38]: df['Values'] = values.where(df.Action == 'Sell', other=-values)

In [39]: df
Out[39]: 
   Prices  Amount Action  Values
0       3      57   Sell     171
1      89      42   Sell    3738
2      45      70    Buy   -3150
3       6      43   Sell     258
4      60      47   Sell    2820
5      19      16    Buy    -304
6      56      89   Sell    4984
7       3      28    Buy     -84
8      56      69   Sell    3864
9      90      49    Buy   -4410
</code></pre>

<p>Further more this should be the fastest solution.</p>
";;4;;2012-12-28T14:47:45.373;;14071265;2016-04-14T05:47:30.807;2016-04-14T05:47:30.807;;1301710.0;;1301710.0;14059094.0;2;50;;;
1881;1881;;;"<p>With this setup:</p>

<pre><code>import pandas as pd
import io

text = '''\
STK_ID RPT_Date sales cash
000568 20120930 80.093 57.488
000596 20120930 32.585 26.177
000799 20120930 14.784 8.157
'''

df = pd.read_csv(io.BytesIO(text), delimiter = ' ', 
                 converters = {0:str})
df.set_index(['STK_ID','RPT_Date'], inplace = True)
</code></pre>

<p>The index, <code>df.index</code> can be reassigned to a new <code>MultiIndex</code> like this:</p>

<pre><code>index = df.index
names = index.names
index = [('000999','20121231')] + df.index.tolist()[1:]
df.index = pd.MultiIndex.from_tuples(index, names = names)
print(df)
#                   sales    cash
# STK_ID RPT_Date                
# 000999 20121231  80.093  57.488
# 000596 20120930  32.585  26.177
# 000799 20120930  14.784   8.157
</code></pre>

<p>Or, the index could be made into columns, the values in the columns could be then reassigned, and then the columns returned to indices:</p>

<pre><code>df.reset_index(inplace = True)
df.ix[0, ['STK_ID', 'RPT_Date']] = ('000999','20121231')
df = df.set_index(['STK_ID','RPT_Date'])
print(df)

#                   sales    cash
# STK_ID RPT_Date                
# 000999 20121231  80.093  57.488
# 000596 20120930  32.585  26.177
# 000799 20120930  14.784   8.157
</code></pre>

<hr>

<p>Benchmarking with IPython <code>%timeit</code> suggests reassigning the index (the first method, above) is significantly faster than resetting the index, modifying column values, and then setting the index again (the second method, above):</p>

<pre><code>In [2]: %timeit reassign_index(df)
10000 loops, best of 3: 158 us per loop

In [3]: %timeit reassign_columns(df)
1000 loops, best of 3: 843 us per loop
</code></pre>
";;1;;2013-01-01T13:40:39.977;;14110955;2017-08-25T18:23:34.540;2017-08-25T18:23:34.540;;190597.0;;190597.0;14110721.0;2;16;;;
1898;1898;;;"<p>As of today, Google App Engine Python 2.7 runtime <a href=""https://developers.google.com/appengine/docs/python/runtime#Pure_Python"" rel=""noreferrer"">doesn't currently allow</a> running arbitrary C extensions.</p>

<p>But it does allow <a href=""https://developers.google.com/appengine/docs/python/config/appconfig#Configuring_Libraries"" rel=""noreferrer"">inclusion</a> of <a href=""https://developers.google.com/appengine/docs/python/tools/libraries27"" rel=""noreferrer"">third party library</a> that have been hand selected by the engineering team based on their popularity and developer demand.</p>

<p>You can request for a specific library to be included by filling a feature request on the <a href=""https://code.google.com/p/googleappengine/issues/detail?id=8620"" rel=""noreferrer""><strong>public issue tracker</strong></a>. If it gets enough ??? it is likely to be considered for inclusion in an upcoming release.</p>
";;5;;2013-01-03T22:47:21.133;;14148511;2013-01-03T23:04:30.363;2013-01-03T23:04:30.363;;656408.0;;656408.0;14144867.0;2;17;;;
1903;1903;;;"<p>You can replace <code>nan</code> with <code>None</code> in your numpy array:</p>

<pre><code>&gt;&gt;&gt; x = np.array([1, np.nan, 3])
&gt;&gt;&gt; y = np.where(np.isnan(x), None, x)
&gt;&gt;&gt; print y
[1.0 None 3.0]
&gt;&gt;&gt; print type(y[1])
&lt;type 'NoneType'&gt;
</code></pre>
";;1;;2013-01-04T18:57:52.763;;14163174;2013-01-04T18:57:52.763;;;;;1361822.0;14162723.0;2;8;;;
1904;1904;;;"<p>@bogatron has it right, you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#where-and-masking"" rel=""noreferrer""><code>where</code></a>, it's worth noting that you can do this natively in pandas:</p>

<pre><code>df1 = df.where((pd.notnull(df)), None)
</code></pre>

<p>Note: this changes the dtype of <strong>all columns</strong> to <code>object</code>.</p>

<p>Example:</p>

<pre><code>In [1]: df = pd.DataFrame([1, np.nan])

In [2]: df
Out[2]: 
    0
0   1
1 NaN

In [3]: df1 = df.where((pd.notnull(df)), None)

In [4]: df1
Out[4]: 
      0
0     1
1  None
</code></pre>

<hr>

<p>Note: what you cannot do recast the DataFrames <code>dtype</code> to allow all datatypes types, using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.astype.html#pandas.DataFrame.astype"" rel=""noreferrer""><code>astype</code></a>, and then the DataFrame <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>fillna</code></a> method:</p>

<pre><code>df1 = df.astype(object).replace(np.nan, 'None')
</code></pre>

<p><em>Unfortunately neither this, nor using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html#pandas.DataFrame.replace"" rel=""noreferrer""><code>replace</code></a>, works with <code>None</code> see <a href=""https://github.com/pydata/pandas/issues/1972"" rel=""noreferrer"">this (closed) issue</a>.</em></p>

<hr>

<p>As an aside, it's worth noting that for most use cases you don't need to replace NaN with None, see this question about <a href=""https://stackoverflow.com/q/17534106/1240268""><strong>the difference between NaN and None in pandas</strong></a>.</p>

<p>However, in this specific case it seems you do (at least at the time of this answer).</p>
";;5;;2013-01-04T19:01:25.247;;14163209;2016-06-28T23:51:08.403;2017-05-23T11:47:20.577;;-1.0;;1240268.0;14162723.0;2;56;;;
1914;1914;;;"<p>You're so close!</p>

<p>You can specify the colors in the styles list:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

testdataframe = pd.DataFrame(np.arange(12).reshape(4,3), columns=['A', 'B', 'C'])
styles = ['bs-','ro-','y^-']
linewidths = [2, 1, 4]
fig, ax = plt.subplots()
for col, style, lw in zip(testdataframe.columns, styles, linewidths):
    testdataframe[col].plot(style=style, lw=lw, ax=ax)
</code></pre>

<p>Also note that the <code>plot</code> method can take a <code>matplotlib.axes</code> object, so you can make multiple calls like this (if you want to):</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

testdataframe1 = pd.DataFrame(np.arange(12).reshape(4,3), columns=['A', 'B', 'C'])
testdataframe2 = pd.DataFrame(np.random.normal(size=(4,3)), columns=['D', 'E', 'F'])
styles1 = ['bs-','ro-','y^-']
styles2 = ['rs-','go-','b^-']
fig, ax = plt.subplots()
testdataframe1.plot(style=styles1, ax=ax)
testdataframe2.plot(style=styles2, ax=ax)
</code></pre>

<p>Not really practical in this case, but the concept might come in handy later.</p>
";;5;;2013-01-06T06:35:53.407;;14179954;2013-01-06T21:37:22.733;2013-01-06T21:37:22.733;;1552748.0;;1552748.0;14178194.0;2;28;;;
1925;1925;;;"<p>How about simply reassigning <code>df.columns</code>:</p>

<pre><code>levels = df.columns.levels
labels = df.columns.labels
df.columns = levels[1][labels[1]]
</code></pre>

<p>For example:</p>

<pre><code>import pandas as pd

columns = pd.MultiIndex.from_arrays([['basic_amt']*4,
                                     ['NSW','QLD','VIC','All']])
index = pd.Index(['All', 'Full Time', 'Part Time'], name = 'Faculty')
df = pd.DataFrame([(1,1,2,4),
                   (0,01,0,1),
                   (1,0,2,3)])
df.columns = columns
df.index = index
</code></pre>

<p>Before:</p>

<pre><code>print(df)

           basic_amt               
                 NSW  QLD  VIC  All
Faculty                            
All                1    1    2    4
Full Time          0    1    0    1
Part Time          1    0    2    3
</code></pre>

<p>After:</p>

<pre><code>levels = df.columns.levels
labels = df.columns.labels
df.columns = levels[1][labels[1]]
print(df)

           NSW  QLD  VIC  All
Faculty                      
All          1    1    2    4
Full Time    0    1    0    1
Part Time    1    0    2    3
</code></pre>
";;10;;2013-01-07T04:32:47.640;;14189912;2013-01-10T22:57:29.763;2013-01-10T22:57:29.763;;190597.0;;190597.0;14189695.0;2;11;;;
1928;1928;;;"<p>The pandas documentation says:</p>

<blockquote>
  <p>Returning a view versus a copy</p>
  
  <p>The rules about when a view on the data is returned are entirely
  dependent on NumPy. Whenever an array of labels or a boolean vector
  are involved in the indexing operation, the result will be a copy.
  With single label / scalar indexing and slicing, e.g. df.ix[3:6] or
  df.ix[:, 'A'], a view will be returned.</p>
</blockquote>

<p>In <code>df[df.key==1]['D']</code> you first do boolean slicing (leading to a copy of the <strong>Dataframe</strong>), then you choose a column ['D'].</p>

<p>In <code>df.D[df.key==1] = 3.4</code>, you first choose a column, then do boolean slicing on the resulting <strong>Series</strong>. </p>

<p>This seems to make the difference, although I must admit that it is a little counterintuitive.</p>

<p><em><strong>Edit</em></strong>: The difference was identified by Dougal, see his comment: With version 1, the copy is made as the <code>__getitem__</code> method is called for the boolean slicing. For version 2, only the <code>__setitem__</code> method is accessed - thus not returning a copy but just assigning.</p>
";;4;;2013-01-07T09:32:27.723;;14193170;2013-01-07T10:32:39.153;2013-01-07T10:32:39.153;;344821.0;;1156006.0;14192741.0;2;15;;;
1948;1948;;;"<p>You could sort the columns using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.sort.html"" rel=""nofollow""><code>sort</code></a>:</p>

<pre><code>df1.sort(axis=1) == df2.sort(axis=1)
</code></pre>

<p>This will evaluate to a dataframe of all <code>True</code> values.</p>

<hr>

<p>As @osa comments this fails for NaN's and isn't particularly robust either, in practise using something similar to @quant's answer is probably recommended (Note: we want a bool rather than raise if there's an issue):</p>

<pre><code>def my_equal(df1, df2):
    from pandas.util.testing import assert_frame_equal
    try:
        assert_frame_equal(df1.sort(axis=1), df2.sort(axis=1), check_names=True)
        return True
    except (AssertionError, ValueError, TypeError):  perhaps something else?
        return False
</code></pre>
";;3;;2013-01-08T21:38:58.643;;14224489;2016-10-10T18:09:11.843;2016-10-10T18:09:11.843;;3130926.0;;1240268.0;14224172.0;2;8;;;
1950;1950;;;"<p>You should be using pandas own <code>ExcelWriter</code> class:</p>

<pre><code>from pandas import ExcelWriter
# from pandas.io.parsers import ExcelWriter
</code></pre>

<p>Then the <code>save_xls</code> function works as expected:</p>

<pre><code>def save_xls(list_dfs, xls_path):
    writer = ExcelWriter(xls_path)
    for n, df in enumerate(list_dfs):
        df.to_excel(writer,'sheet%s' % n)
    writer.save()
</code></pre>
";;5;;2013-01-08T23:27:40.787;;14225838;2013-02-04T21:55:52.057;2013-02-04T21:55:52.057;;1240268.0;;1240268.0;14225676.0;2;67;;;
1963;1963;;;"<p>[Updated to adapt to modern <code>pandas</code>, which has <code>isnull</code> as a method of <code>DataFrame</code>s..]</p>

<p>You can use <code>isnull</code> and <code>any</code> to build a boolean Series and use that to index into your frame:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])
&gt;&gt;&gt; df.isnull()
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False
&gt;&gt;&gt; df.isnull().any(axis=1)
0    False
1     True
2     True
3    False
4    False
dtype: bool
&gt;&gt;&gt; df[df.isnull().any(axis=1)]
   0   1   2
1  0 NaN   0
2  0   0 NaN
</code></pre>

<hr>

<p>[For older <code>pandas</code>:]</p>

<p>You could use the function <code>isnull</code> instead of the method:</p>

<pre><code>In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])

In [57]: df
Out[57]: 
   0   1   2
0  0   1   2
1  0 NaN   0
2  0   0 NaN
3  0   1   2
4  0   1   2

In [58]: pd.isnull(df)
Out[58]: 
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False

In [59]: pd.isnull(df).any(axis=1)
Out[59]: 
0    False
1     True
2     True
3    False
4    False
</code></pre>

<p>leading to the rather compact:</p>

<pre><code>In [60]: df[pd.isnull(df).any(axis=1)]
Out[60]: 
   0   1   2
1  0 NaN   0
2  0   0 NaN
</code></pre>
";;0;;2013-01-09T22:33:07.177;;14247708;2014-05-30T16:19:09.497;2014-05-30T16:19:09.497;;487339.0;;487339.0;14247586.0;2;119;;;
1968;1968;;;"<p>You could use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts</code></a> method:</p>

<pre><code>In [10]: ser.value_counts()
Out[10]: 
two      3
one      1
three    1
</code></pre>

<p>and then <a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html#bar-plots"" rel=""noreferrer"">plot this as a bar chart</a>:</p>

<pre><code>ser.value_counts().plot(kind='bar')
</code></pre>

<p>Edit: I noticed that this doesn't keep the desired order. If you have a list/Series for this ordering (in this case <code>ser[:3]</code> will do) you can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reindex.html"" rel=""noreferrer""><code>reindex</code></a> before plotting:</p>

<pre><code>In [12]: ser.value_counts().reindex(ser[:3])
Out[12]: 
one      1
two      3
three    1
</code></pre>
";;2;;2013-01-10T00:33:02.837;;14248948;2013-01-10T00:43:33.317;2013-01-10T00:43:33.317;;1240268.0;;1240268.0;14248706.0;2;41;;;
1980;1980;;;"<p>I routinely use tens of gigabytes of data in just this fashion
e.g. I have tables on disk that I read via queries, create data and append back.</p>

<p>It's worth reading <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#hdf5-pytables"">the docs</a> and <a href=""https://groups.google.com/forum/m/?fromgroups#!topic/pydata/cmw1F3OFJSc"">late in this thread</a> for several suggestions for how to store your data.</p>

<p>Details which will affect how you store your data, like:<br>
<em>Give as much detail as you can; and I can help you develop a structure.</em></p>

<ol>
<li>Size of data, # of rows, columns, types of columns; are you appending
rows, or just columns? </li>
<li>What will typical operations look like. E.g. do a query on columns to select a bunch of rows and specific columns, then do an operation (in-memory), create new columns, save these.<br>
(Giving a toy example could enable us to offer more specific recommendations.)</li>
<li>After that processing, then what do you do? Is step 2 ad hoc, or repeatable?</li>
<li>Input flat files: how many, rough total size in Gb. How are these organized e.g. by records? Does each one contains different fields, or do they have some records per file with all of the fields in each file?</li>
<li>Do you ever select subsets of rows (records) based on criteria (e.g. select the rows with field A > 5)? and then do something, or do you just select fields A, B, C with all of the records (and then do something)?</li>
<li>Do you 'work on' all of your columns (in groups), or are there a good proportion that you may only use for reports (e.g. you want to keep the data around, but don't need to pull in that column explicity until final results time)?</li>
</ol>

<h2>Solution</h2>

<p><em>Ensure you have <a href=""http://pandas.pydata.org/getpandas.html"">pandas at least <code>0.10.1</code></a> installed.</em></p>

<p>Read <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#iterating-through-files-chunk-by-chunk"">iterating files chunk-by-chunk</a> and <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#multiple-table-queries"">multiple table queries</a>.</p>

<p>Since pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it's easy to select a small group of fields (which will work with a big table, but it's more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):<br>
(The following is pseudocode.)</p>

<pre><code>import numpy as np
import pandas as pd

# create a store
store = pd.HDFStore('mystore.h5')

# this is the key to your storage:
#    this maps your fields to a specific group, and defines 
#    what you want to have as data_columns.
#    you might want to create a nice class wrapping this
#    (as you will want to have this map and its inversion)  
group_map = dict(
    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),
    B = dict(fields = ['field_10',......        ], dc = ['field_10']),
    .....
    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),

)

group_map_inverted = dict()
for g, v in group_map.items():
    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))
</code></pre>

<p>Reading in the files and creating the storage (essentially doing what <code>append_to_multiple</code> does):</p>

<pre><code>for f in files:
   # read in the file, additional options hmay be necessary here
   # the chunksize is not strictly necessary, you may be able to slurp each 
   # file into memory in which case just eliminate this part of the loop 
   # (you can also change chunksize if necessary)
   for chunk in pd.read_table(f, chunksize=50000):
       # we are going to append to each table by group
       # we are not going to create indexes at this time
       # but we *ARE* going to create (some) data_columns

       # figure out the field groupings
       for g, v in group_map.items():
             # create the frame for this group
             frame = chunk.reindex(columns = v['fields'], copy = False)    

             # append it
             store.append(g, frame, index=False, data_columns = v['dc'])
</code></pre>

<p>Now you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn't necessary).</p>

<p>This is how you get columns and create new ones:</p>

<pre><code>frame = store.select(group_that_I_want)
# you can optionally specify:
# columns = a list of the columns IN THAT GROUP (if you wanted to
#     select only say 3 out of the 20 columns in this sub-table)
# and a where clause if you want a subset of the rows

# do calculations on this frame
new_frame = cool_function_on_frame(frame)

# to 'add columns', create a new group (you probably want to
# limit the columns in this new_group to be only NEW ones
# (e.g. so you don't overlap from the other tables)
# add this info to the group_map
store.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)
</code></pre>

<p>When you are ready for post_processing:</p>

<pre><code># This may be a bit tricky; and depends what you are actually doing.
# I may need to modify this function to be a bit more general:
report_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1&gt;0', 'field_1000=foo'], selector = group_1)
</code></pre>

<p>About data_columns, you don't actually need to define <strong>ANY</strong> data_columns; they allow you to sub-select rows based on the column. E.g. something like:</p>

<pre><code>store.select(group, where = ['field_1000=foo', 'field_1001&gt;0'])
</code></pre>

<p>They may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).</p>

<p>You also might want to:</p>

<ul>
<li>create a function which takes a list of fields, looks up the groups in the groups_map, then selects these and concatenates the results so you get the resulting frame (this is essentially what select_as_multiple does). <em>This way the structure would be pretty transparent to you.</em></li>
<li>indexes on certain data columns (makes row-subsetting much faster).</li>
<li>enable compression.</li>
</ul>

<p>Let me know when you have questions!</p>
";;19;;2013-01-10T22:57:22.433;;14268804;2016-08-18T01:51:46.190;2016-08-18T01:51:46.190;;604930.0;;644898.0;14262433.0;2;364;;;
1997;1997;;;"<p>This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (<code>dict</code> of attributes).  Many people form a collection and you can have many collections (people, stock market, income).</p>

<p>pd.dateframe -> pymongo Note: I use the <code>chunksize</code> in <code>read_csv</code> to keep it to 5 to 10k records(pymongo drops the socket if larger)</p>

<pre><code>aCollection.insert((a[1].to_dict() for a in df.iterrows()))
</code></pre>

<p>querying: gt = greater than...</p>

<pre><code>pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))
</code></pre>

<p><code>.find()</code> returns an iterator so I commonly use <code>ichunked</code> to chop into smaller iterators.  </p>

<p>How about a join since I normally get 10 data sources to paste together:</p>

<pre><code>aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))
</code></pre>

<p>then (in my case sometimes I have to agg on <code>aJoinDF</code> first before its ""mergeable"".)</p>

<pre><code>df = pandas.merge(df, aJoinDF, on=aKey, how='left')
</code></pre>

<p>And you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).</p>

<pre><code>collection.update({primarykey:foo},{key:change})
</code></pre>

<p>On smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a <code>dict</code> lookup as you create documents.</p>

<p>Now you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...</p>

<p>You can also use the two methods built into MongoDB (MapReduce and aggregate framework). <a href=""http://docs.mongodb.org/manual/tutorial/aggregation-examples/"">See here for more info about the aggregate framework</a>, as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn't need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)</p>
";;2;;2013-01-11T22:11:52.907;;14287518;2014-08-14T13:50:41.227;2014-08-14T13:50:41.227;;1461210.0;;1649635.0;14262433.0;2;41;;;
2022;2022;;;"<p>Try passing columns of the <code>DataFrame</code> directly to matplotlib, as in the examples below, instead of extracting them as numpy arrays.</p>

<pre><code>df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])
df['col3'] = np.arange(len(df))**2 * 100 + 100

In [5]: df
Out[5]: 
       col1      col2  col3
0 -1.000075 -0.759910   100
1  0.510382  0.972615   200
2  1.872067 -0.731010   500
3  0.131612  1.075142  1000
4  1.497820  0.237024  1700
</code></pre>

<h3>Vary scatter point size based on another column</h3>

<pre><code>plt.scatter(df.col1, df.col2, s=df.col3)
# OR (with pandas 0.13 and up)
df.plot(kind='scatter', x='col1', y='col2', s=df.col3)
</code></pre>

<p><img src=""https://i.stack.imgur.com/FA5KP.png"" alt=""enter image description here""></p>

<h3>Vary scatter point color based on another column</h3>

<pre><code>colors = np.where(df.col3 &gt; 300, 'r', 'k')
plt.scatter(df.col1, df.col2, s=120, c=colors)
# OR (with pandas 0.13 and up)
df.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)
</code></pre>

<p><img src=""https://i.stack.imgur.com/rghSv.png"" alt=""enter image description here""></p>

<h3>Scatter plot with legend</h3>

<p>However, the easiest way I've found to create a scatter plot with legend is to call <code>plt.scatter</code> once for each point type.</p>

<pre><code>cond = df.col3 &gt; 300
subset_a = df[cond].dropna()
subset_b = df[~cond].dropna()
plt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 &gt; 300')
plt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 &lt;= 300') 
plt.legend()
</code></pre>

<p><img src=""https://i.stack.imgur.com/tlibK.png"" alt=""enter image description here""></p>

<h3><em>Update</em></h3>

<p>From what I can tell, matplotlib simply skips points with NA x/y coordinates or NA style settings (e.g., color/size).  To find points skipped due to NA, try the <code>isnull</code> method: <code>df[df.col3.isnull()]</code></p>

<p>To split a list of points into many types, take a look at <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""noreferrer"">numpy <code>select</code></a>, which is a vectorized if-then-else implementation and accepts an optional default value.  For example:</p>

<pre><code>df['subset'] = np.select([df.col3 &lt; 150, df.col3 &lt; 400, df.col3 &lt; 600],
                         [0, 1, 2], -1)
for color, label in zip('bgrm', [0, 1, 2, -1]):
    subset = df[df.subset == label]
    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))
plt.legend()
</code></pre>

<p><img src=""https://i.stack.imgur.com/CtA9s.png"" alt=""enter image description here""></p>
";;6;;2013-01-13T18:48:58.397;;14306902;2017-06-27T22:28:09.407;2017-06-27T22:28:09.407;;243434.0;;243434.0;14300137.0;2;76;;;
2023;2023;;;"<p>Since you're not aggregating similarly indexed rows, try setting the index with a list of column names.</p>

<pre><code>In [2]: df.set_index(['Name', 'Destination'])
Out[2]: 
                   Length
Name  Destination        
Bob   Athens            3
      Rome              5
      Athens            2
Alice Rome              1
      Athens            3
      Rome              5
</code></pre>
";;0;;2013-01-13T18:51:30.860;;14306921;2013-01-13T18:51:30.860;;;;;243434.0;14301913.0;2;12;;;
2024;2024;;;"<p>I think this does what you want:</p>

<pre><code>In [1]: df
Out[1]:
   RollBasis  ToRoll
0          1       1
1          1       4
2          1      -5
3          2       2
4          3      -4
5          5      -2
6          8       0
7         10     -13
8         12      -2
9         13      -5

In [2]: def f(x):
   ...:     ser = df.ToRoll[(df.RollBasis &gt;= x) &amp; (df.RollBasis &lt; x+5)]
   ...:     return ser.sum()
</code></pre>

<p>The above function takes a value, in this case RollBasis, and then indexes the data frame column ToRoll based on that value.  The returned series consists of ToRoll values that meet the RollBasis + 5 criterion.  Finally, that series is summed and returned.</p>

<pre><code>In [3]: df['Rolled'] = df.RollBasis.apply(f)

In [4]: df
Out[4]:
   RollBasis  ToRoll  Rolled
0          1       1      -4
1          1       4      -4
2          1      -5      -4
3          2       2      -4
4          3      -4      -6
5          5      -2      -2
6          8       0     -15
7         10     -13     -20
8         12      -2      -7
9         13      -5      -5
</code></pre>

<p>Code for the toy example DataFrame in case someone else wants to try:</p>

<pre><code>In [1]: from pandas import *

In [2]: import io

In [3]: text = """"""\
   ...:    RollBasis  ToRoll
   ...: 0          1       1
   ...: 1          1       4
   ...: 2          1      -5
   ...: 3          2       2
   ...: 4          3      -4
   ...: 5          5      -2
   ...: 6          8       0
   ...: 7         10     -13
   ...: 8         12      -2
   ...: 9         13      -5
   ...: """"""

In [4]: df = read_csv(io.BytesIO(text), header=0, index_col=0, sep='\s+')
</code></pre>
";;1;;2013-01-13T19:49:12.047;;14307460;2013-01-13T19:49:12.047;;;;;919872.0;14300768.0;2;11;;;
2025;2025;;;"<p>Based on Zelazny7's answer, I created this more general solution:</p>

<pre><code>def rollBy(what, basis, window, func):
    def applyToWindow(val):
        chunk = what[(val&lt;=basis) &amp; (basis&lt;val+window)]
        return func(chunk)
    return basis.apply(applyToWindow)

&gt;&gt;&gt; rollBy(d.ToRoll, d.RollBasis, 5, sum)
0    -4
1    -4
2    -4
3    -4
4    -6
5    -2
6   -15
7   -20
8    -7
9    -5
Name: RollBasis
</code></pre>

<p>It's still not ideal as it is very slow compared to <code>rolling_apply</code>, but perhaps this is inevitable.</p>
";;1;;2013-01-13T20:46:14.157;;14307961;2013-01-13T21:57:16.270;2013-01-13T21:57:16.270;;1427416.0;;1427416.0;14300768.0;2;11;;;
2045;2045;;;"<p><code>strip</code> only removes the specified characters at the beginning and end of the string.  If you want to remove <em>all</em> <code>\n</code>, you need to use <code>replace</code>.</p>

<pre><code>misc['product_desc'] = misc['product_desc'].str.replace('\n', '')
</code></pre>
";;0;;2013-01-15T20:02:32.267;;14345875;2013-01-15T20:02:32.267;;;;;1427416.0;14345739.0;2;43;;;
2047;2047;;;"<p>I would check out <a href=""https://github.com/ContinuumIO/Bokeh/"" rel=""nofollow"">Bokeh</a> which aims to ""provide a compelling Python equivalent of ggplot in R"". Example <a href=""http://htmlpreview.github.com/?https://github.com/ContinuumIO/Bokeh/blob/master/tests/cdx/bokeh-facets.html"" rel=""nofollow"">here</a></p>

<p>EDIT: Also check out <a href=""http://stanford.edu/~mwaskom/software/seaborn/"" rel=""nofollow"">Seaborn</a>, which attempts to reproduce the visual style and syntax of ggplot2.</p>
";;2;;2013-01-16T00:44:17.643;;14349645;2014-01-18T16:41:12.637;2014-01-18T16:41:12.637;;1574687.0;;1574687.0;14349055.0;2;10;;;
2048;2048;;;"<h3>Edit 1 year later:</h3>

<p>With <code>seaborn</code>, the example below becomes:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn
seaborn.set(style='ticks')
# Data to be represented
X = np.random.randn(256)

# Actual plotting
fig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")
axes = plt.subplot(111)
heights, positions, patches = axes.hist(X, color='white')
seaborn.despine(ax=axes, offset=10, trim=True)
fig.tight_layout()
plt.show()
</code></pre>

<p>Pretty dang easy.</p>

<h3>Original post:</h3>

<p>This blog post is the best I've seen so far.
<a href=""http://messymind.net/making-matplotlib-look-like-ggplot/"" rel=""noreferrer"">http://messymind.net/making-matplotlib-look-like-ggplot/</a></p>

<p>It doesn't focus on your standard R plots like you see in most of the ""getting started""-type examples. Instead it tries to emulate the style of ggplot2, which seems to be nearly universally heralded as stylish and well-designed.  </p>

<p>To get the axis spines like you see the in bar plot, try to follow one of the first few examples here: <a href=""http://www.loria.fr/~rougier/coding/gallery/"" rel=""noreferrer"">http://www.loria.fr/~rougier/coding/gallery/</a></p>

<p>Lastly, to get the axis tick marks pointing outward, you can edit your <code>matplotlibrc</code> files to say <code>xtick.direction : out</code> and <code>ytick.direction : out</code>.</p>

<p>Combining these concepts together we get something like this:</p>

<pre><code>import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# Data to be represented
X = np.random.randn(256)

# Actual plotting
fig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")
axes = plt.subplot(111)
heights, positions, patches = axes.hist(X, color='white')

axes.spines['right'].set_color('none')
axes.spines['top'].set_color('none')
axes.xaxis.set_ticks_position('bottom')

# was: axes.spines['bottom'].set_position(('data',1.1*X.min()))
axes.spines['bottom'].set_position(('axes', -0.05))
axes.yaxis.set_ticks_position('left')
axes.spines['left'].set_position(('axes', -0.05))

axes.set_xlim([np.floor(positions.min()), np.ceil(positions.max())])
axes.set_ylim([0,70])
axes.xaxis.grid(False)
axes.yaxis.grid(False)
fig.tight_layout()
plt.show()
</code></pre>

<p>The position of the spines can be specified a number of ways. If you run the code above in IPython, you can then do <code>axes.spines['bottom'].set_position?</code> to see all of your options.</p>

<p><img src=""https://i.stack.imgur.com/f2zrz.png"" alt=""R-style bar plot in python""></p>

<p>So yeah. It's not exactly trivial, but you can get close.</p>
";;9;;2013-01-16T00:57:14.217;;14349766;2015-01-19T16:36:31.050;2015-01-19T16:36:31.050;;1552748.0;;1552748.0;14349055.0;2;41;;;
2050;2050;;;"<p>#
#
#
#
#
#</p>

<p>EDIT 10/14/2013:
For information, ggplot has now been implemented for python (built on matplotlib). </p>

<p>See this <a href=""http://blog.yhathq.com/posts/ggplot-for-python.html"" rel=""nofollow noreferrer"">blog</a> or go directly to the <a href=""https://github.com/yhat/ggplot"" rel=""nofollow noreferrer"">github page</a> of the project for more information and examples.</p>

<p>#
#
#
#
#
#</p>

<p>To my knowledge, there is no built-in solution in matplotlib that will directly give to your figures a similar look than the ones made with R. </p>

<p>Some packages, like <a href=""http://tonysyu.github.com/mpltools/index.html"" rel=""nofollow noreferrer"">mpltools</a>, adds support for stylesheets using Matplotlibs rc-parameters, and can help you to obtain a ggplot look (see the <a href=""http://tonysyu.github.com/mpltools/auto_examples/style/plot_ggplot.html"" rel=""nofollow noreferrer"">ggplot style</a> for an example).</p>

<p>However, since everything can be tweaked in matplotlib, it might be easier for you to directly develop your own functions to achieve exactly what you want.  As an example, below is a snippet that will allow you to easily customize the axes of any matplotlib plot.</p>

<pre><code>def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',
               lw=3, size=20, pad=8):

    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],
                              ['left', 'bottom', 'right', 'top']):
        if c_spine != 'none':
            ax.spines[spine].set_color(c_spine)
            ax.spines[spine].set_linewidth(lw)
        else:
            ax.spines[spine].set_color('none')
    if (c_bottom == 'none') &amp; (c_top == 'none'): # no bottom and no top
        ax.xaxis.set_ticks_position('none')
    elif (c_bottom != 'none') &amp; (c_top != 'none'): # bottom and top
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                      color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom != 'none') &amp; (c_top == 'none'): # bottom but not top
        ax.xaxis.set_ticks_position('bottom')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom == 'none') &amp; (c_top != 'none'): # no bottom but top
        ax.xaxis.set_ticks_position('top')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_top, labelsize=size, pad=pad)
    if (c_left == 'none') &amp; (c_right == 'none'): # no left and no right
        ax.yaxis.set_ticks_position('none')
    elif (c_left != 'none') &amp; (c_right != 'none'): # left and right
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left != 'none') &amp; (c_right == 'none'): # left but not right
        ax.yaxis.set_ticks_position('left')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left == 'none') &amp; (c_right != 'none'): # no left but right
        ax.yaxis.set_ticks_position('right')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_right, labelsize=size, pad=pad)
</code></pre>

<p><strong>EDIT:</strong> for non touching spines, see the function below which induces a 10 pts displacement of the spines (taken from <a href=""http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html"" rel=""nofollow noreferrer"">this example</a> on the matplotlib website).</p>

<pre><code>def adjust_spines(ax,spines):
    for loc, spine in ax.spines.items():
        if loc in spines:
            spine.set_position(('outward',10)) # outward by 10 points
            spine.set_smart_bounds(True)
        else:
            spine.set_color('none') # don't draw spine
</code></pre>

<p>For example, the code and the two plots below show you the default output from matplotib (on the left), and the output when the functions are called (on the right):</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

fig,(ax1,ax2) = plt.subplots(figsize=(8,5), ncols=2)
ax1.plot(np.random.rand(20), np.random.rand(20), 'ok')
ax2.plot(np.random.rand(20), np.random.rand(20), 'ok')

customaxis(ax2) # remove top and right spines, ticks out
adjust_spines(ax2, ['left', 'bottom']) # non touching spines

plt.show()
</code></pre>

<p><img src=""https://i.imgur.com/X97AT.png"" alt=""image""></p>

<p>Of course, it will take time for you to figure out which parameters have to be tweaked in matplotlib to make your plots look exactly like the R ones, but I am not sure there are other options right now. </p>
";;3;;2013-01-16T04:49:32.503;;14351567;2013-10-14T23:17:18.507;2013-10-14T23:17:18.507;;975979.0;;975979.0;14349055.0;2;27;;;
2055;2055;;;"<p>You could use np.diff() to test where a segment starts/ends and iterate over those results. Its a very simple solution, so probably not the most performent one.</p>

<pre><code>a = np.array([3,3,3,3,3,4,4,4,4,4,1,1,1,1,4,4,12,12,12])

prev = 0
splits = np.append(np.where(np.diff(a) != 0)[0],len(a)+1)+1

for split in splits:
    print np.arange(1,a.size+1,1)[prev:split]
    prev = split
</code></pre>

<p>Results in:</p>

<pre><code>[1 2 3 4 5]
[ 6  7  8  9 10]
[11 12 13 14]
[15 16]
[17 18 19]
</code></pre>
";;2;;2013-01-16T13:07:53.240;;14359211;2013-01-16T13:37:18.627;2013-01-16T13:37:18.627;;1755432.0;;1755432.0;14358567.0;2;13;;;
2058;2058;;;"<p>One-liner:</p>

<pre><code>df.reset_index().groupby('A')['index'].apply(np.array)
</code></pre>

<p>Code for example:</p>

<pre><code>In [1]: import numpy as np

In [2]: from pandas import *

In [3]: df = DataFrame([3]*4+[4]*4+[1]*4, columns=['A'])
In [4]: df
Out[4]:
    A
0   3
1   3
2   3
3   3
4   4
5   4
6   4
7   4
8   1
9   1
10  1
11  1

In [5]: df.reset_index().groupby('A')['index'].apply(np.array)
Out[5]:
A
1    [8, 9, 10, 11]
3      [0, 1, 2, 3]
4      [4, 5, 6, 7]
</code></pre>

<p>You can also directly access the information from the groupby object:</p>

<pre><code>In [1]: grp = df.groupby('A')

In [2]: grp.indices
Out[2]:
{1L: array([ 8,  9, 10, 11], dtype=int64),
 3L: array([0, 1, 2, 3], dtype=int64),
 4L: array([4, 5, 6, 7], dtype=int64)}

In [3]: grp.indices[3]
Out[3]: array([0, 1, 2, 3], dtype=int64)
</code></pre>

<p>To address the situation that DSM mentioned you could do something like:</p>

<pre><code>In [1]: df['block'] = (df.A.shift(1) != df.A).astype(int).cumsum()

In [2]: df
Out[2]:
    A  block
0   3      1
1   3      1
2   3      1
3   3      1
4   4      2
5   4      2
6   4      2
7   4      2
8   1      3
9   1      3
10  1      3
11  1      3
12  3      4
13  3      4
14  3      4
15  3      4
</code></pre>

<p>Now groupby both columns and apply the lambda function:</p>

<pre><code>In [77]: df.reset_index().groupby(['A','block'])['index'].apply(np.array)
Out[77]:
A  block
1  3          [8, 9, 10, 11]
3  1            [0, 1, 2, 3]
   4        [12, 13, 14, 15]
4  2            [4, 5, 6, 7]
</code></pre>
";;4;;2013-01-16T14:16:30.810;;14360423;2016-12-05T21:34:29.980;2016-12-05T21:34:29.980;;939971.0;;919872.0;14358567.0;2;28;;;
2062;2062;;;"<p>You can use the <code>DataFrame</code> <code>drop</code> function to remove columns.  You have to pass the <code>axis=1</code> option for it to work on columns and not rows. Note that it returns a copy so you have to assign the result to a new <code>DataFrame</code>:</p>

<pre><code>In [1]: from pandas import *

In [2]: df = DataFrame(dict(x=[0,0,1,0,1], y=[1,0,1,1,0], z=[0,0,1,0,1]))

In [3]: df
Out[3]:
   x  y  z
0  0  1  0
1  0  0  0
2  1  1  1
3  0  1  0
4  1  0  1

In [4]: df = df.drop(['x','y'], axis=1)

In [5]: df
Out[5]:
   z
0  0
1  0
2  1
3  0
4  1
</code></pre>
";;2;;2013-01-16T17:01:31.107;;14363721;2013-01-16T17:01:31.107;;;;;919872.0;14363640.0;2;34;;;
2063;2063;;;"<p>Basically the same as Zelazny7's answer -- just specifying what to keep:</p>

<pre><code>In [68]: df
Out[68]: 
   x  y  z
0  0  1  0
1  0  0  0
2  1  1  1
3  0  1  0
4  1  0  1

In [70]: df = df[['x','z']]                                                                

In [71]: df
Out[71]: 
   x  z
0  0  0
1  0  0
2  1  1
3  0  0
4  1  1
</code></pre>

<h2>*Edit*</h2>

<p>You can specify a large number of columns through indexing/slicing into the <code>Dataframe.columns</code> object.<br>
This object of <code>type(pandas.Index)</code> can be viewed as a <code>dict</code> of column labels (with some extended functionality).</p>

<p>See this extension of above examples:</p>

<pre><code>In [4]: df.columns
Out[4]: Index([x, y, z], dtype=object)

In [5]: df[df.columns[1:]]
Out[5]: 
   y  z
0  1  0
1  0  0
2  1  1
3  1  0
4  0  1

In [7]: df.drop(df.columns[1:], axis=1)
Out[7]: 
   x
0  0
1  0
2  1
3  0
4  1
</code></pre>
";;6;;2013-01-16T17:03:39.730;;14363758;2014-01-08T14:50:18.317;2014-01-08T14:50:18.317;;733291.0;;733291.0;14363640.0;2;13;;;
2066;2066;;;"<p><a href=""http://pandas.pydata.org/"" rel=""noreferrer"">pandas</a> to the rescue:</p>

<pre><code>import pandas as pd
print pd.read_csv('value.txt')

        Date    price  factor_1  factor_2
0  2012-06-11  1600.20     1.255     1.548
1  2012-06-12  1610.02     1.258     1.554
2  2012-06-13  1618.07     1.249     1.552
3  2012-06-14  1624.40     1.253     1.556
4  2012-06-15  1626.15     1.258     1.552
5  2012-06-16  1626.15     1.263     1.558
6  2012-06-17  1626.15     1.264     1.572
</code></pre>

<p>This returns pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe"" rel=""noreferrer"">DataFrame</a> that is similar to <code>R's</code>.</p>
";;5;;2013-01-16T18:56:20.563;;14365647;2013-01-16T19:01:29.573;2013-01-16T19:01:29.573;;1199589.0;;1199589.0;14365542.0;2;70;;;
2069;2069;;;"<p>Here's an alternative to pandas library using Python's built-in <a href=""http://docs.python.org/2/library/csv.html"" rel=""noreferrer"">csv module</a>.</p>

<pre><code>import csv
from pprint import pprint
with open('foo.csv', 'rb') as f:
    reader = csv.reader(f)
    headers = reader.next()
    column = {h:[] for h in headers}
    for row in reader:
        for h, v in zip(headers, row):
            column[h].append(v)
    pprint(column)    # Pretty printer
</code></pre>

<p>will print</p>

<pre><code>{'Date': ['2012-06-11',
          '2012-06-12',
          '2012-06-13',
          '2012-06-14',
          '2012-06-15',
          '2012-06-16',
          '2012-06-17'],
 'factor_1': ['1.255', '1.258', '1.249', '1.253', '1.258', '1.263', '1.264'],
 'factor_2': ['1.548', '1.554', '1.552', '1.556', '1.552', '1.558', '1.572'],
 'price': ['1600.20',
           '1610.02',
           '1618.07',
           '1624.40',
           '1626.15',
           '1626.15',
           '1626.15']}
</code></pre>
";;0;;2013-01-16T19:20:20.000;;14366084;2013-01-16T19:34:35.880;2013-01-16T19:34:35.880;;1221677.0;;1221677.0;14365542.0;2;6;;;
2075;2075;;;"<p>DataFrames have a <code>to_latex</code> method:</p>

<pre><code>In [42]: df = pd.DataFrame(np.random.random((5, 5)))

In [43]: df
Out[43]: 
          0         1         2         3         4
0  0.886864  0.518538  0.359964  0.167291  0.940414
1  0.834130  0.022920  0.265131  0.059002  0.530584
2  0.648019  0.953043  0.263551  0.595798  0.153969
3  0.207003  0.015721  0.931170  0.045044  0.432870
4  0.039886  0.898780  0.728195  0.112069  0.468485

In [44]: print df.to_latex()
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
{} &amp;         0 &amp;         1 &amp;         2 &amp;         3 &amp;         4 \\
\hline
0 &amp;  0.886864 &amp;  0.518538 &amp;  0.359964 &amp;  0.167291 &amp;  0.940414 \\
1 &amp;  0.834130 &amp;  0.022920 &amp;  0.265131 &amp;  0.059002 &amp;  0.530584 \\
2 &amp;  0.648019 &amp;  0.953043 &amp;  0.263551 &amp;  0.595798 &amp;  0.153969 \\
3 &amp;  0.207003 &amp;  0.015721 &amp;  0.931170 &amp;  0.045044 &amp;  0.432870 \\
4 &amp;  0.039886 &amp;  0.898780 &amp;  0.728195 &amp;  0.112069 &amp;  0.468485 \\
\hline
\end{tabular}
</code></pre>

<p>You can simply write this to a tex file.</p>

<p>By default latex will render this as:</p>

<p><img src=""https://i.stack.imgur.com/W4Mqu.png"" alt=""as it would appear in latex""></p>

<p><em>Note: the <code>to_latex</code> method offers several configuration options.</em></p>
";;5;;2013-01-17T16:30:24.830;;14383654;2013-01-17T17:54:50.583;2013-01-17T17:54:50.583;;1240268.0;;1301710.0;14380371.0;2;50;;;
2087;2087;;;"<p>If you are still looking at this, take a look at this post on google groups. It shows how to exchange data between pandas/R via HDF5. </p>

<p><a href=""https://groups.google.com/forum/?fromgroups#!topic/pydata/0LR72GN9p6w"" rel=""noreferrer"">https://groups.google.com/forum/?fromgroups#!topic/pydata/0LR72GN9p6w</a></p>
";;2;;2013-01-18T20:45:51.937;;14407329;2013-01-18T20:45:51.937;;;;;644898.0;12278347.0;2;8;;;
2098;2098;;;"<h3>Update: I now recommend installing the scientific python stack using <a href=""http://docs.continuum.io/anaconda/"" rel=""noreferrer"">Anaconda</a>.</h3>

<p>Pandas comes bundled and can easily be updated using conda:</p>

<pre><code>conda update pandas
</code></pre>

<p>It also comes bundled with cython, scipy (which is tricky to install via pip), statsmodels, and manages the dependencies/reationships between these packages for you.</p>

<p><em>It's worth emphaising that you don't need admin/sudo access to install it on the machine to install Anaconda.</em></p>

<hr>

<p>If you're not using Anaconda, the recommended way to <a href=""http://pandas.pydata.org/pandas-docs/stable/install.html#overview"" rel=""noreferrer"">install pandas</a> is via pip (on Mac and Windows):</p>

<pre><code>pip install pandas
</code></pre>

<p><em>On Linux you can also install with <code>python-pandas</code> in whichever repository, but be aware you may be installing an older version of pandas, ideally you should be using the latest stable version.</em></p>

<hr>

<p>It looks like you have tried to <a href=""http://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source"" rel=""noreferrer"">install from source</a>, about which the docs mention:</p>

<blockquote>
  <p>Installing from the git repository <strong>requires a recent installation of Cython as the cythonized C sources are no longer checked into source control</strong>. Released source distributions will contain the built C files. I recommend installing the latest Cython via <code>easy_install -U Cython</code></p>
  
  <p>Note that <strong>you will not be able to import pandas</strong> if you open an interpreter in the source directory <strong>unless you build the C extensions in place</strong>:</p>

<pre><code>python setup.py build_ext --inplace
</code></pre>
</blockquote>

<p><em>Without compiling <a href=""https://github.com/pydata/pandas/tree/master/pandas"" rel=""noreferrer""><code>hashtables.pyx</code></a> (and a few other cython files), pandas is unable to import them. These are required for pandas (which explains your error message).</em></p>

<p><em>Note: this <a href=""https://github.com/pydata/pandas/pull/3827"" rel=""noreferrer"">error message has been made more descriptive for 0.11.1 onwards</a>, it will say that the C-extensions were not built.</em></p>
";;8;;2013-01-20T19:37:25.950;;14428450;2014-08-19T02:10:08.143;2014-08-19T02:10:08.143;;1240268.0;;1240268.0;14422976.0;2;19;;;
2105;2105;;;"<p>As you mention, at the moment you save the index, but what we can do is <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a>, saving the old index as a column (<code>'Date'</code>).</p>

<pre><code>price2 = price.reset_index()

In [11]: price2
Out[11]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1006 entries, 0 to 1005
Data columns:
Date    1006  non-null values
AAPL    1006  non-null values
GE      1006  non-null values
dtypes: datetime64[ns](1), float64(2)
</code></pre>

<p>Following the <a href=""https://github.com/pydata/pandas/blob/master/doc/source/io.rst#sql-queries"" rel=""noreferrer"">docs</a> (setting a SQLite connection in memory):</p>

<pre><code>import sqlite3
from pandas.io import sql
# Create your connection.
cnx = sqlite3.connect(':memory:')
</code></pre>

<p>We can save <code>price2</code> to <code>cnx</code>:</p>

<pre><code>sql.write_frame(price2, name='price2', con=cnx)
</code></pre>

<p>We can retrieve via <code>read_frame</code>:</p>

<pre><code>p2 = sql.read_frame('select * from price2', cnx)
</code></pre>

<p>However, when stored (and retrieved) <strong>dates are <code>unicode</code></strong> rather than <code>Timestamp</code>. To convert back to what we started with we could <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html"" rel=""noreferrer""><code>apply</code></a> <code>Timestamp</code> to the column and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.set_index.html"" rel=""noreferrer""><code>set_index</code></a>:</p>

<pre><code>from pandas.lib import Timestamp
p2.Date = p2.Date.apply(Timestamp)
p = p2.set_index('Date')
</code></pre>

<p>We get back the same DataFrame as <code>prices</code>:</p>

<pre><code>In [20]: p
Out[20]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 1006 entries, 2009-01-02 00:00:00 to 2012-12-31 00:00:00
Data columns:
AAPL    1006  non-null values
GE      1006  non-null values
dtypes: float64(2)
</code></pre>
";;2;;2013-01-21T05:07:41.777;;14432914;2014-03-26T15:01:22.137;2014-03-26T15:01:22.137;;1678416.0;;1240268.0;14431646.0;2;24;;;
2128;2128;;;"<p>Perhaps you are looking for <a href=""http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.cut.html"" rel=""noreferrer"">pandas.cut</a>:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(range(50), columns  = ['filtercol'])
w = 0
x = 5
y = 17
z = 33
filter_values = [w, x, y, z]
out = pd.cut(df.filtercol, bins = filter_values)
counts = pd.value_counts(out)
# counts is a Series
print(counts)
</code></pre>

<p>yields</p>

<pre><code>(17, 33]    16
(5, 17]     12
(0, 5]       5
</code></pre>

<hr>

<p>To reorder the result so the bin ranges appear in order, you could use</p>

<pre><code>counts.reindex(out.cat.categories)
</code></pre>

<p>which yields</p>

<pre><code>(0, 5]       5
(5, 17]     12
(17, 33]    16
</code></pre>

<hr>

<p>See also <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#discretization-and-quantiling"" rel=""noreferrer"">Discretization and quantiling</a>.</p>
";;4;;2013-01-22T03:46:12.397;;14451264;2016-06-05T14:29:22.167;2016-06-05T14:29:22.167;;190597.0;;190597.0;14451185.0;2;29;;;
2143;2143;;;"<p><strong>Edit: Mar. 2015</strong></p>

<p>As noted below, pandas now uses <a href=""http://www.sqlalchemy.org/"" rel=""noreferrer"">SQLAlchemy</a> to both read from (<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html"" rel=""noreferrer"">read_sql</a>) and insert into (<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html"" rel=""noreferrer"">to_sql</a>) a database. The following should work</p>

<pre><code>import pandas as pd

df = pd.read_sql(sql, cnxn)
</code></pre>

<p><strong>Previous answer:</strong>
Via mikebmassey from a <a href=""https://stackoverflow.com/a/13570851/386279"">similar question</a></p>

<pre><code>import pyodbc
import pandas.io.sql as psql

cnxn = pyodbc.connect(connection_info) 
cursor = cnxn.cursor()
sql = ""SELECT * FROM TABLE""

df = psql.frame_query(sql, cnxn)
cnxn.close()
</code></pre>
";;4;;2013-01-23T19:38:02.580;;14487936;2015-03-13T20:35:07.297;2017-05-23T11:47:32.197;;-1.0;;386279.0;12047193.0;2;60;;;
2159;2159;;;"<p>I think the easiest way to do this would be to set the columns to the top level:</p>

<pre><code>df.columns = df.columns.get_level_values(0)
</code></pre>

<p><em>Note: if the to level has a name you can also access it by this, rather than 0.</em></p>

<p>.</p>

<p>If you want to combine/<a href=""http://docs.python.org/2/library/stdtypes.html#str.join""><code>join</code></a> your MultiIndex into one Index <em>(assuming you have just string entries in your columns)</em> you could:</p>

<pre><code>df.columns = [' '.join(col).strip() for col in df.columns.values]
</code></pre>

<p><em>Note: we must <a href=""http://docs.python.org/2/library/stdtypes.html#str.strip""><code>strip</code></a> the whitespace for when there is no second index.</em></p>

<pre><code>In [11]: [' '.join(col).strip() for col in df.columns.values]
Out[11]: 
['USAF',
 'WBAN',
 'day',
 'month',
 's_CD sum',
 's_CL sum',
 's_CNT sum',
 's_PC sum',
 'tempf amax',
 'tempf amin',
 'year']
</code></pre>
";;3;;2013-01-24T18:37:10.217;;14508355;2014-01-17T19:36:17.260;2014-01-17T19:36:17.260;;1240268.0;;1240268.0;14507794.0;2;151;;;
2161;2161;;;"<p>Andy Hayden's answer is certainly the easiest way -- if you want to avoid duplicate column labels you need to tweak a bit</p>

<pre><code>In [34]: df
Out[34]: 
     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year
                               sum   sum    sum   sum   amax   amin      
0  702730  26451    1      1    12     0     13     1  30.92  24.98  1993
1  702730  26451    2      1    13     0     13     0  32.00  24.98  1993
2  702730  26451    3      1     2    10     13     1  23.00   6.98  1993
3  702730  26451    4      1    12     0     13     1  10.04   3.92  1993
4  702730  26451    5      1    10     0     13     3  19.94  10.94  1993


In [35]: mi = df.columns

In [36]: mi
Out[36]: 
MultiIndex
[(USAF, ), (WBAN, ), (day, ), (month, ), (s_CD, sum), (s_CL, sum), (s_CNT, sum), (s_PC, sum), (tempf, amax), (tempf, amin), (year, )]


In [37]: mi.tolist()
Out[37]: 
[('USAF', ''),
 ('WBAN', ''),
 ('day', ''),
 ('month', ''),
 ('s_CD', 'sum'),
 ('s_CL', 'sum'),
 ('s_CNT', 'sum'),
 ('s_PC', 'sum'),
 ('tempf', 'amax'),
 ('tempf', 'amin'),
 ('year', '')]

In [38]: ind = pd.Index([e[0] + e[1] for e in mi.tolist()])

In [39]: ind
Out[39]: Index([USAF, WBAN, day, month, s_CDsum, s_CLsum, s_CNTsum, s_PCsum, tempfamax, tempfamin, year], dtype=object)

In [40]: df.columns = ind




In [46]: df
Out[46]:
  USAF  WBAN day month s_CDsum s_CLsum s_CNTsum s_PCsum tempfamax tempfamin \
0 702730 26451  1   1    12    0    13    1   30.92   24.98 
1 702730 26451  2   1    13    0    13    0   32.00   24.98 
2 702730 26451  3   1    2    10    13    1   23.00    6.98 
3 702730 26451  4   1    12    0    13    1   10.04    3.92 
4 702730 26451  5   1    10    0    13    3   19.94   10.94 




 year 
0 1993 
1 1993 
2 1993 
3 1993 
4 1993
</code></pre>
";;1;;2013-01-24T18:54:14.357;;14508639;2013-01-24T18:54:14.357;;;;;733291.0;14507794.0;2;13;;;
2182;2182;;;"<p>For the first part you can pass a dict of column names for keys and a list of functions for the values:</p>

<pre><code>In [28]: df
Out[28]:
          A         B         C         D         E  GRP
0  0.395670  0.219560  0.600644  0.613445  0.242893    0
1  0.323911  0.464584  0.107215  0.204072  0.927325    0
2  0.321358  0.076037  0.166946  0.439661  0.914612    1
3  0.133466  0.447946  0.014815  0.130781  0.268290    1

In [26]: f = {'A':['sum','mean'], 'B':['prod']}

In [27]: df.groupby('GRP').agg(f)
Out[27]:
            A                   B
          sum      mean      prod
GRP
0    0.719580  0.359790  0.102004
1    0.454824  0.227412  0.034060
</code></pre>

<p>UPDATE 1:</p>

<p>Because the aggregate function works on Series, references to the other column names are lost.  To get around this, you can reference the full dataframe and index it using the group indices within the lambda function.</p>

<p>Here's a hacky workaround:</p>

<pre><code>In [67]: f = {'A':['sum','mean'], 'B':['prod'], 'D': lambda g: df.ix[g.index].E.sum()}

In [69]: df.groupby('GRP').agg(f)
Out[69]:
            A                   B         D
          sum      mean      prod  &lt;lambda&gt;
GRP
0    0.719580  0.359790  0.102004  1.170219
1    0.454824  0.227412  0.034060  1.182901
</code></pre>

<p>Here, the resultant 'D' column is made up of the summed 'E' values.</p>

<p>UPDATE 2:</p>

<p>Here's a method that I think will do everything you ask.  First make a custom lambda function.  Below, g references the group.  When aggregating, g will be a Series.  Passing <code>g.index</code> to <code>df.ix[]</code> selects the current group from df.  I then test if column C is less than 0.5.  The returned boolean series is passed to <code>g[]</code> which selects only those rows meeting the criteria.</p>

<pre><code>In [95]: cust = lambda g: g[df.ix[g.index]['C'] &lt; 0.5].sum()

In [96]: f = {'A':['sum','mean'], 'B':['prod'], 'D': {'my name': cust}}

In [97]: df.groupby('GRP').agg(f)
Out[97]:
            A                   B         D
          sum      mean      prod   my name
GRP
0    0.719580  0.359790  0.102004  0.204072
1    0.454824  0.227412  0.034060  0.570441
</code></pre>
";;6;;2013-01-25T20:40:24.770;;14530027;2013-01-25T21:28:31.060;2013-01-25T21:28:31.060;;919872.0;;919872.0;14529838.0;2;78;;;
2189;2189;;;"<p>You can use the <code>indexer_between_time</code> <code>Index</code> method.</p>

<p>For example, to <em>include</em> those times between 9am and 6pm (<em>inclusive</em>):</p>

<pre><code>ts.ix[ts.index.indexer_between_time(datetime.time(9), datetime.time(18))]
</code></pre>

<p>to do the opposite and <em>exclude</em> those times between 6pm and 9am (<em>exclusive</em>):</p>

<pre><code>ts.ix[ts.index.indexer_between_time(datetime.time(18), datetime.time(9),
                                    include_start=False, include_end=False)]
</code></pre>

<p><em>Note: <code>indexer_between_time</code>'s arguments <code>include_start</code> and <code>include_end</code> are by default <code>True</code>, setting <code>include_start</code> to <code>False</code> means that datetimes whose time-part is precisely <code>start_time</code> (the first argument), in this case 6pm, will not be included.</em></p>

<p>Example:</p>

<pre><code>In [1]: rng = pd.date_range('1/1/2000', periods=24, freq='H')

In [2]: ts = pd.Series(pd.np.random.randn(len(rng)), index=rng)

In [3]: ts.ix[ts.index.indexer_between_time(datetime.time(10), datetime.time(14))] 
Out[3]: 
2000-01-01 10:00:00    1.312561
2000-01-01 11:00:00   -1.308502
2000-01-01 12:00:00   -0.515339
2000-01-01 13:00:00    1.536540
2000-01-01 14:00:00    0.108617
</code></pre>

<p>Note: the same syntax (using <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#advanced-indexing-with-labels""><code>ix</code></a>) works for a DataFrame:</p>

<pre><code>In [4]: df = pd.DataFrame(ts)

In [5]: df.ix[df.index.indexer_between_time(datetime.time(10), datetime.time(14))]
Out[5]: 
                            0
2000-01-03 10:00:00  1.312561
2000-01-03 11:00:00 -1.308502
2000-01-03 12:00:00 -0.515339
2000-01-03 13:00:00  1.536540
2000-01-03 14:00:00  0.108617
</code></pre>
";;7;;2013-01-26T19:08:54.680;;14540509;2013-01-26T20:09:52.483;2013-01-26T20:09:52.483;;1240268.0;;1240268.0;14539992.0;2;15;;;
2206;2206;;;"<p>I've received the same error (<code>ImportError: cannot import name hashtable</code>) when trying to import pandas from the source code directory.  Try starting the python interpreter from a different directory and import pandas again.</p>
";;6;;2013-01-28T18:12:24.997;;14568392;2013-01-28T18:12:24.997;;;;;243434.0;14568070.0;2;30;;;
2235;2235;;;"<p>You can use the DataFrame <a href=""http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.DataFrame.to_html.html?highlight=to_html#pandas.DataFrame.to_html"" rel=""nofollow noreferrer""><code>to_html</code></a> method, which comes with <code>formatters</code> argument.</p>

<p><em>An easier solution would be to surround by <code>&lt;span class=""significant""&gt;</code> and <code>&lt;/span&gt;</code>, (rather than <code>*</code>). Note: by default this will be escaped (i.e. <code>&lt;</code> becomes <code>&amp;lt;</code>) so you will need to use the <code>escape=False</code> argument.</em></p>
";;4;;2013-01-31T16:16:27.443;;14630250;2017-06-09T18:17:21.440;2017-06-09T18:17:21.440;;100190.0;;1240268.0;14627380.0;2;18;;;
2254;2254;;;"<p>Method #1: print all rows where the ID is one of the IDs in duplicated:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.read_csv(""dup.csv"")
&gt;&gt;&gt; ids = df[""ID""]
&gt;&gt;&gt; df[ids.isin(ids[ids.duplicated()])].sort(""ID"")
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12
</code></pre>

<p>but I couldn't think of a nice way to prevent repeating <code>ids</code> so many times.  I prefer method #2: <code>groupby</code> on the ID.</p>

<pre><code>&gt;&gt;&gt; pd.concat(g for _, g in df.groupby(""ID"") if len(g) &gt; 1)
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12
</code></pre>
";;3;;2013-02-02T01:01:09.933;;14657511;2013-02-02T01:01:09.933;;;;;487339.0;14657241.0;2;21;;;
2259;2259;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">DataFrame.drop</a> and pass it a Series of index labels:</p>

<pre><code>In [65]: df
Out[65]: 
       one  two
one      1    4
two      2    3
three    3    2
four     4    1


In [66]: df.drop(df.index[[1,3]])
Out[66]: 
       one  two
one      1    4
three    3    2
</code></pre>
";;5;;2013-02-02T12:11:11.497;;14661768;2015-07-28T03:38:52.707;2015-07-28T03:38:52.707;;633403.0;;733291.0;14661701.0;2;201;;;
2264;2264;;;"<p>This is because of using integer indices (<code>ix</code> selects those by <em>label</em> over -3 rather than <em>position</em>, and this is by design: see <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#integer-indexing"" rel=""nofollow noreferrer"">integer indexing in pandas ""gotchas""</a>*).</p>

<p>*In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label:</p>

<pre><code>df.iloc[-3:]
</code></pre>

<p><em>see the <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#different-choices-for-indexing-loc-iloc-and-ix"" rel=""nofollow noreferrer"">docs</a></em>.</p>

<p>As Wes points out, in this specific case you should just use tail!</p>

<p>It should also be noted that in Pandas pre-0.14 <code>iloc</code> will raise an <code>IndexError</code> on an out-of-bounds access, while <code>.head()</code> and <code>.tail()</code> will not:</p>

<pre><code>&gt;&gt;&gt; pd.__version__
'0.12.0'
&gt;&gt;&gt; df = pd.DataFrame([{""a"": 1}, {""a"": 2}])
&gt;&gt;&gt; df.iloc[-5:]
...
IndexError: out-of-bounds on slice (end)
&gt;&gt;&gt; df.tail(5)
   a
0  1
1  2
</code></pre>

<hr>

<p>Old answer (depreciated method):</p>

<p>You can use the <code>irows</code> DataFrame method to overcome this ambiguity:</p>

<pre><code>In [11]: df1.irow(slice(-3, None))
Out[11]: 
    STK_ID  RPT_Date  TClose   sales  discount
8      568  20080331   38.75  12.668       NaN
9      568  20080630   30.09  21.102       NaN
10     568  20080930   26.00  30.769       NaN
</code></pre>

<p><em>Note: Series has <a href=""https://stackoverflow.com/a/14466665/1240268"">a similar <code>iget</code> method</a>.</em></p>
";;1;;2013-02-03T05:02:23.223;;14669654;2014-09-15T23:36:50.183;2017-05-23T11:55:07.343;;-1.0;;1240268.0;14663004.0;2;22;;;
2271;2271;;;"<p>Sure, like most Python objects, you can attach new attributes to a <code>pandas.DataFrame</code>:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([])
df.instrument_name = 'Binky'
</code></pre>

<p>Note, however, that while you can attach attributes to a DataFrame, operations performed on the DataFrame (such as <code>groupby</code>, <code>pivot</code>, <code>join</code> or <code>loc</code> to name just a few) may return a new DataFrame <em>without</em> the metadata attached. Pandas does not yet have a robust method of <em>propagating</em> <a href=""https://github.com/pydata/pandas/issues/2485"" rel=""noreferrer"">metadata attached to DataFrames</a>.</p>

<p>Preserving the metadata <em>in a file</em> is possible. You can find an example of how to store metadata in an HDF5 file <a href=""https://stackoverflow.com/a/29130146/190597"">here</a>.</p>
";;5;;2013-02-04T14:03:39.750;;14688398;2015-03-23T13:13:38.433;2017-05-23T12:10:19.517;;-1.0;;190597.0;14688306.0;2;40;;;
2272;2272;;;"<p>Not really. Although you could add attributes containing metadata to the DataFrame class as @unutbu mentions, many DataFrame methods return a new DataFrame, so your meta data would be lost. If you need to manipulate your dataframe, then the best option would be to wrap your metadata and DataFrame in another class. See this discussion on GitHub: <a href=""https://github.com/pydata/pandas/issues/2485"">https://github.com/pydata/pandas/issues/2485</a></p>

<p>There is currently an open <a href=""https://github.com/pydata/pandas/pull/2695"">pull request</a> to add a MetaDataFrame object, which would support metadata better. </p>
";;0;;2013-02-04T14:12:33.190;;14688529;2013-02-04T14:12:33.190;;;;;1452002.0;14688306.0;2;9;;;
2286;2286;;;"<pre><code>In [1]: df
Out[1]:
   data
0     1
1     2
2     3
3     4
</code></pre>

<p>You want to apply a function that conditionally returns a value based on the selected dataframe column.  </p>

<pre><code>In [2]: df['data'].apply(lambda x: 'true' if x &lt;= 2.5 else 'false')
Out[2]:
0     true
1     true
2    false
3    false
Name: data
</code></pre>

<p>You can then assign that returned column to a new column in your dataframe:</p>

<pre><code>In [3]: df['desired_output'] = df['data'].apply(lambda x: 'true' if x &lt;= 2.5 else 'false')

In [4]: df
Out[4]:
   data desired_output
0     1           true
1     2           true
2     3          false
3     4          false
</code></pre>
";;0;;2013-02-05T18:35:28.293;;14714452;2013-02-05T18:35:28.293;;;;;919872.0;14714181.0;2;28;;;
2291;2291;;;"<p>Just compare the column with that value:</p>

<pre><code>In [9]: df = pandas.DataFrame([1,2,3,4], columns=[""data""])

In [10]: df
Out[10]: 
   data
0     1
1     2
2     3
3     4

In [11]: df[""desired""] = df[""data""] &gt; 2.5
In [11]: df
Out[12]: 
   data desired
0     1   False
1     2   False
2     3    True
3     4    True
</code></pre>
";;0;2013-09-30T15:45:39.833;2013-02-05T21:34:52.340;;14717374;2013-09-30T15:45:39.833;2013-09-30T15:45:39.833;;1380673.0;;1380673.0;14714181.0;2;18;;;
2292;2292;;;"<p>In this specific example, where the DataFrame is only one column, you can write this elegantly as:</p>

<pre><code>df['desired_output'] = df.le(2.5)
</code></pre>

<p><em><code>le</code> tests whether elements are less than or equal 2.5, similarly <code>lt</code> for less than, <code>gt</code> and <code>ge</code>.</em></p>
";;0;;2013-02-05T21:58:25.553;;14717761;2013-02-05T21:58:25.553;;;;;1240268.0;14714181.0;2;6;;;
2314;2314;;;"<p>When sorting by a MultiIndex you need to contain the tuple describing the column inside a list*:</p>

<pre><code>In [11]: df.sort([('Group1', 'C')], ascending=False)
Out[11]: 
  Group1       Group2      
       A  B  C      A  B  C
2      5  6  9      1  0  0
1      1  0  3      2  5  7
3      7  0  2      0  3  5
</code></pre>

<p>* <em>so as not to confuse pandas into thinking you want to sort first by Group1 then by C.</em></p>
";;6;;2013-02-06T16:36:27.460;;14734148;2013-02-06T16:36:27.460;;;;;1240268.0;14733871.0;2;53;;;
2316;2316;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.get_group.html""><code>get_group</code></a> method:</p>

<pre><code>In [21]: gb.get_group('foo')
Out[21]: 
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p><em>Note: This doesn't require creating an intermediary dictionary / copy of every subdataframe for every group, so will be much more memory-efficient that creating the naive dictionary with <code>dict(iter(gb))</code>. This is because it uses data-structures already available in the groupby object.</em></p>

<hr>

<p>You can select different columns using the groupby slicing:</p>

<pre><code>In [22]: gb[[""A"", ""B""]].get_group(""foo"")
Out[22]:
     A         B
0  foo  1.624345
2  foo -0.528172
4  foo  0.865408

In [23]: gb[""C""].get_group(""foo"")
Out[23]:
0     5
2    11
4    14
Name: C, dtype: int64
</code></pre>
";;2;;2013-02-06T17:00:27.800;;14734627;2016-06-14T21:45:14.487;2016-06-14T21:45:14.487;;1240268.0;;1240268.0;14734533.0;2;94;;;
2327;2327;;;"<p>There might be a better way, but this here's one approach:</p>

<pre><code>In [34]: import pandas as pd

In [35]: df
Out[35]: 
                        row
0       00000 UNITED STATES
1             01000 ALABAMA
2  01001 Autauga County, AL
3  01003 Baldwin County, AL
4  01005 Barbour County, AL

In [36]: df = pd.DataFrame(df.row.str.split(' ',1).tolist(),
                                   columns = ['flips','row'])

In [37]: df
Out[37]: 
   flips                 row
0  00000       UNITED STATES
1  01000             ALABAMA
2  01001  Autauga County, AL
3  01003  Baldwin County, AL
4  01005  Barbour County, AL
</code></pre>
";;3;;2013-02-07T07:03:46.033;;14745484;2014-05-05T10:31:23.020;2014-05-05T10:31:23.020;;544059.0;;1199589.0;14745022.0;2;34;;;
2328;2328;;;"<p>You can first add it as a normal column and then append it to the current index, so:</p>

<pre><code>df['Firstlevel'] = 'Foo'
df.set_index('Firstlevel', append=True, inplace=True)
</code></pre>

<p>And change the order if needed with:</p>

<pre><code>df.reorder_levels(['Firstlevel', 'A', 'B'])
</code></pre>

<p>Which results in:</p>

<pre><code>                      Vals
Firstlevel A  B           
Foo        a1 b1  0.871563
              b2  0.494001
           a2 b3 -0.167811
           a3 b4 -1.353409
</code></pre>
";;1;;2013-02-07T08:37:45.803;;14746845;2013-02-07T08:37:45.803;;;;;1755432.0;14744068.0;2;51;;;
2339;2339;;;"<p>Don't forget <code>DataFrame.tail</code>! e.g. <code>df1.tail(10)</code></p>
";;0;;2013-02-07T21:03:22.430;;14760930;2013-02-07T21:03:22.430;;;;;776560.0;14663004.0;2;155;;;
2342;2342;;;"<p>You need to pass in the labels explicitly when calling the r.plot function. </p>

<pre><code>r.plot([1,2,3],[1,2,3], xlab=""X"", ylab=""Y"")
</code></pre>

<p>When you plot in R, it grabs the labels via <code>deparse(substitute(x))</code> which essentially grabs the variable name from the <code>plot(testX, testY)</code>. When you're passing in python objects via rpy2, it's an anonymous R object and akin to the following in R:</p>

<pre><code>&gt; deparse(substitute(c(1,2,3)))
[1] ""c(1, 2, 3)""
</code></pre>

<p>which is why you're getting the crazy labels.</p>

<p>A lot of times it's saner to use rpy2 to <strong>only</strong> push data back and forth. </p>

<pre><code>r.assign('testX', df.A)
r.assign('testY', df.B)
%R plot(testX, testY)

rdf = com.convert_to_r_dataframe(df)
r.assign('bob', rdf)
%R plot(bob$$A, bob$$B)
</code></pre>

<p><a href=""http://nbviewer.ipython.org/4734581/"">http://nbviewer.ipython.org/4734581/</a></p>
";;6;;2013-02-07T22:55:16.463;;14762651;2013-02-07T22:55:16.463;;;;;376837.0;14656852.0;2;6;;;
2359;2359;;;"<p>[note: Your code in ""edit 2"" is working here (Python 2.7, rpy2-2.3.2, R-1.15.2).]</p>

<p>As @dale mentions it whenever R objects are anonymous (that is no R symbol exists for the object) the R <code>deparse(substitute())</code> will end up returning the <code>structure()</code> of the R object, and a possible fix is to specify the ""xlab"" and ""ylab"" parameters; for some plots you'll have to also specify <code>main</code> (the title).</p>

<p>An other way to work around that is to use R's formulas and feed the data frame (more below, after we work out the conversion part).</p>

<p>Forget about what is in <code>pandas.rpy</code>. It is both broken and seem to ignore features available in rpy2.</p>

<p>An earlier <a href=""https://stackoverflow.com/questions/14356577/rmagic-ipython-and-summary-information/14359050#14359050"">quick fix to conversion with ipython</a> can be turned into a proper conversion rather easily. I am considering adding one to the rpy2 codebase (with more bells and whistles), but in the meantime just add the following snippet after all your imports in your code examples. It will transparently convert pandas' <code>DataFrame</code> objects into rpy2's <code>DataFrame</code> whenever an R call is made.</p>

<pre><code>from collections import OrderedDict
py2ri_orig = rpy2.robjects.conversion.py2ri
def conversion_pydataframe(obj):
    if isinstance(obj, pandas.core.frame.DataFrame):
        od = OrderedDict()
        for name, values in obj.iteritems():
            if values.dtype.kind == 'O':
                od[name] = rpy2.robjects.vectors.StrVector(values)
            else:
                od[name] = rpy2.robjects.conversion.py2ri(values)
        return rpy2.robjects.vectors.DataFrame(od)
    elif isinstance(obj, pandas.core.series.Series):
        # converted as a numpy array
        res = py2ri_orig(obj) 
        # ""index"" is equivalent to ""names"" in R
        if obj.ndim == 1:
            res.names = ListVector({'x': ro.conversion.py2ri(obj.index)})
        else:
            res.dimnames = ListVector(ro.conversion.py2ri(obj.index))
        return res
    else:
        return py2ri_orig(obj) 
rpy2.robjects.conversion.py2ri = conversion_pydataframe
</code></pre>

<p>Now the following code will ""just work"":</p>

<pre><code>r.plot(rpy2.robjects.Formula('c3~c2'), data)
# `data` was converted to an rpy2 data.frame on the fly
# and the a scatter plot c3 vs c2 (with ""c2"" and ""c3"" the labels on
# the ""x"" axis and ""y"" axis).
</code></pre>

<p>I also note that you are importing <code>ggplot2</code>, without using it. Currently the conversion
will have to be explicitly requested. For example:</p>

<pre><code>p = ggplot2.ggplot(rpy2.robjects.conversion.py2ri(data)) +\
    ggplot2.geom_histogram(ggplot2.aes_string(x = 'c3'))
p.plot()
</code></pre>
";;9;;2013-02-09T15:17:40.383;;14789513;2015-03-20T00:06:35.013;2017-05-23T12:16:58.947;;-1.0;;294017.0;14656852.0;2;7;;;
2372;2372;;;"<p>Use the built-in isinstance() function.</p>

<pre><code>import pandas as pd

def f(var):
    if isinstance(var, pd.DataFrame):
        print ""do stuff""
</code></pre>
";;1;;2013-02-11T09:15:10.797;;14809026;2013-02-11T09:15:10.797;;;;;1755432.0;14808945.0;2;59;;;
2373;2373;;;"<p><a href=""http://docs.python.org/2/library/functions.html#isinstance"" rel=""nofollow noreferrer"">isinstance</a>, nothing else.</p>

<p><a href=""http://www.python.org/dev/peps/pep-0008/"" rel=""nofollow noreferrer"">PEP8</a> says explicitly that <code>isinstance</code> is the preferred way to check types</p>

<pre><code>Yes: if isinstance(obj, int):
No:  if type(obj) is type(1):
</code></pre>

<p>And don't even think about</p>

<pre><code>if obj.__class__.__name__ = ""MyInheritedClass"":
    expect_problems_some_day()
</code></pre>

<p><code>isinstance</code> handles inheritance (see <a href=""https://stackoverflow.com/questions/1549801/differences-between-isinstance-and-type-in-python"">Differences between isinstance() and type() in python</a>). For example, it will tell you if a variable is a string (either <code>str</code> or <code>unicode</code>), because they derive from <code>basestring</code>)</p>

<pre><code>if isinstance(obj, basestring):
    i_am_string(obj)
</code></pre>
";;0;;2013-02-11T09:23:16.907;;14809149;2013-02-12T09:14:32.203;2017-05-23T12:34:08.020;;-1.0;;497208.0;14808945.0;2;27;;;
2376;2376;;;"<p>A (slightly naive) way would be to store the structure of your data frame, i.e. its columns and index, separately, and then create a new data frame from your preprocessed results like so:</p>

<pre><code>In [15]: data = np.zeros((2,2))

In [16]: data
Out[16]: 
array([[ 0.,  0.],
       [ 0.,  0.]])

In [17]: from pandas import DataFrame

In [21]: df  = DataFrame(data, index = ['first', 'second'], columns=['c1','c2'])

In [22]: df
Out[22]: 
        c1  c2
first    0   0
second   0   0

In [26]: i = df.index

In [27]: c = df.columns

# generate new data as a numpy array    
In [29]: df  = DataFrame(np.random.rand(2,2), index=i, columns=c)

In [30]: df
Out[30]: 
              c1        c2
first   0.821354  0.936703
second  0.138376  0.482180
</code></pre>

<p>As you can see in <code>Out[22]</code>, we start off with a data frame, and then in <code>In[29]</code> we place some new data inside the frame, leaving the rows and columns unchanged. I am assuming your preprocessing will <code>not</code> shuffle the rows/ columns of the data.</p>
";;0;;2013-02-11T14:11:00.817;;14813733;2017-01-12T18:12:10.030;2017-01-12T18:12:10.030;;419338.0;;419338.0;14813289.0;2;8;;;
2377;2377;;;"<p>This can be done by wrapping the returned data in a dataframe, with <code>index</code> and <code>columns</code> information in.</p>

<pre><code>import pandas as pd
pd.DataFrame(preprocessing.scale(data), index = data.index, columns = data.columns) 
</code></pre>
";;0;;2013-02-11T14:37:44.550;;14814282;2017-02-02T02:37:22.227;2017-02-02T02:37:22.227;;500207.0;;395239.0;14813289.0;2;14;;;
2386;2386;;;"<p><code>pandas.Series</code>, has a <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html#numpy-ndarray-tolist""><code>tolist</code> method</a>:</p>

<pre><code>In [10]: import pandas as pd

In [11]: s = pd.Series([0,1,8,9], name = 'BayFail')

In [12]: s.tolist()
Out[12]: [0L, 1L, 8L, 9L]
</code></pre>

<hr>

<p>Technical note: In my original answer I said that <code>Series</code> was a subclass of <code>numpy.ndarray</code> and inherited its <code>tolist</code> method. While that's true for Pandas version 0.12 or older, In the soon-to-be-released Pandas version 0.13, <code>Series</code> has been refactored to be a subclass of <code>NDFrame</code>. <code>Series</code> still has a <code>tolist</code> method, but it has no direct relationship to the <code>numpy.ndarray</code> method of the same name.</p>
";;2;;2013-02-11T23:13:34.130;;14822703;2013-11-11T19:39:50.217;2013-11-11T19:39:50.217;;190597.0;;190597.0;14822680.0;2;30;;;
2398;2398;;;"<p>If think you have to ""postprocess"" the barplot with matplotlib as pandas internally sets the width of the bars. </p>

<p>The rectangles which form the bars are in container objects.
So you have to iterate through these containers and set the width of the rectangles individually: </p>

<pre><code>In [208]: df = pd.DataFrame(np.random.random((6, 5)) * 10,               
                        index=list('abcdef'), columns=list('ABCDE'))

In [209]: df
Out[209]: 
     A    B    C    D    E
a  4.2  6.7  1.0  7.1  1.4
b  1.3  9.5  5.1  7.3  5.6
c  8.9  5.0  5.0  6.7  3.8
d  5.5  0.5  2.4  8.4  6.4
e  0.3  1.4  4.8  1.7  9.3
f  3.3  0.2  6.9  8.0  6.1

In [210]: ax = df.plot(kind='bar', stacked=True, align='center')

In [211]: for container in ax.containers:
              plt.setp(container, width=1)
   .....:         

In [212]: x0, x1 = ax.get_xlim()

In [213]: ax.set_xlim(x0 -0.5, x1 + 0.25)
Out[213]: (-0.5, 6.5)

In [214]: plt.tight_layout()
</code></pre>

<p><img src=""https://i.stack.imgur.com/Z8nql.png"" alt=""stacked_bar.png""></p>
";;4;;2013-02-12T23:14:31.553;;14843650;2013-02-14T09:39:54.253;2013-02-14T09:39:54.253;;1301710.0;;1301710.0;14824456.0;2;10;;;
2408;2408;;;"<p>You can use the <code>base</code> argument of <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html"" rel=""noreferrer""><code>resample</code></a>:</p>

<pre><code>sample.resample('60Min', how=conversion, base=30)
</code></pre>

<p>From <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html"" rel=""noreferrer"">the above docs-link</a>:</p>

<blockquote>
  <p><code>base</code> : <code>int</code>, default 0<br>
  &nbsp;&nbsp;&nbsp; For frequencies that evenly subdivide 1 day, the origin of the aggregated intervals.<br>
  &nbsp;&nbsp;&nbsp;&nbsp;For example, for 5min frequency, base could range from 0 through 4. Defaults to 0</p>
</blockquote>
";;1;;2013-02-13T19:10:13.217;;14861132;2013-02-13T19:10:13.217;;;;;1240268.0;14861023.0;2;16;;;
2419;2419;;;"<h2>Update October 2015</h2>

<p>Seaborn handles this use-case splendidly:</p>

<pre><code>import numpy 
import pandas
from  matplotlib import pyplot
import seaborn
seaborn.set(style='ticks')

numpy.random.seed(0)
N = 37
_genders= ['Female', 'Male', 'Non-binary', 'No Response']
df = pandas.DataFrame({
    'Height (cm)': numpy.random.uniform(low=130, high=200, size=N),
    'Weight (kg)': numpy.random.uniform(low=30, high=100, size=N),
    'Gender': numpy.random.choice(_genders, size=N)
})

fg = seaborn.FacetGrid(data=df, hue='Gender', hue_order=_genders, aspect=1.61)
fg.map(pyplot.scatter, 'Weight (kg)', 'Height (cm)').add_legend()
</code></pre>

<p>Which immediately outputs:</p>

<p><a href=""https://i.stack.imgur.com/GnmJP.png""><img src=""https://i.stack.imgur.com/GnmJP.png"" alt=""enter image description here""></a></p>

<h2>Old Answer</h2>

<p>In this case, I would use matplotlib directly.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def dfScatter(df, xcol='Height', ycol='Weight', catcol='Gender'):
    fig, ax = plt.subplots()
    categories = np.unique(df[catcol])
    colors = np.linspace(0, 1, len(categories))
    colordict = dict(zip(categories, colors))  

    df[""Color""] = df[catcol].apply(lambda x: colordict[x])
    ax.scatter(df[xcol], df[ycol], c=df.Color)
    return fig

if 1:
    df = pd.DataFrame({'Height':np.random.normal(size=10),
                       'Weight':np.random.normal(size=10),
                       'Gender': [""Male"",""Male"",""Unknown"",""Male"",""Male"",
                                  ""Female"",""Did not respond"",""Unknown"",""Female"",""Female""]})    
    fig = dfScatter(df)
    fig.savefig('fig1.png')
</code></pre>

<p>And that gives me:</p>

<p><img src=""https://i.stack.imgur.com/hFLmZ.png"" alt=""scalle plot with categorized colors"">
As far as I know, that color column can be any matplotlib compatible color (RBGA tuples, HTML names, hex values, etc).</p>

<p>I'm having trouble getting anything but numerical values to work with the colormaps.</p>
";;5;;2013-02-15T01:37:49.507;;14887119;2015-10-28T16:46:11.527;2015-10-28T16:46:11.527;;1552748.0;;1552748.0;14885895.0;2;24;;;
2434;2434;;;"<p>A simple solution is to use <code>drop_duplicates</code></p>

<pre><code>df4 = df3.drop_duplicates(subset='rownum', keep='last')
</code></pre>

<p>For me, this operated quickly on large data sets.</p>

<p>This requires that 'rownum' be the column with duplicates.  In the modified example, 'rownum' has no duplicates, therefore nothing gets eliminated.  What we really want is to have the 'cols' be set to the index.  I've not found a way to tell drop_duplicates to only consider the index.</p>

<p>Here is a solution that adds the index as a dataframe column, drops duplicates on that, then removes the new column:</p>

<pre><code>df3 = df3.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')
</code></pre>

<p>And if you want things back in the proper order, just call <code>sort</code> on the dataframe.</p>

<pre><code>df3 = df3.sort()
</code></pre>

<h2>Edit: Better answer below</h2>

<p>Take a look at n8yoder's answer using 'duplicated'.  I don't believe this existed in older versions of Pandas, where this answer might still apply.</p>
";;4;;2013-02-15T17:27:14.987;;14900065;2017-01-03T17:06:30.950;2017-01-03T17:06:30.950;;1638871.0;;1638871.0;13035764.0;2;82;;;
2444;2444;;;"<p>I'm assuming you already have the DataFrame. In which case you can just turn the columns into a MultiIndex and use stack then reset_index. Note that you'll then have to rename and reorder the columns and sort by sample to get <em>exactly</em> what you posted in the question:</p>

<pre><code>In [4]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})

In [5]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])

In [6]: df.stack(0).reset_index(1)
Out[6]: 
  level_1         x         y
0      s1  0.897994 -0.278357
0      s2 -0.008126 -1.701865
1      s1 -1.354633 -0.890960
1      s2 -0.773428  0.003501
2      s1 -1.499422 -1.518993
2      s2  0.240226  1.773427
3      s1 -1.090921  0.847064
3      s2 -1.061303  1.557871
4      s1 -1.697340 -0.160952
4      s2 -0.930642  0.182060
5      s1 -0.356076 -0.661811
5      s2  0.539875 -1.033523
6      s1 -0.687861 -1.450762
6      s2  0.700193  0.658959
7      s1 -0.130422 -0.826465
7      s2 -0.423473 -1.281856
8      s1  0.306983  0.433856
8      s2  0.097279 -0.256159
9      s1  0.498057  0.147243
9      s2  1.312578  0.111837
</code></pre>

<p>You can save the MultiIndex conversion if you can just create the DataFrame with a MultiIndex instead.</p>

<p>Edit: use merge to join original ids back in </p>

<pre><code>In [59]: df
Out[59]: 
   names      s1_x      s1_y      s2_x      s2_y
0      0  0.732099  0.018387  0.299856  0.737142
1      1  0.914755 -0.798159 -0.732868 -1.279311
2      2 -1.063558  0.161779 -0.115751 -0.251157
3      3 -1.185501  0.095147 -1.343139 -0.003084
4      4  0.622400 -0.299726  0.198710 -0.383060
5      5  0.179318  0.066029 -0.635507  1.366786
6      6 -0.820099  0.066067  1.113402  0.002872
7      7  0.711627 -0.182925  1.391194 -2.788434
8      8 -1.124092  1.303375  0.202691 -0.225993
9      9 -0.179026  0.847466 -1.480708 -0.497067

In [60]: id = df.ix[:, ['names']]

In [61]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])

In [62]: pandas.merge(df.stack(0).reset_index(1), id, left_index=True, right_index=True)
Out[62]: 
  level_1         x         y  names
0      s1  0.732099  0.018387      0
0      s2  0.299856  0.737142      0
1      s1  0.914755 -0.798159      1
1      s2 -0.732868 -1.279311      1
2      s1 -1.063558  0.161779      2
2      s2 -0.115751 -0.251157      2
3      s1 -1.185501  0.095147      3
3      s2 -1.343139 -0.003084      3
4      s1  0.622400 -0.299726      4
4      s2  0.198710 -0.383060      4
5      s1  0.179318  0.066029      5
5      s2 -0.635507  1.366786      5
6      s1 -0.820099  0.066067      6
6      s2  1.113402  0.002872      6
7      s1  0.711627 -0.182925      7
7      s2  1.391194 -2.788434      7
8      s1 -1.124092  1.303375      8
8      s2  0.202691 -0.225993      8
9      s1 -0.179026  0.847466      9
9      s2 -1.480708 -0.497067      9
</code></pre>

<p>Alternatively:</p>

<pre><code>    In [64]: df
Out[64]: 
   names      s1_x      s1_y      s2_x      s2_y
0      0  0.744742 -1.123403  0.212736  0.005440
1      1  0.465075 -0.673491  1.467156 -0.176298
2      2 -1.111566  0.168043 -0.102142 -1.072461
3      3  1.226537 -1.147357 -1.583762 -1.236582
4      4  1.137675  0.224422  0.738988  1.528416
5      5 -0.237014 -1.110303 -0.770221  1.389714
6      6 -0.659213  2.305374 -0.326253  1.416778
7      7  1.524214 -0.395451 -1.884197  0.524606
8      8  0.375112 -0.622555  0.295336  0.927208
9      9  1.168386 -0.291899 -1.462098  0.250889

In [65]: df = df.set_index('names')

In [66]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])

In [67]: df.stack(0).reset_index(1)
Out[67]: 
      level_1         x         y
names                            
0          s1  0.744742 -1.123403
0          s2  0.212736  0.005440
1          s1  0.465075 -0.673491
1          s2  1.467156 -0.176298
2          s1 -1.111566  0.168043
2          s2 -0.102142 -1.072461
3          s1  1.226537 -1.147357
3          s2 -1.583762 -1.236582
4          s1  1.137675  0.224422
4          s2  0.738988  1.528416
5          s1 -0.237014 -1.110303
5          s2 -0.770221  1.389714
6          s1 -0.659213  2.305374
6          s2 -0.326253  1.416778
7          s1  1.524214 -0.395451
7          s2 -1.884197  0.524606
8          s1  0.375112 -0.622555
8          s2  0.295336  0.927208
9          s1  1.168386 -0.291899
9          s2 -1.462098  0.250889
</code></pre>
";;5;;2013-02-17T03:05:57.237;;14917572;2013-02-27T22:12:56.040;2013-02-27T22:12:56.040;;1306530.0;;1306530.0;14916358.0;2;12;;;
2455;2455;;;"<p>Two approaches that come to mind:</p>

<pre><code>&gt;&gt;&gt; df
          A         B         C         D
0  0.424634  1.716633  0.282734  2.086944
1 -1.325816  2.056277  2.583704 -0.776403
2  1.457809 -0.407279 -1.560583 -1.316246
3 -0.757134 -1.321025  1.325853 -2.513373
4  1.366180 -1.265185 -2.184617  0.881514
&gt;&gt;&gt; df.ix[:, 2]
0    0.282734
1    2.583704
2   -1.560583
3    1.325853
4   -2.184617
Name: C
&gt;&gt;&gt; df[df.columns[2]]
0    0.282734
1    2.583704
2   -1.560583
3    1.325853
4   -2.184617
Name: C
</code></pre>
";;0;;2013-02-18T16:44:55.830;;14941170;2013-02-18T16:44:55.830;;;;;487339.0;14941097.0;2;34;;;
2459;2459;;;"<p>One way to do this is to insert a dummy column with the sums in order to sort:</p>

<pre><code>In [10]: sum_B_over_A = df.groupby('A').sum().B

In [11]: sum_B_over_A
Out[11]: 
A
bar    0.253652
baz   -2.829711
foo    0.551376
Name: B

in [12]: df['sum_B_over_A'] = df.A.apply(sum_B_over_A.get_value)

In [13]: df
Out[13]: 
     A         B      C  sum_B_over_A
0  foo  1.624345  False      0.551376
1  bar -0.611756   True      0.253652
2  baz -0.528172  False     -2.829711
3  foo -1.072969   True      0.551376
4  bar  0.865408  False      0.253652
5  baz -2.301539   True     -2.829711

In [14]: df.sort(['sum_B_over_A', 'A', 'B'])
Out[14]: 
     A         B      C   sum_B_over_A
5  baz -2.301539   True      -2.829711
2  baz -0.528172  False      -2.829711
1  bar -0.611756   True       0.253652
4  bar  0.865408  False       0.253652
3  foo -1.072969   True       0.551376
0  foo  1.624345  False       0.551376
</code></pre>

<p><em>and maybe you would drop the dummy row:</em></p>

<pre><code>In [15]: df.sort(['sum_B_over_A', 'A', 'B']).drop('sum_B_over_A', axis=1)
Out[15]: 
     A         B      C
5  baz -2.301539   True
2  baz -0.528172  False
1  bar -0.611756   True
4  bar  0.865408  False
3  foo -1.072969   True
0  foo  1.624345  False
</code></pre>
";;5;;2013-02-18T18:06:42.850;;14942625;2013-02-19T16:37:58.233;2013-02-19T16:37:58.233;;1240268.0;;1240268.0;14941366.0;2;8;;;
2461;2461;;;"<p>Groupby A:</p>

<pre><code>In [0]: grp = df.groupby('A')
</code></pre>

<p>Within each group, sum over B and broadcast the values using transform.  Then sort by B:</p>

<pre><code>In [1]: grp[['B']].transform(sum).sort('B')
Out[1]:
          B
2 -2.829710
5 -2.829710
1  0.253651
4  0.253651
0  0.551377
3  0.551377
</code></pre>

<p>Index the original df by passing the index from above.  This will re-order the A values by the aggregate sum of the B values:</p>

<pre><code>In [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]

In [3]: sort1
Out[3]:
     A         B      C
2  baz -0.528172  False
5  baz -2.301539   True
1  bar -0.611756   True
4  bar  0.865408  False
0  foo  1.624345  False
3  foo -1.072969   True
</code></pre>

<p>Finally, sort the 'C' values within groups of 'A' using the <code>sort=False</code> option to preserve the A sort order from step 1:</p>

<pre><code>In [4]: f = lambda x: x.sort('C', ascending=False)

In [5]: sort2 = sort1.groupby('A', sort=False).apply(f)

In [6]: sort2
Out[6]:
         A         B      C
A
baz 5  baz -2.301539   True
    2  baz -0.528172  False
bar 1  bar -0.611756   True
    4  bar  0.865408  False
foo 3  foo -1.072969   True
    0  foo  1.624345  False
</code></pre>

<p>Clean up the df index by using <code>reset_index</code> with <code>drop=True</code>:</p>

<pre><code>In [7]: sort2.reset_index(0, drop=True)
Out[7]:
     A         B      C
5  baz -2.301539   True
2  baz -0.528172  False
1  bar -0.611756   True
4  bar  0.865408  False
3  foo -1.072969   True
0  foo  1.624345  False
</code></pre>
";;4;;2013-02-18T22:11:48.840;;14946246;2013-02-18T22:11:48.840;;;;;919872.0;14941366.0;2;43;;;
2487;2487;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html"">xs</a> may be what you want. Here's a few examples:</p>

<pre><code>In [63]: df.xs(('B',), level='Alpha')
Out[63]:
                  I        II       III        IV         V        VI       VII
Int Bool                                                                       
0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250
    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403
    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897
2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582
    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494

In [64]: df.xs(('B', False), level=('Alpha', 'Bool'))
Out[64]:
            I        II       III        IV         V        VI       VII
Int                                                                      
0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897
2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 
</code></pre>

<p><strong>Edit</strong>:</p>

<p>For the last requirement you can chain <code>get_level_values</code> and <code>isin</code>:</p>

<p>Get the even values in the index (other ways to do this too)</p>

<pre><code>In [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)
         ix_vals

Out[87]: set([0L, 2L])
</code></pre>

<p>Use these with <code>isin</code></p>

<pre><code>In [89]: ix = df.index.get_level_values('Int').isin(ix_vals)
In [90]: df[ix]
Out[90]:                I        II       III        IV         V        VI       VII
Alpha Int Bool                                                                       
A     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572
          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265
      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165
          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474
B     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250
          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582
          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 
</code></pre>
";;1;;2013-02-19T18:22:12.957;;14964637;2013-02-19T18:41:23.273;2013-02-19T18:41:23.273;;386279.0;;386279.0;14964493.0;2;26;;;
2498;2498;;;"<p>It sounds like you already know the unique column names. If that's the case, then <code>df = df['Time', 'Time Relative', 'N2']</code> would work.</p>

<p>If not, your solution should work:</p>

<pre><code>In [101]: vals = np.random.randint(0,20, (4,3))
          vals
Out[101]:
array([[ 3, 13,  0],
       [ 1, 15, 14],
       [14, 19, 14],
       [19,  5,  1]])

In [106]: df = pd.DataFrame(np.hstack([vals, vals]), columns=['Time', 'H1', 'N2', 'Time Relative', 'N2', 'Time'] )
          df
Out[106]:
   Time  H1  N2  Time Relative  N2  Time
0     3  13   0              3  13     0
1     1  15  14              1  15    14
2    14  19  14             14  19    14
3    19   5   1             19   5     1

In [107]: df.T.drop_duplicates().T
Out[107]:
   Time  H1  N2
0     3  13   0
1     1  15  14
2    14  19  14
3    19   5   1
</code></pre>

<p>You probably have something specific to your data that's messing it up. We could give more help if there's more details you could give us about the data. </p>

<p><strong>Edit:</strong>
Like Andy said, the problem is probably with the duplicate column titles.</p>

<p>For a sample table file 'dummy.csv' I made up:</p>

<pre><code>Time    H1  N2  Time    N2  Time Relative
3   13  13  3   13  0
1   15  15  1   15  14
14  19  19  14  19  14
19  5   5   19  5   1
</code></pre>

<p>using <code>read_table</code> gives unique columns and works properly:</p>

<pre><code>In [151]: df2 = pd.read_table('dummy.csv')
          df2
Out[151]:
         Time  H1  N2  Time.1  N2.1  Time Relative
      0     3  13  13       3    13              0
      1     1  15  15       1    15             14
      2    14  19  19      14    19             14
      3    19   5   5      19     5              1
In [152]: df2.T.drop_duplicates().T
Out[152]:
             Time  H1  Time Relative
          0     3  13              0
          1     1  15             14
          2    14  19             14
          3    19   5              1  
</code></pre>

<p>If your version doesn't let your, you can hack together a solution to make them unique:</p>

<pre><code>In [169]: df2 = pd.read_table('dummy.csv', header=None)
          df2
Out[169]:
              0   1   2     3   4              5
        0  Time  H1  N2  Time  N2  Time Relative
        1     3  13  13     3  13              0
        2     1  15  15     1  15             14
        3    14  19  19    14  19             14
        4    19   5   5    19   5              1
In [171]: from collections import defaultdict
          col_counts = defaultdict(int)
          col_ix = df2.first_valid_index()
In [172]: cols = []
          for col in df2.ix[col_ix]:
              cnt = col_counts[col]
              col_counts[col] += 1
              suf = '_' + str(cnt) if cnt else ''
              cols.append(col + suf)
          cols
Out[172]:
          ['Time', 'H1', 'N2', 'Time_1', 'N2_1', 'Time Relative']
In [174]: df2.columns = cols
          df2 = df2.drop([col_ix])
In [177]: df2
Out[177]:
          Time  H1  N2 Time_1 N2_1 Time Relative
        1    3  13  13      3   13             0
        2    1  15  15      1   15            14
        3   14  19  19     14   19            14
        4   19   5   5     19    5             1
In [178]: df2.T.drop_duplicates().T
Out[178]:
          Time  H1 Time Relative
        1    3  13             0
        2    1  15            14
        3   14  19            14
        4   19   5             1 
</code></pre>
";;2;;2013-02-20T17:06:21.030;;14985695;2013-02-20T17:49:53.990;2013-02-20T17:49:53.990;;386279.0;;386279.0;14984119.0;2;20;;;
2506;2506;;;"<p>Ah, this is to do with how I created the DataFrame, not with how I was combining them. The long and the short of it is, if you are creating a frame using a loop and a statement that looks like this:</p>

<pre><code>Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))
</code></pre>

<p><strong>You must ignore the index</strong></p>

<pre><code>Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)
</code></pre>

<p>Or you will have issues later when combining data.</p>
";;0;;2013-02-20T20:05:49.260;;14988913;2015-08-17T14:10:57.743;2015-08-17T14:10:57.743;;854988.0;;1319312.0;14988480.0;2;18;;;
2507;2507;;;"<p>This worked for me:</p>

<pre><code>import numpy as np
import pandas as pd

dates = np.asarray(pd.date_range('1/1/2000', periods=8))
df1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])
df2 = df1.copy()
df = df1.append(df2)
</code></pre>

<p>Yields:</p>

<pre><code>                   A         B         C         D
2000-01-01 -0.327208  0.552500  0.862529  0.493109
2000-01-02  1.039844 -2.141089 -0.781609  1.307600
2000-01-03 -0.462831  0.066505 -1.698346  1.123174
2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791
2000-01-05  0.693749  0.544329 -1.606851  0.527733
2000-01-06 -2.461177 -0.339378 -0.236275  0.155569
2000-01-07 -0.597156  0.904511  0.369865  0.862504
2000-01-08 -0.958300 -0.583621 -2.068273  0.539434
2000-01-01 -0.327208  0.552500  0.862529  0.493109
2000-01-02  1.039844 -2.141089 -0.781609  1.307600
2000-01-03 -0.462831  0.066505 -1.698346  1.123174
2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791
2000-01-05  0.693749  0.544329 -1.606851  0.527733
2000-01-06 -2.461177 -0.339378 -0.236275  0.155569
2000-01-07 -0.597156  0.904511  0.369865  0.862504
2000-01-08 -0.958300 -0.583621 -2.068273  0.539434
</code></pre>

<p>If you don't already use the latest version of <code>pandas</code> I highly recommend upgrading. It is now possible to operate with DataFrames which contain duplicate indices.</p>
";;2;;2013-02-20T20:14:06.343;;14989047;2013-02-20T20:14:06.343;;;;;956539.0;14988480.0;2;10;;;
2510;2510;;;"<p>You can call <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.dropna.html"" rel=""noreferrer""><code>dropna</code></a> with arguments <code>subset</code> and <code>how</code>:</p>

<pre><code>df2.dropna(subset=['three', 'four', 'five'], how='all')
</code></pre>

<p>As the names suggests:</p>

<ul>
<li><code>how='all'</code> requires every column (of <code>subset</code>) in the row to be <code>NaN</code> in order to be dropped, as opposed to the default <code>'any'</code>.</li>
<li><code>subset</code> is those columns to inspect for <code>NaN</code>s.</li>
</ul>

<p>As <a href=""https://stackoverflow.com/users/1552748/paul-h"">@PaulH</a> points out, we can generalise to drop the last <code>k</code> columns with:</p>

<pre><code>subset=df2.columns[-k:]
</code></pre>

<p>Indeed, we could even do something more complicated if desired:</p>

<pre><code>subset=filter(lambda x: len(x) &gt; 3, df2.columns)
</code></pre>
";;2;;2013-02-20T23:58:41.307;;14992237;2013-02-21T13:51:46.367;2017-05-23T12:25:45.393;;-1.0;;1240268.0;14991195.0;2;53;;;
2521;2521;;;"<p>I think the issue is that there are two different <code>first</code> methods which share a name but act differently, one is for <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#splitting-an-object-into-groups"" rel=""noreferrer"">groupby objects</a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.first.html"" rel=""noreferrer"">another for a Series/DataFrame</a> (to do with timeseries).</p>

<p>To replicate the behaviour of the groupby <code>first</code> method over a DataFrame using <code>agg</code> you could use <code>iloc[0]</code> (which gets the first row in each group (DataFrame/Series) by index):</p>

<pre><code>grouped.agg(lambda x: x.iloc[0])
</code></pre>

<p>For example:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2], [3, 4]])

In [2]: g = df.groupby(0)

In [3]: g.first()
Out[3]: 
   1
0   
1  2
3  4

In [4]: g.agg(lambda x: x.iloc[0])
Out[4]: 
   1
0   
1  2
3  4
</code></pre>

<p><em>Analogously you can replicate <code>last</code> using <code>iloc[-1]</code>.</em></p>

<p>Note: This will works column-wise, et al:</p>

<pre><code>g.agg({1: lambda x: x.iloc[0]})
</code></pre>

<p><em>In older version of pandas you could would use the irow method (e.g. <code>x.irow(0)</code>, see previous edits.</em></p>

<hr>

<p>A couple of updated notes:</p>

<p>This is better done using the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.nth.html"" rel=""noreferrer""><code>nth</code></a> groupby method, which is much faster >=0.13:</p>

<pre><code>g.nth(0)  # first
g.nth(-1)  # last
</code></pre>

<p><em>You have to take care a little, as the default behaviour for <code>first</code> and <code>last</code> ignores NaN rows... and IIRC for DataFrame groupbys it was broken pre-0.13... there's a <code>dropna</code> option for <code>nth</code>.</em></p>

<p>You can use the strings rather than built-ins (though IIRC pandas spots it's the <code>sum</code> builtin and applies <code>np.sum</code>):</p>

<pre><code>grouped['D'].agg({'result1' : ""sum"", 'result2' : ""mean""})
</code></pre>
";;4;;2013-02-21T12:51:42.893;;15002718;2014-12-05T17:30:51.290;2014-12-05T17:30:51.290;;1240268.0;;1240268.0;15001237.0;2;19;;;
2532;2532;;;"<p>In this case, where the <code>DataFrame</code> is long but not too wide, you can simply slice it:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": range(1000), ""B"": range(1000)})
&gt;&gt;&gt; df
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000 entries, 0 to 999
Data columns:
A    1000  non-null values
B    1000  non-null values
dtypes: int64(2)
&gt;&gt;&gt; df[:5]
   A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4
</code></pre>

<p>If it's both wide and long, I tend to use <code>.ix</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({i: range(1000) for i in range(100)})
&gt;&gt;&gt; df.ix[:5, :10]
   0   1   2   3   4   5   6   7   8   9   10
0   0   0   0   0   0   0   0   0   0   0   0
1   1   1   1   1   1   1   1   1   1   1   1
2   2   2   2   2   2   2   2   2   2   2   2
3   3   3   3   3   3   3   3   3   3   3   3
4   4   4   4   4   4   4   4   4   4   4   4
5   5   5   5   5   5   5   5   5   5   5   5
</code></pre>
";;1;;2013-02-21T15:50:02.550;;15006495;2013-02-21T15:50:02.550;;;;;487339.0;15006298.0;2;27;;;
2536;2536;;;"<p>I think you can use the <code>nrows</code> parameter.  From <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">the docs</a>:</p>

<pre><code>nrows : int, default None

    Number of rows of file to read. Useful for reading pieces of large files
</code></pre>

<p>which seems to work.  Using one of the standard large test files (988504479 bytes, 5344499 lines):</p>

<pre><code>In [1]: import pandas as pd

In [2]: time z = pd.read_csv(""P00000001-ALL.csv"", nrows=20)
CPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s
Wall time: 0.00 s

In [3]: len(z)
Out[3]: 20

In [4]: time z = pd.read_csv(""P00000001-ALL.csv"")
CPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s
Wall time: 30.23 s
</code></pre>
";;1;;2013-02-21T18:00:29.233;;15009160;2016-09-27T04:53:12.897;2016-09-27T04:53:12.897;;3491991.0;;487339.0;15008970.0;2;48;;;
2558;2558;;;"<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html"">documentation</a>, you can use either a regex or <code>delim_whitespace</code>:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; for line in open(""whitespace.csv""):
...     print repr(line)
...     
'a\t  b\tc 1 2\n'
'd\t  e\tf 3 4\n'
&gt;&gt;&gt; pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\s+"")
   0  1  2  3  4
0  a  b  c  1  2
1  d  e  f  3  4
&gt;&gt;&gt; pd.read_csv(""whitespace.csv"", header=None, delim_whitespace=True)
   0  1  2  3  4
0  a  b  c  1  2
1  d  e  f  3  4
</code></pre>
";;1;;2013-02-22T14:51:51.297;;15026839;2013-02-22T14:51:51.297;;;;;487339.0;15026698.0;2;52;;;
2560;2560;;;"<p>This code achieves what you want --- also its weird and certainly buggy:</p>

<p>I observed that it works when:</p>

<p>a) you specify the <code>index_col</code> rel. to the number of columns you really use -- so its three columns in this example, not four (you drop <code>dummy</code> and start counting from then onwards)</p>

<p>b) same for <code>parse_dates</code></p>

<p>c) not so for <code>usecols</code> ;) for obvious reasons</p>

<p>d) here I adapted the <code>names</code> to mirror this behaviour</p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = """"""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5
""""""

df = pd.read_csv(StringIO(csv),
        index_col=[0,1],
        usecols=[1,2,3], 
        parse_dates=[0],
        header=0,
        names=[""date"", ""loc"", """", ""x""])

print df
</code></pre>

<p>which prints</p>

<pre><code>                x
date       loc   
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;1;;2013-02-22T18:04:28.777;;15030455;2013-02-22T18:04:28.777;;;;;733291.0;15017072.0;2;7;;;
2592;2592;;;"<p>For a simple data frame. First, without formatters:</p>

<pre><code>In [11]: df
Out[11]: 
              c1        c2
first   0.821354  0.936703
second  0.138376  0.482180

In [12]: print df.to_latex()
\begin{tabular}{|l|c|c|c|}
\hline
{} &amp;        c1 &amp;        c2 \\
\hline
first  &amp;  0.821354 &amp;  0.936703 \\
second &amp;  0.138376 &amp;  0.482180 \\
\hline
\end{tabular}
</code></pre>

<p><em>Copy-pasting the output (of <code>[12]</code>) to latex, we get: <img src=""https://i.stack.imgur.com/Td5KD.png"" alt=""latex without formatters""></em></p>

<p>If we create two functions <code>f1</code> and <code>f2</code> and put them into <code>to_latex</code> as <code>formatters</code>:</p>

<pre><code>def f1(x):
    return 'blah_%1.2f' % x

def f2(x):
    return 'f2_%1.2f' % x

In [15]: print df.to_latex(formatters=[f1, f2])
\begin{tabular}{|l|c|c|c|}
\hline
{} &amp;        c1 &amp;      c2 \\
\hline
first  &amp; blah\_0.82 &amp; f2\_0.94 \\
second &amp; blah\_0.14 &amp; f2\_0.48 \\
\hline
\end{tabular}
</code></pre>

<p><em>Copy-pasting the output to latex, we get:</em>
<img src=""https://i.stack.imgur.com/9ETVG.png"" alt=""latex with formatters f1 and f2""></p>

<p>Note: how the formatter function <code>f1</code> is applied to the first column and <code>f2</code> to the second.</p>
";;2;;2013-02-25T15:10:52.470;;15070110;2013-02-25T15:41:57.243;2013-02-25T15:41:57.243;;1240268.0;;419338.0;15069814.0;2;19;;;
2598;2598;;;"<p>IIUC, the simplest way I can think of would be something like this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(5, 10))
&gt;&gt;&gt; df[list(df.columns[:2]) + [7]]
          0         1         7
0  0.210139  0.533249  1.780426
1  0.382136  0.083999 -0.392809
2 -0.237868  0.493646 -1.208330
3  1.242077 -0.781558  2.369851
4  1.910740 -0.643370  0.982876
</code></pre>

<p>where the <code>list</code> call isn't optional because otherwise the <code>Index</code> object will try to vector-add itself to the 7.</p>

<p>It would be possible to special-case something like numpy's <code>r_</code> so that </p>

<pre><code>df[col_[:2, ""col5"", 3:6]]
</code></pre>

<p>would work, although I don't know if it would be worth the trouble.</p>
";;0;;2013-02-25T18:37:44.927;;15073977;2013-02-25T18:37:44.927;;;;;487339.0;15072005.0;2;13;;;
2599;2599;;;"<p>A lot of handy things are stored in the <code>DataFrameGroupBy.grouper</code> object.  For example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,
                   'Rank': np.random.randint(0,3,6),
                   'Val': np.random.rand(6)})
&gt;&gt;&gt; grouped = df.groupby([""Name"", ""Rank""])
&gt;&gt;&gt; grouped.grouper.
grouped.grouper.agg_series        grouped.grouper.indices
grouped.grouper.aggregate         grouped.grouper.labels
grouped.grouper.apply             grouped.grouper.levels
grouped.grouper.axis              grouped.grouper.names
grouped.grouper.compressed        grouped.grouper.ngroups
grouped.grouper.get_group_levels  grouped.grouper.nkeys
grouped.grouper.get_iterator      grouped.grouper.result_index
grouped.grouper.group_info        grouped.grouper.shape
grouped.grouper.group_keys        grouped.grouper.size
grouped.grouper.groupings         grouped.grouper.sort
grouped.grouper.groups            
</code></pre>

<p>and so:</p>

<pre><code>&gt;&gt;&gt; df[""GroupId""] = df.groupby([""Name"", ""Rank""]).grouper.group_info[0]
&gt;&gt;&gt; df
  Name  Rank       Val  GroupId
0  foo     0  0.302482        2
1  bar     0  0.375193        0
2  foo     2  0.965763        4
3  bar     2  0.166417        1
4  foo     1  0.495124        3
5  bar     2  0.728776        1
</code></pre>

<p>There may be a nicer alias for for <code>grouper.group_info[0]</code> lurking around somewhere, but this should work, anyway.</p>
";;6;;2013-02-25T19:00:54.073;;15074395;2013-02-25T19:00:54.073;;;;;487339.0;15072626.0;2;22;;;
2618;2618;;;"<p>For the record, here is an example using a sqlite database:</p>

<pre><code>import pandas as pd
import sqlite3

with sqlite3.connect(""whatever.sqlite"") as con:
    sql = ""SELECT * FROM table_name""
    df = pd.read_sql_query(sql, con)
    print df.shape
</code></pre>
";;2;;2013-02-26T18:58:37.440;;15097125;2015-11-01T22:15:49.327;2015-11-01T22:15:49.327;;1452688.0;;419338.0;10065051.0;2;22;;;
2619;2619;;;"<p>If your csv file contains extra data, columns can be <a href=""http://pandas.pydata.org/pandas-docs/dev/dsintro.html#column-selection-addition-deletion"">deleted</a> from the DataFrame after import.   </p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""

df = pd.read_csv(StringIO(csv),
        index_col=[""date"", ""loc""], 
        usecols=[""dummy"", ""date"", ""loc"", ""x""],
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
del df['dummy']
</code></pre>

<p>Which gives us:</p>

<pre><code>                x
date       loc
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;0;;2013-02-26T22:01:28.703;;15100193;2013-02-26T22:01:28.703;;;;;653689.0;15017072.0;2;8;;;
2627;2627;;;"<p>It looks like a transposed matrix:</p>

<pre><code>df.values.T.tolist()
</code></pre>
";;2;;2013-02-27T12:34:42.583;;15112264;2013-02-27T12:39:57.463;2013-02-27T12:39:57.463;;449449.0;;449449.0;15112234.0;2;32;;;
2650;2650;;;"<p>To make the first approach work, try returning a Series instead of a tuple (apply is throwing an exception because it doesn't know how to glue the rows back together as the number of columns doesn't match the original frame).</p>

<pre><code>def calculate(s):
    a = s['path'] + 2*s['row'] # Simple calc for example
    b = s['path'] * 0.153
    return pd.Series(dict(col1=a, col2=b))
</code></pre>

<p>The second approach should work if you replace:</p>

<pre><code>st.ix[i]['a'] = a
</code></pre>

<p>with:</p>

<pre><code>st.ix[i, 'a'] = a
</code></pre>
";;2;;2013-02-28T01:21:16.033;;15125793;2013-02-28T01:21:16.033;;;;;243434.0;15118111.0;2;18;;;
2668;2668;;;"<p>It looks like you may have some nulls in the column. You can drop them with <code>df = df.dropna(subset=['item'])</code>. Then <code>df['item'].value_counts().max()</code> should give you the max counts, and <code>df['item'].value_counts().idxmax()</code> should give you the most frequent value.</p>
";;2;;2013-02-28T15:43:31.177;;15139677;2013-02-28T15:43:31.177;;;;;386279.0;15138973.0;2;14;;;
2675;2675;;;"<p>Even though <code>df.loc[idx]</code> may be a copy of a portion of <code>df</code>, <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#why-does-the-assignment-when-using-chained-indexing-fail"" rel=""nofollow noreferrer""><strong>assignment</strong> to <code>df.loc[idx]</code></a> modifies <code>df</code> itself. (This is also true of <code>df.iloc</code> and <code>df.ix</code>.)</p>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A':[9,10]*6,
                   'B':range(23,35),
                   'C':range(-6,6)})

print(df)
#      A   B  C
# 0    9  23 -6
# 1   10  24 -5
# 2    9  25 -4
# 3   10  26 -3
# 4    9  27 -2
# 5   10  28 -1
# 6    9  29  0
# 7   10  30  1
# 8    9  31  2
# 9   10  32  3
# 10   9  33  4
# 11  10  34  5
</code></pre>

<p>Here is our boolean index:</p>

<pre><code>idx = (df['C']!=0) &amp; (df['A']==10) &amp; (df['B']&lt;30)
</code></pre>

<p>We can modify those rows of <code>df</code> where <code>idx</code> is True by assigning to <code>df.loc[idx, ...]</code>. For example,</p>

<pre><code>df.loc[idx, 'A'] += df.loc[idx, 'B'] * df.loc[idx, 'C']
print(df)
</code></pre>

<p>yields</p>

<pre><code>      A   B  C
0     9  23 -6
1  -110  24 -5
2     9  25 -4
3   -68  26 -3
4     9  27 -2
5   -18  28 -1
6     9  29  0
7    10  30  1
8     9  31  2
9    10  32  3
10    9  33  4
11   10  34  5
</code></pre>
";;9;;2013-02-28T20:16:14.217;;15144847;2015-09-18T02:27:47.257;2015-09-18T02:27:47.257;;190597.0;;190597.0;15143842.0;2;8;;;
2704;2704;;;"<p>Note: Timestamp is just unix time with nanoseconds (so divide it by 10**9):</p>

<pre class=""lang-py prettyprint-override""><code>[t.value // 10 ** 9 for t in tsframe.index]
</code></pre>

<p>For example:</p>

<pre><code>In [1]: t = pd.Timestamp('2000-02-11 00:00:00')

In [2]: t
Out[2]: &lt;Timestamp: 2000-02-11 00:00:00&gt;

In [3]: t.value
Out[3]: 950227200000000000L

In [4]: time.mktime(t.timetuple())
Out[4]: 950227200.0
</code></pre>

<p>As @root points out it's faster to extract the array of values directly:</p>

<pre class=""lang-py prettyprint-override""><code>tsframe.index.astype(np.int64) // 10 ** 9
</code></pre>
";;2;;2013-03-04T14:31:21.453;;15203886;2013-03-04T15:03:12.480;2013-03-04T15:03:12.480;;1240268.0;;1240268.0;15203623.0;2;23;;;
2705;2705;;;"<p>As <code>DatetimeIndex</code> is <code>ndarray</code> under the hood, you can do the conversion without a comprehension (much faster).</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: from datetime import datetime

In [4]: dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]
   ...: index = pd.DatetimeIndex(dates)
   ...: 
In [5]: index.astype(np.int64)
Out[5]: array([1335830400000000000, 1335916800000000000, 1336003200000000000], 
        dtype=int64)

In [6]: index.astype(np.int64) // 10**9
Out[6]: array([1335830400, 1335916800, 1336003200], dtype=int64)

%timeit [t.value // 10 ** 9 for t in index]
10000 loops, best of 3: 119 us per loop

%timeit index.astype(np.int64) // 10**9
100000 loops, best of 3: 18.4 us per loop
</code></pre>
";;2;;2013-03-04T14:47:36.513;;15204235;2013-03-04T15:03:48.133;2013-03-04T15:03:48.133;;1240268.0;;1199589.0;15203623.0;2;50;;;
2715;2715;;;"<p>0.10.1 doesn't really support float32 very much</p>

<p>see this <a href=""http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#dtype-specification"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#dtype-specification</a></p>

<p>you can do this in 0.11 like this:</p>

<pre><code># dont' use dtype converters explicity for the columns you care about
# they will be converted to float64 if possible, or object if they cannot
df = pd.read_csv('test.csv'.....)

#### this is optional and related to the issue you posted ####
# force anything that is not a numeric to nan
# columns are the list of columns that you are interesetd in
df[columns] = df[columns].convert_objects(convert_numeric=True)


    # astype
    df[columns] = df[columns].astype('float32')

see http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion

Its not as efficient as doing it directly in read_csv (but that requires
</code></pre>

<p>I have confirmed that with 0.11-dev, this DOES work (on 32-bit and 64-bit, results are the same)</p>

<pre><code>In [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)

In [6]: x
Out[6]: 
         a        b
0  0.76398  0.81394
1  0.32136  0.91063

In [7]: x.dtypes
Out[7]: 
a    float32
b    float64
dtype: object

In [8]: pd.__version__
Out[8]: '0.11.0.dev-385ff82'

In [9]: quit()
vagrant@precise32:~/pandas$ uname -a
Linux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux

 some low-level changes)
</code></pre>
";;5;;2013-03-04T23:33:05.293;;15213171;2013-03-09T17:30:32.973;2013-03-09T17:30:32.973;;644898.0;;644898.0;15210962.0;2;18;;;
2719;2719;;;"<pre><code>In [22]: df.a.dtype = pd.np.float32

In [23]: df.a.dtype
Out[23]: dtype('float32')
</code></pre>

<p>the above works fine for me under pandas 0.10.1</p>
";;5;;2013-03-05T09:38:10.550;;15220374;2013-03-05T09:38:10.550;;;;;1987630.0;15210962.0;2;6;;;
2721;2721;;;"<p>For <code>agg</code>, the lambba function gets a <code>Series</code>, which does not have a <code>'Short name'</code> attribute.</p>

<p><code>stats.mode</code> returns a tuple of two arrays, so you have to take the first element of the first array in this tuple.</p>

<p>With these two simple changements:</p>

<pre><code>source.groupby(['Country','City']).agg(lambda x: stats.mode(x)[0][0])
</code></pre>

<p>returns</p>

<pre><code>                         Short name
Country City                       
Russia  Sankt-Petersburg        Spb
USA     New-York                 NY
</code></pre>
";;3;;2013-03-05T11:45:26.543;;15222976;2013-03-05T11:45:26.543;;;;;449449.0;15222754.0;2;7;;;
2722;2722;;;"<p>You can use <code>value_counts()</code> to get a count series, and get the first row:</p>

<pre><code>import pandas as pd

source = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], 
                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],
                  'Short name' : ['NY','New','Spb','NY']})

source.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])
</code></pre>
";;3;;2013-03-05T11:48:38.883;;15223034;2013-03-05T11:48:38.883;;;;;772649.0;15222754.0;2;52;;;
2737;2737;;;"<p>I'd also be interested to know if this is possible, from the doc it doesn't seem to be the case. What you could probably do is read the file line by line, and concatenate each reading to a DataFrame:</p>

<pre><code>import pandas as pd

df = pd.DataFrame()

with open(filepath, 'r') as f:
    for line in f:
        df = pd.concat( [df, pd.DataFrame([tuple(line.strip().split(','))])], ignore_index=True )
</code></pre>

<p>It works but not in the most elegant way, I guess...</p>
";;0;;2013-03-06T09:58:12.473;;15244074;2013-03-06T09:58:12.473;;;;;567989.0;15242746.0;2;7;;;
2742;2742;;;"<p>If you want to find the rows with duplicated name (except the first time we see that), you can try this</p>

<pre><code>In [16]: import pandas as pd
In [17]: p1 = {'name': 'willy', 'age': 10}
In [18]: p2 = {'name': 'willy', 'age': 11}
In [19]: p3 = {'name': 'zoe', 'age': 10}
In [20]: df = pd.DataFrame([p1, p2, p3])

In [21]: df
Out[21]: 
   age   name
0   10  willy
1   11  willy
2   10    zoe

In [22]: df.duplicated('name')
Out[22]: 
0    False
1     True
2    False
</code></pre>
";;2;;2013-03-06T13:10:39.193;;15248239;2013-03-06T13:10:39.193;;;;;1426056.0;15247628.0;2;33;;;
2747;2747;;;"<p>One way which seems to work (at least in 0.10.1 and 0.11.0.dev-fc8de6d):</p>

<pre><code>&gt;&gt;&gt; !cat ragged.csv
1,2,3
1,2,3,4
1,2,3,4,5
1,2
1,2,3,4
&gt;&gt;&gt; my_cols = [""A"", ""B"", ""C"", ""D"", ""E""]
&gt;&gt;&gt; pd.read_csv(""ragged.csv"", names=my_cols, engine='python')
   A  B   C   D   E
0  1  2   3 NaN NaN
1  1  2   3   4 NaN
2  1  2   3   4   5
3  1  2 NaN NaN NaN
4  1  2   3   4 NaN
</code></pre>

<p>Note that this approach requires that you give names to the columns you want, though.  Not as general as some other ways, but works well enough when it applies.</p>
";;4;;2013-03-06T15:55:05.310;;15252012;2013-03-06T15:55:05.310;;;;;487339.0;15242746.0;2;42;;;
2759;2759;;;"<p>To complement unutbu's answer, here's an approach using <code>apply</code> on the groupby object.</p>

<pre><code>&gt;&gt;&gt; df.groupby('A_id').apply(lambda x: pd.Series(dict(
    sum_up=(x.B == 'up').sum(),
    sum_down=(x.B == 'down').sum(),
    over_200_up=((x.B == 'up') &amp; (x.C &gt; 200)).sum()
)))
      over_200_up  sum_down  sum_up
A_id                               
a1              0         0       1
a2              0         1       0
a3              1         0       2
a4              0         0       0
a5              0         0       0
</code></pre>
";;0;;2013-03-07T02:45:52.397;;15262146;2013-03-07T02:45:52.397;;;;;243434.0;15259547.0;2;12;;;
2787;2787;;;"<p>First make a grouper:</p>

<pre><code>import pandas as pd

In [1]: grouper = pd.TimeGrouper(""1M"")
</code></pre>

<p>Then make your new column:</p>

<pre><code>In [2]: df['normed'] = df.groupby(grouper).transform(lambda x: x/x.mean())
</code></pre>

<p>By passing grouper to the <code>groupby</code> method you group your data into one month chunks.  Within each chunk you divide the 15 minute interval datum by the mean for that month.</p>
";;0;;2013-03-08T18:35:03.990;;15300930;2013-03-08T18:35:03.990;;;;;919872.0;15297053.0;2;21;;;
2792;2792;;;"<p>Sure!  Setup:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from random import randint
&gt;&gt;&gt; df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                   'B': [randint(1, 9)*10 for x in xrange(10)],
                   'C': [randint(1, 9)*100 for x in xrange(10)]})
&gt;&gt;&gt; df
   A   B    C
0  9  40  300
1  9  70  700
2  5  70  900
3  8  80  900
4  7  50  200
5  9  30  900
6  2  80  700
7  2  80  400
8  5  80  300
9  7  70  800
</code></pre>

<p>We can apply column operations and get boolean Series objects:</p>

<pre><code>&gt;&gt;&gt; df[""B""] &gt; 50
0    False
1     True
2     True
3     True
4    False
5    False
6     True
7     True
8     True
9     True
Name: B
&gt;&gt;&gt; (df[""B""] &gt; 50) &amp; (df[""C""] == 900)
0    False
1    False
2     True
3     True
4    False
5    False
6    False
7    False
8    False
9    False
</code></pre>

<p>[Update, to switch to new-style <code>.loc</code>]:</p>

<p>And then we can use these to index into the object.  For read access, you can chain indices:</p>

<pre><code>&gt;&gt;&gt; df[""A""][(df[""B""] &gt; 50) &amp; (df[""C""] == 900)]
2    5
3    8
Name: A, dtype: int64
</code></pre>

<p>but you can get yourself into trouble because of the difference between a view and a copy doing this for write access.  You can use <code>.loc</code> instead:</p>

<pre><code>&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""]
2    5
3    8
Name: A, dtype: int64
&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""].values
array([5, 8], dtype=int64)
&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""] *= 1000
&gt;&gt;&gt; df
      A   B    C
0     9  40  300
1     9  70  700
2  5000  70  900
3  8000  80  900
4     7  50  200
5     9  30  900
6     2  80  700
7     2  80  400
8     5  80  300
9     7  70  800
</code></pre>

<p>Note that I accidentally did <code>== 900</code> and not <code>!= 900</code>, or <code>~(df[""C""] == 900)</code>, but I'm too lazy to fix it. Exercise for the reader. :^)</p>
";;5;;2013-03-09T20:24:23.220;;15315507;2013-11-11T15:54:10.077;2013-11-11T15:54:10.077;;487339.0;;487339.0;15315452.0;2;156;;;
2794;2794;;;"<p><code>agg</code> is the same as <code>aggregate</code>. It's callable is passed the columns (<code>Series</code> objects) of the <code>DataFrame</code>, one at a time.</p>

<hr>

<p>You could use <code>idxmax</code> to collect the index labels of the rows with the maximum
count:</p>

<pre><code>idx = df.groupby('word')['count'].idxmax()
print(idx)
</code></pre>

<p>yields</p>

<pre><code>word
a       2
an      3
the     1
Name: count
</code></pre>

<p>and then use <code>loc</code> to select those rows in the <code>word</code> and <code>tag</code> columns:</p>

<pre><code>print(df.loc[idx, ['word', 'tag']])
</code></pre>

<p>yields</p>

<pre><code>  word tag
2    a   T
3   an   T
1  the   S
</code></pre>

<p>Note that <code>idxmax</code> returns index <em>labels</em>. <code>df.loc</code> can be used to select rows
by label. But if the index is not unique -- that is, if there are rows with duplicate index labels -- then <code>df.loc</code> will select <em>all rows</em> with the labels listed in <code>idx</code>. So be careful that <code>df.index.is_unique</code> is <code>True</code> if you use <code>idxmax</code> with <code>df.loc</code></p>

<hr>

<p>Alternative, you could use <code>apply</code>. <code>apply</code>'s callable is passed a sub-DataFrame which gives you access to all the columns:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'word':'a the a an the'.split(),
                   'tag': list('SSTTT'),
                   'count': [30, 20, 60, 5, 10]})

print(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))
</code></pre>

<p>yields</p>

<pre><code>word
a       T
an      T
the     S
</code></pre>

<hr>

<p>Using <code>idxmax</code> and <code>loc</code> is typically faster than <code>apply</code>, especially for large DataFrames. Using IPython's %timeit:</p>

<pre><code>N = 10000
df = pd.DataFrame({'word':'a the a an the'.split()*N,
                   'tag': list('SSTTT')*N,
                   'count': [30, 20, 60, 5, 10]*N})
def using_apply(df):
    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))

def using_idxmax_loc(df):
    idx = df.groupby('word')['count'].idxmax()
    return df.loc[idx, ['word', 'tag']]

In [22]: %timeit using_apply(df)
100 loops, best of 3: 7.68 ms per loop

In [23]: %timeit using_idxmax_loc(df)
100 loops, best of 3: 5.43 ms per loop
</code></pre>

<hr>

<p>If you want a dictionary mapping words to tags, then you could use <code>set_index</code>
and <code>to_dict</code> like this:</p>

<pre><code>In [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')

In [37]: df2
Out[37]: 
     tag
word    
a      T
an     T
the    S

In [38]: df2.to_dict()['tag']
Out[38]: {'a': 'T', 'an': 'T', 'the': 'S'}
</code></pre>
";;2;;2013-03-10T13:24:45.360;;15322715;2015-08-24T12:39:17.967;2015-08-24T12:39:17.967;;190597.0;;190597.0;15322632.0;2;63;;;
2795;2795;;;"<p>Here's a simple way to figure out what is being passed (the unutbu) solution then 'applies'!</p>

<pre><code>In [33]: def f(x):
....:     print type(x)
....:     print x
....:     

In [34]: df.groupby('word').apply(f)
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
0    a   S     30
2    a   T     60
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
0    a   S     30
2    a   T     60
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
3   an   T      5
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
1  the   S     20
4  the   T     10
</code></pre>

<p>your function just operates (in this case) on a sub-section of the frame with the grouped variable all having the same value (in this cas 'word'), if you are passing a function, then you have to deal with the aggregation of potentially non-string columns; standard functions, like 'sum' do this for you</p>

<p>Automatically does NOT aggregate on the string columns</p>

<pre><code>In [41]: df.groupby('word').sum()
Out[41]: 
      count
word       
a        90
an        5
the      30
</code></pre>

<p>You ARE aggregating on all columns</p>

<pre><code>In [42]: df.groupby('word').apply(lambda x: x.sum())
Out[42]: 
        word tag count
word                  
a         aa  ST    90
an        an   T     5
the   thethe  ST    30
</code></pre>

<p>You can do pretty much anything within the function</p>

<pre><code>In [43]: df.groupby('word').apply(lambda x: x['count'].sum())
Out[43]: 
word
a       90
an       5
the     30
</code></pre>
";;1;;2013-03-10T13:47:18.227;;15322920;2013-03-10T13:47:18.227;;;;;644898.0;15322632.0;2;17;;;
2802;2802;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"">contains</a> instead:</p>

<pre><code>In [10]: df.b.str.contains('^f')
Out[10]: 
0    False
1     True
2     True
3    False
Name: b, dtype: bool
</code></pre>
";;1;;2013-03-11T07:27:34.647;;15333283;2013-03-11T07:27:34.647;;;;;1426056.0;15325182.0;2;72;;;
2813;2813;;;"<p>You can get the first column as a Series by following code:</p>

<pre><code>x[x.columns[0]]
</code></pre>
";;4;;2013-03-12T12:42:57.013;;15361537;2013-03-12T12:42:57.013;;;;;772649.0;15360925.0;2;50;;;
2815;2815;;;"<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})
&gt;&gt;&gt; df
   x  y
0  1  4
1  2  5
2  3  6
3  4  7
&gt;&gt;&gt; s = df.ix[:,0]
&gt;&gt;&gt; type(s)
&lt;class 'pandas.core.series.Series'&gt;
&gt;&gt;&gt;
</code></pre>
";;5;;2013-03-12T13:33:39.193;;15362700;2013-03-12T13:33:39.193;;;;;567989.0;15360925.0;2;66;;;
2819;2819;;;"<pre><code>in 0.11

In [7]: df.iloc[:,0]
Out[7]: 
0    1
1    2
2    3
3    4
Name: x, dtype: int64
</code></pre>
";;0;;2013-03-12T14:49:17.780;;15364468;2013-03-12T14:49:17.780;;;;;644898.0;15360925.0;2;24;;;
2830;2830;;;"<p><code>apply</code> takes a function to apply to <em>each</em> value, not the series, and accepts kwargs.
So, the values do not have the <code>.size()</code> method.</p>

<p>Perhaps this would work:</p>

<pre><code>from pandas import *

d = {""my_label"": Series(['A','B','A','C','D','D','E'])}
df = DataFrame(d)


def as_perc(value, total):
    return value/float(total)

def get_count(values):
    return len(values)

grouped_count = df.groupby(""my_label"").my_label.agg(get_count)
data = grouped_count.apply(as_perc, total=df.my_label.count())
</code></pre>

<p>The <code>.agg()</code> method here takes a function that is applied to <em>all</em> values of the <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#groupby-object-attributes"">groupby object</a>.</p>
";;0;;2013-03-13T01:00:05.190;;15375176;2013-03-13T08:20:58.283;2013-03-13T08:20:58.283;;24718.0;;24718.0;15374597.0;2;20;;;
2857;2857;;;"<p>I believe this is what you want:</p>

<pre><code>table.groupby('YEARMONTH').CLIENTCODE.nunique()
</code></pre>

<p>Example:</p>

<pre><code>In [2]: table
Out[2]: 
   CLIENTCODE  YEARMONTH
0           1     201301
1           1     201301
2           2     201301
3           1     201302
4           2     201302
5           2     201302
6           3     201302

In [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()
Out[3]: 
YEARMONTH
201301       2
201302       3
</code></pre>
";;5;;2013-03-14T14:09:06.103;;15411596;2015-06-10T17:24:37.327;2015-06-10T17:24:37.327;;2071807.0;;1221924.0;15411158.0;2;147;;;
2873;2873;;;"<p>I'm not the best at date manipulations, but maybe something like this:</p>

<pre><code>import pandas as pd
from datetime import timedelta

df = pd.read_csv(""hourmelt.csv"", sep=r""\s+"")

df = pd.melt(df, id_vars=[""Date""])
df = df.rename(columns={'variable': 'hour'})
df['hour'] = df['hour'].apply(lambda x: int(x.lstrip('h'))-1)

combined = df.apply(lambda x: 
                    pd.to_datetime(x['Date'], dayfirst=True) + 
                    timedelta(hours=int(x['hour'])), axis=1)

df['Date'] = combined
del df['hour']

df = df.sort(""Date"")
</code></pre>

<hr>

<p>Some explanation follows. </p>

<p>Starting from</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from datetime import datetime, timedelta
&gt;&gt;&gt; 
&gt;&gt;&gt; df = pd.read_csv(""hourmelt.csv"", sep=r""\s+"")
&gt;&gt;&gt; df
         Date  h1  h2  h3  h4  h24
0  14.03.2013  60  50  52  49   73
1  14.04.2013   5   6   7   8    9
</code></pre>

<p>We can use <code>pd.melt</code> to make the hour columns into one column with that value:</p>

<pre><code>&gt;&gt;&gt; df = pd.melt(df, id_vars=[""Date""])
&gt;&gt;&gt; df = df.rename(columns={'variable': 'hour'})
&gt;&gt;&gt; df
         Date hour  value
0  14.03.2013   h1     60
1  14.04.2013   h1      5
2  14.03.2013   h2     50
3  14.04.2013   h2      6
4  14.03.2013   h3     52
5  14.04.2013   h3      7
6  14.03.2013   h4     49
7  14.04.2013   h4      8
8  14.03.2013  h24     73
9  14.04.2013  h24      9
</code></pre>

<p>Get rid of those <code>h</code>s:</p>

<pre><code>&gt;&gt;&gt; df['hour'] = df['hour'].apply(lambda x: int(x.lstrip('h'))-1)
&gt;&gt;&gt; df
         Date  hour  value
0  14.03.2013     0     60
1  14.04.2013     0      5
2  14.03.2013     1     50
3  14.04.2013     1      6
4  14.03.2013     2     52
5  14.04.2013     2      7
6  14.03.2013     3     49
7  14.04.2013     3      8
8  14.03.2013    23     73
9  14.04.2013    23      9
</code></pre>

<p>Combine the two columns as a date:</p>

<pre><code>&gt;&gt;&gt; combined = df.apply(lambda x: pd.to_datetime(x['Date'], dayfirst=True) + timedelta(hours=int(x['hour'])), axis=1)
&gt;&gt;&gt; combined
0    2013-03-14 00:00:00
1    2013-04-14 00:00:00
2    2013-03-14 01:00:00
3    2013-04-14 01:00:00
4    2013-03-14 02:00:00
5    2013-04-14 02:00:00
6    2013-03-14 03:00:00
7    2013-04-14 03:00:00
8    2013-03-14 23:00:00
9    2013-04-14 23:00:00
</code></pre>

<p>Reassemble and clean up:</p>

<pre><code>&gt;&gt;&gt; df['Date'] = combined
&gt;&gt;&gt; del df['hour']
&gt;&gt;&gt; df = df.sort(""Date"")
&gt;&gt;&gt; df
                 Date  value
0 2013-03-14 00:00:00     60
2 2013-03-14 01:00:00     50
4 2013-03-14 02:00:00     52
6 2013-03-14 03:00:00     49
8 2013-03-14 23:00:00     73
1 2013-04-14 00:00:00      5
3 2013-04-14 01:00:00      6
5 2013-04-14 02:00:00      7
7 2013-04-14 03:00:00      8
9 2013-04-14 23:00:00      9
</code></pre>
";;2;;2013-03-15T13:13:10.457;;15433426;2013-03-15T13:13:10.457;;;;;487339.0;15432659.0;2;15;;;
2884;2884;;;"<pre><code>df = pd.Panel.from_dict(d).to_frame()
</code></pre>

<p>output:</p>

<pre><code>                   col1        col2
major minor                        
data1 row1         0.87        0.87
      row2      15352.3     15352.3
      row3            0           0
data2 row1   Title col1  Title col2
      row2   Title col1  Title col2
      row3   Title col1  Title col2
data3 row1      14.4878     24.4878
      row2      14.9561     24.9561
      row3      16.8293     26.8293
data4 row1   Title row1  Title row1
      row2   Title row2  Title row2
      row3   Title row3  Title row3
</code></pre>

<p>If you don't want use Panel:</p>

<pre><code>pd.concat(map(pd.DataFrame, d.itervalues()), keys=d.keys()).stack().unstack(0)
</code></pre>
";;6;;2013-03-16T22:28:42.773;;15455455;2013-03-16T22:49:45.313;2013-03-16T22:49:45.313;;772649.0;;772649.0;15455388.0;2;15;;;
2904;2904;;;"<p>I think @herrfz hit all the high points. I'll just flesh out the details:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

sin = np.sin
cos = np.cos
pi = np.pi
N = 100

x = np.linspace(0, pi, N)
a = sin(x)
b = cos(x)

df = pd.DataFrame({
    'A': [True]*N + [False]*N,
    'B': np.hstack((a,b))
    })

for key, grp in df.groupby(['A']):
    plt.plot(grp['B'], label=key)
    grp['D'] = pd.rolling_mean(grp['B'], window=5)    
    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))
plt.legend(loc='best')    
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/HeH3b.png"" alt=""enter image description here""></p>
";;3;;2013-03-17T20:52:57.310;;15466103;2013-03-17T20:52:57.310;;;;;190597.0;15465645.0;2;38;;;
2905;2905;;;"<p>I couldn't find a more direct way other than using <code>select</code>:</p>

<pre><code>&gt;&gt;&gt; df

       last   tod
A SPY     1  1600
  SLV     2  1600
  GLD     3  1600

&gt;&gt;&gt; df.select(lambda x: x[1] in ['SPY','GLD'])

       last   tod
A SPY     1  1600
  GLD     3  1600
</code></pre>
";;3;;2013-03-17T22:47:35.157;;15467189;2013-03-17T22:47:35.157;;;;;919872.0;15463729.0;2;7;;;
2975;2975;;;"<p>One way is to use <code>reset_index</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(range(5))
&gt;&gt;&gt; eq2 = df[0] == 2
&gt;&gt;&gt; df_no_2 = df[~eq2]
&gt;&gt;&gt; df_no_2
   0
0  0
1  1
3  3
4  4
&gt;&gt;&gt; df_no_2.reset_index(drop=True)
   0
0  0
1  1
2  3
3  4
</code></pre>
";;4;;2013-03-21T20:41:07.893;;15557663;2013-03-21T20:41:07.893;;;;;487339.0;15557542.0;2;35;;;
2978;2978;;;"<p>I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.</p>

<p>My basic workflow is to first get a CSV file from the database. I gzip it, so it's not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn't use any memory, since it's only operating row-by-row. Then I ""transpose"" the row-oriented HDF5 file into a column-oriented HDF5 file.</p>

<p>The table transpose looks like:</p>

<pre><code>def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):
    # Get a reference to the input data.
    tb = h_in.getNode(table_path)
    # Create the output group to hold the columns.
    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))
    for col_name in tb.colnames:
        logger.debug(""Processing %s"", col_name)
        # Get the data.
        col_data = tb.col(col_name)
        # Create the output array.
        arr = h_out.createCArray(grp,
                                 col_name,
                                 tables.Atom.from_dtype(col_data.dtype),
                                 col_data.shape)
        # Store the data.
        arr[:] = col_data
    h_out.flush()
</code></pre>

<p>Reading it back in then looks like:</p>

<pre><code>def read_hdf5(hdf5_path, group_path=""/data"", columns=None):
    """"""Read a transposed data set from a HDF5 file.""""""
    if isinstance(hdf5_path, tables.file.File):
        hf = hdf5_path
    else:
        hf = tables.openFile(hdf5_path)

    grp = hf.getNode(group_path)
    if columns is None:
        data = [(child.name, child[:]) for child in grp]
    else:
        data = [(child.name, child[:]) for child in grp if child.name in columns]

    # Convert any float32 columns to float64 for processing.
    for i in range(len(data)):
        name, vec = data[i]
        if vec.dtype == np.float32:
            data[i] = (name, vec.astype(np.float64))

    if not isinstance(hdf5_path, tables.file.File):
        hf.close()
    return pd.DataFrame.from_items(data)
</code></pre>

<p>Now, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.</p>

<p>This generally works for me, but it's a bit clunky, and I can't use the fancy pytables magic.</p>

<p>Edit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can't handle tables. Or, at least, I've been unable to get it to load heterogeneous tables.</p>
";;4;;2013-03-21T21:19:30.510;;15558350;2013-03-22T15:38:07.163;2013-03-22T15:38:07.163;;250839.0;;250839.0;14262433.0;2;34;;;
2984;2984;;;"<p>You can get the summarized values by using groupby() on the State column.</p>

<p>Lets make some sample data first:</p>

<pre><code>import pandas as pd
import StringIO

incsv = StringIO.StringIO(""""""Date,State,City,SalesToday,SalesMTD,SalesYTD
20130320,stA,ctA,20,400,1000
20130320,stA,ctB,30,500,1100
20130320,stB,ctC,10,500,900
20130320,stB,ctD,40,200,1300
20130320,stC,ctF,30,300,800"""""")

df = pd.read_csv(incsv, index_col=['Date'], parse_dates=True)
</code></pre>

<p>Then apply the groupby function and add a column City:</p>

<pre><code>dfsum = df.groupby('State', as_index=False).sum()
dfsum['City'] = 'All'

print dfsum

  State  SalesToday  SalesMTD  SalesYTD City
0   stA          50       900      2100  All
1   stB          50       700      2200  All
2   stC          30       300       800  All
</code></pre>

<p>We can append the original data to the summed df by using append:</p>

<pre><code>dfsum.append(df).set_index(['State','City']).sort_index()

print dfsum

            SalesMTD  SalesToday  SalesYTD
State City                                
stA   All        900          50      2100
      ctA        400          20      1000
      ctB        500          30      1100
stB   All        700          50      2200
      ctC        500          10       900
      ctD        200          40      1300
stC   All        300          30       800
      ctF        300          30       800
</code></pre>

<p>I added the set_index and sort_index to make it look more like your example output, its not strictly necessary to get the results.</p>
";;0;;2013-03-22T12:38:30.780;;15570546;2013-03-22T12:38:30.780;;;;;1755432.0;15570099.0;2;14;;;
2985;2985;;;"<p>If you put State and City not both in the rows, you'll get separate margins. Reshape and you get the table you're after:</p>

<pre><code>In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\
                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)


In [11]: table.stack('City')
Out[11]: 
            SalesMTD  SalesToday  SalesYTD
State City                                
stA   All        900          50      2100
      ctA        400          20      1000
      ctB        500          30      1100
stB   All        700          50      2200
      ctC        500          10       900
      ctD        200          40      1300
stC   All        300          30       800
      ctF        300          30       800
All   All       1900         130      5100
      ctA        400          20      1000
      ctB        500          30      1100
      ctC        500          10       900
      ctD        200          40      1300
      ctF        300          30       800
</code></pre>

<p>I admit this isn't totally obvious. </p>
";;1;;2013-03-22T16:07:31.223;;15574875;2013-03-22T16:07:31.223;;;;;776560.0;15570099.0;2;33;;;
2997;2997;;;"<p>This:</p>

<pre><code>d1.groupby('ExamenYear').agg({'Participated': len, 
                              'Passed': lambda x: sum(x == 'yes')})
</code></pre>

<p>doesn't look way more awkward than the R solution, IMHO.</p>
";;1;;2013-03-23T17:41:51.737;;15590006;2013-03-23T17:41:51.737;;;;;567989.0;15589354.0;2;7;;;
3011;3011;;;"<p>I finally decided to use <strong>apply</strong>.</p>

<p>I am posting what I came up with hoping that it can be useful for others. </p>

<p>From what I understand from Wes' book ""Python for Data analysis"" </p>

<ul>
<li><strong>apply</strong> is more flexible than agg and transform because you can define your own function. </li>
<li>the only requirement is that the functions returns a <em>pandas object</em> or a <em>scalar value</em>.</li>
<li>the inner mechanics: the function is called on each piece of the grouped object abd results are glued together using <strong>pandas.concat</strong></li>
<li>One needs to ""hard-code"" structure you want at the end</li>
</ul>

<p>Here is what I came up with </p>

<pre><code>def ZahlOccurence_0(x):
      return pd.Series({'All': len(x['StudentID']),
                       'Part': sum(x['Participated'] == 'yes'),
                       'Pass' :  sum(x['Passed'] == 'yes')})
</code></pre>

<p>when I run it :     </p>

<pre><code> d1.groupby('ExamenYear').apply(ZahlOccurence_0)
</code></pre>

<p>I get the correct results </p>

<pre><code>            All  Part  Pass
ExamenYear                 
2007          3     2     2
2008          4     3     3
2009          3     3     2
</code></pre>

<p>This approach would also allow me to combine frequencies with other stats</p>

<pre><code>import numpy as np
d1['testValue'] = np.random.randn(len(d1))

def ZahlOccurence_1(x):
    return pd.Series({'All': len(x['StudentID']),
        'Part': sum(x['Participated'] == 'yes'),
        'Pass' :  sum(x['Passed'] == 'yes'),
        'test' : x['testValue'].mean()})


d1.groupby('ExamenYear').apply(ZahlOccurence_1)


            All  Part  Pass      test
ExamenYear                           
2007          3     2     2  0.358702
2008          4     3     3  1.004504
2009          3     3     2  0.521511
</code></pre>

<p>I hope someone else will find this useful </p>
";;0;;2013-03-25T09:38:12.973;;15611666;2013-03-25T09:38:12.973;;;;;1043144.0;15589354.0;2;11;;;
3091;3091;;;"<pre><code>In [1]: df
Out[1]:
    Sp  Mt Value  count
0  MM1  S1     a      3
1  MM1  S1     n      2
2  MM1  S3    cb      5
3  MM2  S3    mk      8
4  MM2  S4    bg     10
5  MM2  S4   dgd      1
6  MM4  S2    rd      2
7  MM4  S2    cb      2
8  MM4  S2   uyi      7

In [2]: df.groupby(['Mt'], sort=False)['count'].max()
Out[2]:
Mt
S1     3
S3     8
S4    10
S2     7
Name: count
</code></pre>

<p>To get the indices of the original DF you can do:</p>

<pre><code>In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']

In [4]: df[idx]
Out[4]:
    Sp  Mt Value  count
0  MM1  S1     a      3
3  MM2  S3    mk      8
4  MM2  S4    bg     10
8  MM4  S2   uyi      7
</code></pre>

<p>Note that if you have multiple max values per group, all will be returned.</p>

<p><strong>Update</strong></p>

<p>On a hail mary chance that this is what the OP is requesting:</p>

<pre><code>In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)

In [6]: df
Out[6]:
    Sp  Mt Value  count  count_max
0  MM1  S1     a      3          3
1  MM1  S1     n      2          3
2  MM1  S3    cb      5          8
3  MM2  S3    mk      8          8
4  MM2  S4    bg     10         10
5  MM2  S4   dgd      1         10
6  MM4  S2    rd      2          7
7  MM4  S2    cb      2          7
8  MM4  S2   uyi      7          7
</code></pre>
";;2;;2013-03-29T15:09:40.397;;15705958;2013-03-29T15:42:14.357;2013-03-29T15:42:14.357;;919872.0;;919872.0;15705630.0;2;78;;;
3107;3107;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""noreferrer""><code>astype</code></a> method to cast a Series (one column):</p>

<pre><code>df['col_name'] = df['col_name'].astype(object)
</code></pre>

<p>Or the entire DataFrame:</p>

<pre><code>df = df.astype(object)
</code></pre>

<hr>

<h3>Update</h3>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"" rel=""noreferrer"">Since version 0.15, you can use the category datatype</a> in a Series/column:</p>

<pre><code>df['col_name'] = df['col_name'].astype('category')
</code></pre>

<p><em>Note: <code>pd.Factor</code> was been deprecated and has been removed in favor of <code>pd.Categorical</code>.</em></p>
";;4;;2013-03-30T21:54:21.303;;15723905;2015-04-30T23:33:54.007;2015-04-30T23:33:54.007;;1240268.0;;1240268.0;15723628.0;2;43;;;
3108;3108;;;"<p><code>Factor</code> and <code>Categorical</code> are the same, as far as I know. I think it was initially called Factor, and then changed to Categorical. To convert to Categorical maybe you can use <code>pandas.Categorical.from_array</code>, something like this:</p>

<pre><code>In [27]: df = pd.DataFrame({'a' : [1, 2, 3, 4, 5], 'b' : ['yes', 'no', 'yes', 'no', 'absent']})

In [28]: df
Out[28]: 
   a       b
0  1     yes
1  2      no
2  3     yes
3  4      no
4  5  absent

In [29]: df['c'] = pd.Categorical.from_array(df.b).labels

In [30]: df
Out[30]: 
   a       b  c
0  1     yes  2
1  2      no  1
2  3     yes  2
3  4      no  1
4  5  absent  0
</code></pre>
";;0;;2013-03-30T22:04:14.190;;15723994;2013-03-30T22:04:14.190;;;;;567989.0;15723628.0;2;12;;;
3119;3119;;;"<p>Assuming <code>df</code> has a unique index, this gives the row with the maximum value:</p>

<pre><code>In [34]: df.loc[df['Value'].idxmax()]
Out[34]: 
Country        US
Place      Kansas
Value         894
Name: 7
</code></pre>

<p>Note that <code>idxmax</code> returns index <em>labels</em>. So if the DataFrame as duplicates in the index, the label may not uniquely identify the row, so <code>df.loc</code> may return more than one row.</p>

<p>Therefore, if <code>df</code> does not have a unique index, you must make the index unique before proceeding as above. Depending on the DataFrame, sometimes you can use <code>stack</code> or <code>set_index</code> to make the index unique. Or, you can simply reset the index (so the rows become renumbered, starting at 0):</p>

<pre><code>df = df.reset_index()
</code></pre>
";;1;;2013-04-01T10:58:15.587;;15742147;2017-07-07T01:55:56.137;2017-07-07T01:55:56.137;;190597.0;;190597.0;15741759.0;2;36;;;
3127;3127;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.set_index.html"" rel=""noreferrer""><code>set_index</code></a>:</p>

<pre><code>df.set_index('month')
</code></pre>

<p>For example:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, datetime(2011,1,1)], [2, datetime(2011,1,2)]], columns=['a', 'b'])

In [2]: df
Out[2]: 
   a                   b
0  1 2011-01-01 00:00:00
1  2 2011-01-02 00:00:00

In [3]: df.set_index('b')
Out[3]: 
            a
b            
2011-01-01  1
2011-01-02  2
</code></pre>
";;5;;2013-04-01T21:54:37.990;;15752582;2013-04-01T21:54:37.990;;;;;1240268.0;15752422.0;2;18;;;
3132;3132;;;"<p>Try this:</p>

<pre><code>data2 = data1.reset_index()
data3 = data2.set_index([""Bool"", ""Dir"", ""index""])   # index is the new column created by reset_index
running_sum = data3.groupby(level=[0,1,2]).sum().groupby(level=[0,1]).cumsum()
</code></pre>

<p>The reason you cannot simply use <code>cumsum</code> on <code>data3</code> has to do with how your data is structured.  Grouping by <code>Bool</code> and <code>Dir</code>  and applying an aggregation function (<code>sum</code>, <code>mean</code>, etc) would produce a DataFrame of a smaller size than you started with, as whatever function you used would aggregate values based on your group keys.  However <code>cumsum</code> is not an aggreagation function.  It wil return a DataFrame that is the same size as the one it's called with.  So unless your input DataFrame is in a format where the output can be the same size after calling <code>cumsum</code>, it will throw an error.  That's why I called <code>sum</code> first, which returns a DataFrame in the correct input format.</p>

<p>Sorry if I haven't explained this well enough.  Maybe someone else could help me out?</p>
";;5;;2013-04-02T04:30:29.847;;15756128;2013-04-02T13:38:08.807;2013-04-02T13:38:08.807;;1649780.0;;1649780.0;15755057.0;2;12;;;
3143;3143;;;"<p>What about something like this:</p>

<p>First resample the data frame into 1D intervals.  This takes the mean of the values for all duplicate days.  Use the <code>fill_method</code> option to fill in missing date values.  Next, pass the resampled frame into <code>pd.rolling_mean</code> with a window of 3 and min_periods=1 :</p>

<pre><code>pd.rolling_mean(df.resample(""1D"", fill_method=""ffill""), window=3, min_periods=1)

            favorable  unfavorable     other
enddate
2012-10-25   0.495000     0.485000  0.025000
2012-10-26   0.527500     0.442500  0.032500
2012-10-27   0.521667     0.451667  0.028333
2012-10-28   0.515833     0.450000  0.035833
2012-10-29   0.488333     0.476667  0.038333
2012-10-30   0.495000     0.470000  0.038333
2012-10-31   0.512500     0.460000  0.029167
2012-11-01   0.516667     0.456667  0.026667
2012-11-02   0.503333     0.463333  0.033333
2012-11-03   0.490000     0.463333  0.046667
2012-11-04   0.494000     0.456000  0.043333
2012-11-05   0.500667     0.452667  0.036667
2012-11-06   0.507333     0.456000  0.023333
2012-11-07   0.510000     0.443333  0.013333
</code></pre>

<p><strong>UPDATE</strong>: As Ben points out in the comments, <a href=""http://pandas.pydata.org/pandas-docs/stable/computation.html#window-functions"">with pandas 0.18.0 the syntax has changed</a>.  With the new syntax this would be:</p>

<pre class=""lang-py prettyprint-override""><code>df.resample(""1d"").sum().fillna(0).rolling(window=3, min_periods=1).mean()
</code></pre>
";;5;;2013-04-02T19:03:58.173;;15772263;2016-05-10T19:59:38.120;2016-05-10T19:59:38.120;;970766.0;;919872.0;15771472.0;2;32;;;
3144;3144;;;"<p>Use numpy's random.permuation function:</p>

<pre><code>In [1]: df = pd.DataFrame({'A':range(10), 'B':range(10)})

In [2]: df
Out[2]:
   A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4
5  5  5
6  6  6
7  7  7
8  8  8
9  9  9


In [3]: df.reindex(np.random.permutation(df.index))
Out[3]:
   A  B
0  0  0
5  5  5
6  6  6
3  3  3
8  8  8
7  7  7
9  9  9
1  1  1
2  2  2
4  4  4
</code></pre>
";;4;;2013-04-02T19:09:05.050;;15772330;2013-04-02T19:09:05.050;;;;;919872.0;15772009.0;2;162;;;
3145;3145;;;"<pre><code>In [16]: def shuffle(df, n=1, axis=0):     
    ...:     df = df.copy()
    ...:     for _ in range(n):
    ...:         df.apply(np.random.shuffle, axis=axis)
    ...:     return df
    ...:     

In [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})

In [18]: shuffle(df)

In [19]: df
Out[19]: 
   A  B
0  8  5
1  1  7
2  7  3
3  6  2
4  3  4
5  0  1
6  9  0
7  4  6
8  2  8
9  5  9
</code></pre>
";;6;;2013-04-02T19:10:49.460;;15772356;2013-04-02T19:41:27.820;2013-04-02T19:41:27.820;;1199589.0;;1199589.0;15772009.0;2;21;;;
3151;3151;;;"<p>Found this on <a href=""https://github.com/pydata/pandas/issues/2841"" rel=""noreferrer"">github</a>...</p>

<pre><code>import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
</code></pre>
";;2;;2013-04-03T03:19:42.940;;15778297;2017-04-26T03:59:49.850;2017-04-26T03:59:49.850;;809572.0;;1649780.0;15777951.0;2;47;;;
3159;3159;;;"<p>I write a method to show the four corners of the data and monkey-patch to dataframe to do so:</p>

<pre><code>def _sw(df, up_rows=10, down_rows=5, left_cols=4, right_cols=3, return_df=False):
    ''' display df data at four corners
        A,B (up_pt)
        C,D (down_pt)
        parameters : up_rows=10, down_rows=5, left_cols=4, right_cols=3
        usage:
            df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])
            df.sw(5,2,3,2)
            df1 = df.set_index(['A','B'], drop=True, inplace=False)
            df1.sw(5,2,3,2)
    '''
    #pd.set_printoptions(max_columns = 80, max_rows = 40)
    ncol, nrow = len(df.columns), len(df)

    # handle columns
    if ncol &lt;= (left_cols + right_cols) :
        up_pt = df.ix[0:up_rows, :]         # screen width can contain all columns
        down_pt = df.ix[-down_rows:, :]
    else:                                   # screen width can not contain all columns
        pt_a = df.ix[0:up_rows,  0:left_cols]
        pt_b = df.ix[0:up_rows,  -right_cols:]
        pt_c = df[-down_rows:].ix[:,0:left_cols]
        pt_d = df[-down_rows:].ix[:,-right_cols:]

        up_pt   = pt_a.join(pt_b, how='inner')
        down_pt = pt_c.join(pt_d, how='inner')
        up_pt.insert(left_cols, '..', '..')
        down_pt.insert(left_cols, '..', '..')

    overlap_qty = len(up_pt) + len(down_pt) - len(df)
    down_pt = down_pt.drop(down_pt.index[range(overlap_qty)]) # remove overlap rows

    dt_str_list = down_pt.to_string().split('\n') # transfer down_pt to string list

    # Display up part data
    print up_pt

    start_row = (1 if df.index.names[0] is None else 2) # start from 1 if without index

    # Display omit line if screen height is not enought to display all rows
    if overlap_qty &lt; 0:
        print ""."" * len(dt_str_list[start_row])

    # Display down part data row by row
    for line in dt_str_list[start_row:]:
        print line

    # Display foot note
    print ""\n""
    print ""Index :"",df.index.names
    print ""Column:"","","".join(list(df.columns.values))
    print ""row: %d    col: %d""%(len(df), len(df.columns))
    print ""\n""

    return (df if return_df else None)
DataFrame.sw = _sw  #add a method to DataFrame class
</code></pre>

<p>Here is the sample:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])

&gt;&gt;&gt; df.sw()
         A       B       C       D  ..       H       I       J
0  -0.8166  0.0102  0.0215 -0.0307  .. -0.0820  1.2727  0.6395
1   1.0659 -1.0102 -1.3960  0.4700  ..  1.0999  1.1222 -1.2476
2   0.4347  1.5423  0.5710 -0.5439  ..  0.2491 -0.0725  2.0645
3  -1.5952 -1.4959  2.2697 -1.1004  .. -1.9614  0.6488 -0.6190
4  -1.4426 -0.8622  0.0942 -0.1977  .. -0.7802 -1.1774  1.9682
5   1.2526 -0.2694  0.4841 -0.7568  ..  0.2481  0.3608 -0.7342
6   0.2108  2.5181  1.3631  0.4375  .. -0.1266  1.0572  0.3654
7  -1.0617 -0.4743 -1.7399 -1.4123  .. -1.0398 -1.4703 -0.9466
8  -0.5682 -1.3323 -0.6992  1.7737  ..  0.6152  0.9269  2.1854
9   0.2361  0.4873 -1.1278 -0.2251  ..  1.4232  2.1212  2.9180
10  2.0034  0.5454 -2.6337  0.1556  ..  0.0016 -1.6128 -0.8093
..............................................................
15  1.4091  0.3540 -1.3498 -1.0490  ..  0.9328  0.3668  1.3948
16  0.4528 -0.3183  0.4308 -0.1818  ..  0.1295  1.2268  0.1365
17 -0.7093  1.3991  0.9501  2.1227  .. -1.5296  1.1908  0.0318
18  1.7101  0.5962  0.8948  1.5606  .. -0.6862  0.9558 -0.5514
19  1.0329 -1.2308 -0.6896 -0.5112  ..  0.2719  1.1478 -0.1459


Index : [None]
Column: A,B,C,D,E,F,G,H,I,J
row: 20    col: 10


&gt;&gt;&gt; df.sw(4,2,3,4)
        A       B       C  ..       G       H       I       J
0 -0.8166  0.0102  0.0215  ..  0.3671 -0.0820  1.2727  0.6395
1  1.0659 -1.0102 -1.3960  ..  1.0984  1.0999  1.1222 -1.2476
2  0.4347  1.5423  0.5710  ..  1.6675  0.2491 -0.0725  2.0645
3 -1.5952 -1.4959  2.2697  ..  0.4856 -1.9614  0.6488 -0.6190
4 -1.4426 -0.8622  0.0942  .. -0.0947 -0.7802 -1.1774  1.9682
..............................................................
18  1.7101  0.5962  0.8948  .. -0.8592 -0.6862  0.9558 -0.5514
19  1.0329 -1.2308 -0.6896  .. -0.3954  0.2719  1.1478 -0.1459


Index : [None]
Column: A,B,C,D,E,F,G,H,I,J
row: 20    col: 10
</code></pre>
";;1;;2013-04-03T11:49:52.807;;15786557;2013-04-03T11:49:52.807;;;;;1072888.0;15006298.0;2;13;;;
3177;3177;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.14.1/generated/pandas.Grouper.html"" rel=""nofollow""><code>pd.Grouper</code></a>
allows you to specify a ""groupby instruction for a target object"". In
particular, you can use it to group by dates even if <code>df.index</code> is not a <code>DatetimeIndex</code>:</p>

<pre><code>df.groupby(pd.Grouper(freq='2D', level=-1))
</code></pre>

<p>The <code>level=-1</code> tells <code>pd.Grouper</code> to look for the dates in the last level of the MultiIndex.
Moreover, you can use this in conjunction with other level values from the index:</p>

<pre><code>level_values = df.index.get_level_values
result = (df.groupby([level_values(i) for i in [0,1]]
                      +[pd.Grouper(freq='2D', level=-1)]).sum())
</code></pre>

<p>It looks a bit awkward, but <code>using_Grouper</code> turns out to be much faster than my original
suggestion, <code>using_reset_index</code>:</p>

<pre><code>import numpy as np
import pandas as pd
import datetime as DT

def using_Grouper(df):
    level_values = df.index.get_level_values
    return (df.groupby([level_values(i) for i in [0,1]]
                       +[pd.Grouper(freq='2D', level=-1)]).sum())

def using_reset_index(df):
    df = df.reset_index(level=[0, 1])
    return df.groupby(['State','City']).resample('2D').sum()

def using_stack(df):
    # http://stackoverflow.com/a/15813787/190597
    return (df.unstack(level=[0,1])
              .resample('2D').sum()
              .stack(level=[2,1])
              .swaplevel(2,0))

def make_orig():
    values_a = range(16)
    values_b = range(10, 26)
    states = ['Georgia']*8 + ['Alabama']*8
    cities = ['Atlanta']*4 + ['Savanna']*4 + ['Mobile']*4 + ['Montgomery']*4
    dates = pd.DatetimeIndex([DT.date(2012,1,1)+DT.timedelta(days = i) for i in range(4)]*4)
    df = pd.DataFrame(
        {'value_a': values_a, 'value_b': values_b},
        index = [states, cities, dates])
    df.index.names = ['State', 'City', 'Date']
    return df

def make_df(N):
    dates = pd.date_range('2000-1-1', periods=N)
    states = np.arange(50)
    cities = np.arange(10)
    index = pd.MultiIndex.from_product([states, cities, dates], 
                                       names=['State', 'City', 'Date'])
    df = pd.DataFrame(np.random.randint(10, size=(len(index),2)), index=index,
                      columns=['value_a', 'value_b'])
    return df

df = make_orig()
print(using_Grouper(df))
</code></pre>

<p>yields</p>

<pre><code>                               value_a  value_b
State   City       Date                        
Alabama Mobile     2012-01-01       17       37
                   2012-01-03       21       41
        Montgomery 2012-01-01       25       45
                   2012-01-03       29       49
Georgia Atlanta    2012-01-01        1       21
                   2012-01-03        5       25
        Savanna    2012-01-01        9       29
                   2012-01-03       13       33
</code></pre>

<hr>

<p>Here is a benchmark comparing <code>using_Grouper</code>, <code>using_reset_index</code>, <code>using_stack</code> on a 5000-row DataFrame:</p>

<pre><code>In [30]: df = make_df(10)

In [34]: len(df)
Out[34]: 5000

In [32]: %timeit using_Grouper(df)
100 loops, best of 3: 6.03 ms per loop

In [33]: %timeit using_stack(df)
10 loops, best of 3: 22.3 ms per loop

In [31]: %timeit using_reset_index(df)
1 loop, best of 3: 659 ms per loop
</code></pre>
";;10;;2013-04-03T22:28:18.550;;15799355;2016-09-16T09:50:47.777;2016-09-16T09:50:47.777;;190597.0;;190597.0;15799162.0;2;15;;;
3180;3180;;;"<p>Heres a complete example.</p>

<pre><code>import numpy as np
import pandas as pd
import os

fname = 'groupby.h5'

# create a frame
df = pd.DataFrame({'A': ['foo', 'foo', 'foo', 'foo',
                         'bar', 'bar', 'bar', 'bar',
                         'foo', 'foo', 'foo'],
                   'B': ['one', 'one', 'one', 'two',
                         'one', 'one', 'one', 'two',
                         'two', 'two', 'one'],
                   'C': ['dull', 'dull', 'shiny', 'dull',
                         'dull', 'shiny', 'shiny', 'dull',
                         'shiny', 'shiny', 'shiny'],
                   'D': np.random.randn(11),
                   'E': np.random.randn(11),
                   'F': np.random.randn(11)})


# create the store and append, using data_columns where I possibily
# could aggregate
with pd.get_store(fname) as store:
    store.append('df',df,data_columns=['A','B','C'])
    print ""store:\n%s"" % store

    print ""\ndf:\n%s"" % store['df']

    # get the groups
    groups = store.select_column('df','A').unique()
    print ""\ngroups:%s"" % groups

    # iterate over the groups and apply my operations
    l = []
    for g in groups:

        grp = store.select('df',where = [ 'A=%s' % g ])

        # this is a regular frame, aggregate however you would like
        l.append(grp[['D','E','F']].sum())


    print ""\nresult:\n%s"" % pd.concat(l, keys = groups)

os.remove(fname)
</code></pre>

<p>Output</p>

<pre><code>store:
&lt;class 'pandas.io.pytables.HDFStore'&gt;
File path: groupby.h5
/df            frame_table  (typ-&gt;appendable,nrows-&gt;11,ncols-&gt;6,indexers-&gt;[index],dc-&gt;[A,B,C])

df:
      A    B      C         D         E         F
0   foo  one   dull -0.815212 -1.195488 -1.346980
1   foo  one   dull -1.111686 -1.814385 -0.974327
2   foo  one  shiny -1.069152 -1.926265  0.360318
3   foo  two   dull -0.472180  0.698369 -1.007010
4   bar  one   dull  1.329867  0.709621  1.877898
5   bar  one  shiny -0.962906  0.489594 -0.663068
6   bar  one  shiny -0.657922 -0.377705  0.065790
7   bar  two   dull -0.172245  1.694245  1.374189
8   foo  two  shiny -0.780877 -2.334895 -2.747404
9   foo  two  shiny -0.257413  0.577804 -0.159316
10  foo  one  shiny  0.737597  1.979373 -0.236070

groups:Index([bar, foo], dtype=object)

result:
bar  D   -0.463206
     E    2.515754
     F    2.654810
foo  D   -3.768923
     E   -4.015488
     F   -6.110789
dtype: float64
</code></pre>

<p>Some caveats:</p>

<p>1) This methodology makes sense if your group density is relatively low. On the order of hundreds or thousands of groups. If you get more than that there are more efficient (but more complicated methods), and your function which you are applying (in this case <code>sum</code>) become more restrictive. </p>

<p>Essentially you would iterator over the entire store by chunks, grouping as you go, but keeping the groups only semi-collapsed (imagine doing a mean, so you would need to keep a running total plus a running count, then divide at the end). So some operations would be a bit trickier, but could potentially handle MANY groups (and is really fast).</p>

<p>2) the efficiency of this could be improved by saving the coordinates (e.g. the group locations, but this is a bit more complicated)</p>

<p>3) multi-grouping is not possible with this scheme (it IS possible, but requires an approach more like 2) above</p>

<p>4) the columns that you want to group, MUST be a data_column!</p>

<p>5) you can combine any other filter you wish in the select btw (which is a sneeky way of doing multi-grouping btw, you just form 2 unique lists of group and iterator over the product of them, not extremely efficient if you have lots of groups, but can work)</p>

<p>HTH</p>

<p>let me know if this works for you</p>
";;6;;2013-04-04T00:00:19.990;;15800314;2014-07-03T00:46:46.740;2014-07-03T00:46:46.740;;644898.0;;644898.0;15798209.0;2;15;;;
3189;3189;;;"<p>An alternative using stack/unstack</p>

<pre><code>df.unstack(level=[0,1]).resample('2D', how='sum').stack(level=[2,1]).swaplevel(2,0)

                               value_a  value_b
State   City       Date
Georgia Atlanta    2012-01-01        1       21
Alabama Mobile     2012-01-01       17       37
        Montgomery 2012-01-01       25       45
Georgia Savanna    2012-01-01        9       29
        Atlanta    2012-01-03        5       25
Alabama Mobile     2012-01-03       21       41
        Montgomery 2012-01-03       29       49
Georgia Savanna    2012-01-03       13       33
</code></pre>

<p>Notes:</p>

<ol>
<li>No idea about performance comparison</li>
<li>Possible pandas bug - stack(level=[2,1]) worked, but stack(level=[1,2]) failed </li>
</ol>
";;2;;2013-04-04T14:16:04.557;;15813787;2013-04-04T14:16:04.557;;;;;1827356.0;15799162.0;2;11;;;
3197;3197;;;"<p>So what are you doing is with append and concat is <em>almost</em> equivalent. The difference is the empty DataFrame. For some reason this causes a big slowdown, not sure exactly why, will have to look at some point. Below is a recreation of basically what you did.</p>

<p>I almost always use concat (though in this case they are equivalent, except for the empty frame);
if you don't use the empty frame they will be the same speed.</p>

<pre><code>In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))

In [18]: df1
Out[18]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39
Freq: S
Data columns (total 1 columns):
A    10000  non-null values
dtypes: int64(1)

In [19]: df4 = pd.DataFrame()

The concat

In [20]: %timeit pd.concat([df1,df2,df3])
1000 loops, best of 3: 270 us per loop

This is equavalent of your append

In [21]: %timeit pd.concat([df4,df1,df2,df3])
10 loops, best of 

 3: 56.8 ms per loop
</code></pre>
";;3;;2013-04-04T22:25:29.713;;15822811;2013-04-04T22:25:29.713;;;;;644898.0;15819050.0;2;23;;;
3221;3221;;;"<p><a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html"">np.correlate</a> calculates the (unnormalized) <a href=""http://en.wikipedia.org/wiki/Cross-correlation"">cross-correlation</a> between two 1-dimensional sequences:</p>

<pre><code>z[k] = sum_n a[n] * conj(v[n+k])
</code></pre>

<p>while <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.corr.html"">df.corr</a> (by default) calculates the <a href=""http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#For_a_sample"">Pearson correlation coefficient</a>. </p>

<p>The correlation coefficient (if it exists) is always between -1 and 1 inclusive.
The cross-correlation is not bounded.</p>

<p>The formulas are somewhat related, but notice that in the cross-correlation formula (above) there is no subtraction of the means, and no division by the standard deviations which is part of the formula for Pearson correlation coefficient.</p>

<p>The fact that the standard deviation of <code>df['a']</code> and <code>df['b']</code> is zero is what causes <code>df.corr</code> to be NaN everywhere. </p>

<hr>

<p>From the comment below, it sounds like you are looking for <a href=""http://en.wikipedia.org/wiki/Beta_%28finance%29"">Beta</a>. It is related to Pearson's correlation coefficient, but instead of dividing by the product of standard deviations:</p>

<p><img src=""https://i.stack.imgur.com/ft4zy.png"" alt=""enter image description here""></p>

<p>you divide by a variance:</p>

<p><img src=""https://i.stack.imgur.com/fHUQO.png"" alt=""enter image description here""></p>

<hr>

<p>You can compute <code>Beta</code> using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html"">np.cov</a></p>

<pre><code>cov = np.cov(a, b)
beta = cov[1, 0] / cov[0, 0]
</code></pre>

<hr>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
np.random.seed(100)


def geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):
    """"""
    http://stackoverflow.com/a/13203189/190597 (unutbu)
    """"""
    dt = float(T) / N
    t = np.linspace(0, T, N)
    W = np.random.standard_normal(size=N)
    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###
    X = (mu - 0.5 * sigma ** 2) * t + sigma * W
    S = S0 * np.exp(X)  # geometric brownian motion ###
    return S

N = 10 ** 6
a = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)
b = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)

cov = np.cov(a, b)
print(cov)
# [[ 0.38234755  0.80525967]
#  [ 0.80525967  1.73517501]]
beta = cov[1, 0] / cov[0, 0]
print(beta)
# 2.10609347015

plt.plot(a)
plt.plot(b)
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/xLLow.png"" alt=""enter image description here""></p>

<p>The ratio of <code>mu</code>s is 2, and <code>beta</code> is ~2.1.</p>

<hr>

<p>And you could also compute it with <code>df.corr</code>, though this is a much more round-about way of doing it (but it is nice to see there is consistency):</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a': a, 'b': b})
beta2 = (df.corr() * df['b'].std() * df['a'].std() / df['a'].var()).ix[0, 1]
print(beta2)
# 2.10609347015
assert np.allclose(beta, beta2)
</code></pre>
";;1;;2013-04-06T21:03:04.647;;15855998;2013-04-07T13:45:54.023;2013-04-07T13:45:54.023;;190597.0;;190597.0;15854878.0;2;60;;;
3230;3230;;;"<p>The former answer is now superseded by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.iloc.html"" rel=""noreferrer""><code>.iloc</code></a>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""date"": range(10, 64, 8)})
&gt;&gt;&gt; df.index += 17
&gt;&gt;&gt; df
    date
17    10
18    18
19    26
20    34
21    42
22    50
23    58
&gt;&gt;&gt; df[""date""].iloc[0]
10
&gt;&gt;&gt; df[""date""].iloc[-1]
58
</code></pre>

<hr>

<p>The shortest way I can think of uses <code>.iget()</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""date"": range(10, 64, 8)})
&gt;&gt;&gt; df.index += 17
&gt;&gt;&gt; df
    date
17    10
18    18
19    26
20    34
21    42
22    50
23    58
&gt;&gt;&gt; df['date'].iget(0)
10
&gt;&gt;&gt; df['date'].iget(-1)
58
</code></pre>

<p>Alternatively:</p>

<pre><code>&gt;&gt;&gt; df['date'][df.index[0]]
10
&gt;&gt;&gt; df['date'][df.index[-1]]
58
</code></pre>

<p>There's also <code>.first_valid_index()</code> and <code>.last_valid_index()</code>, but depending on whether or not you want to rule out <code>NaN</code>s they might not be what you want.</p>

<p>Remember that <code>df.ix[0]</code> doesn't give you the first, but the one indexed by 0.  For example, in the above case, <code>df.ix[0]</code> would produce</p>

<pre><code>&gt;&gt;&gt; df.ix[0]
Traceback (most recent call last):
  File ""&lt;ipython-input-489-494245247e87&gt;"", line 1, in &lt;module&gt;
    df.ix[0]
[...]
KeyError: 0
</code></pre>
";;2;;2013-04-07T13:40:31.360;;15863028;2015-04-27T17:05:20.083;2015-04-27T17:05:20.083;;487339.0;;487339.0;15862034.0;2;26;;;
3253;3253;;;"<p>You could slice and use concat to get what you want.</p>

<pre><code>line = DataFrame({""onset"": 30.0, ""length"": 1.3}, index=[3])
df2 = concat([df.ix[:2], line, df.ix[3:]]).reset_index(drop=True)
</code></pre>

<p>This will produce the dataframe in your example output. As far as I'm aware, concat is the best method to achieve an insert type operation in pandas, but admittedly I'm by no means a pandas expert.</p>
";;1;;2013-04-08T21:10:49.017;;15889056;2013-04-08T21:28:32.083;2013-04-08T21:28:32.083;;1649780.0;;1649780.0;15888648.0;2;21;;;
3285;3285;;;"<p>Do you want to use one of the other columns as the text of the annotation? This is something I did recently.</p>

<p>Starting with some example data</p>

<pre><code>In [1]: df
Out[1]: 
           x         y val
 0 -1.015235  0.840049   a
 1 -0.427016  0.880745   b
 2  0.744470 -0.401485   c
 3  1.334952 -0.708141   d
 4  0.127634 -1.335107   e
</code></pre>

<p>Plot the points. I plot y against x, in this example.</p>

<pre><code>In [2]: ax = df.set_index('x')['y'].plot(style='o')
</code></pre>

<p>Write a function that loops over x, y, and the value to annotate beside the point.</p>

<pre><code>In [3]: def label_point(x, y, val, ax):
    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)
    for i, point in a.iterrows():
        ax.text(point['x'], point['y'], str(point['val']))

In [4]: label_point(df.x, df.y, df.val, ax)

In [5]: draw()
</code></pre>

<p><img src=""https://i.stack.imgur.com/w95Je.png"" alt=""Annotated Points""></p>
";;0;;2013-04-09T19:59:43.870;;15911372;2013-04-09T19:59:43.870;;;;;1221924.0;15910019.0;2;15;;;
3290;3290;;;"<p>The <code>/</code> operator for dv seems equal to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.div.html"" rel=""noreferrer"">div</a> with default axis ""columns"". Set the axis to ""index"", then it'll work.</p>

<pre><code>df = df.div(df.QT, axis='index')
</code></pre>

<p>Another tricky way is to transpose it first, divide it, and then transpose back:</p>

<pre><code>df = (df.T / df.QT).T
</code></pre>
";;1;;2013-04-10T03:59:45.463;;15916686;2013-04-10T04:07:49.097;2013-04-10T04:07:49.097;;1426056.0;;1426056.0;15916612.0;2;34;;;
3294;3294;;;"<p>Something like this?</p>

<pre><code>import random

def some(x, n):
    return x.ix[random.sample(x.index, n)]
</code></pre>
";;4;;2013-04-10T10:55:57.680;;15923878;2013-04-10T10:55:57.680;;;;;449449.0;15923826.0;2;28;;;
3313;3313;;;"<p>You can use the <code>.shape</code> property or just <code>len(DataFrame.index)</code>. However, there are notable performance differences ( the <code>.shape</code> property is faster):</p>

<pre><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))

In [4]: df
Out[4]: 
   0  1  2
0  0  1  2
1  3  4  5
2  6  7  8

In [5]: df.shape
Out[5]: (3, 3)

In [6]: timeit df.shape
1000000 loops, best of 3: 1.17 us per loop

In [7]: timeit df[0].count()
10000 loops, best of 3: 56 us per loop

In [8]: len(df.index)
Out[8]: 3

In [9]: timeit len(df.index)
1000000 loops, best of 3: 381 ns per loop
</code></pre>

<hr>

<p>EDIT: As @Dan Allen noted in the comments <code>len(df.index)</code> and <code>df[0].count()</code> are not interchangeable as <code>count</code> excludes <code>NaN</code>s,</p>
";;7;;2013-04-11T08:24:29.633;;15943975;2017-04-27T08:31:49.570;2017-04-27T08:31:49.570;;1060349.0;;1199589.0;15943769.0;2;331;;;
3314;3314;;;"<p>You can also use <code>df.icol(n)</code> to access a column by integer.</p>

<p>Update: <code>icol</code> is deprecated and the same functionality can be achieved by:</p>

<pre><code>df.iloc[:, n]  # to access the column at the nth position
</code></pre>
";;2;;2013-04-11T12:01:51.120;;15948271;2016-09-13T19:22:27.503;2016-09-13T19:22:27.503;;2285236.0;;228800.0;14941097.0;2;12;;;
3343;3343;;;"<p>This is a doc example: <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#more-concatenating-with-group-keys"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/merging.html#more-concatenating-with-group-keys</a></p>

<pre><code>In [9]: df1 = pd.DataFrame(np.random.randn(3,2),columns=list('AB'),index=pd.date_range('20000101',periods=3))

In [10]: df2 = pd.DataFrame(np.random.randn(3,2),columns=list('AB'),index=pd.date_range('20000101',periods=3))

In [11]: df1
Out[11]: 
                   A         B
2000-01-01  0.129994  1.189608
2000-01-02 -1.126812  1.087617
2000-01-03 -0.930070  0.253098

In [12]: df2
Out[12]: 
                   A         B
2000-01-01  0.535700 -0.769533
2000-01-02 -1.698531 -0.456667
2000-01-03  0.451622 -1.500175

In [13]: pd.concat(dict(df1 = df1, df2 = df2),axis=1)
Out[13]: 
                 df1                 df2          
                   A         B         A         B
2000-01-01  0.129994  1.189608  0.535700 -0.769533
2000-01-02 -1.126812  1.087617 -1.698531 -0.456667
2000-01-03 -0.930070  0.253098  0.451622 -1.500175
</code></pre>
";;2;;2013-04-13T17:10:10.367;;15990537;2016-06-07T08:10:34.717;2016-06-07T08:10:34.717;;620382.0;;644898.0;15989281.0;2;15;;;
3344;3344;;;"<p>If you're looking to do modeling etc, lots of goodies for factor within the <a href=""https://pypi.python.org/pypi/patsy/"" rel=""nofollow"">patsy library</a>. I will admit to having struggled with this myself. I found <a href=""http://jseabold.net/presentations/seabold_pydata2012.html#slide1"" rel=""nofollow"">these slides</a> helpful. Wish I could give a better example, but this is as far as I've gotten myself.</p>
";;0;;2013-04-14T06:08:22.513;;15996274;2014-03-21T03:00:59.980;2014-03-21T03:00:59.980;;2074608.0;;684543.0;15124439.0;2;6;;;
3350;3350;;;"<p>I just give it a shot:</p>

<pre><code>In [9]: s = Series([True, True, True, False])

In [10]: s
Out[10]: 
0     True
1     True
2     True
3    False

In [11]: -s
Out[11]: 
0    False
1    False
2    False
3     True
</code></pre>
";;1;;2013-04-14T10:53:08.627;;15998251;2013-04-14T10:53:08.627;;;;;567989.0;15998188.0;2;8;;;
3353;3353;;;"<p>To invert a boolean Series, <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">use <code>~s</code></a>:</p>

<pre><code>In [7]: s = pd.Series([True, True, False, True])

In [8]: ~s
Out[8]: 
0    False
1    False
2     True
3    False
dtype: bool
</code></pre>

<p>Using Python2.7, NumPy 1.8.0, Pandas 0.13.1:</p>

<pre><code>In [119]: s = pd.Series([True, True, False, True]*10000)

In [10]:  %timeit np.invert(s)
10000 loops, best of 3: 91.8 s per loop

In [11]: %timeit ~s
10000 loops, best of 3: 73.5 s per loop

In [12]: %timeit (-s)
10000 loops, best of 3: 73.5 s per loop
</code></pre>

<p>As of Pandas 0.13.0, Series are no longer subclasses of <code>numpy.ndarray</code>; they are now subclasses of <code>pd.NDFrame</code>. This might have something to do with why <code>np.invert(s)</code> is no longer as fast as <code>~s</code> or <code>-s</code>.</p>

<p>Caveat: <code>timeit</code> results may vary depending on many factors including hardware, compiler, OS, Python, NumPy and Pandas versions. </p>
";;8;;2013-04-14T12:20:27.223;;15998993;2016-10-28T18:58:59.907;2016-10-28T18:58:59.907;;190597.0;;190597.0;15998188.0;2;95;;;
3390;3390;;;"<pre><code>In [10]: df
Out[10]:
          A         B       lat      long
0  1.428987  0.614405  0.484370 -0.628298
1 -0.485747  0.275096  0.497116  1.047605
2  0.822527  0.340689  2.120676 -2.436831
3  0.384719 -0.042070  1.426703 -0.634355
4 -0.937442  2.520756 -1.662615 -1.377490
5 -0.154816  0.617671 -0.090484 -0.191906
6 -0.705177 -1.086138 -0.629708  1.332853
7  0.637496 -0.643773 -0.492668 -0.777344
8  1.109497 -0.610165  0.260325  2.533383
9 -1.224584  0.117668  1.304369 -0.152561

In [11]: df['lat_long'] = df[['lat', 'long']].apply(tuple, axis=1)

In [12]: df
Out[12]:
          A         B       lat      long                             lat_long
0  1.428987  0.614405  0.484370 -0.628298      (0.484370195967, -0.6282975278)
1 -0.485747  0.275096  0.497116  1.047605      (0.497115615839, 1.04760475074)
2  0.822527  0.340689  2.120676 -2.436831      (2.12067574274, -2.43683074367)
3  0.384719 -0.042070  1.426703 -0.634355      (1.42670326172, -0.63435462504)
4 -0.937442  2.520756 -1.662615 -1.377490     (-1.66261469102, -1.37749004179)
5 -0.154816  0.617671 -0.090484 -0.191906  (-0.0904840623396, -0.191905582481)
6 -0.705177 -1.086138 -0.629708  1.332853     (-0.629707821728, 1.33285348929)
7  0.637496 -0.643773 -0.492668 -0.777344   (-0.492667604075, -0.777344111021)
8  1.109497 -0.610165  0.260325  2.533383        (0.26032456699, 2.5333825651)
9 -1.224584  0.117668  1.304369 -0.152561     (1.30436900612, -0.152560909725)
</code></pre>
";;9;;2013-04-16T09:13:01.477;;16033048;2013-04-18T20:42:06.173;2013-04-18T20:42:06.173;;1548051.0;;1548051.0;16031056.0;2;30;;;
3407;3407;;;"<p>Get comfortable with <code>zip</code>. It comes in handy when dealing with column data. </p>

<pre><code>df['new_col'] = list(zip(df.lat, df.long))
</code></pre>

<p>It's less complicated and faster than using <code>apply</code> or <code>map</code>. Something like <code>np.dstack</code> is twice as fast as <code>zip</code>, but wouldn't give you tuples.</p>
";;4;;2013-04-17T19:24:48.227;;16068497;2016-11-07T16:58:03.140;2016-11-07T16:58:03.140;;376837.0;;376837.0;16031056.0;2;57;;;
3410;3410;;;"<p>You may need to give boolean arg in your calls, e.g. use <code>ax.yaxis.grid(True)</code> instead of <code>ax.yaxis.grid()</code>.  Additionally, since you are using both of them you can combine into <code>ax.grid</code>, which works on both, rather than doing it once for each dimension.</p>

<pre><code>ax = plt.gca()
ax.grid(True)
</code></pre>

<p>That should sort you out.</p>
";;7;;2013-04-18T04:19:16.840;;16074407;2013-04-18T04:19:16.840;;;;;674039.0;16074392.0;2;47;;;
3417;3417;;;"<p>It's actually pretty simple (FWIW, I originally thought to do it your way):</p>

<pre><code>df['bar', 'three'] = [0, 1, 2]
df = df.sort_index(axis=1)
print(df)

        bar                        baz          
        one       two  three       one       two
A -0.212901  0.503615      0 -1.660945  0.446778
B -0.803926 -0.417570      1 -0.336827  0.989343
C  3.400885 -0.214245      2  0.895745  1.011671
</code></pre>
";;3;;2013-04-18T17:11:37.177;;16089219;2013-04-18T17:11:37.177;;;;;1742701.0;16088741.0;2;37;;;
3421;3421;;;"<p>You can think DataFrame as a dict of Series. <code>df[key]</code> try to select the column index by <code>key</code> and returns a Series object.</p>

<p>However slicing inside of [] slices the rows, because it's a very common operation.</p>

<p>You can read the document for detail:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics</a></p>
";;0;;2013-04-19T07:33:32.307;;16099579;2013-04-19T07:33:32.307;;;;;772649.0;16096627.0;2;12;;;
3424;3424;;;"<p>echoing @HYRY, see the new docs in 0.11</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html</a></p>

<p>Here we have new operators, <code>.iloc</code> to explicity support only integer indexing, and <code>.loc</code> to explicity support only label indexing</p>

<p>e.g. imagine this scenario</p>

<pre><code>In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))

In [2]: df
Out[2]: 
          A         B
0  1.068932 -0.794307
2 -0.470056  1.192211
4 -0.284561  0.756029
6  1.037563 -0.267820
8 -0.538478 -0.800654

In [5]: df.iloc[[2]]
Out[5]: 
          A         B
4 -0.284561  0.756029

In [6]: df.loc[[2]]
Out[6]: 
          A         B
2 -0.470056  1.192211
</code></pre>

<p><code>[]</code> slices the rows (by label location) only</p>
";;3;;2013-04-19T12:20:25.803;;16104482;2015-10-20T20:41:07.497;2015-10-20T20:41:07.497;;2587908.0;;644898.0;16096627.0;2;247;;;
3425;3425;;;"<p>You need 0.11 for this (0.11rc1 is out, final prob next week)</p>

<pre><code>In [9]: df = DataFrame([ Timestamp('20010101'), Timestamp('20040601') ])

In [10]: df
Out[10]: 
                    0
0 2001-01-01 00:00:00
1 2004-06-01 00:00:00

In [11]: df = DataFrame([ Timestamp('20010101'), 
                          Timestamp('20040601') ],columns=['age'])

In [12]: df
Out[12]: 
                  age
0 2001-01-01 00:00:00
1 2004-06-01 00:00:00

In [13]: df['today'] = Timestamp('20130419')

In [14]: df['diff'] = df['today']-df['age']

In [16]: df['years'] = df['diff'].apply(lambda x: float(x.item().days)/365)

In [17]: df
Out[17]: 
                  age               today                diff      years
0 2001-01-01 00:00:00 2013-04-19 00:00:00 4491 days, 00:00:00  12.304110
1 2004-06-01 00:00:00 2013-04-19 00:00:00 3244 days, 00:00:00   8.887671
</code></pre>

<p>You need this odd apply at the end because not yet full support for timedelta64[ns] scalars (e.g. like how we use Timestamps now for datetime64[ns], coming in 0.12)</p>
";;2;;2013-04-19T12:26:04.357;;16104567;2013-04-19T12:26:04.357;;;;;644898.0;16103238.0;2;8;;;
3453;3453;;;"<p>How about this? </p>

<pre class=""lang-py prettyprint-override""><code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
df = pd.DataFrame(a, columns=['one', 'two', 'three'])
df
Out[16]: 
  one  two three
0   a  1.2   4.2
1   b   70  0.03
2   x    5     0

df.dtypes
Out[17]: 
one      object
two      object
three    object

df[['two', 'three']] = df[['two', 'three']].astype(float)

df.dtypes
Out[19]: 
one       object
two      float64
three    float64
</code></pre>
";;9;;2013-04-21T18:15:27.933;;16134561;2013-06-19T13:39:59.880;2013-06-19T13:39:59.880;;586621.0;;359786.0;15891038.0;2;300;;;
3475;3475;;;"<p>You can use the <code>drop=True</code> option in <code>reset_index()</code>. See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer"">here</a>.</p>
";;0;;2013-04-23T11:30:00.707;;16168245;2015-09-07T22:27:47.277;2015-09-07T22:27:47.277;;2901002.0;;1649780.0;16167829.0;2;29;;;
3482;3482;;;"<p>Use <code>searchsorted</code> to find the nearest times first, and then use it to slice.</p>

<pre><code>In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])

In [16]: df
Out[16]: 
            0
2013-01-01  1
2013-01-03  2
2013-01-05  3

In [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))

In [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))

In [24]: df.ix[start:end]
Out[24]: 
            0
2013-01-03  2
</code></pre>
";;4;;2013-04-23T18:22:19.250;;16176457;2013-04-23T18:22:19.250;;;;;1426056.0;16175874.0;2;37;;;
3486;3486;;;"<p>Short answer: Sort your data (<code>data.sort()</code>) and then I think everything will work the way you are expecting.</p>

<p>Yes, you can slice using datetimes not present in the DataFrame. For example:</p>

<pre><code>In [12]: df
Out[12]: 
                   0
2013-04-20  1.120024
2013-04-21 -0.721101
2013-04-22  0.379392
2013-04-23  0.924535
2013-04-24  0.531902
2013-04-25 -0.957936

In [13]: df['20130419':'20130422']
Out[13]: 
                   0
2013-04-20  1.120024
2013-04-21 -0.721101
2013-04-22  0.379392
</code></pre>

<p>As you can see, you don't even have to build datetime objects; strings work.</p>

<p>Because the datetimes in your index are not sequential, the behavior is weird. If we shuffle the index of my example here...</p>

<pre><code>In [17]: df
Out[17]: 
                   0
2013-04-22  1.120024
2013-04-20 -0.721101
2013-04-24  0.379392
2013-04-23  0.924535
2013-04-21  0.531902
2013-04-25 -0.957936
</code></pre>

<p>...and take the same slice, we get a different result. It returns the first element inside the range and stops at the first element outside the range.</p>

<pre><code>In [18]: df['20130419':'20130422']
Out[18]: 
                   0
2013-04-22  1.120024
2013-04-20 -0.721101
2013-04-24  0.379392
</code></pre>

<p>This is probably not useful behavior. If you want to select ranges of dates, would it make sense to sort it by date first?</p>

<pre><code>df.sort_index()
</code></pre>
";;5;;2013-04-23T21:07:12.130;;16179190;2016-08-05T16:51:03.750;2016-08-05T16:51:03.750;;1221924.0;;1221924.0;16175874.0;2;20;;;
3495;3495;;;"<p>Pandas does not assume that 500+NaN=500, but it is easy to ask it to do that: <code>a.add(b, fill_value=0)</code></p>
";;2;;2013-04-24T21:47:26.403;;16202796;2013-04-24T21:47:26.403;;;;;1221924.0;16202711.0;2;19;;;
3507;3507;;;"<p>This is what I've done in the past</p>

<pre><code>df = pd.DataFrame({'textcol' : np.random.rand(5)})

df
    textcol
0  0.626524
1  0.119967
2  0.803650
3  0.100880
4  0.017859

df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))
   feature1  feature2
0  1.626524 -0.373476
1  1.119967 -0.880033
2  1.803650 -0.196350
3  1.100880 -0.899120
4  1.017859 -0.982141
</code></pre>

<p>Editing for completeness</p>

<pre><code>pd.concat([df, df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))], axis=1)
    textcol feature1  feature2
0  0.626524 1.626524 -0.373476
1  0.119967 1.119967 -0.880033
2  0.803650 1.803650 -0.196350
3  0.100880 1.100880 -0.899120
4  0.017859 1.017859 -0.982141
</code></pre>
";;0;;2013-04-26T17:39:39.793;;16242202;2015-01-22T17:53:40.810;2015-01-22T17:53:40.810;;1827356.0;;1827356.0;16236684.0;2;48;;;
3513;3513;;;"<p>Building off of user1827356 's answer, you can do the assignment in one pass using <code>df.merge</code>:</p>

<pre><code>df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), 
    left_index=True, right_index=True)

    textcol  feature1  feature2
0  0.772692  1.772692 -0.227308
1  0.857210  1.857210 -0.142790
2  0.065639  1.065639 -0.934361
3  0.819160  1.819160 -0.180840
4  0.088212  1.088212 -0.911788
</code></pre>
";;1;;2013-04-26T20:57:06.977;;16245109;2013-04-26T20:57:06.977;;;;;919872.0;16236684.0;2;44;;;
3520;3520;;;"<p><code>pymongo</code> might give you a hand, followings are some codes I'm using:</p>

<pre><code>import pandas as pd
from pymongo import MongoClient


def _connect_mongo(host, port, username, password, db):
    """""" A util for making a connection to mongo """"""

    if username and password:
        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)
        conn = MongoClient(mongo_uri)
    else:
        conn = MongoClient(host, port)


    return conn[db]


def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):
    """""" Read from Mongo and Store into DataFrame """"""

    # Connect to MongoDB
    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)

    # Make a query to the specific DB and Collection
    cursor = db[collection].find(query)

    # Expand the cursor and construct the DataFrame
    df =  pd.DataFrame(list(cursor))

    # Delete the _id
    if no_id:
        del df['_id']

    return df
</code></pre>
";;4;;2013-04-27T18:45:56.430;;16255680;2013-04-27T18:45:56.430;;;;;1426056.0;16249736.0;2;52;;;
3525;3525;;;"<p>Can't you do, where <code>df</code> is your DataFrame:</p>

<pre><code>times = pd.to_datetime(df.timestamp_col)
df.groupby([times.hour, times.minute]).value_col.sum()
</code></pre>
";;2;;2013-04-28T18:36:18.093;;16266318;2013-04-28T18:36:18.093;;;;;776560.0;16266019.0;2;25;;;
3531;3531;;;"<pre><code>import pandas as pd
import datetime
from myapp.models import BlogPost

df = pd.DataFrame(list(BlogPost.objects.all().values()))
df = pd.DataFrame(list(BlogPost.objects.filter(date__gte=datetime.datetime(2012, 5, 1)).values()))

# limit which fields
df = pd.DataFrame(list(BlogPost.objects.all().values('author', 'date', 'slug')))
</code></pre>

<p>The above is how I do the same thing. The most useful addition is specifying which fields you are interested in. If it's only a subset of the available fields you are interested in, then this would give a performance boost I imagine.</p>
";;2;;2013-04-29T05:39:15.383;;16271849;2014-12-16T22:24:08.623;2014-12-16T22:24:08.623;;623735.0;;1505540.0;11697887.0;2;32;;;
3580;3580;;;"<p>IIUC, assignment should fill:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})
&gt;&gt;&gt; df
   A  B
0  1  2
1  2  3
2  3  4
&gt;&gt;&gt; df[""C""] = """"
&gt;&gt;&gt; df[""D""] = np.nan
&gt;&gt;&gt; df
   A  B C   D
0  1  2   NaN
1  2  3   NaN
2  3  4   NaN
</code></pre>
";;2;;2013-05-01T21:52:57.737;;16327135;2013-05-01T21:52:57.737;;;;;487339.0;16327055.0;2;117;;;
3593;3593;;;"<p>I work with the guys that created <a href=""https://store.continuum.io/"" rel=""noreferrer"">Anaconda Python</a>.  You can install multiple versions of python and numpy without corrupting your system python.   It's free and open source (OSX, linux, Windows).  The paid packages are enhancements on top of the free version.  Pandas is included.</p>

<pre><code>conda create --name np17py27 anaconda=1.4 numpy=1.7 python=2.7
export PATH=~/anaconda/envs/np17py27/bin:$PATH
</code></pre>

<p>If you want numpy 1.6:</p>

<pre><code>conda create --name np16py27 anaconda=1.4 numpy=1.6 python=2.7
</code></pre>

<p>Setting your PATH sets up where to find python and ipython.  The environments (np17py27) can be named whatever you would like.</p>
";;2;;2013-05-02T15:50:01.717;;16342396;2013-05-02T15:50:01.717;;;;;268907.0;12436979.0;2;7;;;
3598;3598;;;"<p>You could perform a <a href=""http://pandas.pydata.org/pandas-docs/stable/api.html#id32"" rel=""nofollow noreferrer"">groupby/forward-fill</a> operation  on each group:</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame({'id': [1,1,2,2,1,2,1,1], 'x':[10,20,100,200,np.nan,np.nan,300,np.nan]})
df['x'] = df.groupby(['id'])['x'].ffill()
print(df)
</code></pre>

<p>yields</p>

<pre><code>   id      x
0   1   10.0
1   1   20.0
2   2  100.0
3   2  200.0
4   1   20.0
5   2  200.0
6   1  300.0
7   1  300.0
</code></pre>
";;3;;2013-05-02T18:59:54.107;;16345735;2017-07-18T20:13:55.623;2017-07-18T20:13:55.623;;190597.0;;190597.0;16345583.0;2;13;;;
3607;3607;;;"<p>If you just want to compute (column a) % (column b), you don't need <code>apply</code>, just do it directly:</p>

<pre><code>In [7]: df['a'] % df['c']                                                                                                                                                        
Out[7]: 
0   -1.132022                                                                                                                                                                    
1   -0.939493                                                                                                                                                                    
2    0.201931                                                                                                                                                                    
3    0.511374                                                                                                                                                                    
4   -0.694647                                                                                                                                                                    
5   -0.023486                                                                                                                                                                    
Name: a
</code></pre>
";;1;;2013-05-03T07:56:43.013;;16354103;2013-05-03T07:56:43.013;;;;;567989.0;16353729.0;2;12;;;
3608;3608;;;"<p>Seems you forgot the <code>''</code> of your string.</p>

<pre><code>In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)

In [44]: df
Out[44]:
                    a    b         c     Value
          0 -1.674308  foo  0.343801  0.044698
          1 -2.163236  bar -2.046438 -0.116798
          2 -0.199115  foo -0.458050 -0.199115
          3  0.918646  bar -0.007185 -0.001006
          4  1.336830  foo  0.534292  0.268245
          5  0.976844  bar -0.773630 -0.570417
</code></pre>

<p>BTW, in my opinion, following way is more elegant:</p>

<pre><code>In [53]: def my_test2(row):
....:     return row['a'] % row['c']
....:     

In [54]: df['Value'] = df.apply(my_test2, axis=1)
</code></pre>
";;4;;2013-05-03T08:40:31.377;;16354730;2013-05-03T08:40:31.377;;;;;1426056.0;16353729.0;2;133;;;
3616;3616;;;"<p>Suppose you start with this data:</p>

<pre><code>df = pd.DataFrame({'ID': ('STRSUB BOTDWG'.split())*4,
                   'Days Late': [60, 60, 50, 50, 20, 20, 10, 10],
                   'quantity': [56, 20, 60, 67, 74, 87, 40, 34]})
#    Days Late      ID  quantity
# 0         60  STRSUB        56
# 1         60  BOTDWG        20
# 2         50  STRSUB        60
# 3         50  BOTDWG        67
# 4         20  STRSUB        74
# 5         20  BOTDWG        87
# 6         10  STRSUB        40
# 7         10  BOTDWG        34
</code></pre>

<p>Then you can find the status category using <code>pd.cut</code>. Note that by default, <code>pd.cut</code> splits the Series <code>df['Days Late']</code> into categories which are <em>half-open</em> intervals, <code>(-1, 14], (14, 35], (35, 56], (56, 365]</code>:</p>

<pre><code>df['status'] = pd.cut(df['Days Late'], bins=[-1, 14, 35, 56, 365], labels=False)
labels = np.array('White Yellow Amber Red'.split())
df['status'] = labels[df['status']]
del df['Days Late']
print(df)
#        ID  quantity  status
# 0  STRSUB        56     Red
# 1  BOTDWG        20     Red
# 2  STRSUB        60   Amber
# 3  BOTDWG        67   Amber
# 4  STRSUB        74  Yellow
# 5  BOTDWG        87  Yellow
# 6  STRSUB        40   White
# 7  BOTDWG        34   White
</code></pre>

<p>Now use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pivot</code></a> to get the DataFrame in the desired form:</p>

<pre><code>df = df.pivot(index='ID', columns='status', values='quantity')
</code></pre>

<p>and use <code>reindex</code> to obtain the desired order for the rows and columns:</p>

<pre><code>df = df.reindex(columns=labels[::-1], index=df.index[::-1])
</code></pre>

<hr>

<p>Thus, </p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame({'ID': ('STRSUB BOTDWG'.split())*4,
                   'Days Late': [60, 60, 50, 50, 20, 20, 10, 10],
                   'quantity': [56, 20, 60, 67, 74, 87, 40, 34]})
df['status'] = pd.cut(df['Days Late'], bins=[-1, 14, 35, 56, 365], labels=False)
labels = np.array('White Yellow Amber Red'.split())
df['status'] = labels[df['status']]
del df['Days Late']
df = df.pivot(index='ID', columns='status', values='quantity')
df = df.reindex(columns=labels[::-1], index=df.index[::-1])
print(df)
</code></pre>

<p>yields</p>

<pre><code>        Red  Amber  Yellow  White
ID                               
STRSUB   56     60      74     40
BOTDWG   20     67      87     34
</code></pre>
";;1;;2013-05-03T13:18:06.180;;16359854;2017-07-10T10:16:11.490;2017-07-10T10:16:11.490;;190597.0;;190597.0;16349389.0;2;16;;;
3632;3632;;;"<p>You can make the plots by looping over the groups from <code>groupby</code>:</p>

<pre><code>import matplotlib.pyplot as plt

for title, group in df.groupby('ModelID'):
    group.plot(x='saleDate', y='MeanToDate', title=title)
</code></pre>

<p>See for more information on plotting with pandas dataframes:<br>
<a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/visualization.html</a><br>
and for looping over a groupby-object:<br>
<a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups</a></p>
";;2;;2013-05-04T18:05:32.577;;16377383;2017-07-25T07:52:13.690;2017-07-25T07:52:13.690;;653364.0;;653364.0;16376159.0;2;15;;;
3650;3650;;;"<p>Make the multiple axes first and pass them to the Pandas plot function, like:</p>

<pre><code>fig, axs = plt.subplots(1,2)

df['korisnika'].plot(ax=axs[0])
df['osiguranika'].plot(ax=axs[1])
</code></pre>

<p>It still gives you 1 figure, but with two different plots next to each other.</p>
";;5;;2013-05-06T06:18:52.290;;16393023;2013-05-06T06:18:52.290;;;;;1755432.0;16392921.0;2;48;;;
3655;3655;;;"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""nofollow noreferrer""><code>iloc</code></a> like you were using <code>ix</code>, but apply a different slice...</p>

<pre><code>df2 = df1.iloc[3:] #edited since .ix is now deprecated.
</code></pre>

<p>will give you a new df without the first three rows.</p>
";;4;;2013-05-06T12:04:09.547;;16398361;2017-08-21T16:38:49.460;2017-08-21T16:38:49.460;;1649780.0;;1649780.0;16396903.0;2;63;;;
3689;3689;;;"<p>For version 0.11.0 you need to change both <code>display.height</code> and <code>display.max_rows</code>.</p>

<pre><code>pd.set_option('display.height', 500)
pd.set_option('display.max_rows', 500)
</code></pre>

<p>See also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.describe_option.html"" rel=""noreferrer""><code>pd.describe_option('display')</code></a>.</p>
";;2;;2013-05-08T06:20:24.630;;16433953;2017-02-22T11:08:50.827;2017-02-22T11:08:50.827;;604687.0;;1548051.0;16424493.0;2;79;;;
3725;3725;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html#pandas.DataFrame.iterrows"">iterrows</a> is a generator which yield both index and row</p>

<pre><code>In [18]: for index, row in df.iterrows():
   ....:     print row['c1'], row['c2']
   ....:     
10 100
11 110
12 120
</code></pre>
";;1;;2013-05-10T07:07:58.523;;16476974;2017-01-10T20:37:11.200;2017-01-10T20:37:11.200;;375789.0;;1426056.0;16476924.0;2;556;;;
3727;3727;;;"<h3>Update:</h3>

<p>There is now a <code>to_sql</code> method, which is the preferred way to do this, rather than <code>write_frame</code>:</p>

<pre><code>df.to_sql(con=con, name='table_name_for_df', if_exists='replace', flavor='mysql')
</code></pre>

<p><em>Also note: the syntax may change in pandas 0.14...</em></p>

<p>You can set up the connection with <a href=""http://mysql-python.sourceforge.net/MySQLdb.html"" rel=""noreferrer"">MySQLdb</a>:</p>

<pre class=""lang-py prettyprint-override""><code>from pandas.io import sql
import MySQLdb

con = MySQLdb.connect()  # may need to add some other options to connect
</code></pre>

<p>Setting the <code>flavor</code> of <code>write_frame</code> to <code>'mysql'</code> means you can write to mysql:</p>

<pre class=""lang-py prettyprint-override""><code>sql.write_frame(df, con=con, name='table_name_for_df', 
                if_exists='replace', flavor='mysql')
</code></pre>

<p>The argument <code>if_exists</code> tells pandas how to deal if the table already exists:</p>

<blockquote>
  <p><code>if_exists: {'fail', 'replace', 'append'}</code>, default <code>'fail'</code><br>
      &nbsp;&nbsp;&nbsp;&nbsp; <code>fail</code>: If table exists, do nothing.<br>
      &nbsp;&nbsp;&nbsp;&nbsp; <code>replace</code>: If table exists, drop it, recreate it, and insert data.<br>
      &nbsp;&nbsp;&nbsp;&nbsp; <code>append</code>: If table exists, insert data. Create if does not exist.</p>
</blockquote>

<p><em>Although the <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#sql-queries"" rel=""noreferrer""><code>write_frame</code> docs</a> currently suggest it only works on sqlite, mysql appears to be supported and in fact there is quite a bit of <a href=""https://github.com/pydata/pandas/blob/master/pandas/io/tests/test_sql.py#L223"" rel=""noreferrer"">mysql testing in the codebase</a>.</em></p>
";;4;;2013-05-10T07:58:51.303;;16477603;2014-03-13T01:09:19.410;2014-03-13T01:09:19.410;;1240268.0;;1240268.0;16476413.0;2;37;;;
3759;3759;;;"<p>Put</p>

<pre><code>import matplotlib.pyplot as plt
</code></pre>

<p>at the top, and</p>

<pre><code>plt.show()
</code></pre>

<p>at the end.</p>
";;4;;2013-05-13T12:56:58.833;;16522626;2013-05-13T12:56:58.833;;;;;190597.0;16522380.0;2;134;;;
3774;3774;;;"<p>Here's a more concise approach...</p>

<pre><code>df['a_bsum'] = df.groupby('A')['B'].transform(sum)
df.sort(['a_bsum','C'], ascending=[True, False]).drop('a_bsum', axis=1)
</code></pre>

<p>The first line adds a column to the data frame with the groupwise sum. The second line performs the sort and then removes the extra column.</p>

<p>Result:</p>

<pre><code>    A       B           C
5   baz     -2.301539   True
2   baz     -0.528172   False
1   bar     -0.611756   True
4   bar      0.865408   False
3   foo     -1.072969   True
0   foo      1.624345   False
</code></pre>

<p>NOTE: <code>sort</code> is deprecated, use <code>sort_values</code> instead</p>
";;0;;2013-05-14T14:03:50.240;;16545324;2016-04-27T13:38:41.860;2016-04-27T13:38:41.860;;601224.0;;61974.0;14941366.0;2;19;;;
3795;3795;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html"" rel=""noreferrer""><code>applymap</code></a> with the dictionary <a href=""http://docs.python.org/2/library/stdtypes.html#dict.get"" rel=""noreferrer""><code>get</code></a> method:</p>

<pre><code>In [11]: df[abc_columns].applymap(categories.get)
Out[11]:
   abc1  abc2  abc3
0  Good   Bad   Bad
1   Bad  Good  Good
2   Bad   Bad  Good
3  Good   Bad  Good
4  Good  Good   Bad
</code></pre>

<p>And put this to the specified columns:</p>

<pre><code>In [12]: abc_categories = map(lambda x: x + '_category', abc_columns)

In [13]: abc_categories
Out[13]: ['abc1_category', 'abc2_category', 'abc3_category']

In [14]: df[abc_categories] = df[abc_columns].applymap(categories.get)
</code></pre>

<p>Note: you can construct <code>abc_columns</code> relatively efficiently using a list comprehension:</p>

<pre><code>abc_columns = [col for col in df.columns if str(col).startswith('abc')]
</code></pre>
";;5;;2013-05-15T22:26:35.813;;16576030;2013-05-15T22:45:37.560;2013-05-15T22:45:37.560;;1240268.0;;1240268.0;16575868.0;2;19;;;
3816;3816;;;"<p>That should work:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame()
&gt;&gt;&gt; data = pd.DataFrame({""A"": range(3)})
&gt;&gt;&gt; df.append(data)
   A
0  0
1  1
2  2
</code></pre>

<p>but the <code>append</code> doesn't happen in-place, so you'll have to store the output if you want it:</p>

<pre><code>&gt;&gt;&gt; df
Empty DataFrame
Columns: []
Index: []
&gt;&gt;&gt; df = df.append(data)
&gt;&gt;&gt; df
   A
0  0
1  1
2  2
</code></pre>
";;4;;2013-05-16T20:58:48.373;;16597375;2013-05-16T20:58:48.373;;;;;487339.0;16597265.0;2;156;;;
3843;3843;;;"<p>If you have a list of columns you can just select those:</p>

<pre><code>In [11]: df
Out[11]:
   1  2  3  4  5  6
A  x  x  x  x  x  x
B  x  x  x  x  x  x
C  x  x  x  x  x  x

In [12]: col_list = [3, 5]

In [13]: df = df[col_list]

In [14]: df
Out[14]:
   3  5
A  x  x
B  x  x
C  x  x
</code></pre>
";;1;;2013-05-17T19:24:38.180;;16616454;2013-05-17T19:24:38.180;;;;;1240268.0;16616141.0;2;24;;;
3857;3857;;;"<p>When index is unique, pandas use a hashtable to map key to value O(1). When index is non-unique and sorted, pandas use binary search O(logN), when index is random ordered pandas need to check all the keys in the index O(N).</p>

<p>You can call <code>sort_index</code> method:</p>

<pre><code>import numpy as np
import pandas as pd
x = np.random.randint(0, 200, 10**6)
df1 = pd.DataFrame({'x':x})
df2 = df1.set_index('x', drop=False)
df3 = df2.sort_index()
%timeit df1.loc[100]
%timeit df2.loc[100]
%timeit df3.loc[100]
</code></pre>

<p>result:</p>

<pre><code>10000 loops, best of 3: 71.2 s per loop
10 loops, best of 3: 38.9 ms per loop
10000 loops, best of 3: 134 s per loop
</code></pre>
";;1;;2013-05-18T21:26:37.400;;16629125;2017-07-26T15:38:25.260;2017-07-26T15:38:25.260;;168137.0;;772649.0;16626058.0;2;36;;;
3859;3859;;;"<p>Updated to use pandas 0.13.1</p>

<p>1) No. <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#notes-caveats"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/io.html#notes-caveats</a>. There are various ways to <em>do</em> this, e.g. have your different threads/processes write out the computation results, then have a single process combine.</p>

<p>2) depending the type of data you store, how you do it, and how you want to retrieve, HDF5 can offer vastly better performance. Storing in an <code>HDFStore</code> as a single array, float data, compressed (in other words, not storing it in a format that allows for querying), will be stored/read amazing fast. Even storing in the table format (which slows down the write performance), will offer quite good write performance. You can look at this for some detailed comparsions (which is what <code>HDFStore</code> uses under the hood). <a href=""http://www.pytables.org/"" rel=""noreferrer"">http://www.pytables.org/</a>, here's a nice picture: <img src=""https://i.stack.imgur.com/YNhCa.png"" alt=""""></p>

<p>(and since PyTables 2.3 the queries are now indexed), so perf actually is MUCH better than this
So to answer your question, if you want any kind of performance, HDF5 is the way to go.</p>

<p>Writing:</p>

<pre><code>In [14]: %timeit test_sql_write(df)
1 loops, best of 3: 6.24 s per loop

In [15]: %timeit test_hdf_fixed_write(df)
1 loops, best of 3: 237 ms per loop

In [16]: %timeit test_hdf_table_write(df)
1 loops, best of 3: 901 ms per loop

In [17]: %timeit test_csv_write(df)
1 loops, best of 3: 3.44 s per loop
</code></pre>

<p>Reading</p>

<pre><code>In [18]: %timeit test_sql_read()
1 loops, best of 3: 766 ms per loop

In [19]: %timeit test_hdf_fixed_read()
10 loops, best of 3: 19.1 ms per loop

In [20]: %timeit test_hdf_table_read()
10 loops, best of 3: 39 ms per loop

In [22]: %timeit test_csv_read()
1 loops, best of 3: 620 ms per loop
</code></pre>

<p>And here's the code</p>

<pre><code>import sqlite3
import os
from pandas.io import sql

In [3]: df = DataFrame(randn(1000000,2),columns=list('AB'))
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000000 entries, 0 to 999999
Data columns (total 2 columns):
A    1000000  non-null values
B    1000000  non-null values
dtypes: float64(2)

def test_sql_write(df):
    if os.path.exists('test.sql'):
        os.remove('test.sql')
    sql_db = sqlite3.connect('test.sql')
    sql.write_frame(df, name='test_table', con=sql_db)
    sql_db.close()

def test_sql_read():
    sql_db = sqlite3.connect('test.sql')
    sql.read_frame(""select * from test_table"", sql_db)
    sql_db.close()

def test_hdf_fixed_write(df):
    df.to_hdf('test_fixed.hdf','test',mode='w')

def test_csv_read():
    pd.read_csv('test.csv',index_col=0)

def test_csv_write(df):
    df.to_csv('test.csv',mode='w')    

def test_hdf_fixed_read():
    pd.read_hdf('test_fixed.hdf','test')

def test_hdf_table_write(df):
    df.to_hdf('test_table.hdf','test',format='table',mode='w')

def test_hdf_table_read():
    pd.read_hdf('test_table.hdf','test')
</code></pre>

<p>Of course YMMV.</p>
";;3;;2013-05-18T21:41:40.100;;16629243;2016-10-30T15:46:57.150;2016-10-30T15:46:57.150;;4013571.0;;644898.0;16628329.0;2;58;;;
3867;3867;;;"<ol>
<li><p>As soon as the statement is exectued, eg <code>store['df'] = df</code>. The <code>close</code> just closes the actual file (which will be closed for you if the process exists, but will print a warning message)</p></li>
<li><p>Read the section <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#storing-in-table-format"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/io.html#storing-in-table-format</a></p>

<p>It is generally not a good idea to put a LOT of nodes in an <code>.h5</code> file. You probably want to append and create a smaller number of nodes.</p>

<p>You can just iterate thru your <code>.csv</code> and <code>store/append</code> them one by one. Something like:</p>

<pre><code>for f in files:
  df = pd.read_csv(f)
  df.to_hdf('file.h5',f,df)
</code></pre>

<p>Would be one way (creating a separate node for each file)</p></li>
<li><p>Not appendable - once you write it, you can only retrieve it all at once, e.g. you cannot select a sub-section</p>

<p>If you have a table, then you can do things like:</p>

<pre><code>pd.read_hdf('my_store.h5','a_table_node',['index&gt;100'])
</code></pre>

<p>which is like a database query, only getting part of the data</p>

<p><strong>Thus, a store is not appendable, nor queryable, while a table is both</strong>.</p></li>
</ol>
";;8;;2013-05-19T17:43:49.743;;16637572;2013-05-19T19:16:49.157;2013-05-19T19:16:49.157;;283296.0;;644898.0;16637271.0;2;13;;;
3868;3868;;;"<pre><code>In [9]: s = Series([list('ABC'),list('DEF'),list('ABEF')])

In [10]: s
Out[10]: 
0       [A, B, C]
1       [D, E, F]
2    [A, B, E, F]
dtype: object

In [11]: s.apply(lambda x: Series(1,index=x)).fillna(0)
Out[11]: 
   A  B  C  D  E  F
0  1  1  1  0  0  0
1  0  0  0  1  1  1
2  1  1  0  0  1  1
</code></pre>
";;4;;2013-05-19T17:47:42.400;;16637607;2013-05-19T17:47:42.400;;;;;644898.0;16637171.0;2;27;;;
3876;3876;;;"<p>Copy of my answer from the issue: <a href=""https://github.com/pydata/pandas/issues/3651"">https://github.com/pydata/pandas/issues/3651</a></p>

<p>Your sample is really too small. HDF5 has a fair amount of overhead with really small sizes (even 300k entries is on the smaller side). The following is with no compression on either side. Floats are really more efficiently represented in binary (that as a text representation).</p>

<p>In addition, HDF5 is row based. You get MUCH efficiency by having tables that are not too wide but are fairly long. (Hence your example is not very efficient in HDF5 at all, store it transposed in this case)</p>

<p>I routinely have tables that are 10M+ rows and query times can be in the ms. Even the below example is small. Having 10+GB files is quite common (not to mention the astronomy guys who 10GB+ is a few seconds!)</p>

<pre><code>-rw-rw-r--  1 jreback users 203200986 May 19 20:58 test.csv
-rw-rw-r--  1 jreback users  88007312 May 19 20:59 test.h5

In [1]: df = DataFrame(randn(1000000,10))

In [9]: df
Out[9]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000000 entries, 0 to 999999
Data columns (total 10 columns):
0    1000000  non-null values
1    1000000  non-null values
2    1000000  non-null values
3    1000000  non-null values
4    1000000  non-null values
5    1000000  non-null values
6    1000000  non-null values
7    1000000  non-null values
8    1000000  non-null values
9    1000000  non-null values
dtypes: float64(10)

In [5]: %timeit df.to_csv('test.csv',mode='w')
1 loops, best of 3: 12.7 s per loop

In [6]: %timeit df.to_hdf('test.h5','df',mode='w')
1 loops, best of 3: 825 ms per loop

In [7]: %timeit pd.read_csv('test.csv',index_col=0)
1 loops, best of 3: 2.35 s per loop

In [8]: %timeit pd.read_hdf('test.h5','df')
10 loops, best of 3: 38 ms per loop
</code></pre>

<p>I really wouldn't worry about the size (I suspect you are not, but are merely interested, which is fine). The point of HDF5 is that disk is cheap, cpu is cheap, but you can't have everything in memory at once so we optimize by using chunking</p>
";;0;;2013-05-20T01:29:43.907;;16641346;2013-05-20T01:37:27.203;2013-05-20T01:37:27.203;;644898.0;;644898.0;16639877.0;2;25;;;
3879;3879;;;"<p>Starting in 0.11.1 (coming out this week), replace has a new option to replace with a regex, so this becomes possible</p>

<pre><code>In [14]: df = DataFrame('10.0%',index=range(100),columns=range(10))

In [15]: df.replace('%','',regex=True).astype('float')/100
Out[15]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 100 entries, 0 to 99
Data columns (total 10 columns):
0    100  non-null values
1    100  non-null values
2    100  non-null values
3    100  non-null values
4    100  non-null values
5    100  non-null values
6    100  non-null values
7    100  non-null values
8    100  non-null values
9    100  non-null values
dtypes: float64(10)
</code></pre>

<p>And a bit faster</p>

<pre><code>In [16]: %timeit df.replace('%','',regex=True).astype('float')/100
1000 loops, best of 3: 1.16 ms per loop

 In [18]: %timeit df.applymap(lambda x: float(x[:-1]))/100
1000 loops, best of 3: 1.67 ms per loop
</code></pre>
";;1;;2013-05-20T11:29:21.943;;16648510;2013-05-20T11:29:21.943;;;;;644898.0;16643695.0;2;10;;;
3889;3889;;;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html""><code>rename</code></a> method can take a function, for example:</p>

<pre><code>In [11]: df.columns
Out[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)

In [12]: df.rename(columns=lambda x: x[1:], inplace=True)

In [13]: df.columns
Out[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)
</code></pre>
";;3;;2013-05-21T09:58:59.743;;16667215;2013-05-21T09:58:59.743;;;;;1240268.0;11346283.0;2;216;;;
3891;3891;;;"<p>You can use the <code>parse_dates</code> option from <code>read_csv</code> to do the conversion directly while reading you data.<br>
The trick here is to use <code>dayfirst=True</code> to indicate your dates start with the day and not with the month. See here for more information: <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html</a></p>

<p>When your dates have to be the index:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from StringIO import StringIO
&gt;&gt;&gt; s = StringIO(""""""date,value
... 12/01/2012,1
... 12/01/2012,2
... 30/01/2012,3"""""")
&gt;&gt;&gt; 
&gt;&gt;&gt; pd.read_csv(s, index_col=0, parse_dates=True, dayfirst=True)
            value
date             
2012-01-12      1
2012-01-12      2
2012-01-30      3
</code></pre>

<p>Or when your dates are just in a certain column:</p>

<pre><code>&gt;&gt;&gt; s = StringIO(""""""date
... 12/01/2012
... 12/01/2012
... 30/01/2012"""""")
&gt;&gt;&gt; 
&gt;&gt;&gt; pd.read_csv(s, parse_dates=[0], dayfirst=True)
                 date
0 2012-01-12 00:00:00
1 2012-01-12 00:00:00
2 2012-01-30 00:00:00
</code></pre>
";;1;;2013-05-21T14:25:47.973;;16672514;2013-05-21T14:31:23.167;2013-05-21T14:31:23.167;;653364.0;;653364.0;16672237.0;2;13;;;
3892;3892;;;"<p><em>I think you are calling it correctly, and I posted this as <a href=""https://github.com/pydata/pandas/issues/3669"">an issue on github</a>.</em></p>

<p>You can just specify the format to <code>to_datetime</code> directly, for example:</p>

<pre><code>In [1]: s = pd.Series(['12/1/2012', '30/01/2012'])

In [2]: pd.to_datetime(s, format='%d/%m/%Y')
Out[2]:
0   2012-01-12 00:00:00
1   2012-01-30 00:00:00
dtype: datetime64[ns]
</code></pre>

<p>Update: As OP correctly points out this doesn't work with NaN, if you are happy with <code>dayfirst=True</code> (which works with NaN too):</p>

<pre><code>s.apply(pd.to_datetime, dayfirst=True)
</code></pre>

<p><em>Worth noting that have to be careful using <code>dayfirst</code> (which is easier than specifying the exact format), since <a href=""https://github.com/pydata/pandas/issues/3341""><code>dayfirst</code> isn't strict</a>.</em></p>
";;9;;2013-05-21T14:51:02.790;;16673019;2013-05-21T19:55:10.387;2013-05-21T19:55:10.387;;1240268.0;;1240268.0;16672237.0;2;9;;;
3901;3901;;;"<p>There might be more than one index map to your value, it make more sense to return a list:</p>

<pre><code>In [48]: a
Out[48]: 
   c1  c2
0   0   1
1   2   3
2   4   5
3   6   7
4   8   9

In [49]: a.c1[a.c1 == 8].index.tolist()
Out[49]: [4]
</code></pre>
";;4;;2013-05-22T05:34:39.130;;16684166;2013-05-22T05:34:39.130;;;;;1426056.0;16683701.0;2;21;;;
3909;3909;;;"<p>Simply using <code>df.mean()</code> will Do The Right Thing(tm) with respect to NaNs:</p>

<pre><code>&gt;&gt;&gt; df
                 A      B
DATE                     
2013-05-01  473077  71333
2013-05-02   35131  62441
2013-05-03     727  27381
2013-05-04     481   1206
2013-05-05     226   1733
2013-05-06     NaN   4064
2013-05-07     NaN  41151
2013-05-08     NaN   8144
2013-05-09     NaN     23
2013-05-10     NaN     10
&gt;&gt;&gt; df.mean(axis=1)
DATE
2013-05-01    272205.0
2013-05-02     48786.0
2013-05-03     14054.0
2013-05-04       843.5
2013-05-05       979.5
2013-05-06      4064.0
2013-05-07     41151.0
2013-05-08      8144.0
2013-05-09        23.0
2013-05-10        10.0
dtype: float64
</code></pre>

<p>You can use <code>df[[""A"", ""B""]].mean(axis=1)</code> if there are other columns to ignore.</p>
";;3;;2013-05-22T10:35:54.240;;16689573;2013-05-22T10:35:54.240;;;;;487339.0;16689514.0;2;48;;;
3943;3943;;;"<p>You can try <code>df.column_name = df.column_name.astype(float)</code>. As for the <code>NaN</code> values, you need to specify how they should be converted, but you can use the <code>.fillna</code> method to do it.</p>

<p><em>Example:</em></p>

<pre><code>In [12]: df
Out[12]: 
     a    b
0  0.1  0.2
1  NaN  0.3
2  0.4  0.5

In [13]: df.a.values
Out[13]: array(['0.1', nan, '0.4'], dtype=object)

In [14]: df.a = df.a.astype(float).fillna(0.0)

In [15]: df
Out[15]: 
     a    b
0  0.1  0.2
1  0.0  0.3
2  0.4  0.5

In [16]: df.a.values
Out[16]: array([ 0.1,  0. ,  0.4])
</code></pre>
";;0;;2013-05-24T07:20:32.920;;16729635;2013-05-24T07:27:27.333;2013-05-24T07:27:27.333;;1199589.0;;1199589.0;16729483.0;2;21;;;
3944;3944;;;"<p>If you have a DataFrame with only one row, then access the first (only) row as a Series using <code>iloc</code>, and then the value using the column name:</p>

<pre><code>In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493
</code></pre>
";;5;;2013-05-24T07:31:09.940;;16729808;2013-05-24T07:31:09.940;;;;;1240268.0;16729574.0;2;100;;;
3951;3951;;;"<p>This is available in 0.11. Forces conversion (or set's to nan)
This will work even when <code>astype</code> will fail; its also series by series
so it won't convert say a complete string column</p>

<pre><code>In [10]: df = DataFrame(dict(A = Series(['1.0','1']), B = Series(['1.0','foo'])))

In [11]: df
Out[11]: 
     A    B
0  1.0  1.0
1    1  foo

In [12]: df.dtypes
Out[12]: 
A    object
B    object
dtype: object

In [13]: df.convert_objects(convert_numeric=True)
Out[13]: 
   A   B
0  1   1
1  1 NaN

In [14]: df.convert_objects(convert_numeric=True).dtypes
Out[14]: 
A    float64
B    float64
dtype: object
</code></pre>
";;6;;2013-05-24T12:54:51.137;;16735476;2013-05-24T13:00:40.300;2013-05-24T13:00:40.300;;644898.0;;644898.0;16729483.0;2;43;;;
3953;3953;;;"<p>These are fast access for scalars</p>

<pre><code>In [15]: df = DataFrame(randn(5,3),columns=list('ABC'))

In [16]: df
Out[16]: 
          A         B         C
0 -0.074172 -0.090626  0.038272
1 -0.128545  0.762088 -0.714816
2  0.201498 -0.734963  0.558397
3  1.563307 -1.186415  0.848246
4  0.205171  0.962514  0.037709

In [17]: df.iat[0,0]
Out[17]: -0.074171888537611502

In [18]: df.at[0,'A']
Out[18]: -0.074171888537611502
</code></pre>
";;0;;2013-05-24T12:58:23.933;;16735536;2013-05-24T12:58:23.933;;;;;644898.0;16729574.0;2;51;;;
4002;4002;;;"<p>Note this is using numpy >= 1.7, for numpy &lt; 1.7, see the conversion here: <a href=""http://pandas.pydata.org/pandas-docs/dev/timeseries.html#time-deltas"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/timeseries.html#time-deltas</a></p>

<p>Your original frame, with a datetime index</p>

<pre><code>In [196]: df
Out[196]: 
                     value
2012-03-16 23:50:00      1
2012-03-16 23:56:00      2
2012-03-17 00:08:00      3
2012-03-17 00:10:00      4
2012-03-17 00:12:00      5
2012-03-17 00:20:00      6
2012-03-20 00:43:00      7

In [199]: df.index
Out[199]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2012-03-16 23:50:00, ..., 2012-03-20 00:43:00]
Length: 7, Freq: None, Timezone: None
</code></pre>

<p>Here is the timedelta64 of what you want</p>

<pre><code>In [200]: df['tvalue'] = df.index

In [201]: df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)

In [202]: df
Out[202]: 
                     value              tvalue            delta
2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00
2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00
2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00
2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00
2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00
2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00
2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00
</code></pre>

<p>Getting out the answer while disregarding the day difference (your last day is 3/20, prior is 3/17), actually is tricky</p>

<pre><code>In [204]: df['ans'] = df['delta'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)

In [205]: df
Out[205]: 
                     value              tvalue            delta  ans
2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00    0
2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00    6
2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00   12
2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00    2
2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00    2
2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00    8
2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00   23
</code></pre>
";;0;;2013-05-27T20:53:26.470;;16780413;2013-05-27T20:53:26.470;;;;;644898.0;16777570.0;2;39;;;
4005;4005;;;"<p>As @Jeff mentions there are a few ways to do this, but I recommend using loc/iloc to be more explicit (and raise errors early if your trying something ambiguous):</p>

<pre><code>In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])

In [11]: df
Out[11]:
   A  B
0  1  2
1  3  4

In [12]: df[['A']]

In [13]: df[[0]]

In [14]: df.loc[:, ['A']]

In [15]: df.iloc[:, [0]]

Out[12-15]:  # they all return the same thing:
   A
0  1
1  3
</code></pre>

<p>The latter two choices remove ambiguity in the case of integer column names (precisely why loc/iloc were created). For example:</p>

<pre><code>In [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])

In [17]: df
Out[17]:
   A  0
0  1  2
1  3  4

In [18]: df[[0]]  # ambiguous
Out[18]:
   A
0  1
1  3
</code></pre>
";;3;;2013-05-28T10:03:16.580;;16789254;2013-05-28T10:03:16.580;;;;;1240268.0;16782323.0;2;40;;;
4007;4007;;;"<p>I know this is an old question, but I have just had a similar problem and I think what I did would work for you too.</p>

<p>I used the to_csv() method and wrote to stdout:</p>

<pre><code>import sys

paramdata.to_csv(sys.stdout)
</code></pre>

<p>This should dump the whole dataframe whether it's nicely-printable or not, and you can use the to_csv parameters to configure column separators, whether the index is printed, etc.</p>
";;0;;2013-05-28T10:32:51.073;;16789834;2013-05-28T10:32:51.073;;;;;2428107.0;11361985.0;2;20;;;
4037;4037;;;"<p>Here is one possibility</p>

<pre><code>[3]: df

Out[3]: 
                   A         B         C         D
2013-01-01 -0.024362  0.712035 -0.913923  0.755276
2013-01-02  2.624298  0.285546  0.142265 -0.047871
2013-01-03  1.315157 -0.333630  0.398759 -1.034859
2013-01-04  0.713141 -0.109539  0.263706 -0.588048
2013-01-05 -1.172163 -1.387645 -0.171854 -0.458660
2013-01-06 -0.192586  0.480023 -0.530907 -0.872709

In [4]: df.unstack()
Out[4]: 
A  2013-01-01   -0.024362
   2013-01-02    2.624298
   2013-01-03    1.315157
   2013-01-04    0.713141
   2013-01-05   -1.172163
   2013-01-06   -0.192586
B  2013-01-01    0.712035
   2013-01-02    0.285546
   2013-01-03   -0.333630
   2013-01-04   -0.109539
   2013-01-05   -1.387645
   2013-01-06    0.480023
C  2013-01-01   -0.913923
   2013-01-02    0.142265
   2013-01-03    0.398759
   2013-01-04    0.263706
   2013-01-05   -0.171854
   2013-01-06   -0.530907
D  2013-01-01    0.755276
   2013-01-02   -0.047871
   2013-01-03   -1.034859
   2013-01-04   -0.588048
   2013-01-05   -0.458660
   2013-01-06   -0.872709
dtype: float64
</code></pre>
";;2;;2013-05-29T21:35:44.553;;16824270;2013-05-29T21:35:44.553;;;;;644898.0;16822996.0;2;9;;;
4039;4039;;;"<p>The <code>name</code> of the Series becomes the <code>index</code> of the row in the DataFrame:</p>

<pre><code>In [99]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])

In [100]: s = df.xs(3)

In [101]: s.name = 10

In [102]: df.append(s)
Out[102]: 
           A         B         C         D
0  -2.083321 -0.153749  0.174436  1.081056
1  -1.026692  1.495850 -0.025245 -0.171046
2   0.072272  1.218376  1.433281  0.747815
3  -0.940552  0.853073 -0.134842 -0.277135
4   0.478302 -0.599752 -0.080577  0.468618
5   2.609004 -1.679299 -1.593016  1.172298
6  -0.201605  0.406925  1.983177  0.012030
7   1.158530 -2.240124  0.851323 -0.240378
10 -0.940552  0.853073 -0.134842 -0.277135
</code></pre>
";;3;;2013-05-29T22:05:55.720;;16824696;2013-05-29T22:05:55.720;;;;;190597.0;16824607.0;2;23;;;
4040;4040;;;"<p>The way you have written f it needs two inputs. If you look at the error message it says you are not providing two inputs to f, just one. The error message is correct.<br>
The mismatch is because df[['col1','col2']] returns a single dataframe with two columns, not two separate columns.</p>

<p>You need to change your f so that it takes a single input, keep the above data frame as input, then break it up into x,y <em>inside</em> the function body. Then do whatever you need and return a single value.</p>

<p>You need this function signature because the syntax is .apply(f)
So f needs to take the single thing = dataframe and not two things which is what your current f expects.  </p>

<p>Since you haven't provided the body of f I can't help in anymore detail - but this should provide the way out without fundamentally changing your code or using some other methods rather than apply</p>
";;0;;2013-05-30T00:53:50.313;;16826250;2013-05-30T00:53:50.313;;;;;591702.0;13331698.0;2;7;;;
4042;4042;;;"<p>Sure. <code>ax.legend()</code> has a two argument form that accepts a list of objects (handles) and a list of strings (labels). Use a dummy object (aka a <a href=""http://matplotlib.org/users/legend_guide.html#using-proxy-artist"">""proxy artist""</a>) for your extra string.  I picked a <code>matplotlib.patches.Rectangle</code> with no fill and 0 linewdith below, but you could use any supported artist.</p>

<p>For example, let's say you have 4 bar objects (since you didn't post the code used to generate the graph, I can't reproduce it exactly).</p>

<pre><code>import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
fig = plt.figure()
ax = fig.add_subplot(111)
bar_0_10 = ax.bar(np.arange(0,10), np.arange(1,11), color=""k"")
bar_10_100 = ax.bar(np.arange(0,10), np.arange(30,40), bottom=np.arange(1,11), color=""g"")
# create blank rectangle
extra = Rectangle((0, 0), 1, 1, fc=""w"", fill=False, edgecolor='none', linewidth=0)
ax.legend([extra, bar_0_10, bar_10_100], (""My explanatory text"", ""0-10"", ""10-100""))
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/FQn2U.png"" alt=""example output""></p>
";;3;;2013-05-30T03:00:24.727;;16827257;2013-05-30T03:00:24.727;;;;;1316786.0;16826711.0;2;28;;;
4044;4044;;;"<p>Sort the frame, then select/set using a tuple for the multi-index</p>

<pre><code>In [12]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])

In [13]: df
Out[13]: 
                  A         B         C
bar one 0 -0.694240  0.725163  0.131891
    two 1 -0.729186  0.244860  0.530870
baz one 2  0.757816  1.129989  0.893080
qux one 3 -2.275694  0.680023 -1.054816
    two 4  0.291889 -0.409024 -0.307302
bar one 5  1.697974 -1.828872 -1.004187

In [14]: df = df.sortlevel(0)

In [15]: df
Out[15]: 
                  A         B         C
bar one 0 -0.694240  0.725163  0.131891
        5  1.697974 -1.828872 -1.004187
    two 1 -0.729186  0.244860  0.530870
baz one 2  0.757816  1.129989  0.893080
qux one 3 -2.275694  0.680023 -1.054816
    two 4  0.291889 -0.409024 -0.307302

In [16]: df.loc[('bar','two'),'A'] = 9999

In [17]: df
Out[17]: 
                     A         B         C
bar one 0    -0.694240  0.725163  0.131891
        5     1.697974 -1.828872 -1.004187
    two 1  9999.000000  0.244860  0.530870
baz one 2     0.757816  1.129989  0.893080
qux one 3    -2.275694  0.680023 -1.054816
    two 4     0.291889 -0.409024 -0.307302
</code></pre>

<p>You can also do it with out sorting if you specify the complete index, e.g.</p>

<pre><code>In [23]: df.loc[('bar','two',1),'A'] = 999

In [24]: df
Out[24]: 
                    A         B         C
bar one 0   -0.113216  0.878715 -0.183941
    two 1  999.000000 -1.405693  0.253388
baz one 2    0.441543  0.470768  1.155103
qux one 3   -0.008763  0.917800 -0.699279
    two 4    0.061586  0.537913  0.380175
bar one 5    0.857231  1.144246 -2.369694
</code></pre>

<p>To check the sort depth</p>

<pre><code>In [27]: df.index.lexsort_depth
Out[27]: 0

In [28]: df.sortlevel(0).index.lexsort_depth
Out[28]: 3
</code></pre>

<p>The last part of your question, assigning with a list (note that you must have the 
same number of elements as you are trying to replace), and this MUST be sorted for this to work</p>

<pre><code>In [12]: df.loc[('bar','one'),'A'] = [999,888]

In [13]: df
Out[13]:
          A     B     C
bar one 0 999.000000 -0.645641 0.369443
    5 888.000000 -0.990632 -0.577401
  two 1  -1.071410 2.308711 2.018476
baz one 2  1.211887 1.516925 0.064023
qux one 3  -0.862670 -0.770585 -0.843773
  two 4  -0.644855 -1.431962 0.232528
</code></pre>
";;2;;2013-05-30T11:27:18.597;;16834949;2013-05-30T11:49:55.537;2013-05-30T11:49:55.537;;644898.0;;644898.0;16833842.0;2;10;;;
4066;4066;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"">astype</a></p>

<pre><code>In [31]: df
Out[31]: 
   a        time
0  1  2013-01-01
1  2  2013-01-02
2  3  2013-01-03

In [32]: df['time'] = df['time'].astype('datetime64[ns]')

In [33]: df
Out[33]: 
   a                time
0  1 2013-01-01 00:00:00
1  2 2013-01-02 00:00:00
2  3 2013-01-03 00:00:00
</code></pre>
";;6;;2013-05-31T08:36:33.777;;16853161;2013-05-31T08:36:33.777;;;;;1426056.0;16852911.0;2;42;;;
4068;4068;;;"<p>Essentially equivalent to @waitingkuo, but I would use <code>to_datetime</code> here (it seems a little cleaner, and offers some additional functionality e.g. <code>dayfirst</code>):</p>

<pre><code>In [11]: df
Out[11]:
   a        time
0  1  2013-01-01
1  2  2013-01-02
2  3  2013-01-03

In [12]: pd.to_datetime(df['time'])
Out[12]:
0   2013-01-01 00:00:00
1   2013-01-02 00:00:00
2   2013-01-03 00:00:00
Name: time, dtype: datetime64[ns]

In [13]: df['time'] = pd.to_datetime(df['time'])

In [14]: df
Out[14]:
   a                time
0  1 2013-01-01 00:00:00
1  2 2013-01-02 00:00:00
2  3 2013-01-03 00:00:00
</code></pre>
";;9;;2013-05-31T09:46:26.553;;16854430;2013-05-31T09:46:26.553;;;;;1240268.0;16852911.0;2;58;;;
4077;4077;;;"<p>You could use the to_html() dataframe method to convert the dataframe to html and display it in your browser. Here is an example assuming you have a dataframe called df. You should check the documentation to see what other options are available in the to_html() method.</p>

<pre><code># Format floating point numbers with 2 decimal places.
data_table = df.to_html(float_format=lambda x: '%6.2f' % x,
    classes=""table display"")
# The to_html() method forces a html table border of 1 pixel.
# I use 0  in my table so I  change the html, since there is no 
# border argument in the to_html() method.
data_table = data_table.replace('border=""1""','border=""0""')
# I alson like to display blanks instead on nan.
data_table = data_table.replace('nan', '')
</code></pre>

<p>If you want to get the table to be nicely formatted and scrollable then you can use the datatables plug-in for jQuery <a href=""http://www.datatables.net/"" rel=""noreferrer"">www.datatables.net</a>. Here is the javascript I use to display a table the scrolls in both x and y directiions.</p>

<pre><code>$('.table').dataTable({
    ""bPaginate"": true,
    ""bLengthChange"": true,
    ""bSort"": false,
    ""bStateSave"": true,
    ""sScrollY"": 900,
    ""sScrollX"": 1000,
    ""aLengthMenu"": [[50, 100, 250, 500, 1000, -1], [50, 100, 250, 500, 1000, ""All""]],
    ""iDisplayLength"": 100,
});
</code></pre>
";;0;;2013-06-02T16:27:16.000;;16884805;2013-06-02T16:27:16.000;;;;;918951.0;10636024.0;2;7;;;
4083;4083;;;"<p>I usually create a dictionary containing a <code>DataFrame</code> for every sheet:</p>

<pre><code>xl_file = pd.ExcelFile(file_name)

dfs = {sheet_name: xl_file.parse(sheet_name) 
          for sheet_name in xl_file.sheet_names}
</code></pre>

<hr>

<p>Update: In pandas version 0.20.0+ (edit: perhaps 0.19.2 as well) you will get this behavior more cleanly by passing <a href=""https://github.com/pandas-dev/pandas/issues/9930"" rel=""nofollow noreferrer""><code>sheetname=None</code></a> to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"" rel=""nofollow noreferrer""><code>read_excel</code></a>:</p>

<pre><code>dfs = pd.read_excel(file_name, sheetname=None)
</code></pre>
";;6;;2013-06-03T11:52:16.097;;16896091;2017-01-26T05:16:10.080;2017-01-26T05:16:10.080;;1240268.0;;1240268.0;16888888.0;2;40;;;
4099;4099;;;"<p>To delimit by a tab you can use the <code>sep</code> argument of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html""><code>to_csv</code></a>:</p>

<pre><code>df.to_csv(file_name, sep='\t')
</code></pre>

<p>To use a specific encoding (e.g. 'utf-8') use the <code>encoding</code> argument:</p>

<pre><code>df.to_csv(file_name, sep='\t', encoding='utf-8')
</code></pre>
";;6;;2013-06-04T16:52:17.853;;16923367;2013-06-04T16:52:17.853;;;;;1240268.0;16923281.0;2;353;;;
4130;4130;;;"<p>There may be a more efficient way (I have a feeling <code>pandas.crosstab</code> would be useful here), but here's how I'd do it:</p>

<pre><code>import numpy as np
import pandas

df = pandas.DataFrame({""a"": np.random.random(100),
                       ""b"": np.random.random(100),
                       ""id"": np.arange(100)})

# Bin the data frame by ""a"" with 10 bins...
bins = np.linspace(df.a.min(), df.a.max(), 10)
groups = df.groupby(np.digitize(df.a, bins))

# Get the mean of each bin:
print groups.mean() # Also could do ""groups.aggregate(np.mean)""

# Similarly, the median:
print groups.median()

# Apply some arbitrary function to aggregate binned data
print groups.aggregate(lambda x: np.mean(x[x &gt; 0.5]))
</code></pre>

<hr>

<p>Edit: As the OP was asking specifically for just the means of <code>b</code> binned by the values in <code>a</code>, just do </p>

<pre><code>groups.mean().b
</code></pre>

<p>Also if you wanted the index to look nicer (e.g. display intervals as the index), as they do in @bdiamante's example, use <code>pandas.cut</code> instead of <code>numpy.digitize</code>.  (Kudos to bidamante. I didn't realize <code>pandas.cut</code> existed.)</p>

<pre><code>import numpy as np
import pandas

df = pandas.DataFrame({""a"": np.random.random(100), 
                       ""b"": np.random.random(100) + 10})

# Bin the data frame by ""a"" with 10 bins...
bins = np.linspace(df.a.min(), df.a.max(), 10)
groups = df.groupby(pandas.cut(df.a, bins))

# Get the mean of b, binned by the values in a
print groups.mean().b
</code></pre>

<p>This results in:</p>

<pre><code>a
(0.00186, 0.111]    10.421839
(0.111, 0.22]       10.427540
(0.22, 0.33]        10.538932
(0.33, 0.439]       10.445085
(0.439, 0.548]      10.313612
(0.548, 0.658]      10.319387
(0.658, 0.767]      10.367444
(0.767, 0.876]      10.469655
(0.876, 0.986]      10.571008
Name: b
</code></pre>
";;4;;2013-06-05T20:42:45.117;;16949498;2013-06-06T17:14:45.540;2013-06-06T17:14:45.540;;325565.0;;325565.0;16947336.0;2;41;;;
4131;4131;;;"<p>Not 100% sure if this is what you're looking for, but here's what I think you're getting at:</p>

<pre><code>In [144]: df = DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"":   np.arange(100)})

In [145]: bins = [0, .25, .5, .75, 1]

In [146]: a_bins = df.a.groupby(cut(df.a,bins))

In [147]: b_bins = df.b.groupby(cut(df.b,bins))

In [148]: a_bins.agg([mean,median])
Out[148]:
                 mean    median
a
(0, 0.25]    0.124173  0.114613
(0.25, 0.5]  0.367703  0.358866
(0.5, 0.75]  0.624251  0.626730
(0.75, 1]    0.875395  0.869843

In [149]: b_bins.agg([mean,median])
Out[149]:
                 mean    median
b
(0, 0.25]    0.147936  0.166900
(0.25, 0.5]  0.394918  0.386729
(0.5, 0.75]  0.636111  0.655247
(0.75, 1]    0.851227  0.838805
</code></pre>

<p>Of course, I don't know what bins you had in mind, so you'll have to swap mine out for your circumstance.</p>
";;2;;2013-06-05T20:42:58.407;;16949500;2013-06-05T20:42:58.407;;;;;1649780.0;16947336.0;2;19;;;
4141;4141;;;"<p>There appears to be a bug in the current version of Pandas ('0.11.0'), which means that Matti John's answer will not work.  If you specify columns  for writing to file, they are written in alphabetical order, but simply relabelled according to the list in cols.  For example, this code:</p>

<pre><code>import pandas
dfdict={}
dfdict[""a""]=[1,2,3,4]
dfdict[""b""]=[5,6,7,8]
dfdict[""c""]=[9,10,11,12]
df=pandas.DataFrame(dfdict)
df.to_csv(""dfTest.txt"",""\t"",header=True,cols=[""b"",""a"",""c""])
</code></pre>

<p>results in this (incorrect) output:</p>

<pre><code>    b   a   c
0   1   5   9
1   2   6   10
2   3   7   11
3   4   8   12
</code></pre>

<p>You can check which version of pandas you have installed by executing:</p>

<pre><code>pandas.version.version
</code></pre>

<p>Documentation for to_csv is <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer"">here</a></p>

<p>Actually, it seems that this is a known bug and will be fixed in an upcoming release (0.11.1):</p>

<p><a href=""https://github.com/pydata/pandas/issues/3489"" rel=""nofollow noreferrer"">https://github.com/pydata/pandas/issues/3489</a></p>

<p>UPDATE:  There still hasn't been a new release of pandas, but there is a workaround described here, which doesn't require using a different version of pandas: </p>

<p><a href=""https://github.com/pydata/pandas/issues/3454"" rel=""nofollow noreferrer"">github.com/pydata/pandas/issues/3454</a> </p>

<p>So changing the last line in the block of code above to the following will work correctly: </p>

<pre><code>df.to_csv(""dfTest.txt"",""\t"",header=True,cols=[""b"",""a"",""c""], engine='python')
</code></pre>

<p>UPDATE it seems that the argument ""cols"" has been renamed to ""columns"" and that the argument ""engine"" is deprecated (no longer available) in recent versions of pandas.  Also, this bug is fixed in version 0.19.0.</p>
";;3;;2013-06-06T09:28:50.193;;16958464;2017-05-14T19:44:26.577;2017-05-14T19:44:26.577;;2015542.0;;2015542.0;15653688.0;2;11;;;
4144;4144;;;"<p><strong>Note:</strong> <code>sort</code> is deprecated - use <code>sort_values</code> instead</p>

<p>To <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html"" rel=""noreferrer""><code>sort</code></a> descending use <code>ascending=False</code>:</p>

<pre><code>In [6]: df.sort('Bytes', ascending=False)
Out[6]:
   0  Bytes      Client Ip
1  1   2000  192.168.10.12
0  0   1000   192.168.10.2
2  2    500   192.168.10.4
3  3    159  192.168.10.56
</code></pre>

<p><em>To take the first 10 values use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html"" rel=""noreferrer""><code>.head(10)</code></a>.</em></p>
";;9;;2013-06-06T09:36:46.773;;16958649;2016-04-19T05:44:51.353;2016-04-19T05:44:51.353;;424153.0;;1240268.0;16958499.0;2;16;;;
4182;4182;;;"<p><em>Update: this has <a href=""https://github.com/pydata/pandas/issues/3795"" rel=""noreferrer"">been fixed</a>: from 0.11.1 you passing <code>str</code>/<code>np.str</code> will be equivalent to using <code>object</code>.</em></p>

<p>Use the object dtype:</p>

<pre><code>In [11]: pd.read_csv('a', dtype=object, index_col=0)
Out[11]:
                      A                     B
1A  0.35633069074776547     0.745585398803751
1B  0.20037376323337375  0.013921830784260236
</code></pre>

<p>or better yet, just don't specify a dtype:</p>

<pre><code>In [12]: pd.read_csv('a', index_col=0)
Out[12]:
           A         B
1A  0.356331  0.745585
1B  0.200374  0.013922
</code></pre>

<p><em>It's best to avoid the str dtype, see for example <a href=""https://stackoverflow.com/questions/16929056/pandas-read-csv-dtype-leading-zeros"">here</a>.</em></p>
";;6;;2013-06-07T16:14:05.277;;16988624;2013-06-07T21:53:12.667;2017-05-23T11:46:47.647;;-1.0;;1240268.0;16988526.0;2;17;;;
4187;4187;;;"<p>The most straightforward way I can see is to make them into a DataFrame and then take the row-wise min:</p>

<pre><code>&gt;&gt;&gt; print pandas.concat([s1, s2], axis=1).min(axis=1)
1    1
2    1
3    1
4    1
dtype: float64
</code></pre>
";;1;;2013-06-07T17:52:17.407;;16990140;2013-06-07T17:52:17.407;;;;;1427416.0;16989946.0;2;16;;;
4190;4190;;;"<p>Another similar way:</p>

<pre><code>In [11]: pd.DataFrame([s1, s2]).min()
Out[11]:
1    1
2    1
3    1
4    1
dtype: float64
</code></pre>
";;1;;2013-06-07T21:42:37.423;;16993415;2013-06-07T21:42:37.423;;;;;1240268.0;16989946.0;2;16;;;
4195;4195;;;"<p>You can do it like this. Only trick is that the first time the store table doesn't exist, so <code>get_storer</code> will raise.</p>

<pre><code>import pandas as pd
import numpy as np
import os

files = ['test1.csv','test2.csv']
for f in files:
    pd.DataFrame(np.random.randn(10,2),columns=list('AB')).to_csv(f)

path = 'test.h5'
if os.path.exists(path):
    os.remove(path)

with pd.get_store(path) as store:
    for f in files:
        df = pd.read_csv(f,index_col=0)
        try:
            nrows = store.get_storer('foo').nrows
        except:
            nrows = 0

        df.index = pd.Series(df.index) + nrows
        store.append('foo',df)


In [10]: pd.read_hdf('test.h5','foo')
Out[10]: 
           A         B
0   0.772017  0.153381
1   0.304131  0.368573
2   0.995465  0.799655
3  -0.326959  0.923280
4  -0.808376  0.449645
5  -1.336166  0.236968
6  -0.593523 -0.359080
7  -0.098482  0.037183
8   0.315627 -1.027162
9  -1.084545 -1.922288
10  0.412407 -0.270916
11  1.835381 -0.737411
12 -0.607571  0.507790
13  0.043509 -0.294086
14 -0.465210  0.880798
15  1.181344  0.354411
16  0.501892 -0.358361
17  0.633256  0.419397
18  0.932354 -0.603932
19 -0.341135  2.453220
</code></pre>

<p>You actually don't necessarily need a global unique index, (unless you want one) as <code>HDFStore</code> (through <code>PyTables</code>) provides one by uniquely numbering rows. You can always add these selection parameters.</p>

<pre><code>In [11]: pd.read_hdf('test.h5','foo',start=12,stop=15)
Out[11]: 
           A         B
12 -0.607571  0.507790
13  0.043509 -0.294086
14 -0.465210  0.880798
</code></pre>
";;5;;2013-06-08T12:10:01.417;;16999397;2013-06-08T13:20:49.970;2013-06-08T13:20:49.970;;1240268.0;;644898.0;16997048.0;2;12;;;
4199;4199;;;"<pre><code>B       business day frequency
C       custom business day frequency (experimental)
D       calendar day frequency
W       weekly frequency
M       month end frequency
SM      semi-month end frequency (15th and end of month)
BM      business month end frequency
CBM     custom business month end frequency
MS      month start frequency
SMS     semi-month start frequency (1st and 15th)
BMS     business month start frequency
CBMS    custom business month start frequency
Q       quarter end frequency
BQ      business quarter endfrequency
QS      quarter start frequency
BQS     business quarter start frequency
A       year end frequency
BA      business year end frequency
AS      year start frequency
BAS     business year start frequency
BH      business hour frequency
H       hourly frequency
T       minutely frequency
S       secondly frequency
L       milliseonds
U       microseconds
N       nanoseconds
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html"">timeseries documentation</a>. It includes a list of <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"">offsets</a> (and <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets"">'anchored' offsets</a>), and a section about <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#up-and-downsampling"">resampling</a>. </p>

<p>Note that there isn't a list of all the different <code>how</code> options, because it can be any NumPy array function and any function that is available via <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-dispatch"">groupby dispatching</a> can be passed to <code>how</code> by name.</p>
";;3;;2013-06-08T16:20:07.050;;17001474;2016-11-19T21:12:25.620;2016-11-19T21:12:25.620;;1452002.0;;1452002.0;17001389.0;2;175;;;
4209;4209;;;"<p>To get a Series from a namedtuple you could use the <code>_fields</code> attribute:</p>

<pre><code>In [11]: pd.Series(a, a._fields)
Out[11]:
ticker            GE
date      2010-01-01
price             30
dtype: object
</code></pre>

<p>Similarly you can create a DataFrame like this:</p>

<pre><code>In [12]: df = pd.DataFrame(l, columns=l[0]._fields)

In [13]: df
Out[13]:
  ticker        date  price
0     GE  2010-01-01     30
1     GE  2010-01-02     31
</code></pre>

<p>You have to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""noreferrer""><code>set_index</code></a> after the fact, but you can do this <code>inplace</code>:</p>

<pre><code>In [14]: df.set_index(['ticker', 'date'], inplace=True)

In [15]: df
Out[15]:
                   price
ticker date
GE     2010-01-01     30
       2010-01-02     31
</code></pre>
";;4;;2013-06-09T00:19:32.133;;17005204;2013-06-09T11:23:26.057;2013-06-09T11:23:26.057;;1240268.0;;1240268.0;17004985.0;2;16;;;
4229;4229;;;"<p>I think you can't achieve what you want in a more efficient manner than you proposed.</p>

<p>The underlying problem is that the timestamps (as you seem aware) are made up of two parts.  The data that represents the UTC time, and the timezone, tz_info.  The timezone information is used only for display purposes when printing the timezone to the screen.  At display time, the data is offset appropriately and +01:00 (or similar) is added to the string.  Stripping off the tz_info value (using tz_convert(tz=None)) doesn't doesn't actually change the data that represents the naive part of the timestamp.  </p>

<p>So, the only way to do what you want is to modify the underlying data (pandas doesn't allow this... DatetimeIndex are immutable -- see the help on DatetimeIndex), or to create a new set of timestamp objects and wrap them in a new DatetimeIndex.  Your solution does the latter:</p>

<pre><code>pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
</code></pre>

<p>For reference, here is the <code>replace</code> method of <code>Timestamp</code> (see tslib.pyx):</p>

<pre><code>def replace(self, **kwds):
    return Timestamp(datetime.replace(self, **kwds),
                     offset=self.offset)
</code></pre>

<p>You can refer to the docs on <code>datetime.datetime</code> to see that <code>datetime.datetime.replace</code> also creates a new object.   </p>

<p>If you can, your best bet for efficiency is to modify the source of the data so that it (incorrectly) reports the timestamps without their timezone.  You mentioned:</p>

<blockquote>
  <p>I want to work with timezone naive timeseries (to avoid the extra hassle with timezones, and I do not need them for the case I am working on)</p>
</blockquote>

<p>I'd be curious what extra hassle you are referring to.  I recommend as a general rule for all software development, keep your timestamp 'naive values' in UTC.  There is little worse than looking at two different int64 values wondering which timezone they belong to.  If you always, always, always use UTC for the internal storage, then you will avoid countless headaches.  My mantra is <strong>Timezones are for human I/O only</strong>.</p>
";;1;;2013-06-10T15:30:34.097;;17027507;2013-06-10T15:30:34.097;;;;;1638871.0;16628819.0;2;10;;;
4268;4268;;;"<p>You are actually trying to solve two problems: capturing real-time data and analyzing that data.  The first problem can be solved with <a href=""http://docs.python.org/2/library/logging.html"" rel=""noreferrer"">Python logging</a>, which is designed for this purpose.  Then the other problem can be solved by reading that same log file.</p>
";;3;;2013-06-12T00:40:02.933;;17056022;2013-06-12T00:40:02.933;;;;;584846.0;16740887.0;2;9;;;
4283;4283;;;"<p>Close: first you call <code>ExcelFile</code>, but then you call the <code>.parse</code> method and pass it the sheet name.</p>

<pre><code>&gt;&gt;&gt; xl = pd.ExcelFile(""dummydata.xlsx"")
&gt;&gt;&gt; xl.sheet_names
[u'Sheet1', u'Sheet2', u'Sheet3']
&gt;&gt;&gt; df = xl.parse(""Sheet1"")
&gt;&gt;&gt; df.head()
                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \
0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   
1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   
2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   
3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   
4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   

     dummy6  dummy7    dummy8    dummy9  
0  0.623354       0  2.579108  2.681728  
1  0.554211       0  7.210000  3.028614  
2  0.567841       0  6.940000  3.644147  
3  0.581470       0  6.630000  4.016155  
4  0.595100       0  6.350000  3.974442  
</code></pre>

<p>What you're doing is calling the method which lives on the class itself, rather than the instance, which is okay (although not very idiomatic), but if you're doing that you would also need to pass the sheet name:</p>

<pre><code>&gt;&gt;&gt; parsed = pd.io.parsers.ExcelFile.parse(xl, ""Sheet1"")
&gt;&gt;&gt; parsed.columns
Index([u'Tid', u'dummy1', u'dummy2', u'dummy3', u'dummy4', u'dummy5', u'dummy6', u'dummy7', u'dummy8', u'dummy9'], dtype=object)
</code></pre>
";;4;;2013-06-12T10:52:50.143;;17063653;2013-06-12T10:52:50.143;;;;;487339.0;17063458.0;2;104;;;
4287;4287;;;"<p>Here is the canonical way of doing it, while not necessarily more concise, is more flexible (in that you can apply this to arbitrary columns)</p>

<pre><code>In [39]: df = DataFrame(randn(5,1),columns=['value'])

In [40]: df
Out[40]: 
      value
0  0.092232
1 -0.472784
2 -1.857964
3 -0.014385
4  0.301531

In [41]: df.loc[df['value']&lt;0,'value'] = 0

In [42]: df
Out[42]: 
      value
0  0.092232
1  0.000000
2  0.000000
3  0.000000
4  0.301531
</code></pre>
";;4;;2013-06-12T14:44:08.180;;17068439;2013-06-12T15:17:45.297;2013-06-12T15:17:45.297;;644898.0;;644898.0;17068269.0;2;13;;;
4288;4288;;;"<p>You could use the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.clip.html#pandas-series-clip"">clip method</a>:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'value': np.arange(-5,5)})
df['value'] = df['value'].clip(0, None)
print(df)
</code></pre>

<p>yields</p>

<pre><code>   value
0      0
1      0
2      0
3      0
4      0
5      0
6      1
7      2
8      3
9      4
</code></pre>
";;9;;2013-06-12T14:44:48.590;;17068462;2013-06-12T15:00:58.177;2013-06-12T15:00:58.177;;190597.0;;190597.0;17068269.0;2;11;;;
4291;4291;;;"<p>To select rows whose column value equals a scalar, <code>some_value</code>, use <code>==</code>:</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>To select rows whose column value is in an iterable, <code>some_values</code>, use <code>isin</code>:</p>

<pre><code>df.loc[df['column_name'].isin(some_values)]
</code></pre>

<p>Combine multiple conditions with <code>&amp;</code>: </p>

<pre><code>df.loc[(df['column_name'] == some_value) &amp; df['other_column'].isin(some_values)]
</code></pre>

<hr>

<p>To select rows whose column value <em>does not equal</em> <code>some_value</code>, use <code>!=</code>:</p>

<pre><code>df.loc[df['column_name'] != some_value]
</code></pre>

<p><code>isin</code> returns a boolean Series, so to select rows whose value is <em>not</em> in <code>some_values</code>, negate the boolean Series using <code>~</code>:</p>

<pre><code>df.loc[~df['column_name'].isin(some_values)]
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>If you have multiple values you want to include, put them in a
list (or more generally, any iterable) and use <code>isin</code>:</p>

<pre><code>print(df.loc[df['B'].isin(['one','three'])])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>Note, however, that if you wish to do this many times, it is more efficient to
make an index first, and then use <code>df.loc</code>:</p>

<pre><code>df = df.set_index(['B'])
print(df.loc['one'])
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
one  foo  6  12
</code></pre>

<p>or, to include multiple values from the index use <code>df.index.isin</code>:</p>

<pre><code>df.loc[df.index.isin(['one','two'])]
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
two  foo  2   4
two  foo  4   8
two  bar  5  10
one  foo  6  12
</code></pre>
";;14;;2013-06-12T17:44:20.483;;17071908;2017-03-10T20:16:48.650;2017-03-10T20:16:48.650;;190597.0;;190597.0;17071871.0;2;755;;;
4293;4293;;;"<p>Select those with a specific column:</p>

<pre><code>In [11]: df[df['from_date'] == 19951227]
Out[11]:
   instrument        type  from_date   to_date
0    96000001   W/D &amp; V/L   19951227  19960102
1    96000002  DEED TRUST   19951227  19960102
</code></pre>

<p>Or combine several queries (you can use <code>|</code> for or)</p>

<pre><code>In [12]: df[(19951227 &lt;= df['from_date']) &amp; (df['to_date'] &lt;= 19960102)]
Out[12]:
   instrument         type  from_date   to_date
0    96000001    W/D &amp; V/L   19951227  19960102
1    96000002   DEED TRUST   19951227  19960102
2    96000003  WARNTY DEED   19951228  19960102
3    96000004   DEED TRUST   19951228  19960102
4    96000005    W/D &amp; V/L   19951228  19960102
</code></pre>

<p><em>Worth noting that these columns are not datetime/Timestamp objects...</em></p>

<p>To convert these columns to timestamps you could use:</p>

<pre><code>In [21]: pd.to_datetime(df['from_date'].astype(str))
Out[21]:
0   1995-12-27 00:00:00
1   1995-12-27 00:00:00
2   1995-12-28 00:00:00
3   1995-12-28 00:00:00
4   1995-12-28 00:00:00
Name: from_date, dtype: datetime64[ns]

In [22]: df['from_date'] = pd.to_datetime(df['from_date'].astype(str))

In [23]: pd.to_datetime(df['from_date'].astype(str))  # do same for to_date
</code></pre>

<p>And query via <em>string</em> representation of the date:</p>

<pre><code>In [24]: df['1995-12-27' == df['from_date']]
Out[24]:
   instrument        type           from_date   to_date
0    96000001   W/D &amp; V/L 1995-12-27 00:00:00  19960102
1    96000002  DEED TRUST 1995-12-27 00:00:00  19960102
</code></pre>
";;3;;2013-06-12T19:12:41.830;;17073442;2013-06-12T19:24:42.417;2013-06-12T19:24:42.417;;1240268.0;;1240268.0;17073150.0;2;8;;;
4300;4300;;;"<p>You don't need to create a new DataFrame instance! You can modify the index:</p>

<pre><code>In [11]: df.index = df.index.droplevel(2)

In [12]: df
Out[12]:
     A
1 1  8
  3  9
</code></pre>
";;0;;2013-06-13T10:41:22.240;;17085016;2013-06-13T10:41:22.240;;;;;1240268.0;17084579.0;2;27;;;
4301;4301;;;"<pre><code>df.reset_index(level=2, drop=True)
Out[29]: 
     A
1 1  8
  3  9
</code></pre>
";;1;;2013-06-13T10:43:36.060;;17085044;2013-06-13T10:43:36.060;;;;;1579844.0;17084579.0;2;28;;;
4302;4302;;;"<p>Here is a simple example  </p>

<pre><code>from pandas import DataFrame

# Create data set
d = {'Revenue':[100,111,222], 
     'Cost':[333,444,555]}
df = DataFrame(d)


# mask = Return True when the value in column ""Revenue"" is equal to 111
mask = df['Revenue'] == 111

print mask

# Result:
# 0    False
# 1     True
# 2    False
# Name: Revenue, dtype: bool


# Select * FROM df WHERE Revenue = 111
df[mask]

# Result:
#    Cost    Revenue
# 1  444     111
</code></pre>
";;0;;2013-06-13T11:49:00.543;;17086321;2013-06-13T11:49:00.543;;;;;1821873.0;17071871.0;2;9;;;
4306;4306;;;"<p><code>df['y']</code> will set a column</p>

<p>since you want to set a row, use <code>.loc</code></p>

<p>Note that <code>.ix</code> is equivalent here, yours failed because you tried to assign a dictionary
to each element of the row <code>y</code> probably not what you want; converting to a Series tells pandas
that you want to align the input (for example you then don't have to to specify all of the elements)</p>

<pre><code>In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])

In [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})

In [9]: df
Out[9]: 
     a    b    c    d
x  NaN  NaN  NaN  NaN
y    1    5    2    3
z  NaN  NaN  NaN  NaN
</code></pre>
";;11;;2013-06-13T16:19:28.230;;17092113;2013-06-13T16:19:28.230;;;;;644898.0;17091769.0;2;47;;;
4308;4308;;;"<p>This is a <a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"" rel=""noreferrer"">""gotcha"" in pandas (Support for integer NA)</a>, where integer columns with NaNs are converted to floats.</p>

<blockquote>
  <p>This trade-off is made largely for memory and performance reasons, and also so that the resulting Series continues to be numeric. One possibility is to use <code>dtype=object</code> arrays instead.</p>
</blockquote>
";;2;;2013-06-13T16:50:13.330;;17092718;2013-06-13T16:50:13.330;;;;;1240268.0;17092671.0;2;8;;;
4309;4309;;;"<p>The problem is that since you are assigning things by rows, but dtypes are grouped by columns, so things get cast to <code>object</code> dtype, which is not a good thing, you lose all efficiency. So one way is to convert which will coerce to float/int dtype as needed.</p>

<p>As we answered in another question, if you construct the frame all at once (or construct column by column) this step will not be needed</p>

<pre><code>In [23]: def convert(x):
   ....:     try:
   ....:         return x.astype(int)
   ....:     except:
   ....:         return x
   ....:     

In [24]: df.apply(convert)
Out[24]: 
    a   b   c   d
x  10  10 NaN  10
y   1   5   2   3
z   1   2   3   4

In [25]: df.apply(convert).dtypes
Out[25]: 
a      int64
b      int64
c    float64
d      int64
dtype: object

In [26]: df.apply(convert).to_csv('test.csv')

In [27]: !cat test.csv
,a,b,c,d
x,10,10,,10
y,1,5,2.0,3
z,1,2,3.0,4
</code></pre>
";;9;;2013-06-13T17:05:51.637;;17092986;2013-06-13T17:29:20.643;2013-06-13T17:29:20.643;;644898.0;;644898.0;17092671.0;2;7;;;
4314;4314;;;"<p>The first part is similar to Constantine, you can get the boolean of which rows are empty*:</p>

<pre><code>In [21]: ne = (df1 != df2).any(1)

In [22]: ne
Out[22]:
0    False
1     True
2     True
dtype: bool
</code></pre>

<p>Then we can see which entries have changed:</p>

<pre><code>In [23]: ne_stacked = (df1 != df2).stack()

In [24]: changed = ne_stacked[ne_stacked]

In [25]: changed.index.names = ['id', 'col']

In [26]: changed
Out[26]:
id  col
1   score         True
2   isEnrolled    True
    Comment       True
dtype: bool
</code></pre>

<p><em>Here the first entry is the index and the second the columns which has been changed.</em></p>

<pre><code>In [27]: difference_locations = np.where(df1 != df2)

In [28]: changed_from = df1.values[difference_locations]

In [29]: changed_to = df2.values[difference_locations]

In [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)
Out[30]:
               from           to
id col
1  score       1.11         1.21
2  isEnrolled  True        False
   Comment     None  On vacation
</code></pre>

<p>* Note: it's important that <code>df1</code> and <code>df2</code> share the same index here. To overcome this ambiguity, you can ensure you only look at the shared labels using <code>df1.index &amp; df2.index</code>, but I think I'll leave that as an exercise.</p>
";;7;;2013-06-13T19:39:44.703;;17095620;2013-06-13T20:56:51.863;2013-06-13T20:56:51.863;;1240268.0;;1240268.0;17095101.0;2;56;;;
4316;4316;;;"<pre><code>import pandas as pd
import io

texts = ['''\
id   Name   score                    isEnrolled                        Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.11                     False                           Graduated
113  Zoe    4.12                     True       ''',

         '''\
id   Name   score                    isEnrolled                        Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.21                     False                           Graduated
113  Zoe    4.12                     False                         On vacation''']


df1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,21,20])
df2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,21,20])
df = pd.concat([df1,df2]) 

print(df)
#     id  Name  score isEnrolled               Comment
# 0  111  Jack   2.17       True  He was late to class
# 1  112  Nick   1.11      False             Graduated
# 2  113   Zoe   4.12       True                   NaN
# 0  111  Jack   2.17       True  He was late to class
# 1  112  Nick   1.21      False             Graduated
# 2  113   Zoe   4.12      False           On vacation

df.set_index(['id', 'Name'], inplace=True)
print(df)
#           score isEnrolled               Comment
# id  Name                                        
# 111 Jack   2.17       True  He was late to class
# 112 Nick   1.11      False             Graduated
# 113 Zoe    4.12       True                   NaN
# 111 Jack   2.17       True  He was late to class
# 112 Nick   1.21      False             Graduated
# 113 Zoe    4.12      False           On vacation

def report_diff(x):
    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)

changes = df.groupby(level=['id', 'Name']).agg(report_diff)
print(changes)
</code></pre>

<p>prints</p>

<pre><code>                score    isEnrolled               Comment
id  Name                                                 
111 Jack         2.17          True  He was late to class
112 Nick  1.11 | 1.21         False             Graduated
113 Zoe          4.12  True | False     nan | On vacation
</code></pre>
";;3;;2013-06-13T20:41:36.187;;17096675;2013-06-13T20:41:36.187;;;;;190597.0;17095101.0;2;9;;;
4318;4318;;;"<p>Actually in later versions of pandas this will give a TypeError:</p>

<pre><code>df.replace('-', None)
TypeError: If ""to_replace"" and ""value"" are both None then regex must be a mapping
</code></pre>

<p>You can do it by passing either a list or a dictionary:</p>

<pre><code>In [11]: df.replace('-', df.replace(['-'], [None]) # or .replace('-', {0: None})
Out[11]:
      0
0  None
1     3
2     2
3     5
4     1
5    -5
6    -1
7  None
8     9
</code></pre>

<p>But I recommend using NaNs rather than None:</p>

<pre><code>In [12]: df.replace('-', np.nan)
Out[12]:
     0
0  NaN
1    3
2    2
3    5
4    1
5   -5
6   -1
7  NaN
8    9
</code></pre>
";;8;;2013-06-13T21:29:01.457;;17097397;2013-06-13T21:29:01.457;;;;;1240268.0;17097236.0;2;36;;;
4320;4320;;;"<p>You can use the invert (~) operator (which acts like a not for boolean data):</p>

<pre><code>~df[""col""].str.contains(word)
</code></pre>

<p><em>contains also accepts a regular expression...</em></p>
";;2;;2013-06-13T21:51:44.240;;17097777;2013-06-14T01:21:31.757;2013-06-14T01:21:31.757;;1240268.0;;1240268.0;17097643.0;2;31;;;
4323;4323;;;"<p>The easiest way is to <a href=""http://docs.python.org/2/library/pickle.html"" rel=""noreferrer"">pickle</a> it using <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#pickling"" rel=""noreferrer""><code>to_pickle</code></a>:</p>

<pre><code>df.to_pickle(file_name)  # where to save it, usually as a .pkl
</code></pre>

<p>Then you can load it back using:</p>

<pre><code>df = pd.read_pickle(file_name)
</code></pre>

<p><em>Note: before 0.11.1 <code>save</code> and <code>load</code> were the only way to do this (they are now deprecated in favor of <code>to_pickle</code> and <code>read_pickle</code> respectively).</em></p>

<hr>

<p>Another popular choice is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"" rel=""noreferrer"">HDF5</a> (<a href=""http://www.pytables.org"" rel=""noreferrer"">pytables</a>) which offers <a href=""https://stackoverflow.com/questions/16628329/hdf5-and-sqlite-concurrency-compression-i-o-performance"">very fast</a> access times for large datasets:</p>

<pre><code>store = HDFStore('store.h5')

store['df'] = df  # save it
store['df']  # load it
</code></pre>

<p><em>More advanced strategies are discussed in the <a href=""http://pandas-docs.github.io/pandas-docs-travis/#pandas-powerful-python-data-analysis-toolkit"" rel=""noreferrer"">cookbook</a>.</em></p>

<hr>

<p>Since 0.13 there's also <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#msgpack-experimental"" rel=""noreferrer"">msgpack</a> which may be be better for interoperability, as a faster alternative to JSON, or if you have python object/text-heavy data (see <a href=""https://stackoverflow.com/q/30651724/1240268"">this question</a>).</p>
";;2;;2013-06-13T23:13:34.110;;17098736;2017-03-29T22:40:22.427;2017-05-23T11:54:50.480;;-1.0;;1240268.0;17098654.0;2;170;;;
4324;4324;;;"<p>If I understand correctly, you're already using <code>pandas.read_csv()</code> but would like to speed up the development process so that you don't have to load the file in every time you edit your script, is that right? I have a few recommendations:</p>

<ol>
<li><p>you could load in only part of the CSV file using <code>pandas.read_csv(..., nrows=1000)</code> to only load the top bit of the table, while you're doing the development</p></li>
<li><p>use <a href=""http://ipython.org/"" rel=""nofollow noreferrer"">ipython</a> for an interactive session, such that you keep the pandas table in memory as you edit and reload your script.</p></li>
<li><p>convert the csv to an <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"" rel=""nofollow noreferrer"">HDF5 table</a></p></li>
<li><p><strong>updated</strong> use <code>DataFrame.to_feather()</code> and <code>pd.read_feather()</code> to store data in the R-compatible <a href=""https://github.com/wesm/feather"" rel=""nofollow noreferrer"">feather</a> binary format that is super fast (in my hands, slightly faster than <code>pandas.to_pickle()</code> on numeric data and much faster on string data).</p></li>
</ol>

<p>You might also be interested in <a href=""https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas"">this answer</a> on stackoverflow.</p>
";;0;;2013-06-13T23:28:07.657;;17098885;2017-06-30T17:43:18.457;2017-06-30T17:43:18.457;;843442.0;;28035.0;17098654.0;2;22;;;
4330;4330;;;"<p>cplcloud's answer works, but following code will give you a bit more structure so that you can start configuring more if you do not need the loop.</p>

<pre><code>fig, axes = plt.subplots(nrows=2, ncols=2)
fig.set_figheight(6)
fig.set_figwidth(8)
df[0].plot(ax=axes[0,0], style='r', label='Series'); axes[0,0].set_title(0)
df[1].plot(ax=axes[0,1]); axes[0,1].set_title(1)
df[2].plot(ax=axes[1,0]); axes[1,0].set_title(2)
df[3].plot(ax=axes[1,1]); axes[1,1].set_title(3)
fig.tight_layout()
</code></pre>

<p>Added some example on axis 0 to show how you can further configure it.</p>
";;0;;2013-06-14T10:13:23.807;;17106156;2013-06-14T10:13:23.807;;;;;2107677.0;14888473.0;2;9;;;
4342;4342;;;"<p>What about <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html"" rel=""noreferrer""><code>DataFrame.replace</code></a>?</p>

<pre><code>In [9]: mapping = {'set': 1, 'test': 2}

In [10]: df.replace({'set': mapping, 'tesst': mapping})
Out[10]: 
   Unnamed: 0 respondent  brand engine  country  aware  aware_2  aware_3  age  \
0           0          a  volvo      p      swe      1        0        1   23   
1           1          b  volvo   None      swe      0        0        1   45   
2           2          c    bmw      p       us      0        0        1   56   
3           3          d    bmw      p       us      0        1        1   43   
4           4          e    bmw      d  germany      1        0        1   34   
5           5          f   audi      d  germany      1        0        1   59   
6           6          g  volvo      d      swe      1        0        0   65   
7           7          h   audi      d      swe      1        0        0   78   
8           8          i  volvo      d       us      1        1        1   32   

  tesst set  
0     2   1  
1     1   2  
2     2   1  
3     1   2  
4     2   1  
5     1   2  
6     2   1  
7     1   2  
8     2   1  
</code></pre>

<p>As @Jeff pointed out in the comments, in pandas versions &lt; 0.11.1, manually tack <code>.convert_objects()</code> onto the end to properly convert tesst and set to <code>int64</code> columns, in case that matters in subsequent operations.</p>
";;4;;2013-06-14T18:41:32.443;;17115229;2016-06-03T10:41:25.863;2016-06-03T10:41:25.863;;620382.0;;1221924.0;17114904.0;2;18;;;
4347;4347;;;"<p>This splits the Seatblocks by space and gives each its own row.</p>

<pre><code>In [43]: df
Out[43]: 
   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt
0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60
1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300

In [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()

In [45]: s.index = s.index.droplevel(-1) # to line up with df's index

In [46]: s.name = 'Seatblocks' # needs a name to join

In [47]: s
Out[47]: 
0    2:218:10:4,6
1    1:13:36:1,12
1    1:13:37:1,13
Name: Seatblocks, dtype: object

In [48]: del df['Seatblocks']

In [49]: df.join(s)
Out[49]: 
   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks
0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6
1    31316     Lennon, John       25  F01      300  1:13:36:1,12
1    31316     Lennon, John       25  F01      300  1:13:37:1,13
</code></pre>

<p>Or, to give each colon-separated string in its own column:</p>

<pre><code>In [50]: df.join(s.apply(lambda x: Series(x.split(':'))))
Out[50]: 
   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3
0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6
1    31316     Lennon, John       25  F01      300  1   13  36  1,12
1    31316     Lennon, John       25  F01      300  1   13  37  1,13
</code></pre>

<p>This is a little ugly, but maybe someone will chime in with a prettier solution.</p>
";;6;;2013-06-14T20:44:53.527;;17116976;2014-04-25T14:54:40.533;2014-04-25T14:54:40.533;;1221924.0;;1221924.0;17116814.0;2;145;;;
4359;4359;;;"<pre><code>df.ix[df.type==7, ['var1001', 'var1002']] = 0
</code></pre>

<p>If you're doing it on all columns, you can just do <code>df.ix[df.type==7] = 0</code>.  Or of course if you have a list of the columns whose values you want to replace, you can pass that list in the second slot:</p>

<pre><code>columnsToReplace = ['var1001', 'var1002', ...]
df.ix[df.type==8, columnsToReplace] = 0
</code></pre>
";;2;;2013-06-15T22:07:10.760;;17128356;2013-08-22T18:05:58.623;2013-08-22T18:05:58.623;;1427416.0;;1427416.0;17128302.0;2;20;;;
4368;4368;;;"<p>The easiest way is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html""><code>to_datetime</code></a>:</p>

<pre><code>df['col'] = pd.to_datetime(df['col'])
</code></pre>

<p>It also offers a <code>dayfirst</code> argument for European times (but beware <a href=""https://github.com/pydata/pandas/issues/3341"">this isn't strict</a>).</p>

<p>Here it is in action:</p>

<pre><code>In [11]: pd.to_datetime(pd.Series(['05/23/2005']))
Out[11]:
0   2005-05-23 00:00:00
dtype: datetime64[ns]
</code></pre>

<p>You can pass a specific <a href=""https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior"">format</a>:</p>

<pre><code>In [12]: pd.to_datetime(pd.Series(['05/23/2005']), format=""%m/%d/%Y"")
Out[12]:
0   2005-05-23
dtype: datetime64[ns]
</code></pre>
";;12;;2013-06-16T15:18:23.183;;17134750;2015-12-24T22:47:55.870;2015-12-24T22:47:55.870;;1240268.0;;1240268.0;17134716.0;2;159;;;
4370;4370;;;"<p>You can append using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer""><code>to_csv</code></a> by passing a file which is <a href=""http://docs.python.org/2/library/functions.html#open"" rel=""nofollow noreferrer"">open in append mode</a>:</p>

<pre><code>with open(file_name, 'a') as f:
    df.to_csv(f, header=False)
</code></pre>

<p><em>Use <code>header=None</code>, so as not to append the column names.</em></p>

<p>In fact, pandas has a wrapper to do this in <code>to_csv</code> using the <code>mode</code> argument (<a href=""https://stackoverflow.com/a/17910713/1240268"">see Joe's answer</a>):</p>

<pre><code>df.to_csv(f, mode='a', header=False)
</code></pre>
";;9;;2013-06-16T15:52:32.907;;17135044;2013-08-14T17:50:43.460;2017-05-23T11:54:27.700;;-1.0;;1240268.0;17134942.0;2;24;;;
4381;4381;;;"<p>As of the 0.17.0 release, the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sort.html"" rel=""noreferrer""><code>sort</code></a> method was deprecated in favor of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""noreferrer""><code>sort_values</code></a>.  <code>sort</code> was completely removed in the 0.20.0 release. The arguments (and results) remain the same:</p>

<pre><code>df.sort_values(['a', 'b'], ascending=[True, False])
</code></pre>

<hr>

<p>You can use the ascending argument of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html"" rel=""noreferrer""><code>sort</code></a>:</p>

<pre><code>df.sort(['a', 'b'], ascending=[True, False])
</code></pre>

<p>For example:</p>

<pre><code>In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])

In [12]: df1.sort(['a', 'b'], ascending=[True, False])
Out[12]:
   a  b
2  1  4
7  1  3
1  1  2
3  1  2
4  3  2
6  4  4
0  4  3
9  4  3
5  4  1
8  4  1
</code></pre>

<hr>

<p>As commented by @renadeen</p>

<blockquote>
  <p>Sort isn't in place by default! So you should assign result of the sort method to a variable or add inplace=True to method call.</p>
</blockquote>

<p>that is, if you want to reuse df1 as a sorted DataFrame:</p>

<pre><code>df1 = df1.sort(['a', 'b'], ascending=[True, False])
</code></pre>

<p>or</p>

<pre><code>df1.sort(['a', 'b'], ascending=[True, False], inplace=True)
</code></pre>
";;6;;2013-06-17T06:43:07.113;;17141755;2017-05-18T19:10:08.750;2017-05-18T19:10:08.750;;1240268.0;;1240268.0;17141558.0;2;126;;;
4385;4385;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.replace.html"" rel=""noreferrer"">replace</a></p>

<pre><code>In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], 
                     [1, 2, 3, 4, 5]) 
Out[126]: 
      resp  A  B  C
   0     1  3  3  4
   1     2  4  3  4
   2     3  5  5  5
   3     4  2  3  2
   4     5  1  1  1
   5     6  3  4  1
   6     7  4  4  4
   7     8  5  5  5
   8     9  2  2  1
   9    10  1  1  1
</code></pre>
";;2;;2013-06-17T07:43:01.640;;17142595;2013-06-17T07:43:01.640;;;;;1426056.0;17142304.0;2;49;;;
4387;4387;;;"<p>If your Series was discrete you could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts</code></a>:</p>

<pre><code>In [11]: s = pd.Series([1, 1, 2, 1, 2, 2, 3])

In [12]: s.value_counts()
Out[12]:
2    3
1    3
3    1
dtype: int64
</code></pre>

<p><em>You can see that <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.hist.html"" rel=""noreferrer""><code>s.hist()</code></a> is essentially equivalent to <code>s.value_counts().plot()</code>.</em></p>

<p>If it was of floats an awful hacky solution could be to use groupby:</p>

<pre><code>s.groupby(lambda i: np.floor(2*s[i]) / 2).count()
</code></pre>
";;3;;2013-06-17T13:38:48.173;;17148934;2013-06-17T13:57:00.673;2013-06-17T13:57:00.673;;1240268.0;;1240268.0;17148787.0;2;7;;;
4388;4388;;;"<p>Since <code>hist</code> and <code>value_counts</code> don't use the Series' index, you may as well treat the Series like an ordinary array and use <code>np.histogram</code> directly. Then build a Series from the result.</p>

<pre><code>In [4]: s = Series(randn(100))

In [5]: counts, bins = np.histogram(s)

In [6]: Series(counts, index=bins[:-1])
Out[6]: 
-2.968575     1
-2.355032     4
-1.741488     5
-1.127944    26
-0.514401    23
 0.099143    23
 0.712686    12
 1.326230     5
 1.939773     0
 2.553317     1
dtype: int32
</code></pre>

<p>This is a really convenient way to organize the result of a histogram for subsequent computation.</p>

<p>To index by the <em>center</em> of each bin instead of the left edge, you could use <code>bins[:-1] + np.diff(bins)/2</code>.</p>
";;3;;2013-06-17T15:02:54.617;;17150734;2013-06-17T15:02:54.617;;;;;1221924.0;17148787.0;2;8;;;
4396;4396;;;"<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>

<pre><code>import psycopg2
conn = psycopg2.connect(""dbname='db' user='user' host='host' password='pass'"")
cur = conn.cursor()
cur.execute(""select instrument, price, date from my_prices"")
df = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])
</code></pre>

<p>then set index like</p>

<pre><code>df.set_index('date', drop=False)
</code></pre>

<p>or directly:</p>

<pre><code>df.index =  df['date']
</code></pre>
";;3;;2013-06-17T20:23:30.437;;17156183;2013-06-17T20:23:30.437;;;;;1083888.0;17156084.0;2;10;;;
4397;4397;;;"<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>

<p>First create a db engine (a connection can also work here):</p>

<pre><code>from sqlalchemy import create_engine
# see sqlalchemy docs for how to write this url for your database type:
engine = create_engine('mysql://scott:tiger@localhost/foo')
</code></pre>

<p><em>See <a href=""http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls"" rel=""nofollow noreferrer"">sqlalchemy database urls</a>.</em></p>

<h3><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table"" rel=""nofollow noreferrer"">pandas_read_sql_table</a></h3>

<pre><code>table_name = 'my_prices'
df = pd.read_sql_table(table_name, engine)
</code></pre>

<h3><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query"" rel=""nofollow noreferrer"">pandas_read_sql_query</a></h3>

<pre><code>df = pd.read_sql_query(""SELECT instrument, price, date FROM my_prices;"", engine)
</code></pre>

<hr>

<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=""https://stackoverflow.com/posts/17156233/revisions"">version history</a> of this question for that answer).</p>

<hr>

<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pivot</code></a> the result:</p>

<pre><code>df.reset_index().pivot('date', 'instrument', 'price')
</code></pre>

<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>
";;7;;2013-06-17T20:26:51.660;;17156233;2016-10-12T18:27:02.160;2017-05-23T12:34:34.613;;-1.0;;1240268.0;17156084.0;2;13;;;
4403;4403;;;"<p>One way to do it is to use the minor ticks.  The idea is that you set the minor ticks so that they are located halfway between the major ticks, and you manually specify the labels.</p>

<p>For example:</p>

<pre><code>import matplotlib.ticker as ticker

# a is an axes object, e.g. from figure.get_axes()

# Hide major tick labels
a.xaxis.set_major_formatter(ticker.NullFormatter())

# Customize minor tick labels
a.xaxis.set_minor_locator(ticker.FixedLocator([1.5,2.5,3.5,4.5,5.5]))
a.xaxis.set_minor_formatter(ticker.FixedFormatter(['1','2','3','4','5']))
</code></pre>

<p>The three lines:</p>

<ul>
<li>""Hide"" the 1,2,3,4,... that you have on the major ticks</li>
<li>Set minor ticks halfway between the major ticks (assuming your major ticks are at 1,2,3...)</li>
<li>Manually specifies the labels for the minor ticks.  Here, '1' would be between 1.0 and 2.0 on the graph.</li>
</ul>

<p>This is just a simple example.  You would probably want to streamline it a bit by populating the lists in a loop or something.</p>

<p>You can also experiment with other <a href=""http://matplotlib.org/api/ticker_api.html"" rel=""noreferrer"">locators or formatters</a>.</p>

<p><strong>Edit:</strong> Alternatively, as suggested in the comments:</p>

<pre><code># Hide major tick labels
a.set_xticklabels('')

# Customize minor tick labels
a.set_xticks([1.5,2.5,3.5,4.5,5.5],      minor=True)
a.set_xticklabels(['1','2','3','4','5'], minor=True)
</code></pre>

<hr>

<h3>Example:</h3>

<p><strong>Before:</strong>
<img src=""https://i.stack.imgur.com/LoiF5.png"" alt=""Before""></p>

<p><strong>After:</strong>
<img src=""https://i.stack.imgur.com/vNWuD.png"" alt=""enter image description here""></p>
";;4;;2013-06-18T00:03:51.267;;17158735;2016-09-10T12:50:01.683;2016-09-10T12:50:01.683;;736937.0;;736937.0;17158382.0;2;15;;;
4405;4405;;;"<p>If you set it as the index, it's automatically converted to an Index:</p>

<pre><code>In [11]: dat.index = pd.to_datetime(dat.pop('datetime'), utc=True)

In [12]: dat
Out[12]:
                    label  value
datetime
2011-07-19 07:00:00     a      0
2011-07-19 08:00:00     a      1
2011-07-19 09:00:00     a      2
2011-07-19 07:00:00     b      3
2011-07-19 08:00:00     b      4
2011-07-19 09:00:00     b      5
</code></pre>

<p>Then do the <code>tz_localize</code>:</p>

<pre><code>In [12]: dat.index = dat.index.tz_localize('UTC').tz_convert('US/Pacific')

In [13]: dat
Out[13]:
                          label  value
datetime
2011-07-19 00:00:00-07:00     a      0
2011-07-19 01:00:00-07:00     a      1
2011-07-19 02:00:00-07:00     a      2
2011-07-19 00:00:00-07:00     b      3
2011-07-19 01:00:00-07:00     b      4
2011-07-19 02:00:00-07:00     b      5
</code></pre>

<p><strike>And then you can append the label column to the index:</strike></p>

<p><em>Hmmm this is definitely a bug!</em></p>

<pre><code>In [14]: dat.set_index('label', append=True).swaplevel(0, 1)
Out[14]:
                           value
label datetime
a     2011-07-19 07:00:00      0
      2011-07-19 08:00:00      1
      2011-07-19 09:00:00      2
b     2011-07-19 07:00:00      3
      2011-07-19 08:00:00      4
      2011-07-19 09:00:00      5
</code></pre>

<p>A hacky workaround is to convert the (datetime) level directly (when it's already a MultiIndex):</p>

<pre><code>In [15]: dat.index.levels[1] = dat.index.get_level_values(1).tz_localize('UTC').tz_convert('US/Pacific')

In [16]: dat1
Out[16]:
                                 value
label datetime
a     2011-07-19 00:00:00-07:00      0
      2011-07-19 01:00:00-07:00      1
      2011-07-19 02:00:00-07:00      2
b     2011-07-19 00:00:00-07:00      3
      2011-07-19 01:00:00-07:00      4
      2011-07-19 02:00:00-07:00      5
</code></pre>
";;2;;2013-06-18T01:24:42.940;;17159276;2013-06-18T20:55:08.313;2013-06-18T20:55:08.313;;1240268.0;;1240268.0;17159207.0;2;14;;;
4411;4411;;;"<p>If you know each of these are the same length then you could create the DataFrame directly from the array and then append each column:</p>

<pre><code>df = pd.DataFrame(prcpSeries, columns=['prcp'])
df['tmax'] = tmaxSeries
...
</code></pre>

<hr>

<p>Note: you can also use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.to_frame.html""><code>to_frame</code></a> method (which allows you to (optionally) pass a name - which is useful if the Series doesn't have one):</p>

<pre><code>df = prcpSeries.to_frame(name='prcp')
</code></pre>

<hr>

<p>However, if they are variable length then this will lose some data (any arrays which are longer than <code>prcpSeries</code>). An alternative here is to create each as a DataFrame and then perform an outer join (using <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html""><code>concat</code></a>):</p>

<pre><code>df1 = pd.DataFrame(prcpSeries, columns=['prcp'])
df2 = pd.DataFrame(tmaxSeries, columns=['tmax'])
...

df = pd.concat([df1, df2, ...], join='outer', axis=1)
</code></pre>

<p>For example:</p>

<pre><code>In [21]: dfA = pd.DataFrame([1,2], columns=['A'])

In [22]: dfB = pd.DataFrame([1], columns=['B'])

In [23]: pd.concat([dfA, dfB], join='outer', axis=1)
Out[23]:
   A   B
0  1   1
1  2 NaN
</code></pre>
";;3;;2013-06-18T13:11:22.323;;17169776;2015-01-04T18:52:44.037;2015-01-04T18:52:44.037;;1240268.0;;1240268.0;17165340.0;2;16;;;
4414;4414;;;"<p>I have found that <code>np.random.choice()</code> new in NumPy 1.7.0 works quite well for this.</p>

<p>For example you can pass the index values from a DataFrame and and the integer 10 to select 10 random uniformly sampled rows.</p>

<pre><code>rows = np.random.choice(df.index.values, 10)
sampled_df = df.ix[rows]
</code></pre>
";;5;;2013-06-18T14:41:39.997;;17171819;2013-06-18T14:41:39.997;;;;;2497610.0;12190874.0;2;75;;;
4429;4429;;;"<p>One is a column (aka Series), while the other is a DataFrame:</p>

<pre><code>In [1]: df = pd.DataFrame([[1,2], [3,4]], columns=['a', 'b'])

In [2]: df
Out[2]:
   a  b
0  1  2
1  3  4
</code></pre>

<p>The column 'b' (aka Series):</p>

<pre><code>In [3]: df['b']
Out[3]:
0    2
1    4
Name: b, dtype: int64
</code></pre>

<p>The subdataframe with columns (position) in [1]:</p>

<pre><code>In [4]: df[[1]]
Out[4]:
   b
0  2
1  4
</code></pre>

<p>Note: it's preferable (and less ambiguous) to specify whether you're talking about the column name e.g. ['b'] or the integer location, since sometimes you can have columns named as integers:</p>

<pre><code>In [5]: df.iloc[:, [1]]
Out[5]:
   b
0  2
1  4

In [6]: df.loc[:, ['b']]
Out[6]:
   b
0  2
1  4

In [7]: df.loc[:, 'b']
Out[7]:
0    2
1    4
Name: b, dtype: int64
</code></pre>
";;9;;2013-06-19T14:37:41.087;;17194149;2013-06-19T14:44:12.147;2013-06-19T14:44:12.147;;1240268.0;;1240268.0;17193850.0;2;18;;;
4451;4451;;;"<p>You can <a href=""http://docs.python.org/2/library/warnings.html#the-warnings-filter"" rel=""noreferrer"">filter the warnings</a> to raise which will enable you to debug (e.g. using pdb):</p>

<pre><code>import warnings
warnings.filterwarnings('error')
</code></pre>

<p>*The <a href=""http://docs.python.org/2/library/warnings.html#warnings.filterwarnings"" rel=""noreferrer"">warnings filter</a> can be managed more finely (which is probably more appropriate) e.g.:</p>

<pre><code>warnings.filterwarnings('error', category=UnicodeWarning)
warnings.filterwarnings('error', message='*equal comparison failed*')
</code></pre>

<p><em>Multiple filters will be looked up sequentially. (""Entries closer to the front of the list override entries later in the list, if both match a particular warning."")</em></p>
";;2;;2013-06-20T10:48:35.743;;17211698;2013-06-20T10:48:35.743;;;;;1240268.0;17208567.0;2;11;;;
4460;4460;;;"<p>It is a precedence operator issue.</p>

<p>You should add extra parenthesis to make your multi condition test working:</p>

<pre><code>d[(d['x']&gt;2) &amp; (d['y']&gt;7)]
</code></pre>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">This section</a> of the tutorial you mentioned shows an example with several boolean conditions and the parenthesis are used.</p>
";;0;;2013-06-20T14:43:20.290;;17216674;2017-06-15T20:41:52.533;2017-06-15T20:41:52.533;;832230.0;;624829.0;17216153.0;2;39;;;
4485;4485;;;"<p>You can use <code>df.index</code> to access the index object and then get the values in a list using <code>df.index.tolist()</code>.</p>
";;4;;2013-06-21T17:32:23.687;;17241104;2017-05-20T07:51:40.650;2017-05-20T07:51:40.650;;2285236.0;;1649780.0;17241004.0;2;33;;;
4486;4486;;;"<p>You should use the <code>values</code> attribute:</p>

<pre><code>In [1]: df = pd.DataFrame(index=['a', 'b'])

In [2]: df.index.values
Out[2]: array(['a', 'b'], dtype=object)
</code></pre>

<p><em>This accesses how the data is already stored, so there's no need for a conversion.<br>
Note: This attribute is also available for many other pandas' objects.</em></p>
";;2;;2013-06-21T18:51:13.090;;17242374;2013-06-21T19:01:27.673;2013-06-21T19:01:27.673;;1240268.0;;1240268.0;17241004.0;2;132;;;
4493;4493;;;"<p>A hack would be to change the order of the levels:</p>

<pre><code>In [11]: g
Out[11]:
                                               Sales
Manufacturer Product Name Product Launch Date
Apple        iPad         2010-04-03              30
             iPod         2001-10-23              34
Samsung      Galaxy       2009-04-27              24
             Galaxy Tab   2010-09-02              22

In [12]: g.index = g.index.swaplevel(1, 2)
</code></pre>

<p>Sortlevel, which (as you've found) sorts the MultiIndex levels in order:</p>

<pre><code>In [13]: g = g.sortlevel()
</code></pre>

<p>And swap back:</p>

<pre><code>In [14]: g.index = g.index.swaplevel(1, 2)

In [15]: g
Out[15]:
                                               Sales
Manufacturer Product Name Product Launch Date
Apple        iPod         2001-10-23              34
             iPad         2010-04-03              30
Samsung      Galaxy       2009-04-27              24
             Galaxy Tab   2010-09-02              22
</code></pre>

<p><em>I'm of the opinion that sortlevel should not sort the remaining labels in order, so will create a github issue.</em> :) Although it's worth mentioning the docnote about <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#the-need-for-sortedness"" rel=""noreferrer"">""the need for sortedness""</a>.</p>

<p>Note: you could avoid the first <code>swaplevel</code> by reordering the order of the initial groupby:</p>

<pre><code>g = df.groupby(['Manufacturer', 'Product Launch Date', 'Product Name']).sum()
</code></pre>
";;4;;2013-06-21T19:58:39.647;;17243346;2013-06-21T20:10:00.023;2013-06-21T20:10:00.023;;1240268.0;;1240268.0;17242970.0;2;7;;;
4501;4501;;;"<p>You're looking for the index method <code>get_loc</code>:</p>

<pre><code>In [11]: df.index.get_loc(ds)
Out[11]: 1
</code></pre>
";;2;;2013-06-21T20:49:15.180;;17244095;2013-06-21T20:49:15.180;;;;;1240268.0;17244049.0;2;24;;;
4527;4527;;;"<pre><code>In [13]: df[df &gt; df.quantile(0.8)].dropna()
Out[13]: 
       data
c  0.860467
j  1.887577

In [14]: list(df[df &gt; df.quantile(0.8)].dropna().index)
Out[14]: ['c', 'j']
</code></pre>
";;0;;2013-06-24T23:55:58.377;;17287046;2013-06-24T23:55:58.377;;;;;1221924.0;17286672.0;2;24;;;
4539;4539;;;"<p>You're looking for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.pivot.pivot_table.html"" rel=""noreferrer""><code>pivot_table</code></a>:</p>

<pre><code>In [11]: medals = df.pivot_table('no of medals', ['Year', 'Country'], 'medal')

In [12]: medals
Out[12]:
medal             Bronze  Gold  Silver
Year Country
1896 Afghanistan       3     5       4
     Algeria           3     1       2
</code></pre>

<p>and if you want to reorder the columns:</p>

<pre><code>In [12]: medals.reindex_axis(['Gold', 'Silver', 'Bronze'], axis=1)
Out[12]:
medal             Gold  Silver  Bronze
Year Country
1896 Afghanistan     5       4       3
     Algeria         1       2       3
</code></pre>
";;1;;2013-06-25T13:18:36.270;;17298454;2013-06-25T13:18:36.270;;;;;1240268.0;17298313.0;2;28;;;
4543;4543;;;"<p>Wes McKinney (pandas' author) in Python for Data Analysis provides the following recipe:</p>

<pre><code>groups = dict(list(gb))
</code></pre>

<p>which returns a dictionary whose keys are your group labels and whose values are DataFrames, i.e.</p>

<pre><code>groups['foo']
</code></pre>

<p>will yield what you are looking for:</p>

<pre><code>     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>
";;4;;2013-06-25T16:27:52.617;;17302673;2013-09-01T09:31:45.060;2013-09-01T09:31:45.060;;1240268.0;;887186.0;14734533.0;2;43;;;
4557;4557;;;"<p>Use <code>np.array_split</code>:</p>

<pre><code>Docstring:
Split an array into multiple sub-arrays.

Please refer to the ``split`` documentation.  The only difference
between these functions is that ``array_split`` allows
`indices_or_sections` to be an integer that does *not* equally
divide the axis.
</code></pre>

<hr>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
   ...:                           'foo', 'bar', 'foo', 'foo'],
   ...:                    'B' : ['one', 'one', 'two', 'three',
   ...:                           'two', 'two', 'one', 'three'],
   ...:                    'C' : randn(8), 'D' : randn(8)})

In [3]: print df
     A      B         C         D
0  foo    one -0.174067 -0.608579
1  bar    one -0.860386 -1.210518
2  foo    two  0.614102  1.689837
3  bar  three -0.284792 -1.071160
4  foo    two  0.843610  0.803712
5  bar    two -1.514722  0.870861
6  foo    one  0.131529 -0.968151
7  foo  three -1.002946 -0.257468

In [4]: np.array_split(df, 3)
Out[4]: 
[     A    B         C         D
0  foo  one -0.174067 -0.608579
1  bar  one -0.860386 -1.210518
2  foo  two  0.614102  1.689837,
      A      B         C         D
3  bar  three -0.284792 -1.071160
4  foo    two  0.843610  0.803712
5  bar    two -1.514722  0.870861,
      A      B         C         D
6  foo    one  0.131529 -0.968151
7  foo  three -1.002946 -0.257468]
</code></pre>
";;8;;2013-06-26T09:07:14.693;;17315875;2013-06-26T09:07:14.693;;;;;1199589.0;17315737.0;2;43;;;
4567;4567;;;"<p>You are asking for the condition where all the conditions are true,
so len of the frame is the answer, unless I misunderstand what you are asking</p>

<pre><code>In [17]: df = DataFrame(randn(20,4),columns=list('ABCD'))

In [18]: df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)]
Out[18]: 
           A         B         C         D
12  0.491683  0.137766  0.859753 -1.041487
13  0.376200  0.575667  1.534179  1.247358
14  0.428739  1.539973  1.057848 -1.254489

In [19]: df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)].count()
Out[19]: 
A    3
B    3
C    3
D    3
dtype: int64

In [20]: len(df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)])
Out[20]: 3
</code></pre>
";;1;;2013-06-26T14:14:12.050;;17322585;2013-06-26T14:14:12.050;;;;;644898.0;17322109.0;2;22;;;
4597;4597;;;"<p>How about:</p>

<p><code>df.index.is_monotonic</code></p>
";;3;;2013-06-27T16:01:11.140;;17347945;2013-06-27T16:01:11.140;;;;;776560.0;17315881.0;2;26;;;
4636;4636;;;"<p><code>True</code> is <code>1</code> in Python, and likewise <code>False</code> is <code>0</code><sup>*</sup>:</p>

<pre><code>&gt;&gt;&gt; True == 1
True
&gt;&gt;&gt; False == 0
True
</code></pre>

<p>You should be able to perform any operations you want on them by just treating them as though they were numbers, as they <em>are</em> numbers:</p>

<pre><code>&gt;&gt;&gt; issubclass(bool, int)
True
&gt;&gt;&gt; True * 5
5
</code></pre>

<p>So to answer your question, no work necessary - you already have what you are looking for.</p>

<p><sup>* Note I use <em>is</em> as an English word, not the Python keyword <code>is</code> - <code>True</code> will not be the same object as any random <code>1</code>.</sup></p>
";;3;;2013-06-29T17:58:36.213;;17383140;2013-06-29T18:04:21.843;2013-06-29T18:04:21.843;;722121.0;;722121.0;17383094.0;2;13;;;
4637;4637;;;"<p>You also can do this directly on Frames</p>

<pre><code>In [104]: df = DataFrame(dict(A = True, B = False),index=range(3))

In [105]: df
Out[105]: 
      A      B
0  True  False
1  True  False
2  True  False

In [106]: df.dtypes
Out[106]: 
A    bool
B    bool
dtype: object

In [107]: df.astype(int)
Out[107]: 
   A  B
0  1  0
1  1  0
2  1  0

In [108]: df.astype(int).dtypes
Out[108]: 
A    int64
B    int64
dtype: object
</code></pre>
";;0;;2013-06-29T18:17:18.243;;17383325;2013-06-29T18:17:18.243;;;;;644898.0;17383094.0;2;12;;;
4683;4683;;;"<pre><code>In [9]: Series(df.Letter.values,index=df.Position).to_dict()
Out[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
</code></pre>

<p>Speed comparion (using Wouter's method)</p>

<pre><code>In [6]: df = DataFrame(randint(0,10,10000).reshape(5000,2),columns=list('AB'))

In [7]: %timeit dict(zip(df.A,df.B))
1000 loops, best of 3: 1.27 ms per loop

In [8]: %timeit Series(df.A.values,index=df.B).to_dict()
1000 loops, best of 3: 987 us per loop
</code></pre>
";;10;;2013-07-02T13:08:23.243;;17426500;2013-07-02T14:12:55.977;2013-07-02T14:12:55.977;;644898.0;;644898.0;17426292.0;2;28;;;
4702;4702;;;"<p>Starting from</p>

<pre><code>&gt;&gt;&gt; df
              val1  val2  val3
city_id                       
houston,tx       1     2     0
houston,tx       0     0     1
houston,tx       2     1     1
somewhere,ew     4     3     7
</code></pre>

<p>I might do</p>

<pre><code>&gt;&gt;&gt; df.groupby(df.index).sum()
              val1  val2  val3
city_id                       
houston,tx       3     3     2
somewhere,ew     4     3     7
</code></pre>

<p>or</p>

<pre><code>&gt;&gt;&gt; df.reset_index().groupby(""city_id"").sum()
              val1  val2  val3
city_id                       
houston,tx       3     3     2
somewhere,ew     4     3     7
</code></pre>

<p>The first approach passes the index values (in this case, the <code>city_id</code> values) to <code>groupby</code> and tells it to use those as the group keys, and the second resets the index and then selects the <code>city_id</code> column.  See <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#aggregation"">this section</a> of the docs for more examples.  Note that there are lots of other methods in the <code>DataFrameGroupBy</code> objects, too:</p>

<pre><code>&gt;&gt;&gt; df.groupby(df.index)
&lt;pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790&gt;
&gt;&gt;&gt; df.groupby(df.index).max()
              val1  val2  val3
city_id                       
houston,tx       2     2     1
somewhere,ew     4     3     7
&gt;&gt;&gt; df.groupby(df.index).mean()
              val1  val2      val3
city_id                           
houston,tx       1     1  0.666667
somewhere,ew     4     3  7.000000
</code></pre>
";;0;;2013-07-03T04:29:01.370;;17439693;2013-07-03T04:29:01.370;;;;;487339.0;17438906.0;2;39;;;
4745;4745;;;"<p>You should add <code>parse_dates=True</code>, or <code>parse_dates=['column name']</code> when reading, thats usually enough to magically parse it. But there are always weird formats which need to be defined manually. In such a case you can also add a date parser function, which is the most flexible way possible.</p>

<p>Suppose you have a column 'datetime' with your string, then:</p>

<pre><code>dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)
</code></pre>

<p>This way you can even combine multiple columns into a single datetime column, this merges a 'date' and a 'time' column into a single 'datetime' column:</p>

<pre><code>dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)
</code></pre>
";;2;;2013-07-04T10:32:01.837;;17468012;2013-07-04T11:54:33.973;2013-07-04T11:54:33.973;;1755432.0;;1755432.0;17465045.0;2;110;;;
4746;4746;;;"<p>pandas read_csv method is great for parsing dates.  Complete documentation at <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>

<p>you can even have the different date parts in different columns and pass the parameter:</p>

<pre><code>parse_dates : boolean, list of ints or names, list of lists, or dict
If True -&gt; try parsing the index. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3 each as a
separate date column. If [[1, 3]] -&gt; combine columns 1 and 3 and parse as a single date
column. {foo : [1, 3]} -&gt; parse columns 1, 3 as date and call result foo
</code></pre>

<p>The default sensing of dates works great, but it seems to be biased towards north american Date formats.   If you live elsewhere you might occasionally be caught by the results.   As far as I can remember 1/6/2000  means 6 January in the USA as opposed to 1 Jun where I live.  It is smart enough to swing them around if dates like 23/6/2000 are used.  Probably safer to stay with YYYYMMDD variations of date though.   Apologies to pandas developers,here but i have not tested it with local dates recently.</p>

<p>you can use the date_parser parameter to pass a function to convert your format.</p>

<pre><code>date_parser : function
Function to use for converting a sequence of string columns to an array of datetime
instances. The default uses dateutil.parser.parser to do the conversion.
</code></pre>
";;3;;2013-07-04T10:38:47.637;;17468154;2013-07-04T10:38:47.637;;;;;2107677.0;17465045.0;2;10;;;
4758;4758;;;"<p>The simplest way would be to first <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html""><code>replace</code></a> infs to NaN:</p>

<pre><code>df.replace([np.inf, -np.inf], np.nan)
</code></pre>

<p>and then use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html""><code>dropna</code></a>:</p>

<pre><code>df.replace([np.inf, -np.inf], np.nan).dropna(subset=[""col1"", ""col2""], how=""all"")
</code></pre>

<p>For example:</p>

<pre><code>In [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])

In [12]: df.replace([np.inf, -np.inf], np.nan)
Out[12]:
    0
0   1
1   2
2 NaN
3 NaN
</code></pre>

<p><em>The same method would work for a Series.</em></p>
";;0;;2013-07-04T21:50:51.893;;17478495;2013-07-04T21:50:51.893;;;;;1240268.0;17477979.0;2;116;;;
4765;4765;;;"<p>Recent pandas have a higher level <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries""><code>read_sql</code></a> functions that can do this for you</p>

<pre><code>import pyodbc
import pandas as pd

cnxn = pyodbc.connect(databasez)
DF = pd.read_sql_query(""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"", cnxn)
</code></pre>
";;1;;2013-07-05T14:52:48.283;;17491690;2015-06-13T10:48:51.347;2015-06-13T10:48:51.347;;653364.0;;1097516.0;12704305.0;2;15;;;
4775;4775;;;"<p>You could create a list of dictionaries, where each dictionary corresponds to an input data row. Once the list is complete, then create a data frame. This is a much faster approach. </p>

<p>I had a similar problem where if I created a data frame for each row and appended it to the main data frame it took 30 mins. On the other hand, if I used the below methodology, it was successful within seconds.</p>

<pre><code>rows_list = []
for row in input_rows:

        dict1 = {}
        # get input row in dictionary format
        # key = col_name
        dict1.update(blah..) 

        rows_list.append(dict1)

df = pd.DataFrame(rows_list)               
</code></pre>
";;6;;2013-07-05T20:38:13.783;;17496530;2017-05-07T10:10:43.947;2017-05-07T10:10:43.947;;243392.0;;2441506.0;10715965.0;2;139;;;
4818;4818;;;"<p>You can <em>append</em> to a csv by <a href=""http://docs.python.org/2/library/functions.html#open"">opening the file</a> in append mode:</p>

<pre><code>with open('my_csv.csv', 'a') as f:
    df.to_csv(f, header=False)
</code></pre>

<p>If this was your csv, <code>foo.csv</code>:</p>

<pre><code>,A,B,C
0,1,2,3
1,4,5,6
</code></pre>

<p>If you read that and then append, for example, <code>df + 6</code>:</p>

<pre><code>In [1]: df = pd.read_csv('foo.csv', index_col=0)

In [2]: df
Out[2]:
   A  B  C
0  1  2  3
1  4  5  6

In [3]: df + 6
Out[3]:
    A   B   C
0   7   8   9
1  10  11  12

In [4]: with open('foo.csv', 'a') as f:
             (df + 6).to_csv(f, header=False)
</code></pre>

<p><code>foo.csv</code> becomes:</p>

<pre><code>,A,B,C
0,1,2,3
1,4,5,6
0,7,8,9
1,10,11,12
</code></pre>
";;5;;2013-07-08T15:57:49.360;;17531025;2013-07-08T16:06:49.393;2013-07-08T16:06:49.393;;1240268.0;;1240268.0;17530542.0;2;89;;;
4825;4825;;;"<p><code>NaN</code> can be used as a numerical value on mathematical operations, while <code>None</code> cannot (or at least shouldn't).</p>

<p><code>NaN</code> is a numeric value, as defined in <a href=""http://en.wikipedia.org/wiki/IEEE_floating_point"" rel=""noreferrer"">IEEE 754 floating-point standard</a>.
<code>None</code> is an internal Python tipe (<code>NoneType</code>) and would be more like ""inexistent"" or ""empty"" than ""numerically invalid"" in this context.</p>

<p>The main ""symptom"" of that is that, if you perform, say, an average or a sum on an array containing NaN, even a single one, you get NaN as a result...</p>

<p>In the other hand, you cannot perform mathematical operations using <code>None</code> as operand.</p>

<p>So, depending on the case, you could use <code>None</code> as a way to tell your algorithm not to consider invalid or inexistent values on computations. That would mean the algorithm should test each value to see if it is <code>None</code>.</p>

<p>Numpy has some functions to avoid NaN values to contaminate your results, such as <code>nansum</code> and <code>nan_to_num</code> for example.</p>
";;7;;2013-07-08T19:16:25.820;;17534256;2013-07-08T19:16:25.820;;;;;401828.0;17534106.0;2;7;;;
4826;4826;;;"<p>NaN is used as a placeholder for <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#choice-of-na-representation"" rel=""noreferrer"">missing data <em>consistently</em> in pandas</a>, consistency is good. I usually read/translate NaN as <strong>""missing""</strong>. <em>Also see the <a href=""http://pandas.pydata.org/pandas-docs/dev/missing_data.html"" rel=""noreferrer"">'working with missing data'</a> section in the docs.</em></p>

<p>Wes writes in the docs <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#choice-of-na-representation"" rel=""noreferrer"">'choice of NA-representation'</a>:</p>

<blockquote>
  <p>After years of production use [NaN] has proven, at least in my opinion, to be the best decision given the state of affairs in NumPy and Python in general. The special value NaN (Not-A-Number) is used <em>everywhere</em> as the NA value, and there are API functions <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.isnull.html"" rel=""noreferrer""><code>isnull</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.notnull.html"" rel=""noreferrer""><code>notnull</code></a> which can be used across the dtypes to detect NA values.<br>
  ...<br>
  Thus, I have chosen the Pythonic practicality beats purity approach and traded integer NA capability for a much simpler approach of using a special value in float and object arrays to denote NA, and promoting integer arrays to floating when NAs must be introduced.</p>
</blockquote>

<p><em>Note: the <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#support-for-integer-na"" rel=""noreferrer"">""gotcha"" that integer Series containing missing data are upcast to floats</a>.</em></p>

<p>In my opinion the main reason to use NaN (over None) is that it can be stored with numpy's float64 dtype, rather than the less efficient object dtype, <em>see <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#na-type-promotions"" rel=""noreferrer"">NA type promotions</a></em>.</p>

<pre><code>#  without forcing dtype it changes None to NaN!
s_bad = pd.Series([1, None], dtype=object)
s_good = pd.Series([1, np.nan])

In [13]: s_bad.dtype
Out[13]: dtype('O')

In [14]: s_good.dtype
Out[14]: dtype('float64')
</code></pre>

<p>Jeff comments (below) on this:</p>

<blockquote>
  <p><code>np.nan</code> allows for vectorized operations; its a float value, while <code>None</code>, by definition, forces object type, which basically disables all efficiency in numpy.  </p>
  
  <blockquote>
    <p><strong>So repeat 3 times fast: object==bad, float==good</strong></p>
  </blockquote>
</blockquote>

<p>Saying that, many operations may still work just as well with None vs NaN (but perhaps are not supported i.e. they may sometimes give <a href=""https://stackoverflow.com/a/19866269/1240268"">surprising results</a>):</p>

<pre><code>In [15]: s_bad.sum()
Out[15]: 1

In [16]: s_good.sum()
Out[16]: 1.0
</code></pre>

<p>To answer the second question:<br>
You should be using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.isnull.html"" rel=""noreferrer""><code>pd.isnull</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.notnull.html"" rel=""noreferrer""><code>pd.notnull</code></a> to test for missing data (NaN).</p>
";;2;;2013-07-08T19:43:37.483;;17534682;2014-10-26T17:50:06.163;2017-05-23T12:34:45.273;;-1.0;;1240268.0;17534106.0;2;41;;;
4877;4877;;;"<p>I wonder whether this is a bug in pandas... it's interesting because Index/MultiIndex (index and columns) are in some sense <a href=""https://github.com/pydata/pandas/pull/4039"">supposed to be immutable</a> (however I think these should be copies).</p>

<p>For now, it's easy to create your own method, and add it to DataFrame:</p>

<pre><code>In [11]: def very_deep_copy(self):
    return pd.DataFrame(self.values.copy(), self.index.copy(), self.columns.copy())

In [12]: pd.DataFrame.very_deep_copy = very_deep_copy

In [13]: df2 = df.very_deep_copy()
</code></pre>

<p>As you can see this will create new objects (and preserve names):</p>

<pre><code>In [14]: id(df.columns)
Out[14]: 4370636624

In [15]: id(df2.columns)
Out[15]: 4372118776
</code></pre>
";;1;;2013-07-11T10:50:26.423;;17591423;2013-07-11T10:50:26.423;;;;;1240268.0;17591104.0;2;9;;;
4899;4899;;;"<p>Your code works for me.</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])
&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
</code></pre>

<hr>

<p>Did you paste as is?</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[True,True])
   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>

<hr>

<p><strong>UPDATE</strong> <code>DataFrame.sort</code> is deprecated; use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>DataFrame.sort_values</code></a>.</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
__main__:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
&gt;&gt;&gt; df.sort_values(['c1','c2'], ascending=[False,True])
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
</code></pre>
";;0;;2013-07-12T15:57:02.727;;17619032;2017-08-16T23:15:46.740;2017-08-16T23:15:46.740;;2225682.0;;2225682.0;17618981.0;2;27;;;
4925;4925;;;"<p>A time range doesn't exist as a standalone index type. Generate using a single date</p>

<pre><code>In [1]: pandas.date_range(""11:00"", ""21:30"", freq=""30min"")
Out[1]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-07-14 11:00:00, ..., 2013-07-14 21:30:00]
Length: 22, Freq: 30T, Timezone: None
</code></pre>

<p>The time objects</p>

<pre><code>In [2]: pandas.date_range(""11:00"", ""21:30"", freq=""30min"").time
Out[2]: 
array([datetime.time(11, 0), datetime.time(11, 30), datetime.time(12, 0),
       datetime.time(12, 30), datetime.time(13, 0), datetime.time(13, 30),
       datetime.time(14, 0), datetime.time(14, 30), datetime.time(15, 0),
       datetime.time(15, 30), datetime.time(16, 0), datetime.time(16, 30),
       datetime.time(17, 0), datetime.time(17, 30), datetime.time(18, 0),
       datetime.time(18, 30), datetime.time(19, 0), datetime.time(19, 30),
       datetime.time(20, 0), datetime.time(20, 30), datetime.time(21, 0),
       datetime.time(21, 30)], dtype=object)
</code></pre>

<p>You can also resample if you are spanning multiple dates.</p>

<p>What are you trying to do?</p>
";;1;;2013-07-15T01:19:54.357;;17645475;2013-07-15T01:19:54.357;;;;;644898.0;17644100.0;2;20;;;
4948;4948;;;"<p>Here's one way (though it feels this should work in one go with an apply, I can't get it).</p>

<pre><code>In [11]: g = df.groupby(['A', 'B'])

In [12]: df1 = df.set_index(['A', 'B'])
</code></pre>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation""><code>size</code></a> groupby function is the one you want, we have to match it to the 'A' and 'B' as the index:</p>

<pre><code>In [13]: df1['D'] = g.size()  # unfortunately this doesn't play nice with as_index=False
# Same would work with g['C'].sum()

In [14]: df1.reset_index()
Out[14]:
    A  B  C  D
0   1  5  1  2
1   1  5  1  2
2   1  6  1  1
3   1  7  1  1
4   2  5  1  1
5   2  6  1  2
6   2  6  1  2
7   3  7  1  2
8   3  7  1  2
9   4  6  1  1
10  4  7  1  2
11  4  7  1  2
</code></pre>
";;2;;2013-07-16T00:48:48.483;;17666287;2013-07-16T00:53:53.820;2013-07-16T00:53:53.820;;1240268.0;;1240268.0;17666075.0;2;13;;;
4966;4966;;;"<p>You are looking for <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation""><code>size</code></a>:</p>

<pre><code>In [11]: df.groupby(['col5', 'col2']).size()
Out[11]:
col5  col2
1     A       1
      D       3
2     B       2
3     A       3
      C       1
4     B       1
5     B       2
6     B       1
dtype: int64
</code></pre>

<hr>

<p>To get the same answer as waitingkuo (the ""second question""), but slightly cleaner, is to groupby the level:</p>

<pre><code>In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()
Out[12]:
col2
A       3
B       2
C       1
D       3
dtype: int64
</code></pre>
";;1;;2013-07-16T14:37:33.673;;17679517;2015-04-02T16:45:27.637;2015-04-02T16:45:27.637;;1240268.0;;1240268.0;17679089.0;2;43;;;
4968;4968;;;"<p>Followed by @Andy's answer, you can do following to solve your second question:</p>

<pre><code>In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()
Out[56]: 
      0
col2   
A     3
B     2
C     1
D     3
</code></pre>
";;1;;2013-07-16T14:53:30.280;;17679980;2013-07-16T14:53:30.280;;;;;1426056.0;17679089.0;2;29;;;
4972;4972;;;"<p>Use its value directly:</p>

<pre><code>In [79]: df[df.c &gt; 0.5][['b', 'e']].values
Out[79]: 
array([[ 0.98836259,  0.82403141],
       [ 0.337358  ,  0.02054435],
       [ 0.29271728,  0.37813099],
       [ 0.70033513,  0.69919695]])
</code></pre>
";;4;;2013-07-16T16:59:49.523;;17682662;2013-07-16T16:59:49.523;;;;;1426056.0;17682613.0;2;70;;;
4973;4973;;;"<p>Perhaps something like this for the first problem, you can simply access the columns by their names:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))
&gt;&gt;&gt; df[df['c']&gt;.5][['b','e']]
          b         e
1  0.071146  0.132145
2  0.495152  0.420219
</code></pre>

<p>For the second problem:</p>

<pre><code>&gt;&gt;&gt; df[df['c']&gt;.5][['b','e']].values
array([[ 0.07114556,  0.13214495],
       [ 0.49515157,  0.42021946]])
</code></pre>
";;0;;2013-07-16T17:00:01.160;;17682665;2013-07-16T17:12:29.513;2013-07-16T17:12:29.513;;975477.0;;975477.0;17682613.0;2;16;;;
4974;4974;;;"<p><code>.loc</code> accept row and column selectors simultaneously (as do <code>.ix/.iloc</code> FYI)
This is done in a single pass as well.</p>

<pre><code>In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))

In [2]: df
Out[2]: 
          a         b         c         d         e
0  0.669701  0.780497  0.955690  0.451573  0.232194
1  0.952762  0.585579  0.890801  0.643251  0.556220
2  0.900713  0.790938  0.952628  0.505775  0.582365
3  0.994205  0.330560  0.286694  0.125061  0.575153

In [5]: df.loc[df['c']&gt;0.5,['a','d']]
Out[5]: 
          a         d
0  0.669701  0.451573
1  0.952762  0.643251
2  0.900713  0.505775
</code></pre>

<p>And if you want the values (though this should pass directly to sklearn as is); frames support the array interface</p>

<pre><code>In [6]: df.loc[df['c']&gt;0.5,['a','d']].values
Out[6]: 
array([[ 0.66970138,  0.45157274],
       [ 0.95276167,  0.64325143],
       [ 0.90071271,  0.50577509]])
</code></pre>
";;2;;2013-07-16T17:02:28.953;;17682726;2013-07-16T17:10:06.817;2013-07-16T17:10:06.817;;644898.0;;644898.0;17682613.0;2;7;;;
4984;4984;;;"<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')
&gt;&gt;&gt; a = pd.Series([pd.to_datetime(date) for date in date_stngs])
&gt;&gt;&gt; a
0    2008-12-20 00:00:00
1    2008-12-21 00:00:00
2    2008-12-22 00:00:00
3    2008-12-23 00:00:00
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Use pandas.to_datetime(pd.Series(..)). It's concise and much faster than above code.</p>

<pre><code>&gt;&gt;&gt; pd.to_datetime(pd.Series(date_stngs))
0   2008-12-20 00:00:00
1   2008-12-21 00:00:00
2   2008-12-22 00:00:00
3   2008-12-23 00:00:00
</code></pre>
";;13;;2013-07-17T03:53:47.247;;17690795;2013-07-17T07:33:06.390;2013-07-17T07:33:06.390;;2225682.0;;2225682.0;17690738.0;2;41;;;
4985;4985;;;"<pre><code>In [46]: pd.to_datetime(pd.Series(date_stngs))
Out[46]: 
0   2008-12-20 00:00:00
1   2008-12-21 00:00:00
2   2008-12-22 00:00:00
3   2008-12-23 00:00:00
dtype: datetime64[ns]
</code></pre>

<h2>Update: benchmark</h2>

<pre><code>In [43]: dates = [(dt.datetime(1960, 1, 1)+dt.timedelta(days=i)).date().isoformat() for i in range(20000)]

In [44]: timeit pd.Series([pd.to_datetime(date) for date in dates])
1 loops, best of 3: 1.71 s per loop

In [45]: timeit pd.to_datetime(pd.Series(dates))
100 loops, best of 3: 5.71 ms per loop
</code></pre>
";;3;;2013-07-17T04:00:54.327;;17690868;2013-07-17T06:25:44.623;2013-07-17T06:25:44.623;;1426056.0;;1426056.0;17690738.0;2;33;;;
4988;4988;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"">stack</a> it first and then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"">value_counts</a>:</p>

<pre><code>In [14]: df.stack().value_counts()
Out[14]: 
192.248.8.183    3   
192.168.2.85     3   
66.249.74.52     2   
192.168.2.161    2   
124.43.113.22    1   
dtype: int64
</code></pre>
";;2;;2013-07-17T05:58:05.590;;17692156;2013-07-17T05:58:05.590;;;;;1426056.0;17691447.0;2;31;;;
4997;4997;;;"<p>You can just use <code>map</code>:</p>

<pre><code>In [7]: df = pd.DataFrame({'Status':['Delivered', 'Delivered', 'Undelivered',
                                     'SomethingElse']})

In [8]: df
Out[8]:
          Status
0      Delivered
1      Delivered
2    Undelivered
3  SomethingElse

In [9]: d = {'Delivered': True, 'Undelivered': False}

In [10]: df['Status'].map(d)
Out[10]:
0     True
1     True
2    False
3      NaN
Name: Status, dtype: object
</code></pre>
";;0;;2013-07-17T14:41:08.097;;17702833;2013-07-17T14:41:08.097;;;;;653364.0;17702272.0;2;21;;;
5014;5014;;;"<pre><code>df['Counts'] = df.groupby(['Color'])['Value'].transform('count')
</code></pre>

<p>For example,</p>

<pre><code>In [102]: df = pd.DataFrame({'Color': 'Red Red Blue'.split(), 'Value': [100, 150, 50]})

In [103]: df
Out[103]: 
  Color  Value
0   Red    100
1   Red    150
2  Blue     50

In [104]: df['Counts'] = df.groupby(['Color'])['Value'].transform('count')

In [105]: df
Out[105]: 
  Color  Value  Counts
0   Red    100       2
1   Red    150       2
2  Blue     50       1
</code></pre>

<p>Note that <code>transform('count')</code> ignores NaNs. If you want to count NaNs, use <code>transform(len)</code>. </p>

<hr>

<p>To the anonymous editor: If you are getting an error while using <code>transform('count')</code> it may be due to your version of Pandas being too old. The above works with pandas version 0.15 or newer.</p>
";;10;;2013-07-17T20:20:02.253;;17709453;2015-08-04T12:36:35.040;2015-08-04T12:36:35.040;;190597.0;;190597.0;17709270.0;2;33;;;
5021;5021;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html"" rel=""nofollow noreferrer""><code>mean</code></a> DataFrame method and the Series <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_values.html"" rel=""nofollow noreferrer""><code>sort_values</code></a> method:</p>

<pre><code>In [11]: df = pd.DataFrame(np.random.randn(4,4), columns=list('ABCD'))

In [12]: df
Out[12]:
          A         B         C         D
0  0.933069  1.432486  0.288637 -1.867853
1 -0.455952 -0.725268  0.339908  1.318175
2 -0.894331  0.573868  1.116137  0.508845
3  0.661572  0.819360 -0.527327 -0.925478

In [13]: df.mean()
Out[13]:
A    0.061089
B    0.525112
C    0.304339
D   -0.241578
dtype: float64

In [14]: df.mean().sort_values()
Out[14]:
D   -0.241578
A    0.061089
C    0.304339
B    0.525112
dtype: float64
</code></pre>

<p>Then you can reorder the columns using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex_axis.html"" rel=""nofollow noreferrer""><code>reindex_axis</code></a>:</p>

<pre><code>In [15]: df.reindex_axis(df.mean().sort_values().index, axis=1)
Out[15]:
          D         A         C         B
0 -1.867853  0.933069  0.288637  1.432486
1  1.318175 -0.455952  0.339908 -0.725268
2  0.508845 -0.894331  1.116137  0.573868
3 -0.925478  0.661572 -0.527327  0.819360
</code></pre>

<hr>

<p><em>Note: In earlier versions of pandas, <code>sort_values</code> used to be <code>order</code>, but <code>order</code> was deprecated as part of 0.17 so to be more consistent with the other sorting methods.</em></p>
";;3;;2013-07-18T00:14:06.113;;17712440;2016-12-09T19:11:37.160;2016-12-09T19:11:37.160;;1240268.0;;1240268.0;17712163.0;2;20;;;
5026;5026;;;"<p>Many ways to do that </p>

<h3>1</h3>

<pre><code>In [7]: d.sales[d.sales==24] = 100

In [8]: d
Out[8]: 
   day     flavour  sales  year
0  sat  strawberry     10  2008
1  sun  strawberry     12  2008
2  sat      banana     22  2008
3  sun      banana     23  2008
4  sat  strawberry     11  2009
5  sun  strawberry     13  2009
6  sat      banana     23  2009
7  sun      banana    100  2009
</code></pre>

<h3>2</h3>

<pre><code>In [26]: d.loc[d.sales == 12, 'sales'] = 99

In [27]: d
Out[27]: 
   day     flavour  sales  year
0  sat  strawberry     10  2008
1  sun  strawberry     99  2008
2  sat      banana     22  2008
3  sun      banana     23  2008
4  sat  strawberry     11  2009
5  sun  strawberry     13  2009
6  sat      banana     23  2009
7  sun      banana    100  2009
</code></pre>

<h3>3</h3>

<pre><code>In [28]: d.sales = d.sales.replace(23, 24)

In [29]: d
Out[29]: 
   day     flavour  sales  year
0  sat  strawberry     10  2008
1  sun  strawberry     99  2008
2  sat      banana     22  2008
3  sun      banana     24  2008
4  sat  strawberry     11  2009
5  sun  strawberry     13  2009
6  sat      banana     24  2009
7  sun      banana    100  2009
</code></pre>
";;4;;2013-07-18T17:15:28.100;;17729985;2013-07-18T17:21:51.680;2013-07-18T17:21:51.680;;1426056.0;;1426056.0;17729853.0;2;38;;;
5045;5045;;;"<p>I think you can use the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.argsort.html""><code>argsort</code></a> method:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": 1e4*np.arange(100), ""num"": np.random.random(100)})
&gt;&gt;&gt; x = 0.75
&gt;&gt;&gt; df.ix[(df.num-x).abs().argsort()[:5]]
         A       num
66  660000  0.748261
92  920000  0.754911
59  590000  0.764449
27  270000  0.765633
82  820000  0.732601
&gt;&gt;&gt; x = 0.33
&gt;&gt;&gt; df.ix[(df.num-x).abs().argsort()[:5]]
         A       num
37  370000  0.327928
76  760000  0.327921
8    80000  0.326528
17  170000  0.334702
96  960000  0.324516
</code></pre>
";;3;;2013-07-20T02:38:02.757;;17758115;2013-07-20T02:38:02.757;;;;;487339.0;17758023.0;2;23;;;
5069;5069;;;"<p>You can use <code>DataFrame.values</code> to get an numpy array of the data and then use NumPy functions such as <code>argsort()</code> to get the most correlated pairs. </p>

<p>But if you want to do this in pandas, you can <code>unstack</code> and <code>order</code> the DataFrame:</p>

<pre><code>import pandas as pd
import numpy as np

shape = (50, 4460)

data = np.random.normal(size=shape)

data[:, 1000] += data[:, 2000]

df = pd.DataFrame(data)

c = df.corr().abs()

s = c.unstack()
so = s.order(kind=""quicksort"")

print so[-4470:-4460]
</code></pre>

<p>Here is the output:</p>

<pre><code>2192  1522    0.636198
1522  2192    0.636198
3677  2027    0.641817
2027  3677    0.641817
242   130     0.646760
130   242     0.646760
1171  2733    0.670048
2733  1171    0.670048
1000  2000    0.742340
2000  1000    0.742340
dtype: float64
</code></pre>
";;0;;2013-07-22T01:43:58.870;;17778786;2013-07-22T01:43:58.870;;;;;772649.0;17778394.0;2;19;;;
5070;5070;;;"<p>As nitin points out, you can simply upgrade pandas using pip:</p>

<pre><code>pip install --upgrade pandas
</code></pre>

<p>Since this version of pandas will be installed in <code>site-packages</code> you will, in fact, be at the mercy of any automatic updates to packages within that directory. It's wise to install the versions of packages you want into a <a href=""http://docs.python-guide.org/en/latest/dev/virtualenvs/"" rel=""nofollow noreferrer"">virtual environment</a> so you have a consistent working environment with the bonus of reproducibility.</p>

<p>To answer your last question, the reason Pandas won't ""upgrade"" to 0.11.0 using <code>apt-get update</code> is that packages (of Pandas) from your distribution lag behind or haven't been created yet.</p>
";;2;;2013-07-22T02:54:35.480;;17779230;2016-11-22T18:50:53.570;2016-11-22T18:50:53.570;;700228.0;;700228.0;17759128.0;2;24;;;
5093;5093;;;"<p>There is probably no automatic way to do it right now, but as you use openpyxl, the following line (adapted from another answer by user <a href=""https://stackoverflow.com/users/443457/bufke"">Bufke</a> on <a href=""https://stackoverflow.com/a/14450572/2375855"">how to do in manually</a>) allows you to specify a sane value (in character widths):</p>

<pre><code>writer.sheets['Summary'].column_dimensions['A'].width = 15
</code></pre>
";;0;;2013-07-23T13:43:09.303;;17811984;2014-04-29T13:16:05.483;2017-05-23T12:10:38.810;;-1.0;;2375855.0;17326973.0;2;13;;;
5096;5096;;;"<p>You can specify the <code>style</code> of the plotted line when calling <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.0/generated/pandas.DataFrame.plot.html?highlight=plot#pandas-dataframe-plot"" rel=""noreferrer""><code>df.plot</code></a>:</p>

<pre><code>df.plot(x='col_name_1', y='col_name_2', style='o')
</code></pre>

<p>The <code>style</code> argument can also be a <code>dict</code> or <code>list</code>, e.g.:</p>

<pre><code>import numpy as np
import pandas as pd

d = {'one' : np.random.rand(10),
     'two' : np.random.rand(10)}

df = pd.DataFrame(d)

df.plot(style=['o','rx'])
</code></pre>

<p>All the accepted style formats are listed in the documentation of <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot"" rel=""noreferrer""><code>matplotlib.pyplot.plot</code></a>.</p>

<p><img src=""https://i.stack.imgur.com/3FXAE.png"" alt=""Output""></p>
";;0;;2013-07-23T14:33:45.057;;17813222;2016-04-04T12:34:53.943;2016-04-04T12:34:53.943;;2022086.0;;2022086.0;17812978.0;2;35;;;
5097;5097;;;"<p>For this (and most plotting) I would not rely on the Pandas wrappers to matplotlib. Instead, just use matplotlib directly:</p>

<pre><code>import matplotlib.pyplot as plt
plt.scatter(df['col_name_1'], df['col_name_2'])
plt.show() # Depending on whether you use IPython or interactive mode, etc.
</code></pre>

<p>and remember that you can access a NumPy array of the column's values with <code>df.col_name_1.values</code> for example.</p>

<p>I ran into trouble using this with Pandas default plotting in the case of a column of Timestamp values with millisecond precision. In trying to convert the objects to <code>datetime64</code> type, I also discovered a nasty issue: &lt; <a href=""https://stackoverflow.com/questions/26350364/pandas-gives-incorrect-result-when-asking-if-timestamp-column-values-have-attr-a"">Pandas gives incorrect result when asking if Timestamp column values have attr astype</a> >.</p>
";;1;;2013-07-23T14:36:04.623;;17813277;2014-10-17T15:43:23.170;2017-05-23T12:34:47.163;;-1.0;;567620.0;17812978.0;2;29;;;
5109;5109;;;"<p>A direct conversion is not supported ATM. Contributions are welcome!</p>

<p>Try this, should be ok on memory as the SpareSeries is much like a csc_matrix (for 1 column)
and pretty space efficient</p>

<pre><code>In [37]: col = np.array([0,0,1,2,2,2])

In [38]: data = np.array([1,2,3,4,5,6],dtype='float64')

In [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )

In [40]: m
Out[40]: 
&lt;3x3 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
        with 6 stored elements in Compressed Sparse Column format&gt;

In [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) 
                              for i in np.arange(m.shape[0]) ])
Out[46]: 
   0  1  2
0  1  0  4
1  0  0  5
2  2  3  6

In [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) 
                                   for i in np.arange(m.shape[0]) ])

In [48]: type(df)
Out[48]: pandas.sparse.frame.SparseDataFrame
</code></pre>
";;5;;2013-07-23T19:32:57.817;;17819427;2015-04-28T21:45:59.963;2015-04-28T21:45:59.963;;379593.0;;644898.0;17818783.0;2;22;;;
5155;5155;;;"<p>The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'A': [a], 'B': [b]})
&gt;&gt;&gt; df
   A  B
0  2  3
</code></pre>

<p>or use scalar values and pass an index:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'A': a, 'B': b}, index=[0])
&gt;&gt;&gt; df
   A  B
0  2  3
</code></pre>
";;3;;2013-07-24T16:49:48.613;;17840195;2013-07-24T16:49:48.613;;;;;487339.0;17839973.0;2;111;;;
5156;5156;;;"<p>You need to provide iterables as the values for the Pandas DataFrame columns:</p>

<pre><code>df2 = pd.DataFrame({'A':[a],'B':[b]})
</code></pre>
";;0;;2013-07-24T16:49:49.647;;17840197;2013-07-24T16:49:49.647;;;;;567620.0;17839973.0;2;6;;;
5160;5160;;;"<pre><code>In [4]: df = read_csv(StringIO(data),sep='\s+')

In [5]: df
Out[5]: 
   A         B       C
0  1  0.749065    This
1  2  0.301084      is
2  3  0.463468       a
3  4  0.643961  random
4  1  0.866521  string
5  2  0.120737       !

In [6]: df.dtypes
Out[6]: 
A      int64
B    float64
C     object
dtype: object
</code></pre>

<p>When you apply your own function, there is not automatic exclusions of non-numeric columns. This is slower, though (that the applicatino of <code>.sum()</code> to the groupby</p>

<pre><code>In [8]: df.groupby('A').apply(lambda x: x.sum())
Out[8]: 
   A         B           C
A                         
1  2  1.615586  Thisstring
2  4  0.421821         is!
3  3  0.463468           a
4  4  0.643961      random
</code></pre>

<p>Sum by default concatenates</p>

<pre><code>In [9]: df.groupby('A')['C'].apply(lambda x: x.sum())
Out[9]: 
A
1    Thisstring
2           is!
3             a
4        random
dtype: object
</code></pre>

<p>You can do pretty much what you want</p>

<pre><code>In [11]: df.groupby('A')['C'].apply(lambda x: ""{%s}"" % ', '.join(x))
Out[11]: 
A
1    {This, string}
2           {is, !}
3               {a}
4          {random}
dtype: object
</code></pre>

<p>Doing this a whole frame group at a time. Key is to return a Series</p>

<pre><code>def f(x):
     return Series(dict(A = x['A'].sum(), 
                        B = x['B'].sum(), 
                        C = ""{%s}"" % ', '.join(x['C'])))

In [14]: df.groupby('A').apply(f)
Out[14]: 
   A         B               C
A                             
1  2  1.615586  {This, string}
2  4  0.421821         {is, !}
3  3  0.463468             {a}
4  4  0.643961        {random}
</code></pre>
";;10;;2013-07-24T17:51:18.437;;17841294;2013-07-24T19:54:57.900;2013-07-24T19:54:57.900;;644898.0;;644898.0;17841149.0;2;70;;;
5161;5161;;;"<p>You can use the <code>apply</code> method to apply an arbitrary function to the grouped data.  So if you want a set, apply <code>set</code>.  If you want a list, apply <code>list</code>.</p>

<pre><code>&gt;&gt;&gt; d
   A       B
0  1    This
1  2      is
2  3       a
3  4  random
4  1  string
5  2       !
&gt;&gt;&gt; d.groupby('A')['B'].apply(list)
A
1    [This, string]
2           [is, !]
3               [a]
4          [random]
dtype: object
</code></pre>

<p>If you want something else, just write a function that does what you want and then <code>apply</code> that.</p>
";;1;;2013-07-24T17:51:56.377;;17841308;2013-07-24T17:51:56.377;;;;;1427416.0;17841149.0;2;21;;;
5176;5176;;;"<p>The <code>normed=True</code> returns a histogram for which <code>np.sum(pdf * np.diff(bins))</code> equals 1. If you want the sum of the histogram to be 1 you can use Numpy's histogram() and normalize the results yourself.</p>

<pre><code>x = np.random.randn(30)

fig, ax = plt.subplots(1,2, figsize=(10,4))

ax[0].hist(x, normed=True, color='grey')

hist, bins = np.histogram(x)
ax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')

ax[0].set_title('normed=True')
ax[1].set_title('hist = hist / hist.sum()')
</code></pre>

<p><img src=""https://i.stack.imgur.com/geSyT.png"" alt=""enter image description here""></p>

<p>Btw: Strange plotting glitch at the first bin of the left plot.</p>
";;3;;2013-07-26T09:01:44.683;;17877159;2013-07-26T09:01:44.683;;;;;1755432.0;17874063.0;2;25;;;
5216;5216;;;"<p>You can also pass the file mode as an argument to the to_csv method</p>

<pre><code>df.to_csv(file_name, header=False, mode = 'a')
</code></pre>
";;0;;2013-07-28T17:14:18.867;;17910713;2013-07-28T17:14:18.867;;;;;1855266.0;17134942.0;2;18;;;
5230;5230;;;"<p>With a 'float' like index you always want to use it as a column rather than a direct indexing action. These will all work whether the endpoints exist or not.</p>

<pre><code>In [11]: df
Out[11]: 
          C
A   B      
1.1 111  81
    222  45
3.3 222  98
    333  13
5.5 333  89
6.6 777  98

In [12]: x = df.reset_index()
</code></pre>

<p>Q1</p>

<pre><code>In [13]: x.loc[(x.A&gt;=3.3)&amp;(x.A&lt;=6.6)]
Out[13]: 
     A    B   C
2  3.3  222  98
3  3.3  333  13
4  5.5  333  89
5  6.6  777  98
</code></pre>

<p>Q2</p>

<pre><code>In [14]: x.loc[(x.A&gt;=2.0)&amp;(x.A&lt;=4.0)]
Out[14]: 
     A    B   C
2  3.3  222  98
3  3.3  333  13
</code></pre>

<p>Q3</p>

<pre><code>In [15]: x.loc[(x.B&gt;=111.0)&amp;(x.B&lt;=500.0)]
Out[15]: 
     A    B   C
0  1.1  111  81
1  1.1  222  45
2  3.3  222  98
3  3.3  333  13
4  5.5  333  89
</code></pre>

<p>If you want the indices back, just set them. This is a cheap operation.</p>

<pre><code>In [16]: x.loc[(x.B&gt;=111.0)&amp;(x.B&lt;=500.0)].set_index(['A','B'])
Out[16]: 
          C
A   B      
1.1 111  81
    222  45
3.3 222  98
    333  13
5.5 333  89
</code></pre>

<p>If you REALLY want the actual index values</p>

<pre><code>In [5]: x.loc[(x.B&gt;=111.0)&amp;(x.B&lt;=500.0)].set_index(['A','B']).index
Out[5]: 
MultiIndex
[(1.1, 111), (1.1, 222), (3.3, 222), (3.3, 333), (5.5, 333)]
</code></pre>
";;8;;2013-07-29T12:08:22.183;;17923621;2013-07-29T13:16:22.637;2013-07-29T13:16:22.637;;644898.0;;644898.0;17921010.0;2;8;;;
5233;5233;;;"<pre><code>df.groupby(['col1','col2'])['col3'].nunique().reset_index()
</code></pre>
";;4;;2013-07-29T14:16:46.870;;17926411;2013-07-29T14:16:46.870;;;;;245549.0;17926273.0;2;22;;;
5234;5234;;;"<pre><code>In [17]: df
Out[17]: 
    0  1  2
0   1  1  1
1   1  1  1
2   1  1  2
3   1  2  3
4   1  2  3
5   1  2  3
6   2  1  1
7   2  1  2
8   2  1  3
9   2  2  3
10  2  2  3
11  2  2  3

In [19]: df.groupby([0,1])[2].apply(lambda x: len(x.unique()))
Out[19]: 
0  1
1  1    2
   2    1
2  1    3
   2    1
dtype: int64
</code></pre>
";;0;;2013-07-29T14:18:03.473;;17926436;2013-07-29T14:18:03.473;;;;;644898.0;17926273.0;2;19;;;
5240;5240;;;"<p>The problem with the additional comma, <code>np.genfromtxt</code> does not deal with that.</p>

<p>One simple solution is to read the file with <code>csv.reader()</code> from python's <a href=""http://docs.python.org/2/library/csv.html"" rel=""nofollow"">csv</a> module into a list and then dump it into a numpy array if you like.</p>

<p>If you really want to use <code>np.genfromtxt</code>, note that it can take iterators instead of files, e.g. <code>np.genfromtxt(my_iterator, ...)</code>. So, you can wrap a <code>csv.reader</code> in an iterator and give it to <code>np.genfromtxt</code>.</p>

<p>That would go something like this:</p>

<pre><code>import csv
import numpy as np

np.genfromtxt((""\t"".join(i) for i in csv.reader(open('myfile.csv'))), delimiter=""\t"")
</code></pre>

<p>This essentially replaces on-the-fly only the appropriate commas with tabs.</p>
";;1;;2013-07-29T20:18:04.053;;17933417;2015-12-29T15:33:38.543;2015-12-29T15:33:38.543;;1730674.0;;1358889.0;17933282.0;2;7;;;
5246;5246;;;"<p>You can use <a href=""https://github.com/pydata/pandas/pull/4384"" rel=""noreferrer"">pandas</a> (the becoming default library for working with dataframes (heterogeneous data) in scientific python) for this. It's <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer""><code>read_csv</code></a> can handle this. From the docs:</p>

<blockquote>
  <p>quotechar : string</p>

<pre><code>The character to used to denote the start and end of a quoted item. Quoted items 
can include the delimiter and it will be ignored.
</code></pre>
</blockquote>

<p>The default value is <code>""</code>. An example:</p>

<pre><code>In [1]: import pandas as pd

In [2]: from StringIO import StringIO

In [3]: s=""""""year, city, value
   ...: 2012, ""Louisville KY"", 3.5
   ...: 2011, ""Lexington, KY"", 4.0""""""

In [4]: pd.read_csv(StringIO(s), quotechar='""', skipinitialspace=True)
Out[4]:
   year           city  value
0  2012  Louisville KY    3.5
1  2011  Lexington, KY    4.0
</code></pre>

<p>The trick here is that you also have to use <code>skipinitialspace=True</code> to deal with the spaces after the comma-delimiter.</p>

<p>Apart from a powerful csv reader, I can also strongly advice to use pandas with the heterogeneous data you have (the example output in numpy you give are all strings, although you could use structured arrays).</p>
";;1;;2013-07-30T08:37:54.683;;17942117;2013-07-30T08:37:54.683;;;;;653364.0;17933282.0;2;17;;;
5253;5253;;;"<p>I think a more explicit way of doing this is to use drop.</p>

<p>The syntax is:</p>

<pre><code>df.drop(label)
</code></pre>

<p>And as pointed out by @tim and @ChaimG, this can be done in-place:</p>

<pre><code>df.drop(label, inplace=True)
</code></pre>

<p>One way of implementing this could be:</p>

<pre><code>df.drop(df.index[:3], inplace=True)
</code></pre>

<p>And another ""in place"" use:</p>

<pre><code>df.drop(df.head(3).index, inplace=True)
</code></pre>
";;6;;2013-07-30T14:39:32.237;;17950081;2016-03-08T09:14:59.647;2016-03-08T09:14:59.647;;859496.0;;859496.0;16396903.0;2;69;;;
5255;5255;;;"<pre><code>In [16]: df = DataFrame(np.arange(10).reshape(5,2),columns=list('AB'))

In [17]: df
Out[17]: 
   A  B
0  0  1
1  2  3
2  4  5
3  6  7
4  8  9

In [18]: df.dtypes
Out[18]: 
A    int64
B    int64
dtype: object
</code></pre>

<p>Convert a series</p>

<pre><code>In [19]: df['A'].apply(str)
Out[19]: 
0    0
1    2
2    4
3    6
4    8
Name: A, dtype: object

In [20]: df['A'].apply(str)[0]
Out[20]: '0'
</code></pre>

<p>Convert the whole frame</p>

<pre><code>In [21]: df.applymap(str)
Out[21]: 
   A  B
0  0  1
1  2  3
2  4  5
3  6  7
4  8  9

In [22]: df.applymap(str).iloc[0,0]
Out[22]: '0'
</code></pre>
";;4;;2013-07-30T14:59:10.463;;17950531;2013-07-30T14:59:10.463;;;;;644898.0;17950374.0;2;46;;;
5270;5270;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/dev/basics.html#vectorized-string-methods"" rel=""noreferrer""><code>str.startswith</code></a> DataFrame method to give more consistent results:</p>

<pre><code>In [11]: s = pd.Series(['a', 'ab', 'c', 11, np.nan])

In [12]: s
Out[12]:
0      a
1     ab
2      c
3     11
4    NaN
dtype: object

In [13]: s.str.startswith('a', na=False)
Out[13]:
0     True
1     True
2    False
3    False
4    False
dtype: bool
</code></pre>

<p>and the boolean indexing will work just fine (I prefer to use <code>loc</code>, but it works just the same without):</p>

<pre><code>In [14]: s.loc[s.str.startswith('a', na=False)]
Out[14]:
0     a
1    ab
dtype: object
</code></pre>

<p>.</p>

<p><em>It looks least one of your elements in the Series/column is a float, which doesn't have a startswith method hence the AttributeError, the list comprehension should raise the same error...</em></p>
";;12;;2013-07-30T22:16:05.797;;17958424;2016-08-19T02:20:37.413;2016-08-19T02:20:37.413;;707650.0;;1240268.0;17957890.0;2;25;;;
5275;5275;;;"<p>Your answer lies in the pandas docs: <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html?highlight=view#indexing-view-versus-copy"" rel=""noreferrer"">returning-a-view-versus-a-copy</a>.</p>

<blockquote>
  <p>Whenever an array of labels or a boolean vector are involved
  in the indexing operation, <strong>the result will be a copy</strong>.
  With single label / scalar indexing and slicing,
  e.g. df.ix[3:6] or df.ix[:, 'A'], <strong>a view will be returned</strong>.</p>
</blockquote>

<p>In your example, <code>bar</code> is a <strong>view</strong> of slices of <code>foo</code>.  If you wanted a <strong>copy</strong>, you could have used the <code>copy</code> method.  Modifying <code>bar</code> also modifies <code>foo</code>.  pandas does not appear to have a copy-on-write mechanism.</p>

<p>See my code example below to illustrate:</p>

<pre><code>In [1]: import pandas as pd
   ...: import numpy as np
   ...: foo = pd.DataFrame(np.random.random((10,5)))
   ...: 

In [2]: pd.__version__
Out[2]: '0.12.0.dev-35312e4'

In [3]: np.__version__
Out[3]: '1.7.1'

In [4]: # DataFrame has copy method
   ...: foo_copy = foo.copy()

In [5]: bar = foo.iloc[3:5,1:4]

In [6]: bar == foo.iloc[3:5,1:4] == foo_copy.iloc[3:5,1:4]
Out[6]: 
      1     2     3
3  True  True  True
4  True  True  True

In [7]: # Changing the view
   ...: bar.ix[3,1] = 5

In [8]: # View and DataFrame still equal
   ...: bar == foo.iloc[3:5,1:4]
Out[8]: 
      1     2     3
3  True  True  True
4  True  True  True

In [9]: # It is now different from a copy of original
   ...: bar == foo_copy.iloc[3:5,1:4]
Out[9]: 
       1     2     3
3  False  True  True
4   True  True  True
</code></pre>
";;3;;2013-07-31T04:12:35.957;;17961468;2015-12-01T13:29:00.803;2015-12-01T13:29:00.803;;2626968.0;;2626968.0;17960511.0;2;17;;;
5285;5285;;;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods""><code>str.contains</code></a> method accepts a regular expression pattern:</p>

<pre><code>In [11]: pattern = '|'.join(mylist)

In [12]: pattern
Out[12]: 'dog|cat|fish'

In [13]: frame.a.str.contains(pattern)
Out[13]:
0     True
1    False
2     True
Name: a, dtype: bool
</code></pre>
";;2;;2013-07-31T14:31:27.137;;17973255;2013-07-31T14:31:27.137;;;;;1240268.0;17972938.0;2;39;;;
5292;5292;;;"<p>You can specify a python write mode in the pandas to_csv() function. For append it is 'a'.</p>

<p>In your case:</p>

<blockquote>
  <p>df.to_csv('my_csv.csv', mode='a', header=False)</p>
</blockquote>

<p>The default mode is 'w'.</p>
";;0;;2013-07-31T16:19:00.373;;17975690;2013-07-31T16:19:00.373;;;;;1623360.0;17530542.0;2;130;;;
5296;5296;;;"<p>You can still use the <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#excel-files"">ExcelFile</a> class (and the <code>sheet_names</code> attribute):</p>

<pre><code>xl = pd.ExcelFile('foo.xls')

xl.sheet_names  # see all sheet names

xl.parse(sheet_name)  # read a specific sheet to DataFrame
</code></pre>

<p><em>see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.ExcelFile.parse.html"">docs for parse</a> for more options...</em></p>
";;7;;2013-07-31T18:01:21.350;;17977609;2015-07-18T19:46:19.357;2015-07-18T19:46:19.357;;1373804.0;;1240268.0;17977540.0;2;60;;;
5301;5301;;;"<p>It's worth mentioning that you may have been able to read this in <strong>directly</strong> e.g. if you were using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer""><code>read_csv</code></a> using <code>parse_dates=[['Date', 'Time']]</code>.</p>

<p>Assuming these are just strings you could simply add them together (with a space), allowing you to apply <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.tseries.tools.to_datetime.html"" rel=""noreferrer""><code>to_datetime</code></a>:</p>

<pre><code>In [11]: df['Date'] + ' ' + df['Time']
Out[11]:
0    01-06-2013 23:00:00
1    02-06-2013 01:00:00
2    02-06-2013 21:00:00
3    02-06-2013 22:00:00
4    02-06-2013 23:00:00
5    03-06-2013 01:00:00
6    03-06-2013 21:00:00
7    03-06-2013 22:00:00
8    03-06-2013 23:00:00
9    04-06-2013 01:00:00
dtype: object

In [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])
Out[12]:
0   2013-01-06 23:00:00
1   2013-02-06 01:00:00
2   2013-02-06 21:00:00
3   2013-02-06 22:00:00
4   2013-02-06 23:00:00
5   2013-03-06 01:00:00
6   2013-03-06 21:00:00
7   2013-03-06 22:00:00
8   2013-03-06 23:00:00
9   2013-04-06 01:00:00
dtype: datetime64[ns]
</code></pre>

<p><em>Note: surprisingly (for me), this works fine with NaNs being converted to NaT, but it is worth worrying that the conversion (perhaps using the <code>raise</code> argument).</em></p>
";;3;;2013-07-31T18:33:05.613;;17978188;2013-07-31T18:53:55.213;2013-07-31T18:53:55.213;;1240268.0;;1240268.0;17978092.0;2;49;;;
5302;5302;;;"<p>You could merge the sub-DataFrame (with just those columns):</p>

<pre><code>df2[list('xab')]  # df2 but only with columns x, a, and b

df1.merge(df2[list('xab')])
</code></pre>
";;1;;2013-07-31T18:46:03.367;;17978414;2015-10-27T07:05:27.210;2015-10-27T07:05:27.210;;1681480.0;;1240268.0;17978133.0;2;26;;;
5321;5321;;;"<p>The result of <code>df.groupby(...)</code> is not a DataFrame. To get a DataFrame back, you have to apply a function to each group, transform each element of a group, or filter the groups.</p>

<p>It seems like you want a DataFrame that contains (1) all your original data in <code>df</code> and (2) the count of how much data is in each group. These things have different lengths, so if they need to go into the same DataFrame, you'll need to list the size redundantly, i.e., for each row in each group.</p>

<pre><code>df['size'] = df.groupby(['A','B']).transform(np.size)
</code></pre>

<p>(Aside: It's helpful if you can show succinct sample input and expected results.)</p>
";;4;;2013-08-01T13:15:13.700;;17995122;2013-08-01T13:20:50.837;2013-08-01T13:20:50.837;;1221924.0;;1221924.0;17995024.0;2;9;;;
5351;5351;;;"<p>If <code>lakes</code> is your <code>DataFrame</code>, you can do something like</p>

<pre><code>area_dict = dict(zip(lakes.area, lakes.count))
</code></pre>
";;4;;2013-08-02T09:42:17.350;;18013682;2013-08-02T09:42:17.350;;;;;1225068.0;18012505.0;2;39;;;
5369;5369;;;"<p>You can just get/set the index via its <code>name</code> property</p>

<pre><code>In [7]: df.index.name
Out[7]: 'Index Title'

In [8]: df.index.name = 'foo'

In [9]: df.index.name
Out[9]: 'foo'

In [10]: df
Out[10]: 
         Column 1
foo              
Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>
";;4;;2013-08-02T18:08:39.880;;18023468;2013-08-02T18:08:39.880;;;;;644898.0;18022845.0;2;125;;;
5370;5370;;;"<p><code>df.index.name</code> should do the trick.</p>

<p>Python has a <code>dir</code> function that let's you query object attributes. <code>dir(df.index)</code> was helpful here.</p>
";;1;;2013-08-02T18:09:26.240;;18023485;2013-08-02T18:09:26.240;;;;;7650.0;18022845.0;2;22;;;
5386;5386;;;"<p>The parser is getting confused by the header of the file.  It reads the first row and infers the number of columns from that row.  But the first two rows aren't representative of the actual data in the file.</p>

<p>Try it with <code>data = pd.read_csv(path, skiprows=2)</code></p>
";;1;;2013-08-04T02:24:35.393;;18039175;2013-08-04T02:24:35.393;;;;;1889400.0;18039057.0;2;17;;;
5403;5403;;;"<p>You can also call the show() function after each plot. 
e.g</p>

<pre><code>   plot(a)
   show()
   plot(b)
   show()
</code></pre>

<p>see example at - <a href=""http://nbviewer.ipython.org/6151560"" rel=""noreferrer"">http://nbviewer.ipython.org/6151560</a></p>
";;1;;2013-08-04T19:21:25.463;;18046682;2013-08-04T19:21:25.463;;;;;1652631.0;16392921.0;2;42;;;
5410;5410;;;"<p><code>resample</code> is more general than <code>asfreq</code>. For example, using <code>resample</code> I can pass an arbitrary function to perform binning over a <code>Series</code> or <code>DataFrame</code> object in bins of arbitrary size. <code>asfreq</code> is a concise way of changing the frequency of a <code>DatetimeIndex</code> object. It also provides padding functionality.</p>

<p>As the pandas documentation says, <code>asfreq</code> is a thin wrapper around a call to <code>date_range</code> + a call to <code>reindex</code>. See <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#frequency-conversion"" rel=""noreferrer"">here</a> for an example.</p>

<p>An example of <code>resample</code> that I use in my daily work is computing the number of spikes of a neuron in 1 second bins by resampling a large boolean array where <code>True</code> means ""spike"" and <code>False</code> means ""no spike"". I can do that as easy as <code>large_bool.resample('S', how='sum')</code>. Kind of neat!</p>

<p><code>asfreq</code> can be used when you want to change a <code>DatetimeIndex</code> to have a different frequency while retaining the same values at the current index.</p>

<p>Here's an example where they are equivalent:</p>

<pre><code>In [6]: dr = date_range('1/1/2010', periods=3, freq=3 * datetools.bday)

In [7]: raw = randn(3)

In [8]: ts = Series(raw, index=dr)

In [9]: ts
Out[9]:
2010-01-01   -1.948
2010-01-06    0.112
2010-01-11   -0.117
Freq: 3B, dtype: float64

In [10]: ts.asfreq(datetools.BDay())
Out[10]:
2010-01-01   -1.948
2010-01-04      NaN
2010-01-05      NaN
2010-01-06    0.112
2010-01-07      NaN
2010-01-08      NaN
2010-01-11   -0.117
Freq: B, dtype: float64

In [11]: ts.resample(datetools.BDay())
Out[11]:
2010-01-01   -1.948
2010-01-04      NaN
2010-01-05      NaN
2010-01-06    0.112
2010-01-07      NaN
2010-01-08      NaN
2010-01-11   -0.117
Freq: B, dtype: float64
</code></pre>

<p>As far as when to use either: it depends on the problem you have in mind...care to share?</p>
";;0;;2013-08-05T14:54:40.703;;18061253;2013-08-05T15:10:13.780;2013-08-05T15:10:13.780;;564538.0;;564538.0;18060619.0;2;20;;;
5415;5415;;;"<p>Pandas will automatically align these passed in series and create the joint index
They happen to be the same here. <code>reset_index</code> moves the index to a column.</p>

<pre><code>In [2]: s1 = Series(randn(5),index=[1,2,4,5,6])

In [4]: s2 = Series(randn(5),index=[1,2,4,5,6])

In [8]: DataFrame(dict(s1 = s1, s2 = s2)).reset_index()
Out[8]: 
   index        s1        s2
0      1 -0.176143  0.128635
1      2 -1.286470  0.908497
2      4 -0.995881  0.528050
3      5  0.402241  0.458870
4      6  0.380457  0.072251
</code></pre>
";;0;;2013-08-05T15:53:14.117;;18062430;2013-08-05T15:53:14.117;;;;;644898.0;18062135.0;2;23;;;
5416;5416;;;"<p>I think <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html""><code>concat</code></a> is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):</p>

<pre><code>In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')

In [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')

In [3]: pd.concat([s1, s2], axis=1)
Out[3]:
   s1  s2
A   1   3
B   2   4

In [4]: pd.concat([s1, s2], axis=1).reset_index()
Out[4]:
  index  s1  s2
0     A   1   3
1     B   2   4
</code></pre>

<p><em>Note: This extends to more than 2 Series.</em></p>
";;7;;2013-08-05T15:57:31.060;;18062521;2013-08-05T15:57:31.060;;;;;1240268.0;18062135.0;2;172;;;
5421;5421;;;"<p>You could make use of the <a href=""https://pypi.python.org/pypi/wheel"" rel=""noreferrer""><code>wheel</code></a> package. We do this over at <a href=""http://github.com/pydata/pandas"" rel=""noreferrer"">pandas</a> for our continuous integration builds so that we can basically download them and install them extremely fast.</p>

<p>Take a look at <a href=""https://github.com/pydata/pandas/blob/master/ci/speedpack/build.sh"" rel=""noreferrer"">ci/speedpack/build.sh</a>. This script essentially builds a bunch of wheels that we use (numpy and scipy included) for CI. They are actually stored on server and then pulled from there when travis-ci runs.</p>

<p>Take a look at <a href=""https://github.com/pydata/pandas/blob/master/ci/install.sh"" rel=""noreferrer"">ci/install.sh</a> to see how the installation process works.</p>

<p>In your situation a server might be overkill, but you could setup a local repo and install wheels from there.</p>
";;3;;2013-08-05T20:47:21.757;;18067431;2013-08-05T20:47:21.757;;;;;564538.0;18067073.0;2;13;;;
5434;5434;;;"<p>Place both series in the python set container.  See documentation:  <a href=""http://docs.python.org/2/library/sets.html"">http://docs.python.org/2/library/sets.html</a>  </p>

<p>then use the set intersection method.</p>

<p>s1.intersection(s2)   and then transform back to list if needed.</p>

<p>Just noticed pandas in the tag.   Can translate back to that.</p>

<pre><code>pd.Series(list(set(s1).intersection(set(s2))))
</code></pre>

<p>from comments I have changed this to a more pythonic expression, which is shorter and easier to read:</p>

<pre><code>Series(list(set(s1) &amp; set(s2)))
</code></pre>

<p>should to the trick, except if the index data is also important to you</p>

<p>Have added the list(....)  to translate the set before going to pd.Series   Pandas does not accept a set as direct input for a Series.</p>
";;10;;2013-08-06T12:05:28.283;;18079695;2015-07-10T06:46:25.613;2015-07-10T06:46:25.613;;2107677.0;;2107677.0;18079563.0;2;24;;;
5435;5435;;;"<p>If you are using Panda's, I assume you are also using NumPy. Numpy has a function <code>intersect1d</code> that will work with a Pandas' series. </p>

<p>Example:</p>

<pre><code>pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))
</code></pre>

<p>will return a Series with the values 5 and 42.</p>
";;5;;2013-08-06T12:26:44.563;;18080142;2013-08-06T12:26:44.563;;;;;2653755.0;18079563.0;2;8;;;
5453;5453;;;"<p>If you know the <code>dtype</code>s of your array then you can directly compute the number of bytes that it will take to store your data + some for the Python objects themselves. A useful attribute of <code>numpy</code> arrays is <code>nbytes</code>. You can get the number of bytes from the arrays in a pandas <code>DataFrame</code> by doing</p>

<pre><code>nbytes = sum(block.values.nbytes for block in df.blocks.values())
</code></pre>

<p><code>object</code> dtype arrays store 8 bytes per object (object dtype arrays store a pointer to an opaque <code>PyObject</code>), so if you have strings in your csv you need to take into account that <code>read_csv</code> will turn those into <code>object</code> dtype arrays and adjust your calculations accordingly.</p>

<p>EDIT:</p>

<p>See the <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html#built-in-scalar-types"" rel=""nofollow""><code>numpy</code> scalar types page</a> for more details on the <code>object</code> <code>dtype</code>. Since only a reference is stored you need to take into account the size of the object in the array as well. As that page says, object arrays are somewhat similar to Python <code>list</code> objects.</p>
";;4;;2013-08-06T20:38:29.463;;18090009;2015-03-06T17:14:13.557;2015-03-06T17:14:13.557;;564538.0;;564538.0;18089667.0;2;8;;;
5456;5456;;;"<p>You have to do this in reverse.</p>

<pre><code>In [4]: DataFrame(randn(1000000,20)).to_csv('test.csv')

In [5]: !ls -ltr test.csv
-rw-rw-r-- 1 users 399508276 Aug  6 16:55 test.csv
</code></pre>

<p>Technically memory is about this (which includes the indexes)</p>

<pre><code>In [16]: df.values.nbytes + df.index.nbytes + df.columns.nbytes
Out[16]: 168000160
</code></pre>

<p>So 168MB in memory with a 400MB file, 1M rows of 20 float columns</p>

<pre><code>DataFrame(randn(1000000,20)).to_hdf('test.h5','df')

!ls -ltr test.h5
-rw-rw-r-- 1 users 168073944 Aug  6 16:57 test.h5
</code></pre>

<p>MUCH more compact when written as a binary HDF5 file</p>

<pre><code>In [12]: DataFrame(randn(1000000,20)).to_hdf('test.h5','df',complevel=9,complib='blosc')

In [13]: !ls -ltr test.h5
-rw-rw-r-- 1 users 154727012 Aug  6 16:58 test.h5
</code></pre>

<p>The data was random, so compression doesn't help too much</p>
";;7;;2013-08-06T21:00:08.960;;18090393;2013-08-06T21:05:17.513;2013-08-06T21:05:17.513;;644898.0;;644898.0;18089667.0;2;23;;;
5470;5470;;;"<p>You can do it in javascript using jQuery:</p>

<pre><code> $('table tbody tr').filter(':last').css('background-color', '#FF0000')
</code></pre>

<p>Also newer versions of pandas add a class <code>dataframe</code> to the table html so you can filter out just the pandas tables using:</p>

<pre><code> $('table.dataframe tbody tr').filter(':last').css('background-color', '#FF0000')
</code></pre>

<p>But you can add your own classes if you want:</p>

<pre><code>df.to_html(classes='my_class')
</code></pre>

<p>Or even multiple:</p>

<pre><code>df.to_html(classes=['my_class', 'my_other_class'])
</code></pre>

<p>If you are using the IPython Notebook here is the full working example:</p>

<pre><code>In [1]: import numpy as np
        import pandas as pd
        from IPython.display import HTML, Javascript
In [2]: df = pd.DataFrame({'a': np.arange(10), 'b': np.random.randn(10)})
In [3]: HTML(df.to_html(classes='my_class'))
In [4]: Javascript('''$('.my_class tbody tr').filter(':last')
                                             .css('background-color', '#FF0000');
                   ''')
</code></pre>

<p>Or you can even use plain CSS:</p>

<pre><code>In [5]: HTML('''
        &lt;style&gt;
            .df tbody tr:last-child { background-color: #FF0000; }
        &lt;/style&gt;
        ''' + df.to_html(classes='df'))
</code></pre>

<p>The possibilities are endless :)</p>

<p><em><strong>Edit:</strong> create an html file</em></p>

<pre><code>import numpy as np
import pandas as pd

HEADER = '''
&lt;html&gt;
    &lt;head&gt;
        &lt;style&gt;
            .df tbody tr:last-child { background-color: #FF0000; }
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
'''
FOOTER = '''
    &lt;/body&gt;
&lt;/html&gt;
'''

df = pd.DataFrame({'a': np.arange(10), 'b': np.random.randn(10)})
with open('test.html', 'w') as f:
    f.write(HEADER)
    f.write(df.to_html(classes='df'))
    f.write(FOOTER)
</code></pre>
";;11;;2013-08-07T11:16:18.263;;18101965;2013-08-08T01:25:46.120;2013-08-08T01:25:46.120;;2199958.0;;2199958.0;18096748.0;2;15;;;
5471;5471;;;"<p>To query the <em>df</em> by the <em>MultiIndex</em> values, for example where <em>(A > 1.7) and (B &lt; 666)</em>:</p>

<pre><code>In [536]: result_df = df.loc[(df.index.get_level_values('A') &gt; 1.7) &amp; (df.index.get_level_values('B') &lt; 666)]

In [537]: result_df
Out[537]: 
          C
A   B      
3.3 222  43
    333  59
5.5 333  56
</code></pre>

<p>Hence, to get for example the <em>'A'</em> index values, if still required:</p>

<pre><code>In [538]: result_df.index.get_level_values('A')
Out[538]: Index([3.3, 3.3, 5.5], dtype=object)
</code></pre>

<p>The problem is, that in large data frames the performance of <em>by index</em> selection worse by 10% than the sorted regular rows selection. And in repetitive work, looping, the delay accumulated. See example:</p>

<pre><code>In [558]: df = store.select(STORE_EXTENT_BURSTS_DF_KEY)

In [559]: len(df)
Out[559]: 12857

In [560]: df.sort(inplace=True)

In [561]: df_without_index = df.reset_index()

In [562]: %timeit df.loc[(df.index.get_level_values('END_TIME') &gt; 358200) &amp; (df.index.get_level_values('START_TIME') &lt; 361680)]
1000 loops, best of 3: 562 s per loop

In [563]: %timeit df_without_index[(df_without_index.END_TIME &gt; 358200) &amp; (df_without_index.START_TIME &lt; 361680)]
1000 loops, best of 3: 507 s per loop
</code></pre>
";;0;;2013-08-07T12:51:15.223;;18103894;2013-08-07T12:51:15.223;;;;;1248167.0;17921010.0;2;37;;;
5475;5475;;;"<p><strong>Update: Make sure to check out the answer below, as Pandas now has built-in support for chunked loading.</strong></p>

<p>You could simply try to read the input table chunk-wise and assemble your full dataframe from the individual pieces afterwards, like this:</p>

<pre><code>import pandas as pd
import pandas.io.sql as psql
chunk_size = 10000
offset = 0
dfs = []
while True:
  sql = ""SELECT * FROM MyTable limit %d offset %d order by ID"" % (chunk_size,offset) 
  dfs.append(psql.read_frame(sql, cnxn))
  offset += chunk_size
  if len(dfs[-1]) &lt; chunk_size:
    break
full_df = pd.concat(dfs)
</code></pre>

<p>It might also be possible that the whole dataframe is simply too large to fit in memory, in that case you will have no other option than to restrict the number of rows or columns you're selecting.</p>
";;2;;2013-08-07T16:10:16.237;;18108357;2017-01-02T10:38:51.253;2017-01-02T10:38:51.253;;654162.0;;654162.0;18107953.0;2;16;;;
5493;5493;;;"<p>you could also try;</p>

<pre><code>data = pd.read_csv('file1.csv', error_bad_lines=False)
</code></pre>
";;5;;2013-08-08T14:47:15.083;;18129082;2013-08-08T14:47:15.083;;;;;1948860.0;18039057.0;2;127;;;
5512;5512;;;"<p>The best way to do this in pandas is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">drop</a>:</p>

<pre><code>df = df.drop('column_name', 1)
</code></pre>

<p>where <code>1</code> is the <em>axis</em> number (<code>0</code> for rows and <code>1</code> for columns.)</p>

<p>To delete the column without having to reassign <code>df</code> you can do:</p>

<pre><code>df.drop('column_name', axis=1, inplace=True)
</code></pre>

<p>Finally, to drop by column <em>number</em> instead of by column <em>label</em>, try this to delete, e.g. the 1st, 2nd and 4th columns:</p>

<pre><code>df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index 
</code></pre>
";;14;;2013-08-09T11:12:09.200;;18145399;2016-09-07T12:44:02.227;2016-09-07T12:44:02.227;;2071807.0;;2071807.0;13411544.0;2;1202;;;
5535;5535;;;"<p>Try following code:</p>

<pre><code>import pandas

sample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0},
        'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0},
        'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}

df = pandas.DataFrame([
    [col1,col2,col3] for col1, d in sample.items() for col2, col3 in d.items()
])
</code></pre>
";;4;;2013-08-10T12:45:26.573;;18162021;2013-08-10T12:45:26.573;;;;;2225682.0;18161926.0;2;16;;;
5537;5537;;;"<p>I think the operation you're after -- to unpivot a table -- is called ""melting"".  In this case, the hard part can be done by <code>pd.melt</code>, and everything else is basically renaming and reordering:</p>

<pre><code>df = pd.DataFrame(sample).reset_index().rename(columns={""index"": ""item""})
df = pd.melt(df, ""item"", var_name=""user"").dropna()
df = df[[""user"", ""item"", ""value""]].reset_index(drop=True)
</code></pre>

<hr>

<p>Simply calling <code>DataFrame</code> produces something which has the information we want but has the wrong shape:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(sample)
&gt;&gt;&gt; df
       user1  user2  user3
item1    2.5    2.5    NaN
item2    3.5    3.0    4.5
item3    3.0    3.5    NaN
item4    3.5    4.0    NaN
item5    2.5    NaN    1.0
item6    3.0    NaN    4.0
</code></pre>

<p>So let's promote the index to a real column and improve the name:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(sample).reset_index().rename(columns={""index"": ""item""})
&gt;&gt;&gt; df
    item  user1  user2  user3
0  item1    2.5    2.5    NaN
1  item2    3.5    3.0    4.5
2  item3    3.0    3.5    NaN
3  item4    3.5    4.0    NaN
4  item5    2.5    NaN    1.0
5  item6    3.0    NaN    4.0
</code></pre>

<p>Then we can call <code>pd.melt</code> to turn the columns.  If we don't specify the variable name we want, ""user"", it'll give it the boring name of ""variable"" (just like it gives the data itself the boring name ""value"").</p>

<pre><code>&gt;&gt;&gt; df = pd.melt(df, ""item"", var_name=""user"").dropna()
&gt;&gt;&gt; df
     item   user  value
0   item1  user1    2.5
1   item2  user1    3.5
2   item3  user1    3.0
3   item4  user1    3.5
4   item5  user1    2.5
5   item6  user1    3.0
6   item1  user2    2.5
7   item2  user2    3.0
8   item3  user2    3.5
9   item4  user2    4.0
13  item2  user3    4.5
16  item5  user3    1.0
17  item6  user3    4.0
</code></pre>

<p>Finally, we can reorder and renumber the indices:</p>

<pre><code>&gt;&gt;&gt; df = df[[""user"", ""item"", ""value""]].reset_index(drop=True)
&gt;&gt;&gt; df
     user   item  value
0   user1  item1    2.5
1   user1  item2    3.5
2   user1  item3    3.0
3   user1  item4    3.5
4   user1  item5    2.5
5   user1  item6    3.0
6   user2  item1    2.5
7   user2  item2    3.0
8   user2  item3    3.5
9   user2  item4    4.0
10  user3  item2    4.5
11  user3  item5    1.0
12  user3  item6    4.0
</code></pre>

<p><code>melt</code> is pretty useful once you get used to it.  Usually, as here, you do some renaming/reordering before and after.</p>
";;1;;2013-08-10T13:06:03.240;;18162196;2013-08-10T13:06:03.240;;;;;487339.0;18161926.0;2;13;;;
5544;5544;;;"<p><code>read_csv</code> takes an <code>encoding</code> option to deal with files in different formats. I mostly use <code>read_csv('file', encoding = ""ISO-8859-1"")</code>, or alternatively <code>encoding = utf8</code> for reading, and generally <code>utf-8</code> for <code>to_csv</code>.</p>

<p>You can also use the <a href=""https://en.wikipedia.org/wiki/ISO/IEC_8859-1"" rel=""noreferrer"">alias</a> <code>'latin1'</code> instead of <code>'ISO-8859-1'</code>.</p>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html,"" rel=""noreferrer"">relevant Pandas documentation</a>,
<a href=""http://docs.python.org/2/library/csv.html#examples"" rel=""noreferrer"">python docs examples on csv files</a>, and plenty of related questions here on SO.</p>
";;3;;2013-08-11T13:10:22.297;;18172249;2017-05-26T16:30:47.647;2017-05-26T16:30:47.647;;1267833.0;;1494637.0;18171739.0;2;155;;;
5549;5549;;;"<p>If I'm understanding correctly, it should be as simple as:</p>

<pre><code>df = df[df.line_race != 0]
</code></pre>
";;9;;2013-08-11T14:38:31.897;;18173074;2013-08-11T14:38:31.897;;;;;402468.0;18172851.0;2;352;;;
5550;5550;;;"<p>The best way to do this is with boolean masking:</p>

<pre><code>In [56]: df
Out[56]:
     line_date  daysago  line_race  rating    raw  wrating
0   2007-03-31       62         11      56  1.000   56.000
1   2007-03-10       83         11      67  1.000   67.000
2   2007-02-10      111          9      66  1.000   66.000
3   2007-01-13      139         10      83  0.881   73.096
4   2006-12-23      160         10      88  0.793   69.787
5   2006-11-09      204          9      52  0.637   33.106
6   2006-10-22      222          8      66  0.582   38.408
7   2006-09-29      245          9      70  0.519   36.318
8   2006-09-16      258         11      68  0.486   33.063
9   2006-08-30      275          8      72  0.447   32.160
10  2006-02-11      475          5      65  0.165   10.698
11  2006-01-13      504          0      70  0.142    9.969
12  2006-01-02      515          0      64  0.135    8.627
13  2005-12-06      542          0      70  0.118    8.246
14  2005-11-29      549          0      70  0.114    7.963
15  2005-11-22      556          0      -1  0.110   -0.110
16  2005-11-01      577          0      -1  0.099   -0.099
17  2005-10-20      589          0      -1  0.093   -0.093
18  2005-09-27      612          0      -1  0.083   -0.083
19  2005-09-07      632          0      -1  0.075   -0.075
20  2005-06-12      719          0      69  0.049    3.360
21  2005-05-29      733          0      -1  0.045   -0.045
22  2005-05-02      760          0      -1  0.040   -0.040
23  2005-04-02      790          0      -1  0.034   -0.034
24  2005-03-13      810          0      -1  0.031   -0.031
25  2004-11-09      934          0      -1  0.017   -0.017

In [57]: df[df.line_race != 0]
Out[57]:
     line_date  daysago  line_race  rating    raw  wrating
0   2007-03-31       62         11      56  1.000   56.000
1   2007-03-10       83         11      67  1.000   67.000
2   2007-02-10      111          9      66  1.000   66.000
3   2007-01-13      139         10      83  0.881   73.096
4   2006-12-23      160         10      88  0.793   69.787
5   2006-11-09      204          9      52  0.637   33.106
6   2006-10-22      222          8      66  0.582   38.408
7   2006-09-29      245          9      70  0.519   36.318
8   2006-09-16      258         11      68  0.486   33.063
9   2006-08-30      275          8      72  0.447   32.160
10  2006-02-11      475          5      65  0.165   10.698
</code></pre>

<p><strong>UPDATE:</strong> Now that pandas 0.13 is out, another way to do this is <code>df.query('line_race != 0')</code>.</p>
";;5;;2013-08-11T14:40:32.407;;18173088;2014-02-18T01:00:33.757;2014-02-18T01:00:33.757;;564538.0;;564538.0;18172851.0;2;28;;;
5558;5558;;;"<pre><code>df2 = pd.DataFrame(index=df1.index)
</code></pre>

<p>This will create a DataFrame with no columns but just an index, and it will be the same index as in the df1.</p>
";;4;;2013-08-11T21:37:43.650;;18176957;2013-08-11T21:37:43.650;;;;;2199958.0;18176933.0;2;20;;;
5566;5566;;;"<p>Bit convoluted but if you want to totally ignore the index data.    Convert the contents of the dataframes to sets of tuples containing the columns:</p>

<pre><code>ds1 = set([ tuple(line) for line in df1.values.tolist()])
ds2 = set([ tuple(line) for line in df2.values.tolist()])
</code></pre>

<p>This step will get rid of any duplicates in the dataframes as well (index ignored)</p>

<pre><code>set([(1, 2), (3, 4), (2, 3)])   # ds1
</code></pre>

<p>can then use set methods to find anything.  Eg to find differences:</p>

<pre><code>ds1.difference(ds2)
</code></pre>

<p>gives:
    set([(1, 2), (3, 4)])</p>

<p>can take that back to dataframe if needed.  Note have to transform set to list 1st as set cannot be used to construct dataframe:</p>

<pre><code>pd.DataFrame(list(ds1.difference(ds2)))
</code></pre>
";;3;;2013-08-12T10:43:19.257;;18184990;2013-08-12T11:35:59.577;2013-08-12T11:35:59.577;;2107677.0;;2107677.0;18180763.0;2;18;;;
5585;5585;;;"<pre><code>df.loc[df['line_race'] == 0, 'rating'] = 0
</code></pre>
";;0;;2013-08-12T20:40:21.523;;18196299;2013-08-12T20:40:21.523;;;;;2199958.0;18196203.0;2;47;;;
5595;5595;;;"<p>You could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html#numpy-where"">np.where</a> like this:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(1,7).reshape(2,3),
                  columns = list('abc'), 
                  index=pd.Series([2,5], name='b'))
print(df)
#    a  b  c
# b         
# 2  1  2  3
# 5  4  5  6
print(np.where(df.index==5)[0])
# [1]
print(np.where(df['c']==6)[0])
# [1]
</code></pre>

<p>The value returned is an array since there could be more than one row with a particular index or value in a column.</p>
";;2;;2013-08-13T01:38:30.673;;18199337;2015-01-02T11:57:25.757;2015-01-02T11:57:25.757;;190597.0;;190597.0;18199288.0;2;15;;;
5612;5612;;;"<p>You can convert it to a timedelta with a day precision. To extract the integer value of days you divide it with a timedelta of one day.</p>

<pre><code>&gt;&gt;&gt; x = np.timedelta64(2069211000000000, 'ns')
&gt;&gt;&gt; days = x.astype('timedelta64[D]')
&gt;&gt;&gt; days / np.timedelta64(1, 'D')
23
</code></pre>

<p>Or, as @PhillipCloud suggested, just <code>days.astype(int)</code> since the <code>timedelta</code> is just a 64bit integer that is interpreted in various ways depending on the second parameter you passed in (<code>'D'</code>, <code>'ns'</code>, ...).</p>

<p>You can find more about it <a href=""http://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html"">here</a>.</p>
";;4;;2013-08-13T17:28:41.617;;18215499;2013-08-13T19:34:01.900;2013-08-13T19:34:01.900;;2199958.0;;2199958.0;18215317.0;2;67;;;
5626;5626;;;"<p>Revised, converting to period and then back to timestamp does the trick</p>

<pre><code>In [104]: df = DataFrame(dict(date = [Timestamp('20130101'),Timestamp('20130131'),Timestamp('20130331'),Timestamp('20130330')],value=randn(4))).set_index('date')

In [105]: df
Out[105]: 
               value
date                
2013-01-01 -0.346980
2013-01-31  1.954909
2013-03-31 -0.505037
2013-03-30  2.545073

In [106]: df.index = df.index.to_period('M').to_timestamp('M')

In [107]: df
Out[107]: 
               value
2013-01-31 -0.346980
2013-01-31  1.954909
2013-03-31 -0.505037
2013-03-31  2.545073
</code></pre>

<p>Note that this type of conversion can also be done like this, the above would be slightly faster, though.</p>

<pre><code>In [85]: df.index + pd.offsets.MonthEnd(0) 
Out[85]: DatetimeIndex(['2013-01-31', '2013-01-31', '2013-03-31', '2013-03-31'], dtype='datetime64[ns]', name=u'date', freq=None, tz=None)
</code></pre>
";;8;;2013-08-14T14:01:15.460;;18233876;2015-06-11T17:30:48.790;2015-06-11T17:30:48.790;;644898.0;;644898.0;18233107.0;2;18;;;
5665;5665;;;"<p>You just have to do <code>df.unstack()</code> and that will create a MultiIndexed Series with month as a first level and the year as the second level index. If you want them to be columns then just call <code>reset_index()</code> after that.</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df
      Jan  Feb
2001    3    4
2002    2    7
&gt;&gt;&gt; df.unstack()
Jan  2001    3
     2002    2
Feb  2001    4
     2002    7
&gt;&gt;&gt; df = df.unstack().reset_index(name='value')
&gt;&gt;&gt; df
  level_0  level_1  value
0     Jan     2001      3
1     Jan     2002      2
2     Feb     2001      4
3     Feb     2002      7
&gt;&gt;&gt; df.rename(columns={'level_0': 'month', 'level_1': 'year'}, inplace=True)
&gt;&gt;&gt; df
  month  year  value
0   Jan  2001      3
1   Jan  2002      2
2   Feb  2001      4
3   Feb  2002      7
</code></pre>
";;0;;2013-08-15T18:28:38.057;;18259236;2013-08-15T20:58:34.410;2013-08-15T20:58:34.410;;2199958.0;;2199958.0;18259067.0;2;19;;;
5669;5669;;;"<p>As of pandas 0.12 you can do:</p>

<pre><code>&gt;&gt;&gt; grouped.filter(lambda x: len(x) &gt; 1)

     A  B
0  foo  0
2  foo  2
3  foo  3
</code></pre>
";;3;;2013-08-15T21:13:15.160;;18261958;2013-08-15T22:03:39.643;2013-08-15T22:03:39.643;;1330293.0;;1330293.0;13167391.0;2;28;;;
5705;5705;;;"<p>You should apply your function along the axis=1. Function will receive a row as an argument, and anything it returns will be collected into a new series object</p>

<pre><code>df.apply(you_function, axis=1)
</code></pre>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': np.arange(3),
                       'b': np.random.rand(3)})
&gt;&gt;&gt; df
   a         b
0  0  0.880075
1  1  0.143038
2  2  0.795188
&gt;&gt;&gt; def func(row):
        return row['a'] + row['b']
&gt;&gt;&gt; df.apply(func, axis=1)
0    0.880075
1    1.143038
2    2.795188
dtype: float64
</code></pre>

<p>As for the second part of the question: row wise operations, even optimised ones, using pandas <code>apply</code>, are not the fastest solution there is. They are certainly <strong>a lot</strong> faster than a python for loop, but not the fastest. You can test that by timing operations and you'll see the difference.</p>

<p>Some operation could be converted to column oriented ones (one in my example could be easily converted to just <code>df['a'] + df['b']</code>), but others cannot. Especially if you have a lot of branching, special cases or other logic that should be perform on your row. In that case, if the <code>apply</code> is too slow for you, I would suggest <em>""Cython-izing""</em> your code. Cython plays really nicely with the NumPy C api and will give you the maximal speed you can achieve.</p>

<p>Or you can try <a href=""http://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2/"">numba</a>. :)</p>
";;6;;2013-08-16T22:23:57.447;;18283014;2013-08-17T19:10:28.587;2013-08-17T19:10:28.587;;2199958.0;;2199958.0;18282988.0;2;18;;;
5711;5711;;;"<p><code>df.values</code> gives you the raw NumPy <code>ndarray</code> without the indexes.</p>

<pre><code>&gt;&gt;&gt; df
   x   y
0  4  GE
1  1  RE
2  1  AE
3  4  CD
&gt;&gt;&gt; df.values
array([[4, 'GE'],
       [1, 'RE'],
       [1, 'AE'],
       [4, 'CD']], dtype=object)
</code></pre>

<p>You cannot have a DataFrame without the indexes, they are the whole point of the DataFrame :)</p>

<p>But just to be clear, this operation is not <em>inplace</em>:</p>

<pre><code>&gt;&gt;&gt; df.values is df.values
False
</code></pre>

<p>DataFrame keeps the data in two dimensional arrays grouped by type, so when you want the whole data frame it will have to find the LCD of all the dtypes and construct a 2D array of that type.</p>

<p>To instantiate a new data frame with the values from the old one, just pass the old DataFrame to the new ones constructor and no data will be copied the same data structures will be reused:</p>

<pre><code>&gt;&gt;&gt; df1 = pd.DataFrame([[1, 2], [3, 4]])
&gt;&gt;&gt; df2 = pd.DataFrame(df1)
&gt;&gt;&gt; df2.iloc[0,0] = 42
&gt;&gt;&gt; df1
    0  1
0  42  2
1   3  4
</code></pre>

<p>But you can explicitly specify the <code>copy</code> parameter:</p>

<pre><code>&gt;&gt;&gt; df1 = pd.DataFrame([[1, 2], [3, 4]])
&gt;&gt;&gt; df2 = pd.DataFrame(df1, copy=True)
&gt;&gt;&gt; df2.iloc[0,0] = 42
&gt;&gt;&gt; df1
   0  1
0  1  2
1  3  4
</code></pre>
";;3;;2013-08-17T15:01:51.837;;18290143;2013-08-17T15:13:51.043;2013-08-17T15:13:51.043;;2199958.0;;2199958.0;18290123.0;2;9;;;
5729;5729;;;"<p>I don't believe <code>apply</code> has access to the index; it treats each row as a numpy object, not a Series, as you can see:</p>

<pre><code>In [27]: s.apply(lambda x: type(x))
Out[27]: 
a  b
1  2    &lt;type 'numpy.float64'&gt;
3  6    &lt;type 'numpy.float64'&gt;
4  4    &lt;type 'numpy.float64'&gt;
</code></pre>

<p>To get around this limitation, promote the indexes to columns, apply your function, and recreate a Series with the original index.</p>

<pre><code>Series(s.reset_index().apply(f, axis=1).values, index=s.index)
</code></pre>

<p>Other approaches might use <code>s.get_level_values</code>, which often gets a little ugly in my opinion, or <code>s.iterrows()</code>, which is likely to be slower -- perhaps depending on exactly what <code>f</code> does.</p>
";;4;;2013-08-19T14:52:38.490;;18316830;2013-08-19T14:52:38.490;;;;;1221924.0;18316211.0;2;27;;;
5730;5730;;;"<p>Use <code>len(df)</code>. This works as of pandas 0.11 or maybe even earlier.</p>

<p><code>__len__()</code> is currently (0.12) documented with <code>Returns length of index</code>. Timing info, set up the same way as in root's answer:</p>

<pre><code>In [7]: timeit len(df.index)
1000000 loops, best of 3: 248 ns per loop

In [8]: timeit len(df)
1000000 loops, best of 3: 573 ns per loop
</code></pre>

<p>Due to one additional function call it is a bit slower than calling <code>len(df.index)</code> directly, but this should not play any role in most use cases.</p>
";;0;;2013-08-19T15:02:45.520;;18317067;2013-08-19T15:10:16.217;2013-08-19T15:10:16.217;;145400.0;;145400.0;15943769.0;2;56;;;
5747;5747;;;"<pre><code>&gt;&gt;&gt; myseries[myseries == 7]
3    7
dtype: int64
&gt;&gt;&gt; myseries[myseries == 7].index[0]
3
</code></pre>

<p>Though I admit that there should be a better way to do that, but this at least avoids iterating and looping through the object and moves it to the C level.</p>
";;1;;2013-08-20T05:52:43.623;;18327852;2016-11-07T14:03:06.677;2016-11-07T14:03:06.677;;240490.0;;2199958.0;18327624.0;2;57;;;
5749;5749;;;"<p>Converting to an Index, you can use <code>get_loc</code></p>

<pre><code>In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])

In [3]: Index(myseries).get_loc(7)
Out[3]: 3

In [4]: Index(myseries).get_loc(10)
KeyError: 10
</code></pre>

<p>Duplicate handling</p>

<pre><code>In [5]: Index([1,1,2,2,3,4]).get_loc(2)
Out[5]: slice(2, 4, None)
</code></pre>

<p>Will return a boolean array if non-contiguous returns</p>

<pre><code>In [6]: Index([1,1,2,1,3,2,4]).get_loc(2)
Out[6]: array([False, False,  True, False, False,  True, False], dtype=bool)
</code></pre>

<p>Uses a hashtable internally, so fast</p>

<pre><code>In [7]: s = Series(randint(0,10,10000))

In [9]: %timeit s[s == 5]
1000 loops, best of 3: 203 s per loop

In [12]: i = Index(s)

In [13]: %timeit i.get_loc(5)
1000 loops, best of 3: 226 s per loop
</code></pre>

<p>As Viktor points out, there is a one-time creation overhead to creating an index (its incurred when you actually DO something with the index, e.g. the <code>is_unique</code>)</p>

<pre><code>In [2]: s = Series(randint(0,10,10000))

In [3]: %timeit Index(s)
100000 loops, best of 3: 9.6 s per loop

In [4]: %timeit Index(s).is_unique
10000 loops, best of 3: 140 s per loop
</code></pre>
";;3;;2013-08-20T11:37:59.547;;18334025;2013-08-20T12:19:16.907;2013-08-20T12:19:16.907;;644898.0;;644898.0;18327624.0;2;16;;;
5764;5764;;;"<p>New in 0.12, groupby objects have a <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#filtration""><code>filter</code></a> method, allowing you to do these types of operations:</p>

<pre><code>In [11]: g = data.groupby('tag')

In [12]: g.filter(lambda x: len(x) &gt; 1)  # pandas 0.13.1
Out[12]:
   pid  tag
1    1   45
2    1   62
4    2   45
7    3   62
</code></pre>

<p><em>The function (the first argument of filter) is applied to each group (subframe), and the results include elements of the original DataFrame belonging to groups which evaluated to True.</em></p>

<p>Note: <a href=""https://github.com/pydata/pandas/issues/4621"">in 0.12 the ordering is different than in the original DataFrame</a>, this was fixed in 0.13+:</p>

<pre><code>In [21]: g.filter(lambda x: len(x) &gt; 1)  # pandas 0.12
Out[21]: 
   pid  tag
1    1   45
4    2   45
2    1   62
7    3   62
</code></pre>
";;9;;2013-08-21T12:51:45.967;;18357933;2014-02-28T21:27:20.603;2014-02-28T21:27:20.603;;1240268.0;;1240268.0;13446480.0;2;21;;;
5766;5766;;;"<p>To get the <code>index</code> values as a <code>list</code>/<code>list</code> of <code>tuple</code>s for <code>Index</code>/<code>MultiIndex</code> do:</p>

<pre><code>df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this
</code></pre>

<p>or</p>

<pre><code>list(df.index.values)  # this will always work in pandas
</code></pre>
";;1;;2013-08-21T14:29:34.990;;18360223;2013-08-21T14:29:34.990;;;;;564538.0;18358938.0;2;53;;;
5773;5773;;;"<p>One workaround is to specify skiprows to ignore the first few entries:</p>

<pre><code>In [11]: s = '# notes\na,b,c\n# more notes\n1,2,3'

In [12]: pd.read_csv(StringIO(s), sep=',', comment='#', skiprows=1)
Out[12]: 
    a   b   c
0 NaN NaN NaN
1   1   2   3
</code></pre>

<p>Otherwise <code>read_csv</code> gets a little confused:</p>

<pre><code>In [13]: pd.read_csv(StringIO(s), sep=',', comment='#')
Out[13]: 
        Unnamed: 0
a   b            c
NaN NaN        NaN
1   2            3
</code></pre>

<p><em>This seems to be the case in 0.12.0, I've filed <a href=""https://github.com/pydata/pandas/issues/4623"" rel=""noreferrer"">a bug report</a>.</em></p>

<p>As Viktor points out you can use dropna to remove the NaN after the fact... (there is a <a href=""https://github.com/pydata/pandas/issues/4466"" rel=""noreferrer"">recent open issue</a> to have commented lines be ignored completely):</p>

<pre><code>In [14]: pd.read_csv(StringIO(s2), comment='#', sep=',').dropna(how='all')
Out[14]: 
   a  b  c
1  1  2  3
</code></pre>

<p><em>Note: the default index will ""give away"" the fact there was missing data.</em></p>
";;1;;2013-08-21T20:20:22.847;;18366911;2013-08-21T21:10:06.670;2013-08-21T21:10:06.670;;1240268.0;;1240268.0;18366797.0;2;7;;;
5780;5780;;;"<p>(to expand a bit on my comment)</p>

<p>Numpy developers follow in general a policy of keeping a backward compatible binary interface (ABI). However, the ABI is not forward compatible.</p>

<p>What that means:</p>

<p>A package, that uses numpy in a compiled extension, is compiled against a specific version of numpy. Future version of numpy will be compatible with the compiled extension of the package (for exception see below).
Distributers of those other packages do not need to recompile their package against a newer  versions of numpy and users do not need to update these other packages, when users update to a newer version of numpy.</p>

<p>However, this does not go in the other direction. If a package is compiled against a specific numpy version, say 1.7, then there is no guarantee that the binaries of that package will work with older numpy versions, say 1.6, and very often or most of the time they will not.</p>

<p>The binary distribution of packages like pandas and statsmodels, that are compiled against a recent version of numpy, will not work when an older version of numpy is installed.
Some packages, for example matplotlib, if I remember correctly, compile their extensions against the oldest numpy version that they support. In this case, users with the same old or any more recent version of numpy can use those binaries.</p>

<p>The error message in the question is a typical result of binary incompatibilities.</p>

<p>The solution is to get a binary compatible version, either by updating numpy to at least the version against which pandas or statsmodels were compiled, or to recompile pandas and statsmodels against the older version of numpy that is already installed.</p>

<p>Breaking the ABI backward compatibility:</p>

<p>Sometimes improvements or refactorings in numpy break ABI backward compatibility. This happened (unintentionally) with numpy 1.4.0.
As a consequence, users that updated numpy to 1.4.0, had binary incompatibilities with all other compiled packages, that were compiled against a previous version of numpy. This requires that all packages with binary extensions that use numpy have to be recompiled to work with the ABI incompatible version. </p>
";;0;;2013-08-21T23:32:22.360;;18369312;2013-08-21T23:40:36.893;2013-08-21T23:40:36.893;;333700.0;;333700.0;17709641.0;2;57;;;
5818;5818;;;"<pre><code>import pandas as pd
import numpy as np
data = np.random.random((8,3))*10000
df = pd.DataFrame (data)
pd.options.display.float_format = '{:20,.2f}'.format
print(df)
</code></pre>

<p>yields (random output similar to)</p>

<pre><code>                     0                    1                    2
0             4,839.01             6,170.02               301.63
1             4,411.23             8,374.36             7,336.41
2             4,193.40             2,741.63             7,834.42
3             3,888.27             3,441.57             9,288.64
4               220.13             6,646.20             3,274.39
5             3,885.71             9,942.91             2,265.95
6             3,448.75             3,900.28             6,053.93
</code></pre>

<p>The docstring for <code>pd.set_option</code> or <code>pd.describe_option</code> explains:</p>

<pre><code>display.float_format: [default: None] [currently: None] : callable
        The callable should accept a floating point number and return
        a string with the desired format of the number. This is used
        in some places like SeriesFormatter.
        See core.format.EngFormatter for an example.
</code></pre>
";;3;;2013-08-23T14:23:31.203;;18405221;2013-08-30T20:11:05.003;2013-08-30T20:11:05.003;;190597.0;;190597.0;18404946.0;2;24;;;
5841;5841;;;"<p>This is <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html#na-values-in-groupby"" rel=""noreferrer"">mentioned in the Missing Data section of the docs</a>:</p>

<blockquote>
  <p>NA groups in GroupBy are automatically excluded. This behavior is consistent with R, for example.</p>
</blockquote>

<p>One workaround is to use a placeholder before doing the groupby (e.g. -1):</p>

<pre><code>In [11]: df.fillna(-1)
Out[11]: 
   a   b
0  1   4
1  2  -1
2  3   6

In [12]: df.fillna(-1).groupby('b').sum()
Out[12]: 
    a
b    
-1  2
4   1
6   3
</code></pre>

<p><em>That said, this feels pretty awful hack... perhaps there should be an option to include NaN in groupby (see <a href=""https://github.com/pydata/pandas/issues/3729"" rel=""noreferrer"">this github issue</a> - which uses the same placeholder hack).</em></p>
";;10;;2013-08-25T16:55:34.173;;18431417;2017-05-18T13:37:26.417;2017-05-18T13:37:26.417;;1240268.0;;1240268.0;18429491.0;2;43;;;
5849;5849;;;"<p>First replace all the string values with <code>None</code>, to mark them as missing values and then convert it to float.</p>

<pre><code>df['foo'][df['foo'] == '-'] = None
df['foo'] = df['foo'].astype(float)
</code></pre>
";;2;;2013-08-25T22:08:31.083;;18434234;2013-08-25T22:08:31.083;;;;;2199958.0;18434208.0;2;6;;;
5850;5850;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html""><code>convert_objects</code></a> Series method (and <code>convert_numeric</code>):</p>

<pre><code>In [11]: s
Out[11]: 
0    103.8
1    751.1
2      0.0
3      0.0
4        -
5        -
6      0.0
7        -
8      0.0
dtype: object

In [12]: s.convert_objects(convert_numeric=True)
Out[12]: 
0    103.8
1    751.1
2      0.0
3      0.0
4      NaN
5      NaN
6      0.0
7      NaN
8      0.0
dtype: float64
</code></pre>

<p><em>Note: this is also available as a DataFrame method.</em></p>
";;3;;2013-08-25T22:33:14.673;;18434403;2013-08-25T22:33:14.673;;;;;1240268.0;18434208.0;2;30;;;
5870;5870;;;"<p>I just had the same question but with irregularly spaced datapoints. Resample is not really an option here. So I created my own function. Maybe it will be useful for others too:</p>

<pre><code>from pandas import Series, DataFrame
import pandas as pd
from datetime import datetime, timedelta
import numpy as np

def rolling_mean(data, window, min_periods=1, center=False):
    ''' Function that computes a rolling mean

    Parameters
    ----------
    data : DataFrame or Series
           If a DataFrame is passed, the rolling_mean is computed for all columns.
    window : int or string
             If int is passed, window is the number of observations used for calculating 
             the statistic, as defined by the function pd.rolling_mean()
             If a string is passed, it must be a frequency string, e.g. '90S'. This is
             internally converted into a DateOffset object, representing the window size.
    min_periods : int
                  Minimum number of observations in window required to have a value.

    Returns
    -------
    Series or DataFrame, if more than one column    
    '''
    def f(x):
        '''Function to apply that actually computes the rolling mean'''
        if center == False:
            dslice = col[x-pd.datetools.to_offset(window).delta+timedelta(0,0,1):x]
                # adding a microsecond because when slicing with labels start and endpoint
                # are inclusive
        else:
            dslice = col[x-pd.datetools.to_offset(window).delta/2+timedelta(0,0,1):
                         x+pd.datetools.to_offset(window).delta/2]
        if dslice.size &lt; min_periods:
            return np.nan
        else:
            return dslice.mean()

    data = DataFrame(data.copy())
    dfout = DataFrame()
    if isinstance(window, int):
        dfout = pd.rolling_mean(data, window, min_periods=min_periods, center=center)
    elif isinstance(window, basestring):
        idx = Series(data.index.to_pydatetime(), index=data.index)
        for colname, col in data.iterkv():
            result = idx.apply(f)
            result.name = colname
            dfout = dfout.join(result, how='outer')
    if dfout.columns.size == 1:
        dfout = dfout.ix[:,0]
    return dfout


# Example
idx = [datetime(2011, 2, 7, 0, 0),
       datetime(2011, 2, 7, 0, 1),
       datetime(2011, 2, 7, 0, 1, 30),
       datetime(2011, 2, 7, 0, 2),
       datetime(2011, 2, 7, 0, 4),
       datetime(2011, 2, 7, 0, 5),
       datetime(2011, 2, 7, 0, 5, 10),
       datetime(2011, 2, 7, 0, 6),
       datetime(2011, 2, 7, 0, 8),
       datetime(2011, 2, 7, 0, 9)]
idx = pd.Index(idx)
vals = np.arange(len(idx)).astype(float)
s = Series(vals, index=idx)
rm = rolling_mean(s, window='2min')
</code></pre>
";;5;;2013-08-27T13:38:11.130;;18467097;2014-04-09T06:28:05.570;2014-04-09T06:28:05.570;;2689410.0;;2689410.0;15771472.0;2;30;;;
5919;5919;;;"<p>Since Pandas 0.11.0 you can use dtype argument to explicitly specify data type for each column:</p>

<pre><code>d = pandas.read_csv('foo.csv', dtype={'BAR': 'S10'})
</code></pre>
";;2;;2013-08-29T01:22:23.857;;18500854;2015-10-01T03:19:13.437;2015-10-01T03:19:13.437;;1560062.0;;1560062.0;10591000.0;2;22;;;
5922;5922;;;"<p>You can do this easily manually for each column like this:</p>

<pre><code>df['A_perc'] = df['A']/df['sum']
</code></pre>

<hr>

<p>If you want to do this in one step for all columns, you can use the <code>div</code> method (<a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior"">http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior</a>):</p>

<pre><code>ds.div(ds['sum'], axis=0)
</code></pre>

<p>And if you want this in one step added to the same dataframe:</p>

<pre><code>&gt;&gt;&gt; ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')
          A         B         C         D       sum    A_perc    B_perc  \
1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   
2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   
3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   

     C_perc    D_perc  sum_perc  
1  0.337409  0.307517         1  
2  0.305722  0.385701         1  
3  0.010530  0.500718         1  
</code></pre>
";;3;;2013-08-29T07:49:14.690;;18505101;2013-08-29T07:58:30.573;2013-08-29T07:58:30.573;;653364.0;;653364.0;18504967.0;2;32;;;
5944;5944;;;"<p>You could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.average.html"">numpy.average</a> which allows you to specify weights:</p>

<pre><code>&gt;&gt;&gt; bin_avg[index] = np.average(items_in_bin, weights=my_weights)
</code></pre>

<p>So to calculate the weights you could find the x coordinates of each data point in the bin and calculate their distances to the bin center.</p>
";;1;;2013-08-29T18:34:05.410;;18518561;2013-08-29T18:34:05.410;;;;;2725015.0;18517722.0;2;6;;;
5962;5962;;;"<p>I am doing this all the time, so I tested different ways for speed.
The fastest I found is the following, approx. 3 times faster than Chang She's solution, at least in my case, when taking the total time of file parsing and date parsing into account:</p>

<p>First, parse the data file using pd.read_csv withOUT parsing dates. I find that it is slowing down the file-reading quite a lot. Make sure that the columns of the CSV file are now columns in the dataframe df. Then:</p>

<pre><code>format = ""%Y%m%d %H""
times = pd.to_datetime(df.YYYYMMDD + ' ' + df.HH, format=format)
df.set_index(times, inplace=True)
# and maybe for cleanup
df = df.drop(['YYYYMMDD','HH'], axis=1)
</code></pre>
";;3;;2013-08-30T07:15:45.510;;18527067;2013-08-30T07:15:45.510;;;;;680232.0;11615504.0;2;9;;;
5965;5965;;;"<p>You can use <a href=""https://code.google.com/p/prettytable/"" rel=""noreferrer"">prettytable</a> to render the table as text. The trick is to convert the data_frame to an in-memory csv file and have prettytable read it. Here's the code:</p>

<pre><code>from StringIO import StringIO
import prettytable    

output = StringIO()
data_frame.to_csv(output)
output.seek(0)
pt = prettytable.from_csv(output)
print pt
</code></pre>
";;2;;2013-08-30T08:43:41.777;;18528589;2013-08-30T08:43:41.777;;;;;374047.0;18528533.0;2;11;;;
5998;5998;;;"<p>How about either of:</p>

<pre><code>&gt;&gt;&gt; df
         date  duration user_id
0  2013-04-01        30    0001
1  2013-04-01        15    0001
2  2013-04-01        20    0002
3  2013-04-02        15    0002
4  2013-04-02        30    0002
&gt;&gt;&gt; df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": pd.Series.nunique})
            duration  user_id
date                         
2013-04-01        65        2
2013-04-02        45        1
&gt;&gt;&gt; df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": lambda x: x.nunique()})
            duration  user_id
date                         
2013-04-01        65        2
2013-04-02        45        1
</code></pre>
";;1;;2013-09-01T03:31:03.303;;18554949;2013-09-01T03:31:03.303;;;;;487339.0;18554920.0;2;69;;;
6018;6018;;;"<p>Use the <code>Series.quantile()</code> method:</p>

<pre><code>In [48]: cols = list('abc')

In [49]: df = DataFrame(randn(10, len(cols)), columns=cols)

In [50]: df.a.quantile(0.95)
Out[50]: 1.5776961953820687
</code></pre>

<p>To filter out rows of <code>df</code> where <code>df.a</code> is greater than or equal to the 95th percentile do:</p>

<pre><code>In [72]: df[df.a &lt; df.a.quantile(.95)]
Out[72]:
       a      b      c
0 -1.044 -0.247 -1.149
2  0.395  0.591  0.764
3 -0.564 -2.059  0.232
4 -0.707 -0.736 -1.345
5  0.978 -0.099  0.521
6 -0.974  0.272 -0.649
7  1.228  0.619 -0.849
8 -0.170  0.458 -0.515
9  1.465  1.019  0.966
</code></pre>
";;1;;2013-09-02T20:40:45.257;;18580496;2013-09-02T20:46:15.207;2013-09-02T20:46:15.207;;564538.0;;564538.0;18580461.0;2;27;;;
6029;6029;;;"<p>The answer I was looking for was a slight variation of what @Jeff proposed in his answer. The credit goes to him. This is what solved my problem in the end for reference:</p>

<pre><code>    import pandas
    df = pandas.DataFrame(data, columns=['a','b','c','d'], index=['x','y','z'])
    df = df.fillna(0)
    df = df.astype(int)
    df.to_csv('test.csv', sep='\t')
</code></pre>
";;1;;2013-09-03T09:42:55.590;;18588980;2013-09-03T09:42:55.590;;;;;287297.0;17092671.0;2;7;;;
6036;6036;;;"<p>To overcome the broadcasting issue, you can use the <code>div</code> method:</p>

<pre><code>df.div(df.sum(axis=1), axis=0)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior</a></p>
";;0;;2013-09-03T14:15:46.687;;18594595;2016-06-07T08:11:02.860;2016-06-07T08:11:02.860;;620382.0;;653364.0;18594469.0;2;60;;;
6059;6059;;;"<p>To tweak Jeff's answer (and have this as a reuseable function).</p>

<pre><code>def logged_apply(g, func, *args, **kwargs):
    step_percentage = 100. / len(g)
    import sys
    sys.stdout.write('apply progress:   0%')
    sys.stdout.flush()

    def logging_decorator(func):
        def wrapper(*args, **kwargs):
            progress = wrapper.count * step_percentage
            sys.stdout.write('\033[D \033[D' * 4 + format(progress, '3.0f') + '%')
            sys.stdout.flush()
            wrapper.count += 1
            return func(*args, **kwargs)
        wrapper.count = 0
        return wrapper

    logged_func = logging_decorator(func)
    res = g.apply(logged_func, *args, **kwargs)
    sys.stdout.write('\033[D \033[D' * 4 + format(100., '3.0f') + '%' + '\n')
    sys.stdout.flush()
    return res
</code></pre>

<p><em>Note: the apply progress percentage <a href=""https://stackoverflow.com/a/5426562/1240268"">updates inline</a>. If your function stdouts then this won't work.</em></p>

<pre><code>In [11]: g = df_users.groupby(['userID', 'requestDate'])

In [12]: f = feature_rollup

In [13]: logged_apply(g, f)
apply progress: 100%
Out[13]: 
...
</code></pre>

<p>As usual you can add this to your groupby objects as a method:</p>

<pre><code>from pandas.core.groupby import DataFrameGroupBy
DataFrameGroupBy.logged_apply = logged_apply

In [21]: g.logged_apply(f)
apply progress: 100%
Out[21]: 
...
</code></pre>

<p><em>As mentioned in the comments, this isn't a feature that core pandas would be interested in implementing. But python allows you to create these for many pandas objects/methods (doing so would be quite a bit of work... although you should be able to generalise this approach).</em></p>
";;9;;2013-09-04T10:37:32.340;;18611535;2015-12-19T16:45:12.660;2017-05-23T12:03:02.317;;-1.0;;1240268.0;18603270.0;2;11;;;
6071;6071;;;"<p>Just call <code>reset_index()</code>:</p>

<pre><code>In [130]: s
Out[130]:
0           1
1999-03-31  SOLD_PRICE   NaN
1999-06-30  SOLD_PRICE   NaN
1999-09-30  SOLD_PRICE   NaN
1999-12-31  SOLD_PRICE     3
2000-03-31  SOLD_PRICE     3
Name: 2, dtype: float64

In [131]: s.reset_index()
Out[131]:
            0           1   2
0  1999-03-31  SOLD_PRICE NaN
1  1999-06-30  SOLD_PRICE NaN
2  1999-09-30  SOLD_PRICE NaN
3  1999-12-31  SOLD_PRICE   3
4  2000-03-31  SOLD_PRICE   3
</code></pre>

<p>There are many ways to drop columns:</p>

<p>Call <code>reset_index()</code> twice and specify a column:</p>

<pre><code>In [136]: s.reset_index(0).reset_index(drop=True)
Out[136]:
            0   2
0  1999-03-31 NaN
1  1999-06-30 NaN
2  1999-09-30 NaN
3  1999-12-31   3
4  2000-03-31   3
</code></pre>

<p>Delete the column after resetting the index:</p>

<pre><code>In [137]: df = s.reset_index()

In [138]: df
Out[138]:
            0           1   2
0  1999-03-31  SOLD_PRICE NaN
1  1999-06-30  SOLD_PRICE NaN
2  1999-09-30  SOLD_PRICE NaN
3  1999-12-31  SOLD_PRICE   3
4  2000-03-31  SOLD_PRICE   3

In [139]: del df[1]

In [140]: df
Out[140]:
            0   2
0  1999-03-31 NaN
1  1999-06-30 NaN
2  1999-09-30 NaN
3  1999-12-31   3
4  2000-03-31   3
</code></pre>

<p>Call <code>drop()</code> after resetting:</p>

<pre><code>In [144]: s.reset_index().drop(1, axis=1)
Out[144]:
            0   2
0  1999-03-31 NaN
1  1999-06-30 NaN
2  1999-09-30 NaN
3  1999-12-31   3
4  2000-03-31   3
</code></pre>

<p>Then, after you've reset your index, just rename the columns</p>

<pre><code>In [146]: df.columns = ['Date', 'Sales']

In [147]: df
Out[147]:
         Date  Sales
0  1999-03-31    NaN
1  1999-06-30    NaN
2  1999-09-30    NaN
3  1999-12-31      3
4  2000-03-31      3
</code></pre>
";;0;;2013-09-04T21:25:35.407;;18624069;2013-09-04T21:30:57.803;2013-09-04T21:30:57.803;;564538.0;;564538.0;18624039.0;2;30;;;
6072;6072;;;"<p>When you use double brackets, such as</p>

<pre><code>H3 = H2[['SOLD_PRICE']]
</code></pre>

<p>H3 becomes a DataFrame. If you use single brackets,</p>

<pre><code>H3 = H2['SOLD_PRICE']
</code></pre>

<p>then H3 becomes a Series. If H3 is a Series, then the result you desire follows naturally:</p>

<pre><code>import pandas as pd
import numpy as np
rng = pd.date_range('1/1/2011', periods=72, freq='M')
H2 = pd.DataFrame(np.arange(len(rng)), index=rng, columns=['SOLD_PRICE'])
H3 = H2['SOLD_PRICE']
H5 = H3.resample('Q', how='count')
H6 = pd.rolling_mean(H5,4)
print(H6.head())
</code></pre>

<p>yields</p>

<pre><code>2011-03-31   NaN
2011-06-30   NaN
2011-09-30   NaN
2011-12-31     3
2012-03-31     3
dtype: float64
</code></pre>
";;3;;2013-09-04T21:46:08.830;;18624323;2013-09-04T21:46:08.830;;;;;190597.0;18624039.0;2;13;;;
6097;6097;;;"<pre><code>import numpy as np
import pandas as pd
import scipy.sparse as sparse

df = pd.DataFrame(np.arange(1,10).reshape(3,3))
arr = sparse.coo_matrix(([1,1,1], ([0,1,2], [1,2,0])), shape=(3,3))
df['newcol'] = arr.toarray().tolist()
print(df)
</code></pre>

<p>yields</p>

<pre><code>   0  1  2     newcol
0  1  2  3  [0, 1, 0]
1  4  5  6  [0, 0, 1]
2  7  8  9  [1, 0, 0]
</code></pre>
";;8;;2013-09-05T21:29:40.320;;18646275;2013-09-05T21:29:40.320;;;;;190597.0;18646076.0;2;18;;;
6099;6099;;;"<p>Consider using a higher dimensional datastructure (a <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel"" rel=""noreferrer"">Panel</a>), rather than storing an array in your column:</p>

<pre><code>In [11]: p = pd.Panel({'df': df, 'csc': csc})

In [12]: p.df
Out[12]: 
   0  1  2
0  1  2  3
1  4  5  6
2  7  8  9

In [13]: p.csc
Out[13]: 
   0  1  2
0  0  1  0
1  0  0  1
2  1  0  0
</code></pre>

<p>Look at cross-sections etc, etc, etc.</p>

<pre><code>In [14]: p.xs(0)
Out[14]: 
   csc  df
0    0   1
1    1   2
2    0   3
</code></pre>

<p><em><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel"" rel=""noreferrer"">See the docs for more on Panels</a>.</em></p>
";;5;;2013-09-05T22:13:48.380;;18646802;2013-09-05T22:13:48.380;;;;;1240268.0;18646076.0;2;8;;;
6109;6109;;;"<p>Managed to solve this.</p>

<p>I made a function that goes through my columns that have strings and managed to decode/encode them into utf8 and it now works.</p>

<pre><code>def changeencode(data, cols):
    for col in cols:
        data[col] = data[col].str.decode('iso-8859-1').str.encode('utf-8')
    return data   
</code></pre>
";;1;;2013-09-06T15:35:33.153;;18661440;2013-09-06T15:35:33.153;;;;;1477617.0;18645401.0;2;15;;;
6115;6115;;;"<p>One solution is just to smash it with the Series constructor:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, {'a': 2}], [2, {'a': 1, 'b': 3}]])

In [2]: df
Out[2]: 
   0                   1
0  1           {u'a': 2}
1  2  {u'a': 1, u'b': 3}

In [3]: df[1].apply(pd.Series)
Out[3]: 
   a   b
0  2 NaN
1  1   3
</code></pre>

<p>In some cases you'll want to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html"">concat</a> this to the DataFrame in place of the dict row:</p>

<pre><code>In [4]: dict_col = df.pop(1)  # here 1 is the column name

In [5]: pd.concat([df, dict_col.apply(pd.Series)], axis=1)
Out[5]: 
   0  a   b
0  1  2 NaN
1  2  1   3
</code></pre>

<p><em>If the it goes deeper, you can do this a few times...</em></p>
";;4;;2013-09-06T20:32:29.603;;18666142;2013-09-06T20:32:29.603;;;;;1240268.0;18665284.0;2;19;;;
6128;6128;;;"<p>see docs: <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion</a></p>

<p>using idx = 0 will insert at the beginning</p>

<pre><code>df.insert(idx, col_name, value)
</code></pre>
";;1;;2013-09-07T15:32:55.713;;18674915;2016-06-07T08:10:14.843;2016-06-07T08:10:14.843;;620382.0;;644898.0;18674064.0;2;117;;;
6131;6131;;;"<p>If I understand you correctly:</p>

<p>For (1) do this:</p>

<p>Make some fake data by sampling from the values you gave and some random dates and # of visits:</p>

<pre><code>In [179]: string = Series(np.random.choice(df.string.values, size=100), name='string')

In [180]: visits = Series(poisson(1000, size=100), name='date')

In [181]: date = Series(np.random.choice([df.date[0], now(), Timestamp('1/1/2001'), Timestamp('11/15/2001'), Timestamp('12/1/01'), Timestamp('5/1/01')], size=100), dtype='datetime64[ns]', name='date')

In [182]: df = DataFrame({'string': string, 'visits': visits, 'date': date})

In [183]: df.head()
Out[183]:
                 date   string  visits
0 2001-11-15 00:00:00  current     997
1 2001-11-15 00:00:00  current     974
2 2012-10-02 00:00:00     stem     982
3 2001-12-01 00:00:00     stem     984
4 2001-01-01 00:00:00  current     989

In [186]: resamp = df.set_index('date').groupby('string').resample('M', how='sum')

In [187]: resamp.head()
Out[187]:
                    visits
string  date
current 2001-01-31    2996
        2001-02-28     NaN
        2001-03-31     NaN
        2001-04-30     NaN
        2001-05-31    3016
</code></pre>

<p><code>NaN</code> is there because there were no visits with that query string in those months.</p>

<p>For (2), group by the dates and then divide by the sum:</p>

<pre><code>In [188]: g = resamp.groupby(level='date').apply(lambda x: x / x.sum())

In [189]: g.head()
Out[189]:
                    visits
string  date
current 2001-01-31   0.177
        2001-02-28     NaN
        2001-03-31     NaN
        2001-04-30     NaN
        2001-05-31   0.188
</code></pre>

<p>Just to convince you that (2) is doing what you want:</p>

<pre><code>In [176]: h = g.sortlevel('date').head()

In [177]: h
Out[177]:
                      visits
string    date
current   2001-01-31   0.077
molecular 2001-01-31   0.228
neuron    2001-01-31   0.073
nucleus   2001-01-31   0.234
stem      2001-01-31   0.388

In [178]: h.sum()
Out[178]:
visits    1
dtype: float64
</code></pre>

<p>If you want to convert <code>resamp</code> into a <code>DataFrame</code> and remove the <code>NaN</code>s do:</p>

<pre><code>In [196]: resamp.dropna()
Out[196]:
                      visits
string    date
current   2001-01-31    2996
          2001-05-31    3016
          2001-11-30    5959
          2001-12-31    3998
          2013-09-30    1077
molecular 2001-01-31    3984
          2001-05-31    1911
          2001-11-30    3054
          2001-12-31    1020
          2012-10-31     977
          2013-09-30    1947
neuron    2001-01-31    3961
          2001-05-31    2069
          2001-11-30    5010
          2001-12-31    2065
          2012-10-31    6973
          2013-09-30     994
nucleus   2001-01-31    3060
          2001-05-31    3035
          2001-11-30    2924
          2001-12-31    4144
          2012-10-31    2004
          2013-09-30    7881
stem      2001-01-31    2911
          2001-05-31    5994
          2001-11-30    6072
          2001-12-31    4916
          2012-10-31    1991
          2013-09-30    3977

In [197]: resamp.dropna().reset_index()
Out[197]:
       string                date  visits
0     current 2001-01-31 00:00:00    2996
1     current 2001-05-31 00:00:00    3016
2     current 2001-11-30 00:00:00    5959
3     current 2001-12-31 00:00:00    3998
4     current 2013-09-30 00:00:00    1077
5   molecular 2001-01-31 00:00:00    3984
6   molecular 2001-05-31 00:00:00    1911
7   molecular 2001-11-30 00:00:00    3054
8   molecular 2001-12-31 00:00:00    1020
9   molecular 2012-10-31 00:00:00     977
10  molecular 2013-09-30 00:00:00    1947
11     neuron 2001-01-31 00:00:00    3961
12     neuron 2001-05-31 00:00:00    2069
13     neuron 2001-11-30 00:00:00    5010
14     neuron 2001-12-31 00:00:00    2065
15     neuron 2012-10-31 00:00:00    6973
16     neuron 2013-09-30 00:00:00     994
17    nucleus 2001-01-31 00:00:00    3060
18    nucleus 2001-05-31 00:00:00    3035
19    nucleus 2001-11-30 00:00:00    2924
20    nucleus 2001-12-31 00:00:00    4144
21    nucleus 2012-10-31 00:00:00    2004
22    nucleus 2013-09-30 00:00:00    7881
23       stem 2001-01-31 00:00:00    2911
24       stem 2001-05-31 00:00:00    5994
25       stem 2001-11-30 00:00:00    6072
26       stem 2001-12-31 00:00:00    4916
27       stem 2012-10-31 00:00:00    1991
28       stem 2013-09-30 00:00:00    3977
</code></pre>

<p>You can of course do this for <code>g</code> as well:</p>

<pre><code>In [198]: g.dropna()
Out[198]:
                      visits
string    date
current   2001-01-31   0.177
          2001-05-31   0.188
          2001-11-30   0.259
          2001-12-31   0.248
          2013-09-30   0.068
molecular 2001-01-31   0.236
          2001-05-31   0.119
          2001-11-30   0.133
          2001-12-31   0.063
          2012-10-31   0.082
          2013-09-30   0.123
neuron    2001-01-31   0.234
          2001-05-31   0.129
          2001-11-30   0.218
          2001-12-31   0.128
          2012-10-31   0.584
          2013-09-30   0.063
nucleus   2001-01-31   0.181
          2001-05-31   0.189
          2001-11-30   0.127
          2001-12-31   0.257
          2012-10-31   0.168
          2013-09-30   0.496
stem      2001-01-31   0.172
          2001-05-31   0.374
          2001-11-30   0.264
          2001-12-31   0.305
          2012-10-31   0.167
          2013-09-30   0.251

In [199]: g.dropna().reset_index()
Out[199]:
       string                date  visits
0     current 2001-01-31 00:00:00   0.177
1     current 2001-05-31 00:00:00   0.188
2     current 2001-11-30 00:00:00   0.259
3     current 2001-12-31 00:00:00   0.248
4     current 2013-09-30 00:00:00   0.068
5   molecular 2001-01-31 00:00:00   0.236
6   molecular 2001-05-31 00:00:00   0.119
7   molecular 2001-11-30 00:00:00   0.133
8   molecular 2001-12-31 00:00:00   0.063
9   molecular 2012-10-31 00:00:00   0.082
10  molecular 2013-09-30 00:00:00   0.123
11     neuron 2001-01-31 00:00:00   0.234
12     neuron 2001-05-31 00:00:00   0.129
13     neuron 2001-11-30 00:00:00   0.218
14     neuron 2001-12-31 00:00:00   0.128
15     neuron 2012-10-31 00:00:00   0.584
16     neuron 2013-09-30 00:00:00   0.063
17    nucleus 2001-01-31 00:00:00   0.181
18    nucleus 2001-05-31 00:00:00   0.189
19    nucleus 2001-11-30 00:00:00   0.127
20    nucleus 2001-12-31 00:00:00   0.257
21    nucleus 2012-10-31 00:00:00   0.168
22    nucleus 2013-09-30 00:00:00   0.496
23       stem 2001-01-31 00:00:00   0.172
24       stem 2001-05-31 00:00:00   0.374
25       stem 2001-11-30 00:00:00   0.264
26       stem 2001-12-31 00:00:00   0.305
27       stem 2012-10-31 00:00:00   0.167
28       stem 2013-09-30 00:00:00   0.251
</code></pre>

<p>Lastly, if you want to put your columns in a different order, use <code>reindex</code>:</p>

<pre><code>In [210]: g.dropna().reset_index().reindex(columns=['visits', 'string', 'date'])
Out[210]:
    visits     string                date
0    0.177    current 2001-01-31 00:00:00
1    0.188    current 2001-05-31 00:00:00
2    0.259    current 2001-11-30 00:00:00
3    0.248    current 2001-12-31 00:00:00
4    0.068    current 2013-09-30 00:00:00
5    0.236  molecular 2001-01-31 00:00:00
6    0.119  molecular 2001-05-31 00:00:00
7    0.133  molecular 2001-11-30 00:00:00
8    0.063  molecular 2001-12-31 00:00:00
9    0.082  molecular 2012-10-31 00:00:00
10   0.123  molecular 2013-09-30 00:00:00
11   0.234     neuron 2001-01-31 00:00:00
12   0.129     neuron 2001-05-31 00:00:00
13   0.218     neuron 2001-11-30 00:00:00
14   0.128     neuron 2001-12-31 00:00:00
15   0.584     neuron 2012-10-31 00:00:00
16   0.063     neuron 2013-09-30 00:00:00
17   0.181    nucleus 2001-01-31 00:00:00
18   0.189    nucleus 2001-05-31 00:00:00
19   0.127    nucleus 2001-11-30 00:00:00
20   0.257    nucleus 2001-12-31 00:00:00
21   0.168    nucleus 2012-10-31 00:00:00
22   0.496    nucleus 2013-09-30 00:00:00
23   0.172       stem 2001-01-31 00:00:00
24   0.374       stem 2001-05-31 00:00:00
25   0.264       stem 2001-11-30 00:00:00
26   0.305       stem 2001-12-31 00:00:00
27   0.167       stem 2012-10-31 00:00:00
28   0.251       stem 2013-09-30 00:00:00
</code></pre>
";;9;;2013-09-07T20:16:13.727;;18677517;2013-09-07T20:41:47.573;2013-09-07T20:41:47.573;;564538.0;;564538.0;18677271.0;2;29;;;
6144;6144;;;"<p>Yes, use the <code>level</code> parameter. Take a look <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#groupby-with-multiindex"" rel=""noreferrer"">here</a>. Example:</p>

<pre><code>In [26]: s

first  second  third
bar    doo     one      0.404705
               two      0.577046
baz    bee     one     -1.715002
               two     -1.039268
foo    bop     one     -0.370647
               two     -1.157892
qux    bop     one     -1.344312
               two      0.844885
dtype: float64

In [27]: s.groupby(level=['first','second']).sum()

first  second
bar    doo       0.981751
baz    bee      -2.754270
foo    bop      -1.528539
qux    bop      -0.499427
dtype: float64
</code></pre>
";;1;;2013-09-08T23:05:39.930;;18689514;2013-09-08T23:05:39.930;;;;;1330293.0;18689474.0;2;16;;;
6145;6145;;;"<p>Is your type really arbitrary?  If you know it is just going to be a int float or string you could just do</p>

<pre><code> if val.dtype == float and np.isnan(val):
</code></pre>

<p>assuming it is wrapped in numpy , it will always have a dtype and only float and complex can be NaN</p>
";;4;;2013-09-08T23:15:16.887;;18689589;2013-09-08T23:15:16.887;;;;;852487.0;18689512.0;2;10;;;
6146;6146;;;"<p><code>pandas.isnull()</code> checks for missing values in both numeric and string/object arrays. From the documentation, it checks for:</p>

<blockquote>
  <p>NaN in numeric arrays, None/NaN in object arrays</p>
</blockquote>

<p>Quick example:</p>

<pre><code>import pandas as pd
import numpy as np
s = pd.Series(['apple', np.nan, 'banana'])
pd.isnull(s)
Out[9]: 
0    False
1     True
2    False
dtype: bool
</code></pre>

<p>The idea of using <code>numpy.nan</code> to represent missing values is something that <code>pandas</code> introduced, which is why <code>pandas</code> has the tools to deal with it.</p>

<p>Datetimes too (if you use <code>pd.NaT</code> you won't need to specify the dtype)</p>

<pre><code>In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')

In [25]: s
Out[25]: 
0   2013-01-01 00:00:00
1                   NaT
2   2013-01-02 09:30:00
dtype: datetime64[ns]``

In [26]: pd.isnull(s)
Out[26]: 
0    False
1     True
2    False
dtype: bool
</code></pre>
";;0;;2013-09-08T23:33:44.187;;18689712;2013-09-09T00:20:13.470;2013-09-09T00:20:13.470;;644898.0;;1222578.0;18689512.0;2;100;;;
6154;6154;;;"<p>You can simply use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna"" rel=""noreferrer""><code>DataFrame.fillna</code></a> to fill the <code>nan</code>'s directly:</p>

<pre><code>In [27]: df 
Out[27]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3       NaN -2.027325  1.533582
4       NaN       NaN  0.461821
5 -0.788073       NaN       NaN
6 -0.916080 -0.612343       NaN
7 -0.887858  1.033826       NaN
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431

In [28]: df.mean()
Out[28]: 
A   -0.151121
B   -0.231291
C   -0.530307
dtype: float64

In [29]: df.fillna(df.mean())
Out[29]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3 -0.151121 -2.027325  1.533582
4 -0.151121 -0.231291  0.461821
5 -0.788073 -0.231291 -0.530307
6 -0.916080 -0.612343 -0.530307
7 -0.887858  1.033826 -0.530307
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431
</code></pre>

<p>The docstring of <code>fillna</code> says, that <code>value</code> should be a scalar or a dict, however it seems to work with a <code>Series</code> to. If you want to pass a dict you could use <code>df.mean().to_dict()</code>.</p>
";;0;;2013-09-09T05:27:50.430;;18691949;2013-09-09T05:27:50.430;;;;;1301710.0;18689823.0;2;58;;;
6158;6158;;;"<p>See the docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html"" rel=""noreferrer""><code>to_dict</code></a>. You can use it like this:</p>

<pre><code>df.set_index('id').to_dict()
</code></pre>

<p>And if you have only one column, to avoid the column name is also a level in the dict (actually, in this case you use the <code>Series.to_dict()</code>):</p>

<pre><code>df.set_index('id')['value'].to_dict()
</code></pre>
";;2;;2013-09-09T09:55:13.047;;18695700;2016-01-05T22:19:56.620;2016-01-05T22:19:56.620;;653364.0;;653364.0;18695605.0;2;74;;;
6178;6178;;;"<p>You can select random elements from you index with <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html""><code>np.random.choice</code></a>. Eg to select 5 random rows:</p>

<pre><code>df = pd.DataFrame(np.random.rand(10))

df.loc[np.random.choice(df.index, 5, replace=False)]
</code></pre>

<p>This function is new in 1.7. If you want a solution with an older numpy, you can shuffle the data and taken the first elements of that:</p>

<pre><code>df.loc[np.random.permutation(df.index)[:5]]
</code></pre>

<p>In this way you DataFrame is not sorted anymore, but if this is needed for plottin (for a line plot eg), you can simply do <code>.sort()</code> afterwards.</p>
";;1;;2013-09-10T08:54:05.480;;18714509;2013-09-10T08:54:05.480;;;;;653364.0;18713929.0;2;17;;;
6179;6179;;;"<p>Unfortunately <code>np.random.choice</code> appears to be quite slow for small samples (less than 10% of all rows), you may be better off using plain ol' sample:</p>

<pre><code>from random import sample
df.loc[sample(df.index, 1000)]
</code></pre>

<p>For large DataFrame (a million rows), we see small samples:</p>

<pre><code>In [11]: %timeit df.loc[sample(df.index, 10)]
1000 loops, best of 3: 1.19 ms per loop

In [12]: %timeit df.loc[np.random.choice(df.index, 10, replace=False)]
1 loops, best of 3: 1.36 s per loop

In [13]: %timeit df.loc[np.random.permutation(df.index)[:10]]
1 loops, best of 3: 1.38 s per loop

In [21]: %timeit df.loc[sample(df.index, 1000)]
10 loops, best of 3: 14.5 ms per loop

In [22]: %timeit df.loc[np.random.choice(df.index, 1000, replace=False)]
1 loops, best of 3: 1.28 s per loop    

In [23]: %timeit df.loc[np.random.permutation(df.index)[:1000]]
1 loops, best of 3: 1.3  s per loop
</code></pre>

<p>But around 10% it gets about the same:</p>

<pre><code>In [31]: %timeit df.loc[sample(df.index, 100000)]
1 loops, best of 3: 1.63 s per loop

In [32]: %timeit df.loc[np.random.choice(df.index, 100000, replace=False)]
1 loops, best of 3: 1.36 s per loop

In [33]: %timeit df.loc[np.random.permutation(df.index)[:100000]]
1 loops, best of 3: 1.4 s per loop
</code></pre>

<p>and if you are sampling everything (don't use sample!):</p>

<pre><code>In [41]: %timeit df.loc[sample(df.index, 1000000)]
1 loops, best of 3: 10 s per loop
</code></pre>

<p><em>Note: both numpy.random and random accept a seed, to reproduce randomly generated output.</em></p>

<p>As @joris points out in the comments, choice (without replacement) is actually <a href=""https://github.com/numpy/numpy/blob/37b6baa7c73b9f395b6e4d9f89194d96a79f0090/numpy/random/mtrand/mtrand.pyx#L1067https://github.com/numpy/numpy/blob/v1.7.1/numpy/random/mtrand/mtrand.pyx#L1070"" rel=""noreferrer"">sugar for permutation</a> so it's no suprise it's constant time and slower for smaller samples...</p>
";;8;;2013-09-10T09:11:13.197;;18714869;2013-09-10T10:20:56.200;2013-09-10T10:20:56.200;;1240268.0;;1240268.0;18713929.0;2;14;;;
6267;6267;;;"<p>Joining fails if the DataFrames have some column names in common. The simplest way around it is to include an <code>lsuffix</code> or <code>rsuffix</code> keyword like so:</p>

<pre><code>restaurant_review_frame.join(restaurant_ids_dataframe, on='business_id', how='left', lsuffix=""_review"")
</code></pre>

<p>This way, the columns have distinct names. The documentation <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#overlapping-value-columns"">addresses this very problem</a>.</p>

<p>Or, you could get around this by simply deleting the offending columns before you join. If, for example, the stars in <code>restaurant_ids_dataframe</code> are redundant to the stars in <code>restaurant_review_frame</code>, you could <code>del restaurant_ids_dataframe['stars']</code>.</p>
";;3;;2013-09-13T18:39:35.600;;18793067;2013-09-13T18:39:35.600;;;;;1221924.0;18792918.0;2;15;;;
6270;6270;;;"<p>You can use <strong>merge</strong> to combine two dataframes into one:</p>

<pre><code>import pandas as pd
pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')
</code></pre>

<p>where <strong>on</strong> specifies field name that exists in both dataframes to join on, and <strong>how</strong>
defines whether its inner/outer/left/right join, with outer using 'union of keys from both frames (SQL: full outer join).' Since you have 'star' column in both dataframes, this by default will create two columns star_x and star_y in the combined dataframe. As @DanAllan mentioned for the join method, you can modify the suffixes for merge by passing it as a kwarg. Default is <code>suffixes=('_x', '_y')</code>. if you wanted to do something like <code>star_restaurant_id</code> and <code>star_restaurant_review</code>, you can do:</p>

<pre><code> pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))
</code></pre>

<p>The parameters are explained in detail in this <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging"">link</a>. </p>
";;2;;2013-09-14T08:11:16.573;;18799713;2015-09-03T20:33:53.623;2015-09-03T20:33:53.623;;162264.0;;1637638.0;18792918.0;2;50;;;
6293;6293;;;"<p>One way is to use the <code>get_level_values</code> Index method:</p>

<pre><code>In [11]: df
Out[11]:
     0
A B
1 4  1
2 5  2
3 6  3

In [12]: df.iloc[df.index.get_level_values('A') == 1]
Out[12]:
     0
A B
1 4  1
</code></pre>

<p>In 0.13 you'll be able to use <a href=""https://github.com/pydata/pandas/pull/4180""><code>xs</code> with <code>drop_level</code> argument</a>:</p>

<pre><code>df.xs(1, level='A', drop_level=False) # axis=1 if columns
</code></pre>

<p>Note: if this were column MultiIndex rather than index, you could use the same technique:</p>

<pre><code>In [21]: df1 = df.T

In [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]
Out[22]:
A  1
B  4
0  1
</code></pre>
";;0;;2013-09-16T18:48:54.397;;18835121;2013-09-16T20:43:33.963;2013-09-16T20:43:33.963;;1240268.0;;1240268.0;18835077.0;2;43;;;
6294;6294;;;"<p>You can use <code>DataFrame.xs()</code>:</p>

<pre><code>In [36]: df = DataFrame(np.random.randn(10, 4))

In [37]: df.columns = [np.random.choice(['a', 'b'], size=4).tolist(), np.random.choice(['c', 'd'], size=4)]

In [38]: df.columns.names = ['A', 'B']

In [39]: df
Out[39]:
A      b             a
B      d      d      d      d
0 -1.406  0.548 -0.635  0.576
1 -0.212 -0.583  1.012 -1.377
2  0.951 -0.349 -0.477 -1.230
3  0.451 -0.168  0.949  0.545
4 -0.362 -0.855  1.676 -2.881
5  1.283  1.027  0.085 -1.282
6  0.583 -1.406  0.327 -0.146
7 -0.518 -0.480  0.139  0.851
8 -0.030 -0.630 -1.534  0.534
9  0.246 -1.558 -1.885 -1.543

In [40]: df.xs('a', level='A', axis=1)
Out[40]:
B      d      d
0 -0.635  0.576
1  1.012 -1.377
2 -0.477 -1.230
3  0.949  0.545
4  1.676 -2.881
5  0.085 -1.282
6  0.327 -0.146
7  0.139  0.851
8 -1.534  0.534
9 -1.885 -1.543
</code></pre>

<p>If you want to keep the <code>A</code> level (the <code>drop_level</code> keyword argument is only available in the to-be-released v0.13.0):</p>

<pre><code>In [42]: df.xs('a', level='A', axis=1, drop_level=False)
Out[42]:
A      a
B      d      d
0 -0.635  0.576
1  1.012 -1.377
2 -0.477 -1.230
3  0.949  0.545
4  1.676 -2.881
5  0.085 -1.282
6  0.327 -0.146
7  0.139  0.851
8 -1.534  0.534
9 -1.885 -1.543
</code></pre>
";;3;;2013-09-16T18:51:42.573;;18835174;2017-08-21T16:46:53.860;2017-08-21T16:46:53.860;;2468113.0;;564538.0;18835077.0;2;15;;;
6296;6296;;;"<p>Pass the items of the dictionary to the DataFrame constructor, and give the column names. After that parse the <code>Date</code> column to get <code>Timestamp</code> values.</p>

<pre><code>df = pd.DataFrame(data.items(), columns=['Date', 'DateValue'])
df['Date'] = pd.to_datetime(df['Date'])
</code></pre>
";;2;;2013-09-16T21:11:07.810;;18837378;2013-09-16T21:11:07.810;;;;;2199958.0;18837262.0;2;16;;;
6297;6297;;;"<p>The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):</p>

<pre><code>pd.DataFrame(d)
ValueError: If using all scalar values, you must must pass an index
</code></pre>

<p>You could take the items from the dictionary (i.e. the key-value pairs):</p>

<pre><code>In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3
Out[11]:
             0    1
0   2012-07-02  392
1   2012-07-06  392
2   2012-06-29  391
3   2012-06-28  391
...

In [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])
Out[12]:
          Date  DateValue
0   2012-07-02        392
1   2012-07-06        392
2   2012-06-29        391
</code></pre>

<p>But I think it makes more sense to pass the Series constructor:</p>

<pre><code>In [21]: s = pd.Series(d, name='DateValue')
Out[21]:
2012-06-08    388
2012-06-09    388
2012-06-10    388

In [22]: s.index.name = 'Date'

In [23]: s.reset_index()
Out[23]:
          Date  DateValue
0   2012-06-08        388
1   2012-06-09        388
2   2012-06-10        388
</code></pre>
";;5;;2013-09-16T21:12:01.063;;18837389;2015-02-25T18:58:46.103;2015-02-25T18:58:46.103;;1240268.0;;1240268.0;18837262.0;2;145;;;
6299;6299;;;"<p>The <code>agg</code> method can accept a dict, in which case the keys indicate the column to which the function is applied:</p>

<pre><code>grouped.agg({'numberA':'sum', 'numberB':'min'})
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',
                         'foo', 'bar', 'foo', 'foo'],
                   'B': ['one', 'one', 'two', 'three',
                         'two', 'two', 'one', 'three'],
                   'number A': np.arange(8),
                   'number B': np.arange(8) * 2})
grouped = df.groupby('A')

print(grouped.agg({
    'number A': 'sum',
    'number B': 'min'}))
</code></pre>

<p>yields</p>

<pre><code>     number B  number A
A                      
bar         2         9
foo         0        19
</code></pre>

<p>This also shows that Pandas can handle spaces in column names. I'm not sure what the origin of the problem was, but literal spaces should not have posed a problem. If you wish to investigate this further, </p>

<pre><code>print(df.columns)
</code></pre>

<p><em>without reassigning the column names</em>, will show show us the <code>repr</code> of the names. Maybe there was a character in the column name that looked like a space (or some other character) but was actually a <code>u'\xa0'</code> (NO-BREAK SPACE) or something.</p>
";;4;;2013-09-16T21:34:47.140;;18837709;2013-09-16T23:54:54.147;2013-09-16T23:54:54.147;;190597.0;;190597.0;18837659.0;2;24;;;
6311;6311;;;"<p>Here's one way, using a <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation"">transform</a> with count:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['AppKey', 'B'])

In [2]: df
Out[2]:
   AppKey  B
0       1  2
1       1  4
2       5  6
</code></pre>

<p>Groupby the AppKey column and applying a transform count, means that each occurrence of AppKey is counted and the count is assigned to those rows where it appears:</p>

<pre><code>In [3]: count_appkey = df.groupby('AppKey')['AppKey'].transform('count')

In [4]: count_appkey
Out[4]:
0    2
1    2
2    1
Name: AppKey, dtype: int64

In [5]: count_appkey == 1
Out[5]:
0    False
1    False
2     True
Name: AppKey, dtype: bool
</code></pre>

<p>You can then use this as a boolean mask to the original DataFrame (leaving only those rows whose AppKey occurs precisely once):</p>

<pre><code>In [6]: df[count_appkey == 1]
Out[6]:
   AppKey  B
2       5  6
</code></pre>
";;1;;2013-09-17T14:09:57.407;;18852224;2013-09-17T14:09:57.407;;;;;1240268.0;18851216.0;2;6;;;
6312;6312;;;"<p>As of pandas version 0.12, we have <code>filter</code> for this. It does exactly what @Andy's solution does using <code>transform</code>, but a little more succinctly and somewhat faster.</p>

<pre><code>df.groupby('AppKey').filter(lambda x: x.count() == 1)
</code></pre>

<p>To steal @Andy's example,</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['AppKey', 'B'])

In [2]: df.groupby('AppKey').filter(lambda x: x.count() == 1)
Out[2]: 
   AppKey  B
2       5  6
</code></pre>
";;7;;2013-09-17T14:18:38.723;;18852410;2013-09-17T14:18:38.723;;;;;1221924.0;18851216.0;2;7;;;
6329;6329;;;"<p>HTML receives a custom string of html data. Nobody forbids you to pass in a style tag with the custom CSS style for the <code>.dataframe</code> class (which the <code>to_html</code> method adds to the table).</p>

<p>So the simplest solution would be to just add a style and concatenate it with the output of the <code>df.to_html</code>:</p>

<pre><code>style = '&lt;style&gt;.dataframe td { text-align: right; }&lt;/style&gt;'
HTML( style + df.to_html( formatters=frmt ) )
</code></pre>

<p>But I would suggest to define a custom class for a DataFrame since this will change the style of all the tables in your notebook (style is ""global"").</p>

<pre><code>style = '&lt;style&gt;.right_aligned_df td { text-align: right; }&lt;/style&gt;'
HTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))
</code></pre>

<p>You can also define the style in one of the previous cells, and then just set the <code>classes</code> parameter of the <code>to_html</code> method:</p>

<pre><code># Some cell at the begining of the notebook
In [2]: HTML('''&lt;style&gt;
                    .right_aligned_df td { text-align: right; }
                    .left_aligned_df td { text-align: right; }
                    .pink_df { background-color: pink; }
                &lt;/style&gt;''')

...

# Much later in your notebook
In [66]: HTML(df.to_html(classes='pink_df'))
</code></pre>
";;3;;2013-09-18T17:08:39.047;;18878267;2013-09-18T17:54:05.220;2013-09-18T17:54:05.220;;2199958.0;;2199958.0;18876022.0;2;23;;;
6331;6331;;;"<p>Instead of reindexing, just change the actual index:</p>

<pre><code>dfm.index = range(1,len(dfm) + 1)
</code></pre>

<p>Then that wont change the order, just the index</p>
";;3;;2013-09-18T17:16:30.650;;18878413;2013-09-18T17:16:30.650;;;;;2127988.0;18878308.0;2;36;;;
6332;6332;;;"<p>I think you're misunderstanding what <code>reindex</code> does. It uses the passed index to select values  along the axis passed, then fills with <code>NaN</code> wherever your passed index doesn't match up with the current index. What you're interested in is just setting the index to something else:</p>

<pre><code>In [12]: df = DataFrame(randn(10, 2), columns=['a', 'delt'])

In [13]: df
Out[13]:
       a   delt
0  0.222 -0.964
1  0.038 -0.367
2  0.293  1.349
3  0.604 -0.855
4 -0.455 -0.594
5  0.795  0.013
6 -0.080 -0.235
7  0.671  1.405
8  0.436  0.415
9  0.840  1.174

In [14]: df.reindex(index=arange(1, len(df) + 1))
Out[14]:
        a   delt
1   0.038 -0.367
2   0.293  1.349
3   0.604 -0.855
4  -0.455 -0.594
5   0.795  0.013
6  -0.080 -0.235
7   0.671  1.405
8   0.436  0.415
9   0.840  1.174
10    NaN    NaN

In [16]: df.index = arange(1, len(df) + 1)

In [17]: df
Out[17]:
        a   delt
1   0.222 -0.964
2   0.038 -0.367
3   0.293  1.349
4   0.604 -0.855
5  -0.455 -0.594
6   0.795  0.013
7  -0.080 -0.235
8   0.671  1.405
9   0.436  0.415
10  0.840  1.174
</code></pre>

<p>Remember, if you want <code>len(df)</code> to be in the index you have to add 1 to the endpoint since Python <em>doesn't include endpoints when constructing ranges</em>.</p>
";;6;;2013-09-18T17:17:02.020;;18878425;2013-09-18T17:17:02.020;;;;;564538.0;18878308.0;2;6;;;
6343;6343;;;"<p>I think you want to <a href=""http://docs.python.org/2/library/zipfile#zipfile.ZipFile.open""><code>open</code></a> the ZipFile, which returns a file-like object,  rather than <a href=""http://docs.python.org/2/library/zipfile#zipfile.ZipFile.read""><code>read</code></a>:</p>

<pre><code>In [11]: crime2013 = pd.read_csv(z.open('crime_incidents_2013_CSV.csv'))

In [12]: crime2013
Out[12]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 24567 entries, 0 to 24566
Data columns (total 15 columns):
CCN                            24567  non-null values
REPORTDATETIME                 24567  non-null values
SHIFT                          24567  non-null values
OFFENSE                        24567  non-null values
METHOD                         24567  non-null values
LASTMODIFIEDDATE               24567  non-null values
BLOCKSITEADDRESS               24567  non-null values
BLOCKXCOORD                    24567  non-null values
BLOCKYCOORD                    24567  non-null values
WARD                           24563  non-null values
ANC                            24567  non-null values
DISTRICT                       24567  non-null values
PSA                            24567  non-null values
NEIGHBORHOODCLUSTER            24263  non-null values
BUSINESSIMPROVEMENTDISTRICT    3613  non-null values
dtypes: float64(4), int64(1), object(10)
</code></pre>
";;1;;2013-09-19T02:26:38.543;;18885319;2013-09-19T02:26:38.543;;;;;1240268.0;18885175.0;2;22;;;
6372;6372;;;"<p>You cannot create a DataFrame from a generator with the 0.12 version of pandas. You can either update yourself to the development version (get it from the github and compile it - which is a little bit painful on windows but I would prefer this option).</p>

<p>Or you can, since you said you are filtering the lines, first filter them, write them to a file and then load them using <code>read_csv</code> or something else...</p>

<p>If you want to get super complicated you can create a file like object that will return the lines:</p>

<pre><code>def gen():
    lines = [
        'col1,col2\n',
        'foo,bar\n',
        'foo,baz\n',
        'bar,baz\n'
    ]
    for line in lines:
        yield line

class Reader(object):
    def __init__(self, g):
        self.g = g
    def read(self, n=0):
        try:
            return next(self.g)
        except StopIteration:
            return ''
</code></pre>

<p>And then use the <code>read_csv</code>:</p>

<pre><code>&gt;&gt;&gt; pd.read_csv(Reader(gen()))
  col1 col2
0  foo  bar
1  foo  baz
2  bar  baz
</code></pre>
";;6;;2013-09-20T12:09:30.263;;18916457;2013-09-20T12:16:36.083;2013-09-20T12:16:36.083;;2199958.0;;2199958.0;18915941.0;2;10;;;
6411;6411;;;"<p>Use <code>collections.Counter</code>:</p>

<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; r1=['My nickname is ft.jgt','Someone is going to my place']
&gt;&gt;&gt; Counter("" "".join(r1).split("" "")).items()
[('Someone', 1), ('ft.jgt', 1), ('My', 1), ('is', 2), ('to', 1), ('going', 1), ('place', 1), ('my', 1), ('nickname', 1)]
</code></pre>
";;1;;2013-09-21T20:03:02.673;;18937023;2013-09-21T20:03:02.673;;;;;2713087.0;18936957.0;2;13;;;
6413;6413;;;"<p>Use a <code>set</code> to create the sequence of unique elements.</p>

<p>Do some clean-up on <code>df</code> to get the strings in lower case and split:</p>

<pre><code>df['text'].str.lower().str.split()
Out[43]: 
0             [my, nickname, is, ft.jgt]
1    [someone, is, going, to, my, place]
</code></pre>

<p>Each list in this column can be passed to <code>set.update</code> function to get unique values. Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""noreferrer""><code>apply</code></a> to do so:</p>

<pre><code>results = set()
df['text'].str.lower().str.split().apply(results.update)
print results

set(['someone', 'ft.jgt', 'my', 'is', 'to', 'going', 'place', 'nickname'])
</code></pre>
";;2;;2013-09-21T20:31:45.947;;18937309;2013-09-21T20:31:45.947;;;;;624829.0;18936957.0;2;19;;;
6414;6414;;;"<p>If you want to do it from the DataFrame construct:</p>

<pre><code>import pandas as pd

r1=['My nickname is ft.jgt','Someone is going to my place']

df=pd.DataFrame(r1,columns=['text'])

df.text.apply(lambda x: pd.value_counts(x.split("" ""))).sum(axis = 0)

My          1
Someone     1
ft.jgt      1
going       1
is          2
my          1
nickname    1
place       1
to          1
dtype: float64
</code></pre>

<p>If you want a more flexible tokenization use <code>nltk</code> and its <code>tokenize</code></p>
";;2;;2013-09-21T20:42:17.223;;18937417;2013-09-21T20:42:17.223;;;;;1968405.0;18936957.0;2;10;;;
6420;6420;;;"<p>You just do an opposite comparison. <code>if Col2 &lt;= 1</code>. This will return a boolean Series with <code>False</code> values for those greater than 1 and <code>True</code> values for the other. If you convert it to an <code>int64</code> dtype, <code>True</code> becomes 1 and <code>False</code> become <code>0</code>,</p>

<pre><code>df['Col3'] = (df['Col2'] &lt;= 1).astype(int)
</code></pre>

<p>If you want a more general solution, where you can assign any number to the <code>Col3</code> depending on the value of the <code>Col2</code> you should do something like:</p>

<pre><code>df['Col3'] = df['Col2'].map(lambda x: 42 if x &gt; 1 else 55)
</code></pre>

<p>Or:</p>

<pre><code>df['Col3'] = 0
condition = df['Col2'] &gt; 1
df.loc[condition, 'Col3'] = 42
df.loc[~condition, 'Col3'] = 55
</code></pre>
";;4;;2013-09-22T09:55:58.540;;18942558;2013-09-22T10:02:45.900;2013-09-22T10:02:45.900;;2199958.0;;2199958.0;18942506.0;2;55;;;
6466;6466;;;"<p>Simple, just use <code>.set_color</code></p>

<pre><code>&gt;&gt;&gt; barlist=plt.bar([1,2,3,4], [1,2,3,4])
&gt;&gt;&gt; barlist[0].set_color('r')
&gt;&gt;&gt; plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/SYCFW.png"" alt=""enter image description here""></p>

<p>For your new question, not much harder either, just need to find the bar from your axis, an example:</p>

<pre><code>&gt;&gt;&gt; f=plt.figure()
&gt;&gt;&gt; ax=f.add_subplot(1,1,1)
&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])
&lt;Container object of 4 artists&gt;
&gt;&gt;&gt; ax.get_children()
[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, 
 &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  
 &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, 
 &lt;matplotlib.text.Text object at 0x7836450&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, 
 &lt;matplotlib.spines.Spine object at 0x6529950&gt;, 
 &lt;matplotlib.spines.Spine object at 0x69aef50&gt;,
 &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, 
 &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]
&gt;&gt;&gt; ax.get_children()[2].set_color('r') 
 #You can also try to locate the first patches.Rectangle object 
 #instead of direct calling the index.
</code></pre>

<p>If you have a complex plot and want to identify the bars first, add those:</p>

<pre><code>&gt;&gt;&gt; import matplotlib
&gt;&gt;&gt; childrenLS=ax.get_children()
&gt;&gt;&gt; barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)
[&lt;matplotlib.patches.Rectangle object at 0x3103650&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3103810&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3129850&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3129cd0&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3112ad0&gt;]
</code></pre>
";;6;;2013-09-24T05:15:19.030;;18973430;2016-03-11T18:02:12.313;2016-03-11T18:02:12.313;;2487184.0;;2487184.0;18973404.0;2;49;;;
6469;6469;;;"<p>I assume you are using Series.plot() to plot your data.  If you look at the docs for Series.plot() here:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.plot.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.plot.html</a></p>

<p>there is no <em>color</em> parameter listed where you might be able to set the colors for your bar graph.</p>

<p>However, the Series.plot() docs state the following at the end of the parameter list: </p>

<pre><code>kwds : keywords
Options to pass to matplotlib plotting method
</code></pre>

<p>What that means is that when you specify the <em>kind</em> argument for Series.plot() as <em>bar</em>, Series.plot() will actually call matplotlib.pyplot.bar(), and matplotlib.pyplot.bar() will be sent all the extra keyword arguments that you specify at the end of the argument list for Series.plot().  </p>

<p>If you examine the docs for the matplotlib.pyplot.bar() method here:</p>

<p><a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar"" rel=""nofollow noreferrer"">http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar</a></p>

<p>..it also accepts keyword arguments at the end of it's parameter list, and if you peruse the list of recognized parameter names, one of them is <em>color</em>, which can be a sequence specifying the different colors for your bar graph.  </p>

<p>Putting it all together, if you specify the <em>color</em> keyword argument at the end of your Series.plot() argument list, the keyword argument will be relayed to the matplotlib.pyplot.bar() method.  Here is the proof:  </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

s = pd.Series(
    [5, 4, 4, 1, 12],
    index = [""AK"", ""AX"", ""GA"", ""SQ"", ""WN""]
)

#Set descriptions:
plt.title(""Total Delay Incident Caused by Carrier"")
plt.ylabel('Delay Incident')
plt.xlabel('Carrier')

#Set tick colors:
ax = plt.gca()
ax.tick_params(axis='x', colors='blue')
ax.tick_params(axis='y', colors='red')

#Plot the data:
my_colors = 'rgbkymc'  #red, green, blue, black, etc.

pd.Series.plot(
    s, 
    kind='bar', 
    color=my_colors,
)

plt.show()
</code></pre>

<hr>

<p><img src=""https://i.stack.imgur.com/aE3V0.png"" alt=""enter image description here""></p>

<p>Note that if there are more bars than colors in your sequence, the colors will repeat.</p>
";;0;;2013-09-24T07:04:44.577;;18975065;2015-11-16T09:02:39.760;2015-11-16T09:02:39.760;;926143.0;;926143.0;18973404.0;2;15;;;
6483;6483;;;"<p>Use the <code>Figure.savefig()</code> method, like so:</p>

<pre><code>ax = s.hist()  # s is an instance of Series
fig = ax.get_figure()
fig.savefig('/path/to/figure.pdf')
</code></pre>

<p>It doesn't have to end in <code>pdf</code>, there are many options. Check out <a href=""http://matplotlib.org/api/figure_api.html?highlight=savefig#matplotlib.figure.Figure.savefig"" rel=""noreferrer"">the documentation</a>.</p>

<p>Alternatively, you can use the <code>pyplot</code> interface and just call the <code>savefig</code> as a function to save the most recently created figure:</p>

<pre><code>s.hist()
savefig('path/to/figure.pdf')  # saves the current figure
</code></pre>
";;1;;2013-09-24T21:14:24.980;;18992172;2013-09-24T21:28:43.127;2013-09-24T21:28:43.127;;564538.0;;564538.0;18992086.0;2;69;;;
6513;6513;;;"<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>

<p>Here is one way to do it, starting with a Query object called 'query':</p>

<pre><code>data_records = [rec.__dict__ for rec in query.all()]
df = pandas.DataFrame.from_records(data_records)
</code></pre>

<p>I'm curious to know if there's a better approach, but this did the trick for me in two lines.</p>
";;1;;2013-09-26T14:56:11.163;;19031661;2013-09-26T15:16:47.283;2013-09-26T15:16:47.283;;2147440.0;;2147440.0;12047193.0;2;25;;;
6531;6531;;;"<p>You can simply write:</p>

<pre><code>df = df[['bob']]
</code></pre>

<p><em>and the other columns will be garbage collected.</em></p>
";;0;;2013-09-28T02:40:53.127;;19062640;2013-09-28T02:40:53.127;;;;;1240268.0;19062612.0;2;23;;;
6537;6537;;;"<pre><code>import pandas as pd

import numpy as np

array=np.random.random((2,4))

df=pd.DataFrame(array, columns=('Test1', 'toto', 'test2', 'riri'))

print df

      Test1      toto     test2      riri
0  0.923249  0.572528  0.845464  0.144891
1  0.020438  0.332540  0.144455  0.741412

cols = [c for c in df.columns if c.lower()[:4] != 'test']

df=df[cols]

print df
       toto      riri
0  0.572528  0.144891
1  0.332540  0.741412
</code></pre>
";;1;;2013-09-28T20:55:09.153;;19071572;2013-09-28T20:55:09.153;;;;;2069099.0;19071199.0;2;16;;;
6539;6539;;;"<p>Use the <code>DataFrame.select</code> method:</p>

<pre><code>In [38]: df = DataFrame({'Test1': randn(10), 'Test2': randn(10), 'awesome': randn(10)})

In [39]: df.select(lambda x: not re.search('Test\d+', x), axis=1)
Out[39]:
   awesome
0    1.215
1    1.247
2    0.142
3    0.169
4    0.137
5   -0.971
6    0.736
7    0.214
8    0.111
9   -0.214
</code></pre>
";;2;;2013-09-28T21:07:25.463;;19071679;2013-09-28T21:07:25.463;;;;;564538.0;19071199.0;2;7;;;
6547;6547;;;"<p>This will drop the outermost level from the hierarchical column index:</p>

<pre><code>df = data.groupby(...).agg(...)
df.columns = df.columns.droplevel(0)
</code></pre>

<p>If you'd like to keep the outermost level, you can use the ravel() function on the multi-level column to form new labels:</p>

<pre><code>df.columns = [""_"".join(x) for x in df.columns.ravel()]
</code></pre>

<hr>

<p>For example:</p>

<pre><code>import pandas as pd
import pandas.rpy.common as com
import numpy as np

data = com.load_data('Loblolly')
print(data.head())
#     height  age Seed
# 1     4.51    3  301
# 15   10.89    5  301
# 29   28.72   10  301
# 43   41.74   15  301
# 57   52.70   20  301

df = data.groupby('Seed').agg(
    {'age':['sum'],
     'height':['mean', 'std']})
print(df.head())
#       age     height           
#       sum        std       mean
# Seed                           
# 301    78  22.638417  33.246667
# 303    78  23.499706  34.106667
# 305    78  23.927090  35.115000
# 307    78  22.222266  31.328333
# 309    78  23.132574  33.781667

df.columns = df.columns.droplevel(0)
print(df.head())
</code></pre>

<p>yields</p>

<pre><code>      sum        std       mean
Seed                           
301    78  22.638417  33.246667
303    78  23.499706  34.106667
305    78  23.927090  35.115000
307    78  22.222266  31.328333
309    78  23.132574  33.781667
</code></pre>

<p>Alternatively, to keep the first level of the index:</p>

<pre><code>df = data.groupby('Seed').agg(
    {'age':['sum'],
     'height':['mean', 'std']})
df.columns = [""_"".join(x) for x in df.columns.ravel()]
</code></pre>

<p>yields</p>

<pre><code>      age_sum   height_std  height_mean
Seed                           
301        78    22.638417    33.246667
303        78    23.499706    34.106667
305        78    23.927090    35.115000
307        78    22.222266    31.328333
309        78    23.132574    33.781667
</code></pre>
";;3;;2013-09-29T13:47:39.513;;19078773;2017-07-09T13:30:36.087;2017-07-09T13:30:36.087;;3065216.0;;190597.0;19078325.0;2;39;;;
6558;6558;;;"<p>I had the same problem recently, and indeed setting the default encoding to UTF-8 did the trick:</p>

<pre><code>import sys
reload(sys)
sys.setdefaultencoding(""utf-8"")
</code></pre>

<p>Running <code>sys.getdefaultencoding()</code> yielded <code>'ascii'</code> on my environment (Python 2.7.3), so I guess that's the default.</p>

<p>Also see <a href=""https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python"">this related question</a> and <a href=""http://blog.ianbicking.org/illusive-setdefaultencoding.html"" rel=""nofollow noreferrer"">Ian Bicking's blog post on the subject</a>.</p>
";;1;;2013-09-30T08:01:28.067;;19089210;2013-09-30T08:01:28.067;2017-05-23T12:01:57.687;;-1.0;;350572.0;15420672.0;2;11;;;
6570;6570;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html""><code>pd.read_csv</code></a>:</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv(""test_data2.csv"", index_col=[0,1], skipinitialspace=True)
&gt;&gt;&gt; df
                       dep  freq   arr   code  mode
from       to                                      
RGBOXFD    RGBPADTON   127     0    27  99999     2
           RGBPADTON   127     0    33  99999     2
           RGBRDLEY    127     0  1425  99999     2
           RGBCHOLSEY  127     0    52  99999     2
           RGBMDNHEAD  127     0    91  99999     2
RGBDIDCOTP RGBPADTON   127     0    46  99999     2
           RGBPADTON   127     0     3  99999     2
           RGBCHOLSEY  127     0    61  99999     2
           RGBRDLEY    127     0  1430  99999     2
           RGBPADTON   127     0   115  99999     2
</code></pre>

<p>where I've used <code>skipinitialspace=True</code> to get rid of those annoying spaces in the header row.</p>
";;0;;2013-09-30T21:01:13.500;;19103754;2013-09-30T21:01:13.500;;;;;487339.0;19103624.0;2;17;;;
6573;6573;;;"<p><code>map</code> over the elements:</p>

<pre><code>In [239]: from operator import methodcaller

In [240]: s = Series(date_range(Timestamp('now'), periods=2))

In [241]: s
Out[241]:
0   2013-10-01 00:24:16
1   2013-10-02 00:24:16
dtype: datetime64[ns]

In [238]: s.map(lambda x: x.strftime('%d-%m-%Y'))
Out[238]:
0    01-10-2013
1    02-10-2013
dtype: object

In [242]: s.map(methodcaller('strftime', '%d-%m-%Y'))
Out[242]:
0    01-10-2013
1    02-10-2013
dtype: object
</code></pre>

<p>You can get the raw <code>datetime.date</code> objects by calling the <code>date()</code> method of the <code>Timestamp</code> elements that make up the <code>Series</code>:</p>

<pre><code>In [249]: s.map(methodcaller('date'))

Out[249]:
0    2013-10-01
1    2013-10-02
dtype: object

In [250]: s.map(methodcaller('date')).values

Out[250]:
array([datetime.date(2013, 10, 1), datetime.date(2013, 10, 2)], dtype=object)
</code></pre>

<p>Yet <em>another</em> way you can do this is by calling the unbound <code>Timestamp.date</code> method:</p>

<pre><code>In [273]: s.map(Timestamp.date)
Out[273]:
0    2013-10-01
1    2013-10-02
dtype: object
</code></pre>

<p>This method is the fastest, and IMHO the most readable. <code>Timestamp</code> is accessible in the top-level <code>pandas</code> module, like so: <code>pandas.Timestamp</code>. I've imported it directly for expository purposes.</p>

<p>The <code>date</code> attribute of <code>DatetimeIndex</code> objects does something similar, but returns a <code>numpy</code> object array instead:</p>

<pre><code>In [243]: index = DatetimeIndex(s)

In [244]: index
Out[244]:
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-10-01 00:24:16, 2013-10-02 00:24:16]
Length: 2, Freq: None, Timezone: None

In [246]: index.date
Out[246]:
array([datetime.date(2013, 10, 1), datetime.date(2013, 10, 2)], dtype=object)
</code></pre>

<p>For larger <code>datetime64[ns]</code> <code>Series</code> objects, calling <code>Timestamp.date</code> is faster than <code>operator.methodcaller</code> which is slightly faster than a <code>lambda</code>:</p>

<pre><code>In [263]: f = methodcaller('date')

In [264]: flam = lambda x: x.date()

In [265]: fmeth = Timestamp.date

In [266]: s2 = Series(date_range('20010101', periods=1000000, freq='T'))

In [267]: s2
Out[267]:
0    2001-01-01 00:00:00
1    2001-01-01 00:01:00
2    2001-01-01 00:02:00
3    2001-01-01 00:03:00
4    2001-01-01 00:04:00
5    2001-01-01 00:05:00
6    2001-01-01 00:06:00
7    2001-01-01 00:07:00
8    2001-01-01 00:08:00
9    2001-01-01 00:09:00
10   2001-01-01 00:10:00
11   2001-01-01 00:11:00
12   2001-01-01 00:12:00
13   2001-01-01 00:13:00
14   2001-01-01 00:14:00
...
999985   2002-11-26 10:25:00
999986   2002-11-26 10:26:00
999987   2002-11-26 10:27:00
999988   2002-11-26 10:28:00
999989   2002-11-26 10:29:00
999990   2002-11-26 10:30:00
999991   2002-11-26 10:31:00
999992   2002-11-26 10:32:00
999993   2002-11-26 10:33:00
999994   2002-11-26 10:34:00
999995   2002-11-26 10:35:00
999996   2002-11-26 10:36:00
999997   2002-11-26 10:37:00
999998   2002-11-26 10:38:00
999999   2002-11-26 10:39:00
Length: 1000000, dtype: datetime64[ns]

In [269]: timeit s2.map(f)
1 loops, best of 3: 1.04 s per loop

In [270]: timeit s2.map(flam)
1 loops, best of 3: 1.1 s per loop

In [271]: timeit s2.map(fmeth)
1 loops, best of 3: 968 ms per loop
</code></pre>

<p>Keep in mind that one of the goals of <code>pandas</code> is to provide a layer on top of <code>numpy</code> so that (most of the time) you don't have to deal with the low level details of the <code>ndarray</code>. So getting the raw <code>datetime.date</code> objects in an array is of limited use since they don't correspond to any <code>numpy.dtype</code> that is supported by <code>pandas</code> (<code>pandas</code> only supports <code>datetime64[ns]</code> [that's nanoseconds] dtypes). That said, sometimes you need to do this.</p>
";;7;;2013-10-01T00:19:43.670;;19106012;2013-10-01T13:47:33.783;2013-10-01T13:47:33.783;;564538.0;;564538.0;19105976.0;2;27;;;
6581;6581;;;"<p>The following gives what you want:</p>

<pre><code>df = DataFrame(table, columns=headers)
df
</code></pre>

<p>outputs</p>

<pre><code>Out[7]:
   Heading1  Heading2
0         1         2
1         3         4
</code></pre>
";;0;;2013-10-01T09:41:12.887;;19112890;2013-10-01T09:41:12.887;;;;;704848.0;19112398.0;2;113;;;
6601;6601;;;"<p>You can work out the columns that are only in one dataframe and use this to select a subset of columns in the merge</p>

<pre><code>cols_to_use = df2.columns - df.columns
</code></pre>

<p>then perform the merge using this (note this is an index object but it has a handy <code>tolist()</code> method)</p>

<pre><code>dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')
</code></pre>

<p>This will avoid any columns clashing in the merge</p>

<p><strong>For version 0.15 and above, the new preferred syntax is:</strong></p>

<pre><code>cols_to_use = df2.columns.difference(df.columns)
</code></pre>

<p>thanks @odedbd</p>
";;7;;2013-10-01T20:43:17.660;;19125531;2017-07-13T19:29:46.013;2017-07-13T19:29:46.013;;680232.0;;704848.0;19125091.0;2;33;;;
6605;6605;;;"<p>Sure, if this comes up a lot, make a function like this one. You can even configure it to load every time you start IPython: <a href=""https://ipython.org/ipython-doc/1/config/overview.html"" rel=""noreferrer"">https://ipython.org/ipython-doc/1/config/overview.html</a></p>

<pre><code>def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')
</code></pre>

<p>As for coloring, getting too elaborate with colors sounds counterproductive to me, but I agree something like <a href=""http://getbootstrap.com/2.3.2/base-css.html#tables"" rel=""noreferrer"">bootstrap's <code>.table-striped</code></a> would be nice. You could always <a href=""https://github.com/pydata/pandas/issues"" rel=""noreferrer"">create an issue</a> to suggest this feature.</p>
";;2;;2013-10-01T21:48:30.600;;19126566;2017-02-26T10:25:43.927;2017-02-26T10:25:43.927;;4284627.0;;1221924.0;19124601.0;2;127;;;
6648;6648;;;"<pre><code>    List = [1, 3]
    df.ix[List]
</code></pre>

<p>should do the trick!
Whe I index with data frames I always use the .ix() method. Its so much easier and more flexible...</p>

<p><strong>UPDATE</strong>
This is no longer the accepted method for indexing. While the <code>ix</code> method is not deprecated its use can potentially lead to unintended consequences. Use <code>.iloc</code> for integer based indexing and <code>.loc</code> for label based indexing. </p>
";;0;;2013-10-03T09:43:39.327;;19155860;2017-08-08T11:54:29.667;2017-08-08T11:54:29.667;;2484720.0;;2484720.0;19155718.0;2;52;;;
6659;6659;;;"<p>The is one regular expression and should be in one string:</p>

<pre><code>""nt|nv""  # rather than ""nt"" | "" nv""
f_recs[f_recs['Behavior'].str.contains(""nt|nv"", na=False)]
</code></pre>

<p>Python doesn't let you use the or (<code>|</code>) operator on strings:</p>

<pre><code>In [1]: ""nt"" | ""nv""
TypeError: unsupported operand type(s) for |: 'str' and 'str'
</code></pre>
";;1;;2013-10-03T22:14:31.867;;19170098;2013-10-03T22:14:31.867;;;;;1240268.0;19169649.0;2;25;;;
6713;6713;;;"<pre><code>plt.axvline(x_position)
</code></pre>

<p>It takes the standard plot formatting options (<code>linestlye</code>, <code>color</code>, ect)</p>

<p><a href=""http://matplotlib.org/api/pyplot_api.html"" rel=""noreferrer"">(doc)</a></p>

<p>If you have a reference to your <code>axes</code> object:</p>

<pre><code>ax.axvline(x, color='k', linestyle='--')
</code></pre>
";;1;;2013-10-06T20:54:47.207;;19213836;2013-10-06T20:54:47.207;;;;;380231.0;19213789.0;2;75;;;
6719;6719;;;"<p>Transform your date index back into a simple data column with <code>reset_index</code>, and then generate your json object by using the <code>orient='index'</code> property:</p>

<pre><code>In [11]: aggregated_df.reset_index().to_json(orient='index')
Out[11]: '{""0"":{""created"":""05-16-13"",""counter"":3},""1"":{""created"":""05-17-13"",""counter"":1},""2"":{""created"":""05-18-13"",""counter"":1}}'
</code></pre>
";;0;;2013-10-06T22:27:43.080;;19214708;2013-10-06T22:27:43.080;;;;;624829.0;19214588.0;2;22;;;
6734;6734;;;"<p>One option is to use Python's slicing and indexing features to logically evaluate the places where your condition holds and overwrite the data there.</p>

<p>Assuming you can load your data directly into <code>pandas</code> with <code>pandas.read_csv</code> then the following code might be helpful for you.</p>

<pre><code>import pandas
df = pandas.read_csv(""test.csv"")
df.loc[df.ID == 103, 'FirstName'] = ""Matt""
df.loc[df.ID == 103, 'LastName'] = ""Jones""
</code></pre>

<p>As mentioned in the comments, you can also do the assignment to both columns in one shot:</p>

<pre><code>df.loc[df.ID == 103, ['FirstName', 'LastName']] = 'Matt', 'Jones'
</code></pre>

<p>Note that you'll need <code>pandas</code> version 0.11 or newer to make use of <code>loc</code> for overwrite assignment operations.</p>

<p>Another way to do it is to use what is called chained assignment. The behavior of this is less stable and so it is not considered the best solution, but it is useful to know about:</p>

<pre><code>import pandas
df = pandas.read_csv(""test.csv"")
df['FirstName'][df.ID == 103] = ""Matt""
df['LastName'][df.ID == 103] = ""Jones""
</code></pre>
";;7;;2013-10-07T13:48:43.980;;19226617;2013-10-07T13:55:16.013;2013-10-07T13:55:16.013;;567620.0;;567620.0;19226488.0;2;32;;;
6735;6735;;;"<p>You can use <code>map</code>, it can map vales from a dictonairy or even a custom function.</p>

<p>Suppose this is your df:</p>

<pre><code>    ID First_Name Last_Name
0  103          a         b
1  104          c         d
</code></pre>

<p>Create the dicts:</p>

<pre><code>fnames = {103: ""Matt"", 104: ""Mr""}
lnames = {103: ""Jones"", 104: ""X""}
</code></pre>

<p>And map:</p>

<pre><code>df['First_Name'] = df['ID'].map(fnames)
df['Last_Name'] = df['ID'].map(lnames)
</code></pre>

<p>The result will be:</p>

<pre><code>    ID First_Name Last_Name
0  103       Matt     Jones
1  104         Mr         X
</code></pre>

<p>Or use a custom function:</p>

<pre><code>names = {103: (""Matt"", ""Jones""), 104: (""Mr"", ""X"")}
df['First_Name'] = df['ID'].map(lambda x: names[x][0])
</code></pre>
";;2;;2013-10-07T13:54:34.780;;19226745;2013-10-07T14:56:58.863;2013-10-07T14:56:58.863;;1755432.0;;1755432.0;19226488.0;2;10;;;
6743;6743;;;"<p>These appear to be seconds since epoch.</p>

<pre><code>In [20]: df = DataFrame(data['values'])

In [21]: df.columns = [""date"",""price""]

In [22]: df
Out[22]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 358 entries, 0 to 357
Data columns (total 2 columns):
date     358  non-null values
price    358  non-null values
dtypes: float64(1), int64(1)

In [23]: df.head()
Out[23]: 
         date  price
0  1349720105  12.08
1  1349806505  12.35
2  1349892905  12.15
3  1349979305  12.19
4  1350065705  12.15
In [25]: df['date'] = pd.to_datetime(df['date'],unit='s')

In [26]: df.head()
Out[26]: 
                 date  price
0 2012-10-08 18:15:05  12.08
1 2012-10-09 18:15:05  12.35
2 2012-10-10 18:15:05  12.15
3 2012-10-11 18:15:05  12.19
4 2012-10-12 18:15:05  12.15

In [27]: df.dtypes
Out[27]: 
date     datetime64[ns]
price           float64
dtype: object
</code></pre>
";;9;;2013-10-07T18:25:59.670;;19231939;2013-10-07T18:25:59.670;;;;;644898.0;19231871.0;2;68;;;
6752;6752;;;"<p>I'll assume that <code>Time</code> and <code>Product</code> are columns in a <code>DataFrame</code>,  <code>df</code> is an instance of <code>DataFrame</code>, and that other variables are scalar values:</p>

<p>For now, you'll have to reference the <code>DataFrame</code> instance:</p>

<pre><code>k1 = df.loc[(df.Product == p_id) &amp; (df.Time &gt;= start_time) &amp; (df.Time &lt; end_time), ['Time', 'Product']]
</code></pre>

<p>The parentheses are also necessary, because of the precedence of the <code>&amp;</code> operator vs. the comparison operators. The <code>&amp;</code> operator is actually an overloaded bitwise operator which has the same precedence as arithmetic operators which in turn have a higher precedence than comparison operators.</p>

<p>In <code>pandas</code> 0.13 a new experimental <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>DataFrame.query()</code></a> method will be available. It's extremely similar to subset modulo the <code>select</code> argument:</p>

<p>With <code>query()</code> you'd do it like this:</p>

<pre><code>df[['Time', 'Product']].query('Product == p_id and Month &lt; mn and Year == yr')
</code></pre>

<p>Here's a simple example:</p>

<pre><code>In [9]: df = DataFrame({'gender': np.random.choice(['m', 'f'], size=10), 'price': poisson(100, size=10)})

In [10]: df
Out[10]:
  gender  price
0      m     89
1      f    123
2      f    100
3      m    104
4      m     98
5      m    103
6      f    100
7      f    109
8      f     95
9      m     87

In [11]: df.query('gender == ""m"" and price &lt; 100')
Out[11]:
  gender  price
0      m     89
4      m     98
9      m     87
</code></pre>

<p>The final query that you're interested will even be able to take advantage of chained comparisons, like this:</p>

<pre><code>k1 = df[['Time', 'Product']].query('Product == p_id and start_time &lt;= Time &lt; end_time')
</code></pre>
";;5;;2013-10-08T02:09:57.907;;19237920;2017-07-05T17:40:09.253;2017-07-05T17:40:09.253;;395857.0;;564538.0;19237878.0;2;54;;;
6753;6753;;;"<p><a href=""https://stackoverflow.com/a/11067079/841830"">Tweet's answer</a> can be passed to BrenBarn's answer above with </p>

<pre><code>data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)
</code></pre>

<p>So for your example, say:</p>

<pre><code>vals = randint(low=16, high=80, size=25).reshape(5,5)
cols = ['Q1.3', 'Q6.1', 'Q1.2', 'Q9.1', 'Q10.2']
data = DataFrame(vals, columns = cols)
</code></pre>

<p>You get: </p>

<pre><code>data

    Q1.3    Q6.1    Q1.2    Q9.1    Q10.2
0   73      29      63      51      72
1   61      29      32      68      57
2   36      49      76      18      37
3   63      61      51      30      31
4   36      66      71      24      77
</code></pre>

<p>Then do:</p>

<pre><code>data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)
</code></pre>

<p>resulting in:</p>

<pre><code>data


     Q1.2    Q1.3    Q6.1    Q9.1    Q10.2
0    2       0       1       3       4
1    7       5       6       8       9
2    2       0       1       3       4
3    2       0       1       3       4
4    2       0       1       3       4
</code></pre>
";;0;;2013-10-08T02:22:08.443;;19238029;2016-09-26T10:30:43.817;2017-05-23T12:34:45.403;;-1.0;;2856814.0;11067027.0;2;12;;;
6796;6796;;;"<h2>Single Column in the DF</h2>

<p>As you noted above, this returns a series the same size as the original</p>

<pre><code>In [32]: df.groupby('a')['a'].transform('count')
Out[32]: 
0     4
1     4
2     4
3     4
4     2
5     2
6     3
7     3
8     3
9     7
10    7
11    7
12    7
13    7
14    7
15    7
Name: a, dtype: int64
</code></pre>

<p>However, this is returing an empty frame</p>

<pre><code>In [33]: df.groupby('a').transform('count')
Out[33]: 
Empty DataFrame
Columns: []
Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
</code></pre>

<p>you cannot assign a an empty frame as a column to another frame because this is essentially an ambiguous assignment (you can make a case that it should 'work' though)</p>

<h2>Two columns in the starting DF</h2>

<p>The two column case return a single-column DataFrame</p>

<pre><code>In [42]: df2.groupby('a').transform('count')
Out[42]: 
    b
0   4
1   4
2   4
3   4
4   2
5   2
6   3
7   3
8   3
9   7
10  7
11  7
12  7
13  7
14  7
15  7

In [43]: type(df2.groupby('a').transform('count'))
Out[43]: pandas.core.frame.DataFrame

Or a series

In [45]: df2.groupby('a')['a'].transform('count')
Out[45]: 
0     4
1     4
2     4
3     4
4     2
5     2
6     3
7     3
8     3
9     7
10    7
11    7
12    7
13    7
14    7
15    7
Name: a, dtype: int64

In [46]: type(df.groupby('a')['a'].transform('count'))
Out[46]: pandas.core.series.Series
</code></pre>

<p>This 'works' because pandas DOES allow assignment of a single column frame to work, as it will take the underlying series.</p>

<p>So pandas is actually trying to be helpful. That said, I find this an unclear error message for trying to assign an empty frame.</p>
";;4;;2013-10-09T12:20:19.317;;19271841;2013-10-09T12:20:19.317;;;;;644898.0;19267029.0;2;8;;;
6820;6820;;;"<p>you can also use iloc:</p>

<pre><code>df.iloc[[1,3],:]
</code></pre>
";;0;;2013-10-10T12:17:42.677;;19295539;2013-10-10T12:17:42.677;;;;;2006977.0;19155718.0;2;14;;;
6822;6822;;;"<p>you can also use ranges by using:</p>

<pre><code>b = df[(df['a'] &gt; 1) &amp; (df['a'] &lt; 5)]
</code></pre>
";;0;;2013-10-10T12:26:29.810;;19295726;2013-10-10T12:26:29.810;;;;;2006977.0;12065885.0;2;34;;;
6852;6852;;;"<p>You could use <code>Series.reindex</code>:</p>

<pre><code>import pandas as pd

idx = pd.date_range('09-01-2013', '09-30-2013')

s = pd.Series({'09-02-2013': 2,
               '09-03-2013': 10,
               '09-06-2013': 5,
               '09-07-2013': 1})
s.index = pd.DatetimeIndex(s.index)

s = s.reindex(idx, fill_value=0)
print(s)
</code></pre>

<p>yields</p>

<pre><code>2013-09-01     0
2013-09-02     2
2013-09-03    10
2013-09-04     0
2013-09-05     0
2013-09-06     5
2013-09-07     1
2013-09-08     0
...
</code></pre>
";;4;;2013-10-11T18:08:29.603;;19324591;2013-10-11T18:36:56.977;2013-10-11T18:36:56.977;;190597.0;;190597.0;19324453.0;2;83;;;
6880;6880;;;"<p>I stumbled on this question while trying to do the same thing (I think). Here is how I did it:</p>

<pre><code>df['index_col'] = df.index
</code></pre>

<p>You can then sort on the new index column, if you like.</p>
";;2;;2013-10-13T18:57:05.170;;19349005;2013-10-13T18:57:05.170;;;;user1225054;;12168648.0;2;15;;;
6884;6884;;;"<p>In 0.13 (coming very soon), this is heavily optimized and quite fast (but still pretty fast in 0.12); both orders of magnitude faster than looping</p>

<pre><code>In [3]: df
Out[3]: 
   M  D     Y  Apples  Oranges
0  5  6  1990      12        3
1  5  7  1990      14        4
2  5  8  1990      15       34
3  5  9  1990      23       21

In [4]: df.dtypes
Out[4]: 
M          int64
D          int64
Y          int64
Apples     int64
Oranges    int64
dtype: object

# in 0.12, use this
In [5]: pd.to_datetime((df.Y*10000+df.M*100+df.D).apply(str),format='%Y%m%d')

# in 0.13 the above or this will work
In [5]: pd.to_datetime(df.Y*10000+df.M*100+df.D,format='%Y%m%d')
Out[5]: 
0   1990-05-06 00:00:00
1   1990-05-07 00:00:00
2   1990-05-08 00:00:00
3   1990-05-09 00:00:00
dtype: datetime64[ns]
</code></pre>
";;8;;2013-10-13T22:23:53.500;;19351003;2013-10-13T23:09:54.383;2013-10-13T23:09:54.383;;644898.0;;644898.0;19350806.0;2;25;;;
6900;6900;;;"<p>Upcoming pandas 0.13 version will allow to add rows through <code>loc</code> on non existing index data.</p>

<p>Description is <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#setting-with-enlargement"">here</a> and this new feature is called <em>Setting With Enlargement</em>.</p>
";;10;;2013-10-14T20:04:58.180;;19368360;2015-09-07T09:32:17.583;2015-09-07T09:32:17.583;;210945.0;;624829.0;19365513.0;2;30;;;
6908;6908;;;"<pre><code>dataframe[""period""] = dataframe[""Year""].map(str) + dataframe[""quarter""]
</code></pre>
";;6;;2013-10-15T10:09:51.163;;19378497;2013-10-15T10:09:51.163;;;;;1050484.0;19377969.0;2;110;;;
6923;6923;;;"<p>On <code>groupby</code> object, the <code>agg</code> function can take a list to <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once"">apply several aggregation methods</a> at once. This should give you the result you need:</p>

<pre><code>df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])
</code></pre>
";;3;;2013-10-15T15:49:28.347;;19385591;2015-05-17T03:55:35.873;2015-05-17T03:55:35.873;;2411802.0;;624829.0;19384532.0;2;116;;;
6939;6939;;;"<pre><code>import pandas as pd
df = pd.DataFrame({'A':'foo foo foo bar bar bar'.split(),
                   'B':[0.1, 0.5, 1.0]*2})

df['C'] = df.groupby(['A'])['B'].transform(
                     lambda x: pd.qcut(x, 3, labels=range(1,4)))
print(df)
</code></pre>

<p>yields</p>

<pre><code>     A    B  C
0  foo  0.1  1
1  foo  0.5  2
2  foo  1.0  3
3  bar  0.1  1
4  bar  0.5  2
5  bar  1.0  3
</code></pre>
";;0;;2013-10-16T12:49:34.737;;19403897;2013-10-16T12:49:34.737;;;;;190597.0;19403133.0;2;18;;;
6960;6960;;;"<p>Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html</a></p>

<pre><code>df[b] = df[a].apply(lambda col: do stuff with col here)
</code></pre>
";;7;;2013-10-16T22:38:11.267;;19415186;2015-02-05T04:16:14.887;2015-02-05T04:16:14.887;;202229.0;;1609400.0;7837722.0;2;13;;;
7026;7026;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.shift.html"">shift</a>:</p>

<pre><code>a.loc[a.shift(-1) != a]

Out[3]:

1    1
3    2
4    3
5    2
dtype: int64
</code></pre>

<p>So the above uses boolean critieria, we compare the dataframe against the dataframe shifted by -1 rows to create the mask</p>

<p>Another method is to use <code>diff</code>:</p>

<pre><code>In [82]:

a.loc[a.diff() != 0]
Out[82]:
1    1
2    2
4    3
5    2
dtype: int64
</code></pre>

<p>But this is slower than the original method if you have a large number of rows.</p>

<p><strong>Update</strong></p>

<p>Thanks to Bjarke Ebert for pointing out a subtle error, I should actually use <code>shift(1)</code> or just <code>shift()</code> as the default is a period of 1, this returns the first consecutive value:</p>

<pre><code>In [87]:

a.loc[a.shift() != a]
Out[87]:
1    1
2    2
4    3
5    2
dtype: int64
</code></pre>

<p>Note the difference in index values, thanks @BjarkeEbert!</p>
";;12;;2013-10-19T08:27:32.690;;19464054;2014-06-19T15:16:48.823;2014-06-19T15:16:48.823;;704848.0;;704848.0;19463985.0;2;31;;;
7039;7039;;;"<pre><code>df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)
</code></pre>

<p>If you want the sorted result for future use, <code>inplace=True</code> is required.</p>
";;0;;2013-10-20T03:52:08.657;;19473752;2013-10-20T04:14:40.173;2013-10-20T04:14:40.173;;1679310.0;;2899283.0;13636592.0;2;18;;;
7052;7052;;;"<p>That's available as <code>my_dataframe.columns</code>.</p>
";;0;;2013-10-20T21:20:06.947;;19482988;2014-01-23T18:50:27.977;2014-01-23T18:50:27.977;;1427416.0;;1427416.0;19482970.0;2;11;;;
7053;7053;;;"<p>You can get the values as a list by doing:</p>

<pre><code>list(my_dataframe.columns.values)
</code></pre>

<p>Also you can simply use:</p>

<pre><code>list(my_dataframe)
</code></pre>
";;5;;2013-10-20T21:23:07.363;;19483025;2017-01-06T04:38:06.417;2017-01-06T04:38:06.417;;6933270.0;;1267329.0;19482970.0;2;548;;;
7056;7056;;;"<p>There is a built in method which is the most performant:</p>

<pre><code>my_dataframe.columns.values.tolist()
</code></pre>

<p><code>.columns</code> returns an <code>Index</code>, <code>.columns.values</code> returns an <code>array</code> and this has a helper function to return a <code>list</code>.</p>

<p><strong>EDIT</strong></p>

<p>For those who hate typing this is probably the shortest method:</p>

<pre><code>list(df)
</code></pre>
";;1;;2013-10-20T22:25:15.950;;19483602;2015-03-17T14:34:07.820;2015-03-17T14:34:07.820;;704848.0;;704848.0;19482970.0;2;202;;;
7064;7064;;;"<p>Instead of specifying dtypes, specify a converter for the column you want to keep as str, building on @TomAugspurger's example:</p>

<pre><code>from io import StringIO
import pandas as pd
data = StringIO(u""""""
121301234
121300123
121300012
"""""")

pd.read_fwf(data, colspecs=[(0,3),(4,8)], converters = {1: str})
</code></pre>

<p>Leads to</p>

<pre><code>    \n Unnamed: 1
0  121       0123
1  121       0012
2  121       0001
</code></pre>

<p>Converters are a mapping from a column name or index to a function to convert the value in the cell (eg. int would convert them to integer, float to floats, etc) </p>
";;0;;2013-10-21T04:11:41.270;;19486140;2013-10-21T11:30:14.640;2013-10-21T11:30:14.640;;1316786.0;;1316786.0;19472566.0;2;7;;;
7108;7108;;;"<p>For the first question I think answer would be:</p>

<pre><code>&lt;your DataFrame&gt;.rename(columns={'count':'Total_Numbers'})
</code></pre>

<p>or</p>

<pre><code>&lt;your DataFrame&gt;.columns = ['ID', 'Region', 'Total_Numbers']
</code></pre>

<p>As for second one I'd say the answer would be no. It's possible to use it like 'df.ID' because of <a href=""http://docs.python.org/2/reference/datamodel.html"" rel=""noreferrer"">python datamodel</a>:</p>

<blockquote>
  <p>Attribute references are translated to lookups in this dictionary,
  e.g., m.x is equivalent to m.<strong>dict</strong>[""x""]</p>
</blockquote>
";;1;;2013-10-22T16:35:08.677;;19523512;2016-01-27T16:02:31.357;2016-01-27T16:02:31.357;;424651.0;;1744834.0;19523277.0;2;20;;;
7153;7153;;;"<p>So I'm not entirely sure why this works, but it saves an image with my plot:</p>

<pre><code>dtf = pd.DataFrame.from_records(d,columns=h)
dtf2.plot()
fig = plt.gcf()
fig.savefig('output.png')
</code></pre>

<p>I'm guessing that the last snippet from my original post saved blank because the figure was never getting the axes generated by pandas.  With the above code, the figure object is returned from some magic global state by the gcf() call (get current figure), which automagically bakes in axes plotted in the line above.</p>
";;2;;2013-10-24T02:11:30.370;;19555675;2013-10-24T02:26:18.000;2013-10-24T02:26:18.000;;380231.0;;1046775.0;19555525.0;2;7;;;
7202;7202;;;"<p>You can achieve this using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.iterrows.html""><code>iterrows</code></a>:</p>

<pre><code>temp=[]

for row in df.iterrows():
    index, data = row
    temp.append(data.tolist())
</code></pre>

<p>Alternatively you can also use <code>apply</code>:</p>

<pre><code>df.apply(lambda x: x.tolist(), axis=1)
</code></pre>

<p><strong>Update</strong></p>

<p>After looking at this again there is a built in method which would be the fastest method also, calling <code>tolist</code> on the <code>.values</code> np array:</p>

<pre><code>In [62]:
df.values.tolist()

Out[62]:
[[0.0, 3.61, 380.0, 3.0],
 [1.0, 3.67, 660.0, 3.0],
 [1.0, 3.19, 640.0, 4.0],
 [0.0, 2.93, 520.0, 4.0]]
</code></pre>
";;3;;2013-10-25T08:54:18.947;;19585378;2016-07-12T08:07:21.347;2016-07-12T08:07:21.347;;704848.0;;704848.0;19585280.0;2;31;;;
7203;7203;;;"<p>you can do it like this:</p>

<pre><code>map(list, df.values)
</code></pre>
";;0;;2013-10-25T08:55:56.560;;19585413;2013-10-25T08:55:56.560;;;;;1744834.0;19585280.0;2;19;;;
7211;7211;;;"<p>Your function is failing because the groupby dataframe you end up with has a hierarchical index and two columns (Letter and N) so when you do <code>.hist()</code> it's trying to make a histogram of both columns hence the str error.</p>

<p>This is the default behavior of pandas plotting functions (one plot per column) so if you reshape your data frame so that each letter is a column you will get exactly what you want.</p>

<pre><code>df.reset_index().pivot('index','Letter','N').hist()
</code></pre>

<p>The <code>reset_index()</code> is just to shove the current index into a column called <code>index</code>.  Then <code>pivot</code> will take your data frame, collect all of the values <code>N</code> for each <code>Letter</code> and make them a column.  The resulting data frame as 400 rows (fills missing values with <code>NaN</code>) and three columns (<code>A, B, C</code>).  <code>hist()</code> will then produce one histogram per column and you get format the plots as needed.</p>
";;3;;2013-10-25T14:33:29.247;;19592693;2013-10-25T14:33:29.247;;;;;1968405.0;19584029.0;2;7;;;
7225;7225;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.squeeze.html""><code>squeeze</code></a> function that will remove one dimension from the dataframe:</p>

<pre><code>df[df[""location""] == ""c""].squeeze()
Out[5]: 
date        20130102
location           c
Name: 2, dtype: object
</code></pre>

<p>DataFrame.squeeze method acts the same way of the squeeze argument of the read_csv function when set to True: if the resulting dataframe is a 1-len dataframe, i.e. it has only one dimension like it is a column or a row, then the object is squeezed down to the smaller dimension object. In your case your get a Series object from the DataFrame. The same logic applies if you squeeze a Panel down to a DataFrame. </p>

<p>squeeze is explicit in your code and shows clearly your intent to ""cast down"" the object in hands because its dimension can be projected to a smaller one.</p>

<p>If the dataframe has more than one column or row, squeeze has no effect.</p>
";;5;;2013-10-25T21:24:32.500;;19599661;2013-10-25T22:13:45.517;2013-10-25T22:13:45.517;;624829.0;;624829.0;19599578.0;2;28;;;
7226;7226;;;"<p>You can just take first row with integer indexing (<a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html"">iloc()</a> function):</p>

<pre><code>&gt;&gt;&gt; df[df[""location""] == ""c""].iloc[0]
date        20130102
location           c
Name: 2, dtype: object
</code></pre>
";;1;;2013-10-25T21:34:12.997;;19599776;2013-10-25T21:34:12.997;;;;;1744834.0;19599578.0;2;8;;;
7227;7227;;;"<p>As far as I can tell with updates to pandas, you have to use pivot_table() instead of pivot().</p>

<pre><code>pandas.pivot_table(df,values='count',index='site_id',columns='week')
</code></pre>
";;4;;2013-10-25T22:45:37.453;;19600533;2016-02-16T18:05:00.857;2016-02-16T18:05:00.857;;7536.0;;1807668.0;11232275.0;2;55;;;
7230;7230;;;"<p>I'm on a roll, just found an even simpler way to do it using the <em>by</em> keyword in the hist method:</p>

<pre><code>df['N'].hist(by=df['Letter'])
</code></pre>

<p>That's a very handy little shortcut for quickly scanning your grouped data!</p>

<p>For future visitors, the product of this call is the following chart:
<img src=""https://i.stack.imgur.com/oy4NN.png"" alt=""enter image description here""></p>
";;4;;2013-10-26T06:59:47.310;;19603918;2014-05-03T21:57:40.883;2014-05-03T21:57:40.883;;1460746.0;;2918893.0;19584029.0;2;86;;;
7235;7235;;;"<p>you can do</p>

<pre><code>followers_df.index = range(20)
</code></pre>
";;0;;2013-10-26T17:52:22.143;;19609945;2013-10-26T17:52:22.143;;;;;1744834.0;19609631.0;2;30;;;
7236;7236;;;"<pre><code>followers_df.reset_index()
followers_df.reindex(index=range(0,20))
</code></pre>
";;3;;2013-10-26T17:53:51.860;;19609954;2013-10-26T20:52:56.817;2013-10-26T20:52:56.817;;2006977.0;;2006977.0;19609631.0;2;15;;;
7241;7241;;;"<p>You can use <code>read_csv()</code> on a <code>StringIO</code> object:</p>

<pre><code>from StringIO import StringIO  # got moved to io in python3.

import requests
r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv')
data = r.content

In [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate'])

In [11]: df.head()
Out[11]: 
          City                                            region     Res_Comm  \
0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   
10       Foley                              South_Mobile-Baldwin  Residential   
12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   
38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   
44      Athens                 North_Huntsville-Decatur-Florence  Residential   

          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \
0            Rural 2010-01-15 00:00:00             2            2          3   
10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   
12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   
38           Rural 2010-01-15 00:00:00             3            3          3   
44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   

    Inventory_exp  Price_exp  Credit_exp  
0               2          3           3  
10              4          4           3  
12              2          2           3  
38              3          3           2  
44              4          4           4  
</code></pre>
";;7;;2013-10-26T21:02:16.660;;19611857;2017-06-19T13:36:15.403;2017-06-19T13:36:15.403;;3705840.0;;1889400.0;19611729.0;2;28;;;
7259;7259;;;"<p>If I understand you correctly, you can use a combination of <code>Series.isin()</code> and <code>DataFrame.append()</code>:</p>

<pre><code>In [80]: df1
Out[80]:
   rating  user_id
0       2  0x21abL
1       1  0x21abL
2       1   0xdafL
3       0  0x21abL
4       4  0x1d14L
5       2  0x21abL
6       1  0x21abL
7       0   0xdafL
8       4  0x1d14L
9       1  0x21abL

In [81]: df2
Out[81]:
   rating      user_id
0       2      0x1d14L
1       1    0xdbdcad7
2       1      0x21abL
3       3      0x21abL
4       3      0x21abL
5       1  0x5734a81e2
6       2      0x1d14L
7       0       0xdafL
8       0      0x1d14L
9       4  0x5734a81e2

In [82]: ind = df2.user_id.isin(df1.user_id) &amp; df1.user_id.isin(df2.user_id)

In [83]: ind
Out[83]:
0     True
1    False
2     True
3     True
4     True
5    False
6     True
7     True
8     True
9    False
Name: user_id, dtype: bool

In [84]: df1[ind].append(df2[ind])
Out[84]:
   rating  user_id
0       2  0x21abL
2       1   0xdafL
3       0  0x21abL
4       4  0x1d14L
6       1  0x21abL
7       0   0xdafL
8       4  0x1d14L
0       2  0x1d14L
2       1  0x21abL
3       3  0x21abL
4       3  0x21abL
6       2  0x1d14L
7       0   0xdafL
8       0  0x1d14L
</code></pre>

<p>This is essentially the algorithm you described as ""clunky"", using idiomatic <code>pandas</code> methods. Note the duplicate row indices. Also, note that this won't give you the expected output if <code>df1</code> and <code>df2</code> have no overlapping row indices, i.e., if</p>

<pre><code>In [93]: df1.index &amp; df2.index
Out[93]: Int64Index([], dtype='int64')
</code></pre>

<p>In fact, it won't give the expected output if their row indices are not equal.</p>
";;4;;2013-10-27T14:15:13.493;;19619020;2013-10-27T15:01:29.380;2013-10-27T15:01:29.380;;564538.0;;564538.0;19618912.0;2;6;;;
7279;7279;;;"<p>you can use regex as the delimiter:</p>

<pre><code>pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\s+"")
</code></pre>
";;0;;2013-10-28T10:16:01.017;;19632099;2013-10-28T10:16:01.017;;;;user2927197;;19632075.0;2;11;;;
7280;7280;;;"<p>add <code>delim_whitespace=True</code> argument, it's faster than regex.</p>
";;0;;2013-10-28T11:06:34.097;;19633103;2013-10-28T11:06:34.097;;;;;772649.0;19632075.0;2;43;;;
7402;7402;;;"<p>You can do it like this:</p>

<pre><code>data.columns = map(str.lower, data.columns)
</code></pre>

<p>or</p>

<pre><code>data.columns = [x.lower() for x in data.columns]
</code></pre>

<p>example:</p>

<pre><code>&gt;&gt;&gt; data = pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})
&gt;&gt;&gt; data
   A  B  C
0  0  3  a
1  1  2  b
2  2  1  c
&gt;&gt;&gt; data.columns = map(str.lower, data.columns)
&gt;&gt;&gt; data
   a  b  c
0  0  3  a
1  1  2  b
2  2  1  c
</code></pre>
";;1;;2013-11-01T11:42:38.613;;19726078;2013-11-01T11:42:38.613;;;;;1744834.0;19726029.0;2;46;;;
7414;7414;;;"<p><strong>In Python 3.x:</strong></p>

<pre><code>In [6]: d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )

In [7]: DataFrame(dict([ (k,Series(v)) for k,v in d.items() ]))
Out[7]: 
    A  B
0   1  1
1   2  2
2 NaN  3
3 NaN  4
</code></pre>

<p><strong>In Python 2.x:</strong></p>

<p>replace <code>d.items()</code> with <code>d.iteritems()</code>.</p>
";;9;;2013-11-01T22:27:02.037;;19736406;2017-08-09T17:56:10.000;2017-08-09T17:56:10.000;;4284627.0;;644898.0;19736080.0;2;36;;;
7421;7421;;;"<p>If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you're looking for here, but doing scientific computing on a notebook with 4GB of RAM isn't reasonable.</p>
";;2;;2013-11-02T07:14:07.340;;19739768;2013-11-02T07:14:07.340;;;;;13969.0;14262433.0;2;38;;;
7440;7440;;;"<pre><code>data.rename(columns={'gdp':'log(gdp)'}, inplace=True)
</code></pre>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html#pandas.DataFrame.rename"" rel=""noreferrer""><code>rename</code></a> show that it accepts a dict as a param for <code>columns</code> so you just pass a dict with a single entry.</p>

<p>Also see <a href=""https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas"">related</a></p>
";;8;;2013-11-03T21:32:01.623;;19758398;2015-02-11T09:06:56.967;2017-05-23T12:34:44.877;;-1.0;;704848.0;19758364.0;2;145;;;
7467;7467;;;"<p>You can write to csv without the header using <code>header=False</code> and without the index using <code>index=False</code>. If desired, you also can modify the separator using <code>sep</code>.</p>

<p>CSV example with no header row, omitting the header row:</p>

<pre><code>df.to_csv('filename.csv', header=False)
</code></pre>

<p>TSV (tab-separated) example, omitting the index column:</p>

<pre><code>df.to_csv('filename.tsv', sep='\t', index=False)
</code></pre>
";;0;;2013-11-05T04:55:42.190;;19782137;2016-08-05T21:04:08.363;2016-08-05T21:04:08.363;;310019.0;;461436.0;19781609.0;2;31;;;
7483;7483;;;"<p>Can I ask why not just do it by slicing the data frame. Something like</p>

<pre><code>#create some data with Names column
data = pd.DataFrame({'Names': ['Joe', 'John', 'Jasper', 'Jez'] *4, 'Ob1' : np.random.rand(16), 'Ob2' : np.random.rand(16)})

#create unique list of names
UniqueNames = data.Names.unique()

#create a data frame dictionary to store your data frames
DataFrameDict = {elem : pd.DataFrame for elem in UniqueNames}

for key in DataFrameDict.keys():
    DataFrameDict[key] = data[:][data.Names == key]
</code></pre>

<p>Hey presto you have a dictionary of data frames just as (I think) you want them. Need to access one? Just enter</p>

<pre><code>DataFrameDict['Joe']
</code></pre>

<p>Hope that helps</p>
";;5;;2013-11-05T14:28:44.703;;19791302;2013-11-05T14:28:44.703;;;;;2484720.0;19790790.0;2;28;;;
7502;7502;;;"<p>Straight from Wes McKinney's <a href=""http://shop.oreilly.com/product/0636920023784.do"" rel=""noreferrer"">Python for Data Analysis</a> book, pg. 132 (I highly recommended this book):</p>

<blockquote>
  <p>Another frequent operation is applying a function on 1D arrays to each column or row. DataFrames apply method does exactly this:</p>
</blockquote>

<pre><code>In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])

In [117]: frame
Out[117]: 
               b         d         e
Utah   -0.029638  1.081563  1.280300
Ohio    0.647747  0.831136 -1.549481
Texas   0.513416 -0.884417  0.195343
Oregon -0.485454 -0.477388 -0.309548

In [118]: f = lambda x: x.max() - x.min()

In [119]: frame.apply(f)
Out[119]: 
b    1.133201
d    1.965980
e    2.829781
dtype: float64
</code></pre>

<blockquote>
  <p>Many of the most common array statistics (like sum and mean) are DataFrame methods,
      so using apply is not necessary.</p>
  
  <p>Element-wise Python functions can be used, too. Suppose you wanted to compute a formatted string from each floating point value in frame. You can do this with applymap:</p>
</blockquote>

<pre><code>In [120]: format = lambda x: '%.2f' % x

In [121]: frame.applymap(format)
Out[121]: 
            b      d      e
Utah    -0.03   1.08   1.28
Ohio     0.65   0.83  -1.55
Texas    0.51  -0.88   0.20
Oregon  -0.49  -0.48  -0.31
</code></pre>

<blockquote>
  <p>The reason for the name applymap is that Series has a map method for applying an element-wise function:</p>
</blockquote>

<pre><code>In [122]: frame['e'].map(format)
Out[122]: 
Utah       1.28
Ohio      -1.55
Texas      0.20
Oregon    -0.31
Name: e, dtype: object
</code></pre>

<p>Summing up, <code>apply</code> works on a row / column basis of a DataFrame, <code>applymap</code> works element-wise on a DataFrame, and <code>map</code> works element-wise on a Series.</p>
";;4;;2013-11-05T20:40:33.743;;19798528;2013-11-05T20:40:33.743;;;;;772487.0;19798153.0;2;221;;;
7521;7521;;;"<p>Firstly your approach is inefficient because the appending to the list on a row by basis will be slow as it has to periodically grow the list when there is insufficient space for the new entry, list comprehensions are better in this respect as the size is determined up front and allocated once.</p>

<p>However, I think fundamentally your approach is a little wasteful as you have a dataframe already so why create a new one for each of these users?</p>

<p>I would sort the dataframe by column <code>'name'</code>, set the index to be this and if required not drop the column.</p>

<p>Then generate a list of all the unique entries and then you can perform a lookup using these entries and crucially if you only querying the data, use the selection critieria to return a view on the dataframe without incurring a costly data copy.</p>

<p>So:</p>

<pre><code># sort the dataframe
df.sort(columns=['name'], inplace=True)
# set the index to be this and don't drop
df.set_index(keys=['name'], drop=False,inplace=True)
# get a list of names
names=df['name'].unique().tolist()
# now we can perform a lookup on a 'view' of the dataframe
joe = df.loc[df.name=='joe']
# now you can query all 'joes'
</code></pre>
";;8;;2013-11-06T10:29:02.150;;19809616;2015-12-08T08:58:40.800;2015-12-08T08:58:40.800;;704848.0;;704848.0;19790790.0;2;26;;;
7537;7537;;;"<p>You can use <code>first</code></p>

<pre><code>In [14]: df.groupby('Mt').first()
Out[14]: 
   Sp  Value  count
Mt                 
s1  a      1      3
s2  c      3      5
s3  f      6      6
</code></pre>

<h1>Update</h1>

<p>Set <code>as_index=False</code> to achieve your goal</p>

<pre><code>In [28]: df.groupby('Mt', as_index=False).first()
Out[28]: 
   Mt Sp  Value  count
0  s1  a      1      3
1  s2  c      3      5
2  s3  f      6      6 
</code></pre>

<h1>Update Again</h1>

<p>Sorry for misunderstanding what you mean. You can sort it first if you want the one with max count in a group</p>

<pre><code>In [196]: df.sort('count', ascending=False).groupby('Mt', as_index=False).first()
Out[196]: 
   Mt Sp  Value  count
0  s1  a      1      3
1  s2  e      5     10
2  s3  f      6      6
</code></pre>
";;8;;2013-11-06T17:40:10.743;;19818942;2013-11-06T18:46:17.050;2013-11-06T18:46:17.050;;1426056.0;;1426056.0;19818756.0;2;25;;;
7538;7538;;;"<p>To get first occurence of maximum <code>count</code> you can use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.idxmax.html"" rel=""noreferrer"">pandas.DataFrame.idxmax()</a> function:</p>

<pre><code>&gt;&gt;&gt; df.iloc[df.groupby(['Mt']).apply(lambda x: x['count'].idxmax())]
   Mt Sp  Value  count
0  s1  a      1      3
3  s2  d      4     10
5  s3  f      6      6
</code></pre>
";;1;;2013-11-06T17:48:19.723;;19819118;2013-11-06T18:30:24.413;2013-11-06T18:30:24.413;;1744834.0;;1744834.0;19818756.0;2;16;;;
7543;7543;;;"<p>There's more to it than this, but you're probably looking for this list:</p>

<pre><code>B   business day frequency
C   custom business day frequency (experimental)
D   calendar day frequency
W   weekly frequency
M   month end frequency
BM  business month end frequency
MS  month start frequency
BMS business month start frequency
Q   quarter end frequency
BQ  business quarter endfrequency
QS  quarter start frequency
BQS business quarter start frequency
A   year end frequency
BA  business year end frequency
AS  year start frequency
BAS business year start frequency
H   hourly frequency
T   minutely frequency
S   secondly frequency
L   milliseconds
U   microseconds
</code></pre>

<p>Source: <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"">http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases</a></p>
";;1;;2013-11-06T19:43:41.670;;19821311;2015-04-27T02:13:58.827;2015-04-27T02:13:58.827;;1269634.0;;479673.0;17001389.0;2;62;;;
7556;7556;;;"<p>You can use the attribute <code>df.empty</code> to check whether it's empty or not:</p>

<pre><code>if df.empty:
    print('DataFrame is empty!')
</code></pre>

<p>Source: <a href=""http://pandas.pydata.org/pandas-docs/dev/basics.html#boolean-reductions"">Pandas Documentation</a></p>
";;2;;2013-11-07T05:55:48.427;;19828967;2013-11-07T05:55:48.427;;;;;2106009.0;19828822.0;2;171;;;
7590;7590;;;"<p>The <code>rename</code> method takes a dictionary for the index which applies to index <em>values</em>.<br>
You want to rename to index level's name:</p>

<pre><code>df.index.names = ['Date']
</code></pre>

<p><em>A good way to think about this is that columns and index are the same type of object (<code>Index</code> or <code>MultiIndex</code>), and you can interchange the two via transpose.</em></p>

<p>This is a little bit confusing since the index names have a similar meaning to columns, so here are some more examples:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))

In [2]: df
Out[2]: 
   A  B  C
0  1  2  3
1  4  5  6

In [3]: df1 = df.set_index('A')

In [4]: df1
Out[4]: 
   B  C
A      
1  2  3
4  5  6
</code></pre>

<p>You can see the rename on the index, which can change the <em>value</em> 1:</p>

<pre><code>In [5]: df1.rename(index={1: 'a'})
Out[5]: 
   B  C
A      
a  2  3
4  5  6

In [6]: df1.rename(columns={'B': 'BB'})
Out[6]: 
   BB  C
A       
1   2  3
4   5  6
</code></pre>

<p>Whilst renaming the level names:</p>

<pre><code>In [7]: df1.index.names = ['index']
        df1.columns.names = ['column']
</code></pre>

<p>Note: this attribute is just a list, and you could do the renaming as a list comprehension/map.</p>

<pre><code>In [8]: df1
Out[8]: 
column  B  C
index       
1       2  3
4       5  6
</code></pre>
";;0;;2013-11-08T04:19:28.283;;19851521;2015-07-30T14:42:53.517;2015-07-30T14:42:53.517;;5089491.0;;1240268.0;19851005.0;2;95;;;
7600;7600;;;"<p>You can apply a function that tests row-wise your <code>DataFrame</code> for the presence of strings, e.g., say that <code>df</code> is your <code>DataFrame</code> </p>

<pre><code> rows_with_strings  = df.apply(
       lambda row : 
          any([ isinstance(e, basestring) for e in row ])
       , axis=1) 
</code></pre>

<p>This will produce a mask for your DataFrame indicating which rows contain at least one string. You can hence select the rows without strings through the opposite mask</p>

<pre><code> df_with_no_strings = df[~rows_with_strings]
</code></pre>

<p>.</p>

<p><strong>Example:</strong></p>

<pre><code> a = [[1,2],['a',2], [3,4], [7,'d']]
 df = pd.DataFrame(a,columns = ['a','b'])


 df 
   a  b
0  1  2
1  a  2
2  3  4
3  7  d

select  = df.apply(lambda r : any([isinstance(e, basestring) for e in r  ]),axis=1) 

df[~select]                                                                                                                                

    a  b
 0  1  2
 2  3  4
</code></pre>
";;2;;2013-11-08T14:34:17.597;;19861545;2013-11-08T15:14:44.797;2013-11-08T15:14:44.797;;1714661.0;;1714661.0;19860389.0;2;12;;;
7620;7620;;;"<p>You should use loc and do this <strong>without chaining</strong>:</p>

<pre><code>In [11]: df.loc[df.cherry == 'bad', ['apple', 'banana']] = np.nan

In [12]: df
Out[12]: 
   apple  banana cherry
0      0       3   good
1    NaN     NaN    bad
2      2       5   good
</code></pre>

<p><em>See the docs on <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy"">returning a view vs a copy</a>, if you chain the assignment is made to the copy (and thrown away) but if you do it in one loc then pandas cleverly realises you want to assign to the original.</em></p>
";;3;;2013-11-08T20:14:12.477;;19867768;2013-11-08T20:14:12.477;;;;;1240268.0;19867734.0;2;28;;;
7647;7647;;;"<p>You want the <code>quantile</code> method:</p>

<pre><code>In [47]: df
Out[47]: 
           A         B    C
0   0.719391  0.091693  one
1   0.951499  0.837160  one
2   0.975212  0.224855  one
3   0.807620  0.031284  one
4   0.633190  0.342889  one
5   0.075102  0.899291  one
6   0.502843  0.773424  one
7   0.032285  0.242476  one
8   0.794938  0.607745  one
9   0.620387  0.574222  one
10  0.446639  0.549749  two
11  0.664324  0.134041  two
12  0.622217  0.505057  two
13  0.670338  0.990870  two
14  0.281431  0.016245  two
15  0.675756  0.185967  two
16  0.145147  0.045686  two
17  0.404413  0.191482  two
18  0.949130  0.943509  two
19  0.164642  0.157013  two

In [48]: df.groupby('C').quantile(.95)
Out[48]: 
            A         B
C                      
one  0.964541  0.871332
two  0.826112  0.969558
</code></pre>
";;1;;2013-11-10T21:02:17.957;;19895152;2013-11-10T21:02:17.957;;;;;1889400.0;19894939.0;2;25;;;
7656;7656;;;"<p>I don't understand why there isn't a <code>B2</code> in your dict.  I'm also not sure what you want to happen in the case of repeated column values (every one except the last, I mean.)  Assuming the first is an oversight, we could use recursion:</p>

<pre><code>def recur_dictify(frame):
    if len(frame.columns) == 1:
        if frame.values.size == 1: return frame.values[0][0]
        return frame.values.squeeze()
    grouped = frame.groupby(frame.columns[0])
    d = {k: recur_dictify(g.ix[:,1:]) for k,g in grouped}
    return d
</code></pre>

<p>which produces</p>

<pre><code>&gt;&gt;&gt; df
  name  v1   v2  v3
0    A  A1  A11   1
1    A  A2  A12   2
2    B  B1  B12   3
3    C  C1  C11   4
4    B  B2  B21   5
5    A  A2  A21   6
&gt;&gt;&gt; pprint.pprint(recur_dictify(df))
{'A': {'A1': {'A11': 1}, 'A2': {'A12': 2, 'A21': 6}},
 'B': {'B1': {'B12': 3}, 'B2': {'B21': 5}},
 'C': {'C1': {'C11': 4}}}
</code></pre>

<p>It might be simpler to use a non-pandas approach, though:</p>

<pre><code>def retro_dictify(frame):
    d = {}
    for row in frame.values:
        here = d
        for elem in row[:-2]:
            if elem not in here:
                here[elem] = {}
            here = here[elem]
        here[row[-2]] = row[-1]
    return d
</code></pre>
";;1;;2013-11-11T06:37:44.497;;19900276;2013-11-11T06:47:52.190;2013-11-11T06:47:52.190;;487339.0;;487339.0;19798112.0;2;20;;;
7673;7673;;;"<pre><code>df['color'] = np.where(df['Set']=='Z', 'green', 'red')
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
df['color'] = np.where(df['Set']=='Z', 'green', 'red')
print(df)
</code></pre>

<p>yields</p>

<pre><code>  Set Type  color
0   Z    A  green
1   Z    B  green
2   X    B    red
3   Y    C    red
</code></pre>

<hr>

<p>If you had more conditions then use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""noreferrer""><code>np.select</code></a>. For example, if you want color to be </p>

<ul>
<li><code>yellow</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'A')</code></li>
<li>otherwise <code>blue</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'B')</code> </li>
<li>otherwise <code>purple</code> when <code>(df['Type'] == 'B')</code></li>
<li>otherwise <code>black</code>,</li>
</ul>

<p>then use</p>

<pre><code>df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
conditions = [
    (df['Set'] == 'Z') &amp; (df['Type'] == 'A'),
    (df['Set'] == 'Z') &amp; (df['Type'] == 'B'),
    (df['Type'] == 'B')]
choices = ['yellow', 'blue', 'purple']
df['color'] = np.select(conditions, choices, default='black')
print(df)
</code></pre>

<p>which yields</p>

<pre><code>  Set Type   color
0   Z    A  yellow
1   Z    B    blue
2   X    B  purple
3   Y    C   black
</code></pre>
";;5;;2013-11-11T19:03:15.037;;19913845;2017-08-13T10:32:37.970;2017-08-13T10:32:37.970;;190597.0;;190597.0;19913659.0;2;154;;;
7679;7679;;;"<p>This solves the problem:</p>

<pre><code>df['newcolumn'] = df.A * df.B
</code></pre>

<p>You could also do:</p>

<pre><code>def fab(row):
  return row['A'] * row['B']

df['newcolumn'] = df.apply(fab, axis=1)
</code></pre>
";;1;;2013-11-11T20:17:21.430;;19915115;2013-11-11T20:17:21.430;;;;;1177562.0;19914937.0;2;13;;;
7684;7684;;;"<p>You also need to be careful to create a copy of the DataFrame, otherwise the csvdata_old will be updated with csvdata (since it points to the same object):</p>

<pre><code>csvdata_old = csvdata.copy()
</code></pre>

<p>To check whether they are equal, you can <a href=""https://stackoverflow.com/questions/19322506/pandas-dataframes-with-nans-equality-comparison"">use assert_frame_equal as in this answer</a>:</p>

<pre><code>from pandas.util.testing import assert_frame_equal
assert_frame_equal(csvdata, csvdata_old)
</code></pre>

<p>You can wrap this in a function with something like:</p>

<pre><code>try:
    assert_frame_equal(csvdata, csvdata_old)
    return True
except:  # appeantly AssertionError doesn't catch all
    return False
</code></pre>

<p><em>There was discussion of a better way...</em></p>
";;5;;2013-11-12T00:40:20.467;;19918849;2013-11-12T04:25:39.947;2017-05-23T12:25:36.887;;-1.0;;1240268.0;19917545.0;2;22;;;
7693;7693;;;"<p>You can go with @greenAfrican example, if it's possible for you to rewrite your function. But if you don't want to rewrite your function, you can wrap it into anonymous function inside apply, like this:</p>

<pre><code>&gt;&gt;&gt; def fxy(x, y):
...     return x * y

&gt;&gt;&gt; df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)
&gt;&gt;&gt; df
    A   B  newcolumn
0  10  20        200
1  20  30        600
2  30  10        300
</code></pre>
";;0;;2013-11-12T06:52:41.130;;19922732;2013-11-14T06:43:35.560;2013-11-14T06:43:35.560;;1744834.0;;1744834.0;19914937.0;2;77;;;
7696;7696;;;"<p>Ah, of course there is a solution for this already:</p>

<pre><code>from pandas.util.testing import assert_frame_equal
</code></pre>
";;0;;2013-11-12T11:45:48.087;;19928288;2013-11-12T11:45:48.087;;;;;57215.0;19928284.0;2;30;;;
7702;7702;;;"<p>In case anyone needs to try and merge two dataframes together on the index (instead of another column), this also works!</p>

<p>T1 and T2 are dataframes that have the same indices</p>

<pre><code>import pandas as pd
T1 = pd.merge(T1, T2, on=T1.index, how='outer')
</code></pre>

<p>P.S. I had to use merge because append would fill NaNs in unnecessarily.</p>
";;0;;2013-11-12T19:14:07.657;;19937902;2013-11-12T19:14:07.657;;;;;2217577.0;18792918.0;2;9;;;
7728;7728;;;"<p>You can use <code>something.isin(somewhere)</code> and <code>~something.isin(somewhere)</code>:</p>

<pre><code>&gt;&gt;&gt; df
  countries
0        US
1        UK
2   Germany
3     China
&gt;&gt;&gt; countries
['UK', 'China']
&gt;&gt;&gt; df.countries.isin(countries)
0    False
1     True
2    False
3     True
Name: countries, dtype: bool
&gt;&gt;&gt; df[df.countries.isin(countries)]
  countries
1        UK
3     China
&gt;&gt;&gt; df[~df.countries.isin(countries)]
  countries
0        US
2   Germany
</code></pre>
";;7;;2013-11-13T17:13:39.863;;19960116;2013-11-13T17:13:39.863;;;;;487339.0;19960077.0;2;200;;;
7729;7729;;;"<p>I've been usually doing generic filtering over rows like this:</p>

<pre><code>criterion = lambda row: row['countries'] not in countries
not_in = df[df.apply(criterion, axis=1)]
</code></pre>
";;2;;2013-11-13T17:14:32.250;;19960136;2013-11-13T17:14:32.250;;;;;399317.0;19960077.0;2;6;;;
7731;7731;;;"<p>You can pivot your DataFrame after creating:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(data)
&gt;&gt;&gt; df.pivot(index=0, columns=1, values=2)
# avg DataFrame
1      c1     c2
0               
r1  avg11  avg12
r2  avg21  avg22
&gt;&gt;&gt; df.pivot(index=0, columns=1, values=3)
# stdev DataFrame
1        c1       c2
0                   
r1  stdev11  stdev12
r2  stdev21  stdev22
</code></pre>
";;1;;2013-11-13T18:28:06.063;;19961557;2013-11-13T18:28:06.063;;;;;1744834.0;19961490.0;2;42;;;
7733;7733;;;"<p>I submit that it is better to leave your data stacked as it is:</p>

<pre><code>df = pandas.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])

# Possibly also this if these can always be the indexes:
# df = df.set_index(['R_Number', 'C_Number'])
</code></pre>

<p>Then it's a bit more intuitive to say</p>

<pre><code>df.set_index(['R_Number', 'C_Number']).Avg.unstack(level=1)
</code></pre>

<p>This way it is implicit that you're seeking to reshape the averages, or the standard deviations. Whereas, just using <code>pivot</code>, it's purely based on column convention as to what semantic entity it is that you are reshaping.</p>
";;4;;2013-11-13T18:44:56.647;;19961872;2013-11-13T18:44:56.647;;;;;567620.0;19961490.0;2;24;;;
7742;7742;;;"<p>One way would be to use <code>transform</code>:</p>

<pre><code>&gt;&gt;&gt; df
  name  value
0    A      1
1    A    NaN
2    B    NaN
3    B      2
4    B      3
5    B      1
6    C      3
7    C    NaN
8    C      3
&gt;&gt;&gt; df[""value""] = df.groupby(""name"").transform(lambda x: x.fillna(x.mean()))
&gt;&gt;&gt; df
  name  value
0    A      1
1    A      1
2    B      2
3    B      2
4    B      3
5    B      1
6    C      3
7    C      3
8    C      3
</code></pre>
";;5;;2013-11-13T22:51:24.313;;19966142;2013-11-13T22:51:24.313;;;;;487339.0;19966018.0;2;30;;;
7752;7752;;;"<p>Just change the final line to:</p>

<pre><code>ax2.plot(ax.get_xticks(),df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)
</code></pre>

<p>You will be all set.</p>

<p><img src=""https://i.stack.imgur.com/uxkFB.png"" alt=""enter image description here""></p>

<p>I don't quite get your second question. The 1st and 2nd y axis are of different scale, what do you mean by aligning them to the same line? They can't be aligned to the same grid line (yes you can but the right axis will look ugly, having values like 0.687 and alike). Anyway, you can do:</p>

<pre><code>ax.set_ylim((-10, 80.))
</code></pre>

<p>to align them, and the plot now looks ugly:</p>

<p><img src=""https://i.stack.imgur.com/3HwNd.png"" alt=""enter image description here""></p>
";;3;;2013-11-14T03:52:00.003;;19969224;2013-11-14T03:59:07.403;2013-11-14T03:59:07.403;;2487184.0;;2487184.0;19952290.0;2;6;;;
7754;7754;;;"<p>In <code>ipython</code>, I use this to print a part of the dataframe that works quite well (prints the first 100 rows):</p>

<pre><code>print paramdata.head(100).to_string()
</code></pre>
";;0;;2013-11-14T09:18:43.600;;19973722;2015-10-27T17:51:10.660;2015-10-27T17:51:10.660;;2901002.0;;2991338.0;11361985.0;2;10;;;
7758;7758;;;"<p>Alternatively, you can use numpy underlying function:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})
&gt;&gt;&gt; df['new_column'] = np.multiply(df['A'], df['B'])
&gt;&gt;&gt; df
    A   B  new_column
0  10  20         200
1  20  30         600
2  30  10         300
</code></pre>

<p>or vectorize arbitrary function in general case:</p>

<pre><code>&gt;&gt;&gt; def fx(x, y):
...     return x*y
...
&gt;&gt;&gt; df['new_column'] = np.vectorize(fx)(df['A'], df['B'])
&gt;&gt;&gt; df
    A   B  new_column
0  10  20         200
1  20  30         600
2  30  10         300
</code></pre>
";;1;;2013-11-14T11:17:46.190;;19976286;2013-11-14T11:17:46.190;;;;;1265154.0;19914937.0;2;45;;;
7764;7764;;;"<p>Its not really clear what you are asking. Multi-index docs are <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#hierarchical-indexing-multiindex"" rel=""noreferrer"">here</a></p>

<p>The OP needs to set the index, then sort in place</p>

<pre><code>df.set_index(['fileName','phrase'],inplace=True)
df.sortlevel(inplace=True)
</code></pre>

<p>Then access these levels via a tuple to get a specific result</p>

<pre><code>df.ix[('somePath','somePhrase')]
</code></pre>

<p>Maybe just give a toy example like this and show I want to get a specific result.</p>

<pre><code>In [1]: arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'])
   ...:    .....: ,
   ...:    .....:           np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])
   ...:    .....:           ]

In [2]: df = DataFrame(randn(8, 4), index=arrays)

In [3]: df
Out[3]: 
                0         1         2         3
bar one  1.654436  0.184326 -2.337694  0.625120
    two  0.308995  1.219156 -0.906315  1.555925
baz one -0.180826 -1.951569  1.617950 -1.401658
    two  0.399151 -1.305852  1.530370 -0.132802
foo one  1.097562  0.097126  0.387418  0.106769
    two  0.465681  0.270120 -0.387639 -0.142705
qux one -0.656487 -0.154881  0.495044 -1.380583
    two  0.274045 -0.070566  1.274355  1.172247

In [4]: df.index.lexsort_depth
Out[4]: 2

In [5]: df.ix[('foo','one')]
Out[5]: 
0    1.097562
1    0.097126
2    0.387418
3    0.106769
Name: (foo, one), dtype: float64

In [6]: df.ix['foo']
Out[6]: 
            0         1         2         3
one  1.097562  0.097126  0.387418  0.106769
two  0.465681  0.270120 -0.387639 -0.142705

In [7]: df.ix[['foo']]
Out[7]: 
                0         1         2         3
foo one  1.097562  0.097126  0.387418  0.106769
    two  0.465681  0.270120 -0.387639 -0.142705

In [8]: df.sortlevel(level=1)
Out[8]: 
                0         1         2         3
bar one  1.654436  0.184326 -2.337694  0.625120
baz one -0.180826 -1.951569  1.617950 -1.401658
foo one  1.097562  0.097126  0.387418  0.106769
qux one -0.656487 -0.154881  0.495044 -1.380583
bar two  0.308995  1.219156 -0.906315  1.555925
baz two  0.399151 -1.305852  1.530370 -0.132802
foo two  0.465681  0.270120 -0.387639 -0.142705
qux two  0.274045 -0.070566  1.274355  1.172247

In [10]: df.sortlevel(level=1).index.lexsort_depth
Out[10]: 0
</code></pre>
";;8;;2013-11-14T16:17:55.447;;19982756;2013-11-14T17:57:08.060;2013-11-14T17:57:08.060;;644898.0;;644898.0;19981518.0;2;11;;;
7776;7776;;;"<p>I think you can almost do exactly what you thought would be ideal, using the <a href=""http://statsmodels.sourceforge.net/"">statsmodels</a> package which is one of <code>pandas</code>' optional dependencies (it's used for a few things in <code>pandas.stats</code>.)</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import statsmodels.formula.api as sm
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})
&gt;&gt;&gt; result = sm.ols(formula=""A ~ B + C"", data=df).fit()
&gt;&gt;&gt; print result.params
Intercept    14.952480
B             0.401182
C             0.000352
dtype: float64
&gt;&gt;&gt; print result.summary()
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      A   R-squared:                       0.579
Model:                            OLS   Adj. R-squared:                  0.158
Method:                 Least Squares   F-statistic:                     1.375
Date:                Thu, 14 Nov 2013   Prob (F-statistic):              0.421
Time:                        20:04:30   Log-Likelihood:                -18.178
No. Observations:                   5   AIC:                             42.36
Df Residuals:                       2   BIC:                             41.19
Df Model:                           2                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept     14.9525     17.764      0.842      0.489       -61.481    91.386
B              0.4012      0.650      0.617      0.600        -2.394     3.197
C              0.0004      0.001      0.650      0.583        -0.002     0.003
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   1.061
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.498
Skew:                          -0.123   Prob(JB):                        0.780
Kurtosis:                       1.474   Cond. No.                     5.21e+04
==============================================================================

Warnings:
[1] The condition number is large, 5.21e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre>
";;4;;2013-11-15T01:05:30.447;;19991632;2013-11-16T14:19:42.533;2013-11-16T14:19:42.533;;487339.0;;487339.0;19991445.0;2;78;;;
7785;7785;;;"<p><strong>Note:</strong> <code>pandas.stats</code> <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#whatsnew-0200-prior-deprecations"" rel=""nofollow noreferrer"">has been removed</a> with 0.20.0</p>

<hr>

<p>It's possible to do this with <code>pandas.stats.ols</code>:</p>

<pre><code>&gt;&gt;&gt; from pandas.stats.api import ols
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})
&gt;&gt;&gt; res = ols(y=df['A'], x=df[['B','C']])
&gt;&gt;&gt; res
-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~ &lt;B&gt; + &lt;C&gt; + &lt;intercept&gt;

Number of Observations:         5
Number of Degrees of Freedom:   3

R-squared:         0.5789
Adj R-squared:     0.1577

Rmse:             14.5108

F-stat (2, 2):     1.3746, p-value:     0.4211

Degrees of Freedom: model 2, resid 2

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%
--------------------------------------------------------------------------------
             B     0.4012     0.6497       0.62     0.5999    -0.8723     1.6746
             C     0.0004     0.0005       0.65     0.5826    -0.0007     0.0014
     intercept    14.9525    17.7643       0.84     0.4886   -19.8655    49.7705
---------------------------------End of Summary---------------------------------
</code></pre>

<p>Note that you need to have <code>statsmodels</code> package installed, it is used internally by the <code>pandas.stats.ols</code> function.</p>
";;4;;2013-11-15T08:00:23.610;;19996208;2017-06-30T18:04:07.577;2017-06-30T18:04:07.577;;826983.0;;1744834.0;19991445.0;2;55;;;
7802;7802;;;"<p>Change the type of column ""vals"" prior to exporting the data frame to a CSV file</p>

<pre><code>df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)

df_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')
</code></pre>
";;1;;2013-11-15T17:23:09.623;;20006954;2013-11-15T22:23:24.580;2013-11-15T22:23:24.580;;1175491.0;;1175491.0;20003290.0;2;23;;;
7809;7809;;;"<p>The function for this is <code>from_records</code>, see <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.from_records.html"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.from_records.html</a>.</p>
";;0;;2013-11-15T23:45:28.980;;20012628;2013-11-15T23:45:28.980;;;;;542190.0;20012507.0;2;10;;;
7814;7814;;;"<p>In a similar vein to <a href=""https://stackoverflow.com/a/17005204/1240268"">creating a Series from a namedtuple</a>, you can use the <a href=""http://docs.python.org/2/library/collections.html#collections.somenamedtuple._fields"" rel=""nofollow noreferrer""><code>_fields</code></a> attribute:</p>

<pre><code>In [11]: Point = namedtuple('Point', ['x', 'y'])

In [12]: points = [Point(1, 2), Point(3, 4)]

In [13]: pd.DataFrame(points, columns=Point._fields)
Out[13]: 
   x  y
0  1  2
1  3  4
</code></pre>

<p><em>Assuming they are all of the same type, in this example all <code>Point</code>s.</em></p>
";;0;;2013-11-16T05:24:38.143;;20015080;2013-11-16T05:35:04.143;2017-05-23T12:00:31.437;;-1.0;;1240268.0;20012507.0;2;18;;;
7824;7824;;;"<blockquote>
  <p>This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place.</p>
</blockquote>

<p>No it doesn't, just convert to a NumPy array:</p>

<pre><code>&gt;&gt;&gt; data = np.asarray(df)
</code></pre>

<p>This takes constant time because it just creates a <em>view</em> on your data. Then feed it to scikit-learn:</p>

<pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression
&gt;&gt;&gt; lr = LinearRegression()
&gt;&gt;&gt; X, y = data[:, 1:], data[:, 0]
&gt;&gt;&gt; lr.fit(X, y)
LinearRegression(copy_X=True, fit_intercept=True, normalize=False)
&gt;&gt;&gt; lr.coef_
array([  4.01182386e-01,   3.51587361e-04])
&gt;&gt;&gt; lr.intercept_
14.952479503953672
</code></pre>
";;6;;2013-11-16T14:14:30.840;;20019449;2013-11-17T13:16:02.173;2013-11-17T13:16:02.173;;166749.0;;166749.0;19991445.0;2;10;;;
7832;7832;;;"<p>First, I don't get where in your sample array are features, and where observations.</p>

<p>Second, <code>DictVectorizer</code> holds no data, and is only about transformation utility and metadata storage. After transformation it stores features names and mapping. It returns a numpy array, used for further computations. Numpy array (features matrix) size equals to <code>features count</code> x <code>number of observations</code>, with values equal to feature value for an observation. So if you know your observations and features, you can create this array any other way you like.</p>

<p>In case you expect sklearn do it for you, you don't have to reconstruct dict manually, as  it can be done with <code>to_dict</code> applied to transposed dataframe:</p>

<pre><code>&gt;&gt;&gt; df
  col1 col2
0    A  foo
1    B  bar
2    C  foo
3    A  bar
4    A  foo
5    B  bar
&gt;&gt;&gt; df.T.to_dict().values()
[{'col2': 'foo', 'col1': 'A'}, {'col2': 'bar', 'col1': 'B'}, {'col2': 'foo', 'col1': 'C'}, {'col2': 'bar', 'col1': 'A'}, {'col2': 'foo', 'col1': 'A'}, {'col2': 'bar', 'col1': 'B'}]
</code></pre>

<hr>

<p>Since scikit-learn 0.13.0 (Jan 3, 2014) there is a new parameter <code>'records'</code> for the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html"" rel=""nofollow""><code>to_dict()</code></a> method available, so now you can simple use this method without additional manipulations:</p>

<pre><code>&gt;&gt;&gt; df = pandas.DataFrame({'col1': ['A', 'B', 'C', 'A', 'A', 'B'], 'col2': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar']})
&gt;&gt;&gt; df
  col1 col2
0    A  foo
1    B  bar
2    C  foo
3    A  bar
4    A  foo
5    B  bar
&gt;&gt;&gt; df.to_dict('records')
[{'col2': 'foo', 'col1': 'A'}, {'col2': 'bar', 'col1': 'B'}, {'col2': 'foo', 'col1': 'C'}, {'col2': 'bar', 'col1': 'A'}, {'col2': 'foo', 'col1': 'A'}, {'col2': 'bar', 'col1': 'B'}]
</code></pre>
";;3;;2013-11-16T22:44:23.663;;20024879;2016-02-14T13:26:44.927;2016-02-14T13:26:44.927;;60281.0;;1265154.0;20024584.0;2;10;;;
7837;7837;;;"<pre><code>df['col'] = 'str' + df['col'].astype(str)
</code></pre>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col':['a',0]})
&gt;&gt;&gt; df
  col
0   a
1   0
&gt;&gt;&gt; df['col'] = 'str' + df['col'].astype(str)
&gt;&gt;&gt; df
    col
0  stra
1  str0
</code></pre>
";;1;;2013-11-17T05:00:19.437;;20027386;2013-11-17T05:00:19.437;;;;;1744834.0;20025882.0;2;52;;;
7840;7840;;;"<p>Take a look at <code>sklearn-pandas</code> which provides exactly what you're looking for. The corresponding Github repo is <a href=""https://github.com/paulgb/sklearn-pandas"">here</a>.</p>
";;3;;2013-11-17T15:08:33.747;;20032254;2013-11-17T15:08:33.747;;;;;18601.0;20024584.0;2;9;;;
7842;7842;;;"<pre><code>&gt;&gt;&gt; frame['HighScore'] = frame[['test1','test2','test3']].apply(max, axis=1)
&gt;&gt;&gt; frame
    name  test1  test2  test3  HighScore
0   bill     85     35     51        85
1    joe     75     45     61        75
2  steve     85     83     45        85
</code></pre>
";;1;;2013-11-17T16:33:09.613;;20033218;2013-11-17T16:43:52.773;2013-11-17T16:43:52.773;;1265154.0;;1265154.0;20033111.0;2;11;;;
7843;7843;;;"<pre><code>&gt;&gt;&gt; frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)
&gt;&gt;&gt; frame
    name  test1  test2  test3  HighScore
0   bill     85     35     51         85
1    joe     75     45     61         75
2  steve     85     83     45         85
</code></pre>
";;2;;2013-11-17T16:33:37.450;;20033232;2013-11-17T16:33:37.450;;;;;1744834.0;20033111.0;2;59;;;
7854;7854;;;"<p>It's going to depend on the size of you DataFrames which is faster, but another option is to use join to smash your multi line ""JSON"" (Note: it's not valid json), into valid json and use read_json:</p>

<pre><code>In [11]: '[%s]' % ','.join(test.splitlines())
Out[11]: '[{""a"":1,""b"":2},{""a"":3,""b"":4}]'
</code></pre>

<p>For this tiny example this is slower, if around 100 it's the similar, signicant gains if it's larger...</p>

<pre><code>In [21]: %timeit pd.read_json('[%s]' % ','.join(test.splitlines()))
1000 loops, best of 3: 977 s per loop

In [22]: %timeit l=[ json.loads(l) for l in test.splitlines()]; df = pd.DataFrame(l)
1000 loops, best of 3: 282 s per loop

In [23]: test_100 = '\n'.join([test] * 100)

In [24]: %timeit pd.read_json('[%s]' % ','.join(test_100.splitlines()))
1000 loops, best of 3: 1.25 ms per loop

In [25]: %timeit l = [json.loads(l) for l in test_100.splitlines()]; df = pd.DataFrame(l)
1000 loops, best of 3: 1.25 ms per loop

In [26]: test_1000 = '\n'.join([test] * 1000)

In [27]: %timeit l = [json.loads(l) for l in test_1000.splitlines()]; df = pd.DataFrame(l)
100 loops, best of 3: 9.78 ms per loop

In [28]: %timeit pd.read_json('[%s]' % ','.join(test_1000.splitlines()))
100 loops, best of 3: 3.36 ms per loop
</code></pre>

<p><em>Note: of that time the join is surprisingly fast.</em></p>
";;1;;2013-11-18T02:16:31.543;;20038973;2013-11-18T02:16:31.543;;;;;1240268.0;20037430.0;2;9;;;
7855;7855;;;"<p>You don't really need to convert that into a set:</p>

<pre><code>cols = [col for col in df.columns if col not in ['B', 'D']]
df2 = df[cols]
</code></pre>
";;0;;2013-11-18T02:25:39.110;;20039057;2013-11-18T02:25:39.110;;;;;2102764.0;14940743.0;2;44;;;
7865;7865;;;"<p>I suppose you have to represent whole pandas object as <a href=""http://nbviewer.ipython.org/urls/raw.github.com/ipython/ipython/1.x/examples/notebooks/Part%205%20-%20Rich%20Display%20System.ipynb"">html object</a>, that is </p>

<pre><code>In [1]: from IPython.display import HTML

In [2]: df = pd.DataFrame(list(range(5)), columns=['a'])

In [3]: df['a'] = df['a'].apply(lambda x: '&lt;a href=""http://example.com/{0}""&gt;link&lt;/a&gt;'.format(x))

In [4]: HTML(df.to_html(escape=False))
</code></pre>

<p>Sorry, now don't have IPython at hand, and can't check whether output is correct.</p>
";;4;;2013-11-18T09:03:35.943;;20043785;2016-10-13T03:54:50.247;2016-10-13T03:54:50.247;;621449.0;;1265154.0;20035518.0;2;22;;;
7869;7869;;;"<p>There's also pd.factorize function to use:</p>

<pre><code># use the df data from @herrfz

In [150]: pd.factorize(df.b)
Out[150]: (array([0, 1, 0, 1, 2]), array(['yes', 'no', 'absent'], dtype=object))
In [152]: df['c'] = pd.factorize(df.b)[0]

In [153]: df
Out[153]: 
   a       b  c
0  1     yes  0
1  2      no  1
2  3     yes  0
3  4      no  1
4  5  absent  2
</code></pre>
";;0;;2013-11-18T15:35:07.800;;20051631;2013-11-18T15:35:07.800;;;;;2102764.0;15723628.0;2;7;;;
7879;7879;;;"<p>As of Pandas 0.12 (I believe) you can do:</p>

<pre><code>import pandas
import pyodbc

sql = 'select * from table'
cnn = pyodbc.connect(...)

data = pandas.read_sql(sql, cnn)
</code></pre>

<p>Prior to 0.12, you could do:</p>

<pre><code>import pandas
from pandas.io.sql import read_frame
import pyodbc

sql = 'select * from table'
cnn = pyodbc.connect(...)

data = read_frame(sql, cnn)
</code></pre>
";;6;;2013-11-18T23:01:13.190;;20059818;2013-11-18T23:01:13.190;;;;;1552748.0;20055257.0;2;28;;;
7904;7904;;;"<pre><code>&gt;&gt;&gt; df.groupby('id').first()
     value
id        
1    first
2    first
3    first
4   second
5    first
6    first
7   fourth
</code></pre>

<p>If you need <code>id</code> as column:</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').first().reset_index()
   id   value
0   1   first
1   2   first
2   3   first
3   4  second
4   5   first
5   6   first
6   7  fourth
</code></pre>

<p>To get n first records, you can use head():</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)
    id   value
0    1   first
1    1  second
2    2   first
3    2  second
4    3   first
5    3   third
6    4  second
7    4   fifth
8    5   first
9    6   first
10   6  second
11   7  fourth
12   7   fifth
</code></pre>
";;4;;2013-11-19T09:25:52.597;;20067665;2013-11-19T10:56:36.003;2013-11-19T10:56:36.003;;1744834.0;;1744834.0;20067636.0;2;78;;;
7908;7908;;;"<p>Did you try <code>df.groupby('id').head(2)</code></p>

<p>Ouput generated: </p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2)
       id  value
id             
1  0   1      1
   1   1      2 
2  3   2      1
   4   2      2
3  7   3      1
4  8   4      1
</code></pre>

<p>(Keep in mind that you might need to order/sort before, depending on your data)</p>

<p>EDIT: As mentioned by the questioner, use <code>df.groupby('id').head(2).reset_index(drop=True)</code> to remove the multindex and flatten the results.</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)
    id  value
0   1      1
1   1      2
2   2      1
3   2      2
4   3      1
5   4      1
</code></pre>
";;11;;2013-11-19T10:46:03.760;;20069379;2013-11-19T11:01:41.947;2013-11-19T11:01:41.947;;872535.0;;872535.0;20069009.0;2;61;;;
7912;7912;;;"<p>I think <code>df['word'].value_counts()</code> should serve. By skipping the groupby machinery, you'll save some time. I'm not sure why <code>count</code> should be much slower than <code>max</code>. Both take some time to avoid missing values. (Compare with <code>size</code>.)</p>

<p>In any case, value_counts has been <a href=""https://github.com/pydata/pandas/commit/d1a1fa27e520c0d196981df58edb1bd5d3cc3161"">specifically optimized</a> to handle object type, like your words, so I doubt you'll do much better than that.</p>
";;1;;2013-11-19T16:18:16.993;;20076611;2013-11-19T16:23:56.480;2013-11-19T16:23:56.480;;1221924.0;;1221924.0;20076195.0;2;62;;;
7923;7923;;;"<p>Here is a similar comparison I just did. Its about 1/3 of the data 10M rows. The final size is abou 1.3GB</p>

<p>I define 3 timing functions:</p>

<p>Test the Fixed format (called Storer in 0.12). This writes in a PyTables Array format</p>

<pre><code>def f(df):
    store = pd.HDFStore('test.h5','w')
    store['df'] = df
    store.close()
</code></pre>

<p>Write in the Table format, using PyTables Table format. Do not create an index.</p>

<pre><code>def f2(df):
    store = pd.HDFStore('test.h5','w')
    store.append('df',df,index=False)
    store.close()
</code></pre>

<p>Same as f2, but create an index (which is normally done)</p>

<pre><code>def f3(df):
    store = pd.HDFStore('test.h5','w')
    store.append('df',df)
    store.close()
</code></pre>

<p>Create the frame</p>

<pre><code>In [25]: df = concat([DataFrame(np.random.randn(10000000,10)),DataFrame(np.random.randint(0,10,size=50000000).reshape(10000000,5))],axis=1)

In [26]: df
Out[26]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 10000000 entries, 0 to 9999999
Columns: 15 entries, 0 to 4
dtypes: float64(10), int64(5)


v0.12.0

In [27]: %timeit f(df)
1 loops, best of 3: 14.7 s per loop

In [28]: %timeit f2(df)
1 loops, best of 3: 32 s per loop

In [29]: %timeit f3(df)
1 loops, best of 3: 40.1 s per loop

master/v0.13.0

In [5]: %timeit f(df)
1 loops, best of 3: 12.9 s per loop

In [6]: %timeit f2(df)
1 loops, best of 3: 17.5 s per loop

In [7]: %timeit f3(df)
1 loops, best of 3: 24.3 s per loop
</code></pre>

<p>Timing Runs with the same file as provided by the OP (link is below)</p>

<pre><code>In [4]: df = pd.read_hdf('test.h5','df')

In [5]: df
Out[5]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 28880943 entries, 0 to 28880942
Columns: 14 entries, node_id to kernel_type
dtypes: float64(4), int64(10)
</code></pre>

<p>Like f1, Fixed format</p>

<pre><code>In [6]: %timeit df.to_hdf('test.hdf','df',mode='w')
1 loops, best of 3: 36.2 s per loop
</code></pre>

<p>Like f2, Table format, no index</p>

<pre><code>In [7]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False)
1 loops, best of 3: 45 s per loop

In [8]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False,chunksize=2000000)
1 loops, best of 3: 44.5 s per loop
</code></pre>

<p>Like f3, Table format with index</p>

<pre><code>In [9]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000)
1 loops, best of 3: 1min 36s per loop
</code></pre>

<p>Like f3, Table format with index, compressed with blosc</p>

<pre><code>In [10]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000,complib='blosc')
1 loops, best of 3: 46.5 s per loop

In [11]: %timeit pd.read_hdf('test.hdf','df')
1 loops, best of 3: 10.8 s per loop
</code></pre>

<p>Show original file (test.h5, and compressed, test.hdf)</p>

<pre><code>In [13]: !ls -ltr test.h*
-rw-r--r-- 1 jreback users 3471518282 Nov 20 18:20 test.h5
-rw-rw-r-- 1 jreback users  649327780 Nov 20 21:17 test.hdf
</code></pre>

<p>Several points to note.</p>

<ul>
<li><p>Not creating an index can make a non-trivial difference in time. I also believe that if you have a string based index it can substantially worsen write time. That said, you always want to create an index to make retrieval very fast.</p>

<p>You didn't include what your index is, nor whether its sorted (though I only think this makes a small difference).</p></li>
<li><p>the write penalty in my examples are roughly 2x (though I have seen it be somewhat bigger when INCLUDING the index time). Thus your 7s (1/2 of my time), for 3x the number I am writing is quite suspect. I am using a reasonably fast disk array. If you were using a flash based disk then this is possible, though.</p></li>
<li><p>master/v0.13.0 (release very soon), improves the write times on tables substantially.</p></li>
<li><p>you can try setting the <code>chunksize</code> parameter to a bigger number when you write the data (its default is 100000). The purpose of the 'relatively' low number is to have a constant memory usage. (e.g. if is bigger you will use more memory, in theory it should write faster though).</p></li>
<li><p>Tables offer 2 advantages over Fixed format: 1) query retrieval, and 2) appendability. Reading the entire table doesn't take advantage of either, so if you ONLY want to read the entire table, then Fixed format is recommended. (In my experience the flexibility of Tables greatly outweights the write penalty, but YMMV)</p></li>
</ul>

<p>Bottom line is to repeat the timings (use ipython as it will run multiple tests). If you can reproduce your results, then pls post a %prun and I'll take a look.</p>

<p>Update:</p>

<p>so the recommended way for a table this size is to compress with blosc and use pandas master/0.13.0 along with PyTables 3.0.0</p>
";;21;;2013-11-20T00:05:13.850;;20084843;2013-11-21T02:25:29.500;2013-11-21T02:25:29.500;;644898.0;;644898.0;20083098.0;2;7;;;
7924;7924;;;"<pre><code>In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))

In [2]: df
Out[2]: 
   0  1  2  3  4  5  6  7  8  9
0  2  2  3  2  6  1  9  9  3  3
1  1  2  5  8  5  2  5  0  6  3
2  0  7  0  7  5  5  9  1  0  3
3  5  3  2  3  7  6  8  3  8  4
4  8  0  2  2  3  9  7  1  2  7
5  3  2  8  5  6  4  3  7  0  8
6  4  2  6  5  3  3  4  5  3  2
7  7  6  0  6  6  7  1  7  5  1
8  7  4  3  1  0  6  9  7  7  3
9  5  3  4  5  2  0  8  6  4  7

In [13]: Series(df.values.ravel()).unique()
Out[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])
</code></pre>

<p>Numpy unique sorts, so its faster to do it this way (and then sort if you need to)</p>

<pre><code>In [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))

In [15]: %timeit Series(df.values.ravel()).unique()
10000 loops, best of 3: 137 ?s per loop

In [16]: %timeit np.unique(df.values.ravel())
1000 loops, best of 3: 270 ?s per loop
</code></pre>
";;5;;2013-11-20T00:10:04.870;;20084895;2013-11-20T00:46:26.133;2013-11-20T00:46:26.133;;644898.0;;644898.0;20084382.0;2;70;;;
7944;7944;;;"<p>There are a lot of options for read_csv which will handle all the cases you mentioned.  You might want to try dtype={'A': datetime.datetime}, but often you won't need dtypes as pandas can infer the types.  But if you have dates, then you need to specify the parse_date options.</p>

<pre><code>parse_dates : boolean, list of ints or names, list of lists, or dict
keep_date_col : boolean, default False
date_parser : function
</code></pre>

<p>In general to convert boolean values you will need to specify: </p>

<pre><code>true_values : list    Values to consider as True
false_values : list Values to consider as False
</code></pre>

<p>Which will transform any value in the list to the boolean true/false.  For more general conversions you will most likely need</p>

<p>converters : dict. optional  Dict of functions for converting values in certain columns. Keys can either be integers or column labels</p>

<p>Though dense, check here for the full list: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>
";;4;;2013-11-20T12:51:19.463;;20096494;2013-11-20T13:49:53.303;2013-11-20T13:49:53.303;;832621.0;;2816571.0;20095983.0;2;6;;;
7947;7947;;;"<pre><code>In [44]: df.gdp = df.gdp.shift(-1)

In [45]: df
Out[45]: 
   y  gdp  cap
0  1    3    5
1  2    7    9
2  8    4    2
3  3    7    7
4  6  NaN    7

In [46]: df[:-1]                                                                                                                                                                                                                                                                                                               
Out[46]: 
   y  gdp  cap
0  1    3    5
1  2    7    9
2  8    4    2
3  3    7    7
</code></pre>
";;1;;2013-11-20T13:06:32.420;;20096827;2013-11-20T13:06:32.420;;;;;1548051.0;20095673.0;2;49;;;
7961;7961;;;"<p>DataFrames and Series always have an index. Although it displays alongside the column(s), it is not a column, which is why del df['index'] did not work.</p>

<p>If you want to replace the index with simple sequential numbers, use <code>df.reset_index()</code>. I strongly suggest reading a little bit of the pandas documentation, like <a href=""http://pandas.pydata.org/pandas-docs/stable/10min.html"">10 minutes to Pandas</a> to get a sense for why the index is there is how it is used. </p>
";;5;;2013-11-20T21:53:21.060;;20107825;2013-11-20T21:53:21.060;;;;;1221924.0;20107570.0;2;22;;;
7983;7983;;;"<p>You can <strong>concat two DataFrames</strong>:</p>

<pre><code>&gt;&gt;&gt; df1 = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.sum)
&gt;&gt;&gt; df2 = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)
&gt;&gt;&gt; pd.concat((df1, df2), axis=1)
          D         E
B                    
A  1.810847 -0.524178
B  2.762190 -0.443031
C  0.867519  0.078460
</code></pre>

<p>or you can <strong>pass list of functions</strong> as <code>aggfunc</code> parameter and then reindex:</p>

<pre><code>&gt;&gt;&gt; df3 = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])
&gt;&gt;&gt; df3
        sum                mean          
          D         E         D         E
B                                        
A  1.810847 -4.193425  0.226356 -0.524178
B  2.762190 -3.544245  0.345274 -0.443031
C  0.867519  0.627677  0.108440  0.078460
&gt;&gt;&gt; df3 = df3.ix[:, [('sum', 'D'), ('mean','E')]]
&gt;&gt;&gt; df3.columns = ['D', 'E']
&gt;&gt;&gt; df3
          D         E
B                    
A  1.810847 -0.524178
B  2.762190 -0.443031
C  0.867519  0.078460
</code></pre>

<p>Alghouth, it would be nice to have an option to defin <code>aggfunc</code> for each column individually. Don't know how it could be done, may be pass into <code>aggfunc</code> dict-like parameter, like <code>{'D':np.mean, 'E':np.sum}</code>.</p>

<p><strong>update</strong> Actually, in your case you can <strong>pivot by hand</strong>:</p>

<pre><code>&gt;&gt;&gt; df.groupby('B').aggregate({'D':np.sum, 'E':np.mean})
          E         D
B                    
A -0.524178  1.810847
B -0.443031  2.762190
C  0.078460  0.867519
</code></pre>
";;3;;2013-11-21T11:50:24.517;;20120225;2013-11-21T13:18:17.330;2013-11-21T13:18:17.330;;1744834.0;;1744834.0;20119414.0;2;15;;;
8021;8021;;;"<p>You can use <code>names</code> parameter. For example, if you have csv file like this:</p>

<pre><code>1,2,1
2,3,4,2,3
1,2,3,3
1,2,3,4,5,6
</code></pre>

<p>And try to read it, you'll receive and error </p>

<pre><code>&gt;&gt;&gt; pd.read_csv(r'D:/Temp/tt.csv')
Traceback (most recent call last):
...
Expected 5 fields in line 4, saw 6
</code></pre>

<p>But if you pass <code>names</code> parameters, you'll get result:</p>

<pre><code>&gt;&gt;&gt; pd.read_csv(r'D:/Temp/tt.csv', names=list('abcdef'))
   a  b  c   d   e   f
0  1  2  1 NaN NaN NaN
1  2  3  4   2   3 NaN
2  1  2  3   3 NaN NaN
3  1  2  3   4   5   6
</code></pre>

<p>Hope it helps.</p>
";;0;;2013-11-22T21:02:28.957;;20154429;2013-11-22T21:02:28.957;;;;;1744834.0;20154303.0;2;21;;;
8025;8025;;;"<p><em>Note: The ideas here are pretty generic for StackOverflow, indeed <a href=""http://sscce.org/"" rel=""nofollow noreferrer"">questions</a>.</em></p>

<h3>Disclaimer: Writing a good question is HARD.</h3>

<h2>The Good:</h2>

<ul>
<li><p>do include small* example DataFrame, either as runnable code:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])
</code></pre>

<p>or make it ""copy and pasteable"" using <code>pd.read_clipboard(sep='\s\s+')</code>, you can format the text for StackOverflow highlight and use Ctrl+K (or prepend four spaces to each line):</p>

<pre><code>In [2]: df
Out[2]: 
   A  B
0  1  2
1  1  3
2  4  6
</code></pre>

<p>test <code>pd.read_clipboard(sep='\s\s+')</code> yourself.</p>

<p>* <em>I really do mean <strong>small</strong>, the vast majority of example DataFrames could be fewer than 6 rows<sup>citation needed</sup>, and <strong>I bet I can do it in 5 rows.</strong> Can you reproduce the error with <code>df = df.head()</code>, if not fiddle around to see if you can make up a small DataFrame which exhibits the issue you are facing.</em></p>

<p>* <em>Every rule has an exception, the obvious one is for performance issues  (<a href=""http://ipython.org/ipython-doc/dev/interactive/tutorial.html#magic-functions"" rel=""nofollow noreferrer"">in which case definitely use %timeit and possibly %prun</a>), where you should generate (consider using np.random.seed so we have the exact same frame): <code>df = pd.DataFrame(np.random.randn(100000000, 10))</code>. Saying that, ""make this code fast for me"" is not strictly on topic for the site...</em></p></li>
<li><p>write out the outcome you desire (similarly to above)</p>

<pre><code>In [3]: iwantthis
Out[3]: 
   A  B
0  1  5
1  4  6
</code></pre>

<p><em>Explain what the numbers come from: the 5 is sum of the B column for the rows where A is 1.</em></p></li>
<li><p>do show <em>the code</em> you've tried:</p>

<pre><code>In [4]: df.groupby('A').sum()
Out[4]: 
   B
A   
1  5
4  6
</code></pre>

<p><em>But say what's incorrect: the A column is in the index rather than a column.</em></p></li>
<li><p>do show you've done some research (<a href=""http://pandas.pydata.org/pandas-docs/stable/search.html?q=groupby+sum"" rel=""nofollow noreferrer"">search the docs</a>, <a href=""https://stackoverflow.com/search?q=[pandas]+groupby+sum"">search StackOverflow</a>), give a summary:</p>

<blockquote>
  <p>The docstring for sum simply states ""Compute sum of group values""</p>
  
  <p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#cython-optimized-aggregation-functions"" rel=""nofollow noreferrer"">groupby docs</a> don't give any examples for this.</p>
</blockquote>

<p><em>Aside: the answer here is to use <code>df.groupby('A', as_index=False).sum()</code>.</em></p></li>
<li><p>if it's relevant that you have Timestamp columns, e.g. you're resampling or something, then be explicit and apply <code>pd.to_datetime</code> to them for good measure**.</p>

<pre><code>df['date'] = pd.to_datetime(df['date']) # this column ought to be date..
</code></pre>

<p>** <em>Sometimes this is the issue itself: they were strings.</em></p></li>
</ul>

<h2>The Bad:</h2>

<ul>
<li><p>don't include a MultiIndex, which <strong>we can't copy and paste</strong> (see above), this is kind of a grievance with pandas default display but nonetheless annoying:</p>

<pre><code>In [11]: df
Out[11]:
     C
A B   
1 2  3
  2  6
</code></pre>

<p><em>The correct way is to include an ordinary DataFrame with a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a> call:</em></p>

<pre><code>In [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])

In [13]: df
Out[13]: 
     C
A B   
1 2  3
  2  6
</code></pre></li>
<li><p>do provide insight to what it is when giving the outcome you want:</p>

<pre><code>   B
A   
1  1
5  0
</code></pre>

<p><em>Be specific about how you got the numbers (what are they)... double check they're correct.</em></p></li>
<li><p>If your code throws an error, do include the entire stacktrace (this can be edited out later if it's too noisy). Show the line number (and the corresponding line of your code which it's raising against).</p></li>
</ul>

<h2>The Ugly:</h2>

<ul>
<li><p>don't link to a csv we don't have access to (ideally don't link to an external source at all...)</p>

<pre><code>df = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options
</code></pre>

<p><em><strong>Most data is proprietary</strong> we get that: Make up similar data and see if you can reproduce the problem (something small).</em></p></li>
<li><p>don't explain the situation vaguely in words, like you have a DataFrame which is ""large"", mention some of the column names in passing (be sure not to mention their dtypes). Try and go into lots of detail about something which is completely meaningless without seeing the actual context. Presumably noone is even going to read to the end of this paragraph.</p>

<p><em>Essays are bad, it's easier with small examples.</em></p></li>
<li><p>don't include 10+ (100+??) lines of data munging before getting to your actual question.</p>

<p><em>Please, we see enough of this in our day jobs. We want to help, but <a href=""https://www.youtube.com/watch?v=ECfRp-jwbI4"" rel=""nofollow noreferrer"">not like this...</a>.</em><br>
<em>Cut the intro, and just show the relevant DataFrames (or small versions of them) in the step which is causing you trouble.</em></p></li>
</ul>

<h3>Anyways, have fun learning python, numpy and pandas!</h3>
";;2;;2013-11-23T06:19:13.533;;20159305;2017-08-24T10:32:27.833;2017-08-24T10:32:27.833;;4909087.0;;1240268.0;20109391.0;2;100;;;
8039;8039;;;"<p>I doubt there is a both <em>quickest</em> and <em>simple</em> method. If you don't worry about data conversion, you can do</p>

<pre><code>&gt;&gt;&gt; import json
&gt;&gt;&gt; df = pd.DataFrame.from_dict({'A': {1: datetime.datetime.now()}})
&gt;&gt;&gt; df
                           A
1 2013-11-23 21:14:34.118531

&gt;&gt;&gt; records = json.loads(df.T.to_json()).values()
&gt;&gt;&gt; db.myCollection.insert(records)
</code></pre>

<p>But in case you try to <a href=""https://stackoverflow.com/questions/16249736/how-to-import-data-from-mongodb-to-pandas"">load data back</a>, you'll get:</p>

<pre><code>&gt;&gt;&gt; df = read_mongo(db, 'myCollection')
&gt;&gt;&gt; df
                     A
0  1385241274118531000
&gt;&gt;&gt; df.dtypes
A    int64
dtype: object
</code></pre>

<p>so you'll have to convert 'A' columnt back to <code>datetime</code>s, as well as all not <code>int</code>, <code>float</code> or <code>str</code> fields in your <code>DataFrame</code>. For this example:</p>

<pre><code>&gt;&gt;&gt; df['A'] = pd.to_datetime(df['A'])
&gt;&gt;&gt; df
                           A
0 2013-11-23 21:14:34.118531
</code></pre>
";;1;;2013-11-23T21:17:09.267;;20167984;2014-09-23T02:10:00.823;2017-05-23T12:25:42.197;;-1.0;;1265154.0;20167194.0;2;15;;;
8041;8041;;;"<p>Index is an object, and default index starts from <code>0</code>:</p>

<pre><code>&gt;&gt;&gt; result.index
Int64Index([0, 1, 2], dtype=int64)
</code></pre>

<p>You can shift this index by <code>1</code> with</p>

<pre><code>&gt;&gt;&gt; result.index += 1 
&gt;&gt;&gt; result.index
Int64Index([1, 2, 3], dtype=int64)
</code></pre>
";;1;;2013-11-23T21:57:00.560;;20168416;2013-11-23T21:57:00.560;;;;;1265154.0;20167930.0;2;27;;;
8052;8052;;;"<p>This is a job for <code>groupby</code>:</p>

<pre><code>&gt;&gt;&gt; df.groupby([""score"", ""type""]).sum()
                        count
score    type                
9.397000 advanced  537.331573
9.397995 advanced    9.641728
9.397996 newbie      0.100000
9.399900 expert     19.6541374
&gt;&gt;&gt; df.groupby([""score"", ""type""], as_index=False).sum()
      score      type       count
0  9.397000  advanced  537.331573
1  9.397995  advanced    9.641728
2  9.397996    newbie    0.100000
3  9.399900    expert   19.654137
</code></pre>
";;0;;2013-11-24T22:06:15.077;;20181686;2013-11-24T22:12:02.833;2013-11-24T22:12:02.833;;487339.0;;487339.0;20181456.0;2;23;;;
8080;8080;;;"<pre><code>&gt;&gt;&gt; df[df.groupby(level=0).transform(len)['type'] &gt; 1]
                   type
genome_location1   MIR3
genome_location1  AluJb
</code></pre>
";;4;;2013-11-25T17:47:18.943;;20199798;2013-11-25T17:47:18.943;;;;;1744834.0;20199129.0;2;7;;;
8081;8081;;;"<pre><code>df.groupby(level=0).filter(lambda x: len(x) &gt; 1)['type']
</code></pre>

<p>We added <code>filter</code> method for this kind of operation. You can also use masking and transform for equivalent results, but this is faster, and a little more readable too.</p>

<p><strong>Important:</strong></p>

<p>The <code>filter</code> method was introduced in version 0.12, but it failed to work on DataFrames/Series with nonunique indexes. The issue -- and a related issue with <code>transform</code> on Series -- was fixed for version 0.13, which should be released any day now.</p>

<p>Clearly, nonunique indexes are the heart of this question, so I should point out that this approach will not help until you have pandas 0.13. In the meantime, the <code>transform</code> workaround is the way to go. Be ware that if you try that on a <em>Series</em> with a nonunique index, it too will fail.</p>

<p>There is no good reason why <code>filter</code> and <code>transform</code> should not be applied to nonunique indexes; it was just poorly implemented at first.</p>
";;6;;2013-11-25T18:36:14.827;;20200594;2013-11-25T19:58:22.263;2013-11-25T19:58:22.263;;1221924.0;;1221924.0;20199129.0;2;12;;;
8090;8090;;;"<p>Hopefully someone will provide a better answer, but in case no one does, this will definitely work, so</p>

<p>Zeroth, I'm assuming you don't want to just end up sorted on <code>loan</code>, but to preserve <em>whatever</em> original order was in <code>x</code>, which may or may not have anything to do with the order of the <code>loan</code> column. (Otherwise, the problem is easier, and less interesting.)</p>

<p>First, you're asking it to sort based on the join keys. As <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#database-style-dataframe-joining-merging"" rel=""noreferrer"">the docs</a> explain, that's the default when you don't pass a <code>sort</code> argument.</p>

<hr>

<p>Second, if you <em>don't</em> sort based on the join keys, the rows will end up grouped together, such that two rows that merged from the same source row end up next to each other, which means you're still going to get <code>a</code>, <code>c</code>, <code>b</code>.</p>

<p>You can work around this by getting the rows grouped together in the order they appear in the original <code>x</code> by just merging again with <code>x</code> (on either side, it doesn't really matter), or by reindexing based on <code>x</code> if you prefer. Like this:</p>

<pre><code>x.merge(x.merge(y, how='left', on='state', sort=False))
</code></pre>

<hr>

<p>Alternatively, you can cram an x-index in there with <code>reset_index</code>, then just sort on that, like this:</p>

<pre><code>x.reset_index().merge(y, how='left', on='state', sort=False).sort('index')
</code></pre>

<hr>

<p>Either way obviously seems a bit wasteful, and clumsy so, as I said, hopefully there's a better answer that I'm just not seeing at the moment. But if not, that works.</p>
";;4;;2013-11-26T01:18:23.747;;20206825;2013-11-26T01:18:23.747;;;;;908494.0;20206615.0;2;13;;;
8108;8108;;;"<p>Pandas docs says it uses openpyxl for xlsx files. Quick look through the code in <code>ExcelWriter</code> gives a clue that something like this might work out:</p>

<pre><code>import pandas
from openpyxl import load_workbook

book = load_workbook('Masterfile.xlsx')
writer = pandas.ExcelWriter('Masterfile.xlsx', engine='openpyxl') 
writer.book = book
writer.sheets = dict((ws.title, ws) for ws in book.worksheets)

data_filtered.to_excel(writer, ""Main"", cols=['Diff1', 'Diff2'])

writer.save()
</code></pre>
";;8;;2013-11-26T15:45:59.103;;20221655;2016-10-13T15:40:07.867;2016-10-13T15:40:07.867;;3662204.0;;296069.0;20219254.0;2;61;;;
8117;8117;;;"<p>This approach, <code>df1 != df2</code>, works only for  dataframes  with identical rows and columns. In fact, all dataframes axes are compared with <code>_indexed_same</code> method, and exception is raised if differences found, even in columns/indices order.</p>

<p>If I got you right, you want not to find changes, but symmetric difference. For that, one approach might be concatenate dataframes:</p>

<pre><code>&gt;&gt;&gt; df = pd.concat([df1, df2])
&gt;&gt;&gt; df = df.reset_index(drop=True)
</code></pre>

<p>group by </p>

<pre><code>&gt;&gt;&gt; df_gpby = df.groupby(list(df.columns))
</code></pre>

<p>get index of unique records</p>

<pre><code>&gt;&gt;&gt; idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]
</code></pre>

<p>filter</p>

<pre><code>&gt;&gt;&gt; df.reindex(idx)
         Date   Fruit   Num   Color
9  2013-11-25  Orange   8.6  Orange
8  2013-11-25   Apple  22.1     Red
</code></pre>
";;4;;2013-11-26T21:14:50.397;;20228113;2013-11-26T21:14:50.397;;;;;1265154.0;20225110.0;2;27;;;
8125;8125;;;"<p>use <code>drop</code> method:</p>

<pre><code>df.drop(column_name, axis=1)
</code></pre>
";;3;;2013-11-27T00:39:03.563;;20230859;2013-11-27T00:39:03.563;;;;;772649.0;20230326.0;2;46;;;
8126;8126;;;"<p>you can just select the columns you want without deleting or dropping:</p>

<pre><code>collist = ['col1', 'col2', 'col3']
df1 = df[collist]
</code></pre>

<p>Just pass a list of the columns you desire</p>

<p>You can also retrieve the list of columns and then select from that list</p>

<pre><code>collist = df.columns.tolist()
# you can now select from this list any arbritrary range
df1 = df[collist[0:1]]
# or remove a column
collist.remove('col2')
# now select
df1 = df[collist]
# df1 will now only have 'col1' and 'col3'
</code></pre>
";;4;;2013-11-27T02:06:17.420;;20231632;2014-01-16T10:29:48.203;2014-01-16T10:29:48.203;;704848.0;;704848.0;20230326.0;2;11;;;
8132;8132;;;"<p>You want <code>df.loc[df.index &lt; '2013-10-16 08:00:00']</code> since you're selecting by label (index) and not by value.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label"">selecting by label</a></p>
";;3;;2013-11-27T04:01:40.383;;20233649;2013-11-27T04:01:40.383;;;;;2089197.0;20233071.0;2;30;;;
8136;8136;;;"<pre><code>&gt;&gt;&gt; s = pd.Series([1,2,3,4,np.NaN,5,np.NaN])
&gt;&gt;&gt; s[~s.isnull()]
0    1
1    2
2    3
3    4
5    5
</code></pre>

<p><strong>update</strong> or even better approach as @DSM suggested in comments, using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dropna.html"" rel=""noreferrer""><code>pandas.Series.dropna()</code></a>:</p>

<pre><code>&gt;&gt;&gt; s.dropna()
0    1
1    2
2    3
3    4
5    5
</code></pre>
";;1;;2013-11-27T06:29:56.860;;20235451;2017-03-18T07:26:59.520;2017-03-18T07:26:59.520;;736937.0;;1744834.0;20235401.0;2;62;;;
8151;8151;;;"<p>There is a bit of ambiguity in your question. There are at least <strike>three</strike> two interpretations:</p>

<ol>
<li>the keys in <code>di</code> refer to index values</li>
<li>the keys in <code>di</code> refer to <code>df['col1']</code> values</li>
<li>the keys in <code>di</code> refer to index locations (not the OP's question, but thrown in for fun.)</li>
</ol>

<p>Below is a solution for each case.</p>

<hr>

<p><strong>Case 1:</strong>
If the keys of <code>di</code> are meant to refer to index values, then you could use the <code>update</code> method:</p>

<pre><code>df['col1'].update(pd.Series(di))
</code></pre>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
#   col1 col2
# 1    w    a
# 2   10   30
# 0   20  NaN

di = {0: ""A"", 2: ""B""}

# The value at the 0-index is mapped to 'A', the value at the 2-index is mapped to 'B'
df['col1'].update(pd.Series(di))
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    w    a
2    B   30
0    A  NaN
</code></pre>

<p>I've modified the values from your original post so it is clearer what <code>update</code> is doing.
Note how the keys in <code>di</code> are associated with index values. The order of the index values -- that is, the index <em>locations</em> -- does not matter.</p>

<hr>

<p><strong>Case 2:</strong>
If the keys in <code>di</code> refer to <code>df['col1']</code> values, then @DanAllan and @DSM show how to achieve this with <code>replace</code>:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
print(df)
#   col1 col2
# 1    w    a
# 2   10   30
# 0   20  NaN

di = {10: ""A"", 20: ""B""}

# The values 10 and 20 are replaced by 'A' and 'B'
df['col1'].replace(di, inplace=True)
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    w    a
2    A   30
0    B  NaN
</code></pre>

<p>Note how in this case the keys in <code>di</code> were changed to match <em>values</em> in <code>df['col1']</code>.</p>

<hr>

<p><strong>Case 3:</strong>
If the keys in <code>di</code> refer to index locations, then you could use</p>

<pre><code>df['col1'].put(di.keys(), di.values())
</code></pre>

<p>since</p>

<pre><code>df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
di = {0: ""A"", 2: ""B""}

# The values at the 0 and 2 index locations are replaced by 'A' and 'B'
df['col1'].put(di.keys(), di.values())
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    A    a
2   10   30
0    B  NaN
</code></pre>

<p>Here, the first and third rows were altered, because the keys in <code>di</code> are <code>0</code> and <code>2</code>, which with Python's 0-based indexing refer to the first and third locations.</p>
";;5;;2013-11-27T19:04:34.810;;20250947;2013-11-27T21:53:23.660;2013-11-27T21:53:23.660;;190597.0;;190597.0;20250771.0;2;26;;;
8152;8152;;;"<p>You can use <code>.replace</code>.  For example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col2': {0: 'a', 1: 2, 2: np.nan}, 'col1': {0: 'w', 1: 1, 2: 2}})
&gt;&gt;&gt; di = {1: ""A"", 2: ""B""}
&gt;&gt;&gt; df
  col1 col2
0    w    a
1    1    2
2    2  NaN
&gt;&gt;&gt; df.replace({""col1"": di})
  col1 col2
0    w    a
1    A    2
2    B  NaN
</code></pre>

<p>or directly on the <code>Series</code>, i.e. <code>df[""col1""].replace(di, inplace=True)</code>.</p>
";;1;;2013-11-27T19:06:53.687;;20250996;2013-11-27T19:06:53.687;;;;;487339.0;20250771.0;2;85;;;
8197;8197;;;"<p>Like so:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})

len(df.columns)
3
</code></pre>
";;3;;2013-11-30T07:11:45.840;;20297639;2013-11-30T07:22:52.843;2013-11-30T07:22:52.843;;390388.0;;390388.0;20297332.0;2;86;;;
8202;8202;;;"<p>You can delete column on <code>i</code> index like this:</p>

<pre><code>df.drop(df.columns[i], axis=1)
</code></pre>

<p>It could work strange, if you have duplicate names in columns, so to do this you can rename column you want to delete column by new name. Or you can reassign DataFrame like this:</p>

<pre><code>df = df.iloc[:, [j for j, c in enumerate(df.columns) if j != i]]
</code></pre>
";;0;;2013-11-30T15:06:58.977;;20301769;2016-02-16T10:48:10.050;2016-02-16T10:48:10.050;;5934209.0;;1744834.0;20297317.0;2;45;;;
8206;8206;;;"<p>Alternative: </p>

<pre><code>df.shape[1]
</code></pre>

<p>(<code>df.shape[0]</code> is the number of rows)</p>
";;1;;2013-11-30T18:56:56.027;;20304311;2015-12-20T01:03:42.173;2015-12-20T01:03:42.173;;1435522.0;;2775630.0;20297332.0;2;32;;;
8215;8215;;;"<p>A one liner can be:</p>

<pre><code>x.set_index('name').index.get_duplicates()
</code></pre>

<p>the index contains a method for finding duplicates, columns does not seem to have a similar method.. </p>
";;2;;2013-12-01T13:48:49.013;;20312816;2013-12-01T13:48:49.013;;;;;1945306.0;15247628.0;2;9;;;
8235;8235;;;"<p><code>&amp;</code> has higher precedence than <code>==</code>.  Write:</p>

<pre><code>my_df.ix[(my_df.CHUNK_NAME==chunks[0])&amp;(my_df.LAMBDA==lam_beta[0][0])]
         ^                           ^ ^                            ^
</code></pre>
";;4;;2013-12-02T17:08:57.497;;20333894;2013-12-02T17:08:57.497;;;;;567292.0;20333435.0;2;42;;;
8237;8237;;;"<p>In the IPython notebook you could also use <code>%matplotlib inline</code> at the top of the notebook to automatically display the created plots in the output cells.</p>
";;0;;2013-12-02T18:06:50.563;;20334902;2016-03-06T10:55:14.647;2016-03-06T10:55:14.647;;751572.0;;751572.0;16522380.0;2;26;;;
8251;8251;;;"<p>You can iterate through the items:</p>

<pre><code>In [11]: pd.DataFrame(list(my_dict.iteritems()),
                      columns=['business_id','business_code'])
Out[11]: 
  business_id business_code
0         id2          val2
1         id3          val3
2         id1          val1
</code></pre>
";;1;;2013-12-03T01:19:14.360;;20341058;2013-12-03T01:19:14.360;;;;;1240268.0;20340844.0;2;20;;;
8288;8288;;;"<p>you can use the left_on and right_on options as follows:</p>

<pre><code>pd.merge(frame_1, frame_2, left_on = 'county_ID', right_on = 'countyid')
</code></pre>

<p>I was not sure from the question if you only wanted to merge if the key was in the left hand dataframe. If that is the case then the following will do that (the above will in effect do a many to many merge)</p>

<pre><code>pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid')
</code></pre>
";;2;;2013-12-04T12:41:06.430;;20375692;2013-12-04T12:41:06.430;;;;;2484720.0;20375561.0;2;47;;;
8297;8297;;;"<p>Granted that the behavior is inconsistent, but I think it's easy to imagine cases where this is convenient. Anyway, to get a DataFrame every time, just pass a list to <code>loc</code>. There are other ways, but in my opinion this is the cleanest.</p>

<pre><code>In [2]: type(df.loc[[3]])
Out[2]: pandas.core.frame.DataFrame

In [3]: type(df.loc[[1]])
Out[3]: pandas.core.frame.DataFrame
</code></pre>
";;5;;2013-12-04T19:36:17.370;;20384317;2013-12-04T19:36:17.370;;;;;1221924.0;20383647.0;2;27;;;
8338;8338;;;"<p>As mentioned, it could be worth looking into the <a href=""http://pandas.pydata.org/pandas-docs/stable/search.html?q=rolling"">rolling_ functions</a>, which will mean you won't have as many copies around.</p>

<p>One solution is to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html"">concat</a> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.shift.html"">shifted</a> Series together to make a DataFrame:</p>

<pre><code>In [11]: pd.concat([s, s.shift(), s.shift(2)], axis=1)
Out[11]: 
   0   1   2
1  5 NaN NaN
2  4   5 NaN
3  3   4   5
4  2   3   4
5  1   2   3

In [12]: pd.concat([s, s.shift(), s.shift(2)], axis=1).dropna()
Out[12]: 
   0  1  2
3  3  4  5
4  2  3  4
5  1  2  3
</code></pre>

<p><em>Doing work on this will be more efficient that on lists...</em></p>
";;2;;2013-12-05T21:07:31.610;;20410720;2013-12-05T21:07:31.610;;;;;1240268.0;20410312.0;2;23;;;
8402;8402;;;"<pre><code>data.reindex(index=data.index[::-1])
</code></pre>

<p>or simply:</p>

<pre><code>data.iloc[::-1]
</code></pre>

<p>will reverse your data frame, if you want to have a <code>for</code> loop which goes from down to up you may do:</p>

<pre><code>for idx in reversed(data.index):
    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])
</code></pre>

<p>or</p>

<pre><code>for idx in reversed(data.index):
    print(idx, data.Even[idx], data.Odd[idx])
</code></pre>

<p>You are getting an error because <code>reversed</code> first calls <code>data.__len__()</code> which returns 6. Then it tries to call <code>data[j - 1]</code> for <code>j</code> in <code>range(6, 0, -1)</code>, and the first call would be <code>data[5]</code>; but in pandas dataframe <code>data[5]</code> means column 5, and there is no column 5 so it will throw an exception. ( see <a href=""http://docs.python.org/2/library/functions.html#reversed"">docs</a> )</p>
";;3;;2013-12-07T17:24:42.637;;20444256;2014-10-21T23:06:55.173;2014-10-21T23:06:55.173;;625914.0;;625914.0;20444087.0;2;70;;;
8409;8409;;;"<p>Master has just been updated by this <a href=""https://github.com/pydata/pandas/pull/5661"">issue</a>.</p>

<p>This file be read simply by:</p>

<pre><code> result = pd.read_pickle('pickle_L1cor_s1.pic')
</code></pre>

<p>The objects that are pickled are pandas &lt;= 0.12 versioned. This need a custom unpickler, which
the 0.13/master (releasing shortly) handles. 0.13 saw a refactor of the Series inheritance hierarchy where Series is no longer a sub-class of <code>ndarray</code>, but now of <code>NDFrame</code>, the same base class of <code>DataFrame</code> and <code>Panel</code>. This was done for a great many reasons, mainly to promote code consistency. See <a href=""http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#whatsnew-0130-refactoring"">here</a> for a more complete description.</p>

<p>The error message you are seeing <code>`TypeError: _reconstruct: First argument must be a sub-type of ndarray</code> is that the python default unpickler makes sure that the class hierarchy that was pickled is exactly the same what it is recreating. Since Series has changed between versions this is no longer possible with the default unpickler, (this IMHO is a bug in the way pickle works). In any event, pandas will unpickle pre-0.13 pickles that have Series objects.</p>
";;7;;2013-12-08T15:25:44.920;;20455090;2013-12-08T15:25:44.920;;;;;644898.0;20444593.0;2;17;;;
8418;8418;;;"<p><code>df.values</code> is a numpy array, and accessing values that way is always faster than <code>np.array</code>.</p>

<pre><code>scipy.sparse.csr_matrix(df.values)
</code></pre>

<p>You might need to take the transpose first, like <code>df.values.T</code>. In DataFrames, the columns are axis 0.</p>
";;4;;2013-12-08T22:12:41.337;;20459839;2013-12-08T22:12:41.337;;;;;1221924.0;20459536.0;2;25;;;
8422;8422;;;"<p>either:</p>

<pre><code>df['index1'] = df.index
</code></pre>

<p>or, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index""><code>.reset_index</code></a>:</p>

<pre><code>df.reset_index(level=0, inplace=True)
</code></pre>

<hr>

<p>so, if you have a multi-index frame with 3 levels of index, like:</p>

<pre><code>&gt;&gt;&gt; df
                       val
tick       tag obs        
2016-02-26 C   2    0.0139
2016-02-27 A   2    0.5577
2016-02-28 C   6    0.0303
</code></pre>

<p>and you want to convert the 1st (<code>tick</code>) and 3rd (<code>obs</code>) levels in the index into columns, you would do:</p>

<pre><code>&gt;&gt;&gt; df.reset_index(level=['tick', 'obs'])
          tick  obs     val
tag                        
C   2016-02-26    2  0.0139
A   2016-02-27    2  0.5577
C   2016-02-28    6  0.0303
</code></pre>
";;2;;2013-12-09T00:39:17.983;;20461206;2016-03-06T12:21:48.660;2016-03-06T12:21:48.660;;625914.0;;625914.0;20461165.0;2;231;;;
8423;8423;;;"<p>I ran across the same problem; it turned out that my data was an array of <code>np.float32</code> and the reduced float precision caused the distance matrix to be asymmetric.  I fixed the issue by converting my data to <code>np.float64</code> before running MDS on it.</p>

<p>Here's an example that uses random data to illustrate the issue:</p>

<pre><code>import numpy as np
from sklearn.manifold import MDS
from sklearn.metrics import euclidean_distances
from sklearn.datasets import make_classification

data, labels = make_classification()
mds = MDS(n_components=2)

similarities = euclidean_distances(data.astype(np.float64))
print np.abs(similarities - similarities.T).max()
# Prints 1.7763568394e-15
mds.fit(data.astype(np.float64))
# Succeeds

similarities = euclidean_distances(data.astype(np.float32))
print np.abs(similarities - similarities.T).max()
# Prints 9.53674e-07
mds.fit(data.astype(np.float32))
# Fails with ""ValueError: similarities must be symmetric""
</code></pre>
";;1;;2013-12-09T02:10:27.013;;20461929;2013-12-09T02:10:27.013;;;;;590203.0;16990996.0;2;7;;;
8454;8454;;;"<p>Make it a DatetimeIndex first:</p>

<pre><code>pd.DatetimeIndex(montdist['date']) + pd.DateOffset(1)
</code></pre>

<p><em>Note: I think there is a feature request that this could work with date columns...</em></p>

<p>In action:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])

In [12]: df['date'] = pd.to_datetime(['21-11-2013', '22-11-2013'])

In [13]: pd.DatetimeIndex(df.date) + pd.DateOffset(1)
Out[13]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-11-22 00:00:00, 2013-11-23 00:00:00]
Length: 2, Freq: None, Timezone: None

In [14]: pd.DatetimeIndex(df.date) + pd.offsets.Hour(1)
Out[14]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-11-21 01:00:00, 2013-11-22 01:00:00]
Length: 2, Freq: None, Timezone: Non
</code></pre>
";;3;;2013-12-09T21:22:39.883;;20481080;2013-12-09T21:22:39.883;;;;;1240268.0;20480897.0;2;16;;;
8467;8467;;;"<p><code>reset_index()</code> is what you're looking for. if you don't want it saved as a column, then</p>

<pre><code>df = df.reset_index(drop=True)
</code></pre>
";;2;;2013-12-10T10:19:52.223;;20491748;2013-12-10T10:19:52.223;;;;;2775630.0;20490274.0;2;231;;;
8471;8471;;;"<p>Try to use timedelta():</p>

<pre><code>mondist['shifted_date']=mondist.date + datetime.timedelta(days=1)
</code></pre>
";;2;;2013-12-10T14:03:59.223;;20496583;2013-12-10T14:03:59.223;;;;;2134595.0;20480897.0;2;7;;;
8527;8527;;;"<p>I do not know what you mean by heat map for a time series, but for a dataframe you may do as below:</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from itertools import product
from string import ascii_uppercase
from matplotlib import patheffects

m, n = 4, 7 # 4 rows, 7 columns
df = pd.DataFrame(np.random.randn(m, n),
                  columns=list(ascii_uppercase[:n]),
                  index=list(ascii_uppercase[-m:]))


ax = plt.imshow(df, interpolation='nearest', cmap='Oranges').axes

_ = ax.set_xticks(np.linspace(0, n-1, n))
_ = ax.set_xticklabels(df.columns)
_ = ax.set_yticks(np.linspace(0, m-1, m))
_ = ax.set_yticklabels(df.index)

ax.grid('off')
ax.xaxis.tick_top()
</code></pre>

<p>optionally, to print actual values in the middle of each square, with some shadows for readability, you may do:</p>

<pre><code>path_effects = [patheffects.withSimplePatchShadow(shadow_rgbFace=(1,1,1))]

for i, j in product(range(m), range(n)):
    _ = ax.text(j, i, '{0:.2f}'.format(df.iloc[i, j]),
                size='medium', ha='center', va='center',
                path_effects=path_effects)
</code></pre>

<p><img src=""https://i.stack.imgur.com/YyLfi.png"" alt=""heat-map""></p>
";;5;;2013-12-11T15:34:47.603;;20523271;2015-11-26T14:17:44.933;2015-11-26T14:17:44.933;;625914.0;;625914.0;20520246.0;2;20;;;
8581;8581;;;"<p>It looks like gcc being killed due to insufficient memory (see <a href=""https://stackoverflow.com/questions/20555761/pip-install-pandas-gives-unicodedecodeerror-ascii-codec-cant-decode-byte-0#comment30742213_20555761"">@Blender's comment)</a> exposed a bug in pip. It mixes bytestrings and Unicode while logging that leads to:</p>

<pre><code>&gt;&gt;&gt; '\n'.join(['bytestring with non-ascii character ?', u'unicode'])
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 36: \
   ordinal not in range(128)
</code></pre>

<p>If it is reproducible with the latest <code>pip</code> version; you could <a href=""https://github.com/pypa/pip/issues?state=open"" rel=""nofollow noreferrer"">report the bug</a>.</p>
";;2;;2013-12-13T00:44:49.447;;20557179;2013-12-13T00:44:49.447;2017-05-23T12:30:20.733;;-1.0;;4279.0;20555761.0;2;10;;;
8589;8589;;;"<p>@BrenBarn answer above yields a list of tuples not a list of list as asked in question.  I specifically needed a list of lists to be able to write the dataframe into spreadsheed using DataNitro.   Adapted the above example with list comprehension:</p>

<pre><code>[list(x) for x in dt.T.itertuples()]
</code></pre>

<p>This yields the result as needed</p>
";;0;;2013-12-13T12:21:02.230;;20566408;2014-09-12T11:58:47.513;2014-09-12T11:58:47.513;;2107677.0;;2107677.0;11811392.0;2;9;;;
8604;8604;;;"<p>It's a simple linear algebra, you multiply matrix with its transpose (your example contains strings, don't forget to convert them to integer):</p>

<pre><code>&gt;&gt;&gt; df_asint = df.astype(int)
&gt;&gt;&gt; coocc = df_asint.T.dot(df_asint)
&gt;&gt;&gt; coocc
       Dop  Snack  Trans
Dop      4      2      3
Snack    2      3      2
Trans    3      2      4
</code></pre>

<p>if, as in R answer, you want to reset diagonal, you can use numpy's <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html""><code>fill_diagonal</code></a>:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.fill_diagonal(coocc.values, 0)
&gt;&gt;&gt; coocc
       Dop  Snack  Trans
Dop      0      2      3
Snack    2      0      2
Trans    3      2      0
</code></pre>
";;5;;2013-12-13T19:28:57.220;;20574460;2013-12-13T20:05:24.367;2013-12-13T20:05:24.367;;1265154.0;;1265154.0;20574257.0;2;25;;;
8640;8640;;;"<p>It seems in general you're just looking for a join:</p>

<pre><code>&gt; dat1 = pd.DataFrame({'dat1': [9,5]})
&gt; dat2 = pd.DataFrame({'dat2': [7,6]})
&gt; dat1.join(dat2)
   dat1  dat2
0     9     7
1     5     6
</code></pre>
";;5;;2013-12-16T03:33:50.893;;20603020;2013-12-16T03:33:50.893;;;;;507762.0;20602947.0;2;22;;;
8653;8653;;;"<p>Check <code>pandas.__version__</code>:</p>

<pre><code>In [76]: import pandas as pd

In [77]: pd.__version__
Out[77]: '0.12.0-933-g281dc4e'
</code></pre>

<p>Pandas also provides a utility function, <code>pd.show_versions()</code>, which reports the version of its dependencies as well:</p>

<pre><code>In [53]: pd.show_versions(as_json=False)

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-45-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.15.2-113-g5531341
nose: 1.3.1
Cython: 0.21.1
numpy: 1.8.2
scipy: 0.14.0.dev-371b4ff
statsmodels: 0.6.0.dev-a738b4f
IPython: 2.0.0-dev
sphinx: 1.2.2
patsy: 0.3.0
dateutil: 1.5
pytz: 2012c
bottleneck: None
tables: 3.1.1
numexpr: 2.2.2
matplotlib: 1.4.2
openpyxl: None
xlrd: 0.9.3
xlwt: 0.7.5
xlsxwriter: None
lxml: 3.3.3
bs4: 4.3.2
html5lib: 0.999
httplib2: 0.8
apiclient: None
rpy2: 2.5.5
sqlalchemy: 0.9.8
pymysql: None
psycopg2: 2.4.5 (dt dec mx pq3 ext)
</code></pre>
";;0;;2013-12-16T13:57:54.470;;20612691;2015-02-07T13:17:48.890;2015-02-07T13:17:48.890;;190597.0;;190597.0;20612645.0;2;137;;;
8680;8680;;;"<p>You could use <code>pd.factorize</code>:</p>

<pre><code>import pandas as pd

x = pd.DataFrame({'cat':('A','A','B'), 'val':(10,20,30)})
labels, levels = pd.factorize(x['cat'])
x['cat'] = labels
x = x.set_index('cat')
print(x)
</code></pre>

<p>yields</p>

<pre><code>     val
cat     
0     10
0     20
1     30
</code></pre>

<p>You could add 1 to <code>labels</code> if you wish to replicate Stata's behaviour:</p>

<pre><code>x['cat'] = labels+1
</code></pre>
";;8;;2013-12-16T20:10:37.107;;20619971;2013-12-16T20:16:41.517;2013-12-16T20:16:41.517;;190597.0;;190597.0;20619851.0;2;16;;;
8686;8686;;;"<p>From what I gather, <code>SettingWithCopyWarning</code> was created to flag potentially confusing ""chained"" assignments, such as the following, which don't always work as expected, particularly when the first selection returns a <em>copy</em>.  [see <a href=""https://github.com/pydata/pandas/pull/5390"" rel=""noreferrer"">GH5390</a> and <a href=""https://github.com/pydata/pandas/issues/5597"" rel=""noreferrer"">GH5597</a> for background discussion.]</p>

<pre><code>df[df['A'] &gt; 2]['B'] = new_val  # new_val not set in df
</code></pre>

<p>The warning offers a suggestion to rewrite as follows:</p>

<pre><code>df.loc[df['A'] &gt; 2, 'B'] = new_val
</code></pre>

<p>However, this doesn't fit your usage, which is equivalent to:</p>

<pre><code>df = df[df['A'] &gt; 2]
df['B'] = new_val
</code></pre>

<p>While it's clear that you don't care about writes making it back to the original frame (since you overwrote the reference to it), unfortunately this pattern can not be differentiated from the first chained assignment example, hence the (false positive) warning.  The potential for false positives is addressed in the <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">docs on indexing</a>, if you'd like to read further.  You can safely disable this new warning with the following assignment.</p>

<pre><code>pd.options.mode.chained_assignment = None  # default='warn'
</code></pre>
";;8;;2013-12-17T06:20:23.940;;20627316;2013-12-17T06:20:23.940;;;;;243434.0;20625582.0;2;232;;;
8702;8702;;;"<p>You can try yourself:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from StringIO import StringIO
&gt;&gt;&gt; s = """"""1, 2
... 3, 4
... 5, 6""""""
&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=[1], header=None)
   0  1
0  1  2
1  5  6
&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=1, header=None)
   0  1
0  3  4
1  5  6
</code></pre>
";;2;;2013-12-17T15:04:27.797;;20637559;2013-12-17T15:04:27.797;;;;;1265154.0;20637439.0;2;37;;;
8706;8706;;;"<p>Supposing <code>d</code> is your list of dicts, simply:</p>

<pre><code>pd.DataFrame(d)
</code></pre>
";;9;;2013-12-17T15:35:13.867;;20638258;2013-12-17T15:35:13.867;;;;;653364.0;20638006.0;2;319;;;
8707;8707;;;"<p>Given your import, it appears it is:</p>

<pre><code>com.convert_robj(rdf)
</code></pre>

<p>For example,</p>

<pre><code>In [480]: dfrm
Out[480]:
           A          B  C
0   0.454459  49.916767  1
1   0.943284  50.878174  1
2   0.974856  50.335679  2
3   0.776600  50.782104  1
4   0.553895  50.084505  1
5   0.514018  50.719019  2
6   0.915413  50.513962  0
7   0.771571  49.859855  2
8   0.068619  49.409657  0
9   0.728141  50.945174  2
10  0.388115  47.879653  1
11  0.960172  49.680258  0
12  0.015216  50.067968  0
13  0.495024  50.286287  1
14  0.565954  49.909771  1
15  0.992279  49.009696  1
16  0.179934  49.554256  0
17  0.521243  47.854791  0
18  0.551241  51.076262  1
19  0.713271  49.418503  0
20  0.801716  50.660304  1

In [481]: rdfrm = com.convert_to_r_dataframe(dfrm)

In [482]: rdfrm
Out[482]:
&lt;DataFrame - Python:0x14905cf8 / R:0x1600ee98&gt;
[FloatVector, FloatVector, IntVector]
  A: &lt;class 'rpy2.robjects.vectors.FloatVector'&gt;
  &lt;FloatVector - Python:0xf9d0b00 / R:0x140e2620&gt;
[0.454459, 0.943284, 0.974856, ..., 0.551241, 0.713271, 0.801716]
  B: &lt;class 'rpy2.robjects.vectors.FloatVector'&gt;
  &lt;FloatVector - Python:0xf9d0878 / R:0x125aa240&gt;
[49.916767, 50.878174, 50.335679, ..., 51.076262, 49.418503, 50.660304]
  C: &lt;class 'rpy2.robjects.vectors.IntVector'&gt;
  &lt;IntVector - Python:0x11fceef0 / R:0x13f0d918&gt;
[       1,        1,        2, ...,        1,        0,        1]

In [483]: com.convert_robj(rdfrm)
Out[483]:
           A          B  C
0   0.454459  49.916767  1
1   0.943284  50.878174  1
2   0.974856  50.335679  2
3   0.776600  50.782104  1
4   0.553895  50.084505  1
5   0.514018  50.719019  2
6   0.915413  50.513962  0
7   0.771571  49.859855  2
8   0.068619  49.409657  0
9   0.728141  50.945174  2
10  0.388115  47.879653  1
11  0.960172  49.680258  0
12  0.015216  50.067968  0
13  0.495024  50.286287  1
14  0.565954  49.909771  1
15  0.992279  49.009696  1
16  0.179934  49.554256  0
17  0.521243  47.854791  0
18  0.551241  51.076262  1
19  0.713271  49.418503  0
20  0.801716  50.660304  1
</code></pre>

<p>With docs:</p>

<pre><code>In [475]: com.convert_robj?
Type:       function
String Form:&lt;function convert_robj at 0x13e85848&gt;
File:       /mnt/epd/7.3-2_pandas0.12/lib/python2.7/site-packages/pandas/rpy/common.py
Definition: com.convert_robj(obj, use_pandas=True)
Docstring:
Convert rpy2 object to a pandas-friendly form

Parameters
----------
obj : rpy2 object

Returns
-------
Non-rpy data structure, mix of NumPy and pandas objects
</code></pre>
";;0;;2013-12-17T16:16:56.230;;20639234;2013-12-17T16:16:56.230;;;;;567620.0;20630121.0;2;7;;;
8723;8723;;;"<p>In general the point of the <code>SettingWithCopyWarning</code> is to show users (and esp new users) that they <em>may</em> be operating on a copy and not the original as they think. There <em>are</em> False positives (IOW you know what you are doing, so it <em>ok</em>). One possibility is simply to turn off the (by default <em>warn</em>) warning as @Garrett suggest.</p>

<p>Here is a nother, per option.</p>

<pre><code>In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))

In [2]: dfa = df.ix[:,[1,0]]

In [3]: dfa.is_copy
Out[3]: True

In [4]: dfa['A'] /= 2
/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  #!/usr/local/bin/python
</code></pre>

<p>You can set the <code>is_copy</code> flag to <code>False</code>, which will effectively turn off the check, *for that object``</p>

<pre><code>In [5]: dfa.is_copy = False

In [6]: dfa['A'] /= 2
</code></pre>

<p>If you explicity copy then you <em>know what you are doing</em>, so no further warning will happen.</p>

<pre><code>In [7]: dfa = df.ix[:,[1,0]].copy()

In [8]: dfa['A'] /= 2
</code></pre>

<p>The code the OP is showing above, while legitimate, and probably something I do as well, is technically a case for this warning, and not a False positive. Another way to <em>not</em> have the warning would be to do the selection operation via <code>reindex</code>, e.g.</p>

<pre><code>quote_df = quote_df(columns=['STK',.......])
</code></pre>
";;12;;2013-12-17T20:49:10.263;;20644369;2014-02-19T22:47:07.783;2014-02-19T22:47:07.783;;478288.0;;644898.0;20625582.0;2;68;;;
8725;8725;;;"<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[1,2],[3,4]], columns=list('ab'))
&gt;&gt;&gt; df
   a  b
0  1  2
1  3  4
&gt;&gt;&gt; df['c'] = df['b']**2
&gt;&gt;&gt; df
   a  b   c
0  1  2   4
1  3  4  16
</code></pre>
";;0;;2013-12-17T21:01:26.933;;20644575;2013-12-17T21:01:26.933;;;;;1265154.0;20644536.0;2;17;;;
8739;8739;;;"<p>wouldn't be just easier to do what yourself describe, namely</p>

<pre><code>df.sort(['ticker', 'date'], inplace=True)
df['diffs'] = df['value'].diff()
</code></pre>

<p>and then correct for borders:</p>

<pre><code>mask = df.ticker != df.ticker.shift(1)
df['diffs'][mask] = np.nan
</code></pre>

<p>to maintain the original index you may do <code>idx = df.index</code> in the beginning, and then at the end you can do <code>df.reindex(idx)</code>, or if it is a huge dataframe, perform the operations on </p>

<pre><code>df.filter(['ticker', 'date', 'value'])
</code></pre>

<p>and then <code>join</code> the two dataframes at the end.</p>

<p><strong>edit</strong>: alternatively, ( though still not using <code>groupby</code> )</p>

<pre><code>df.set_index(['ticker','date'], inplace=True)
df.sort_index(inplace=True)
df['diffs'] = np.nan 

for idx in df.index.levels[0]:
    df.diffs[idx] = df.value[idx].diff()
</code></pre>

<p>for</p>

<pre><code>   date ticker  value
0    63      C   1.65
1    88      C  -1.93
2    22      C  -1.29
3    76      A  -0.79
4    72      B  -1.24
5    34      A  -0.23
6    92      B   2.43
7    22      A   0.55
8    32      A  -2.50
9    59      B  -1.01
</code></pre>

<p>this will produce:</p>

<pre><code>             value  diffs
ticker date              
A      22     0.55    NaN
       32    -2.50  -3.05
       34    -0.23   2.27
       76    -0.79  -0.56
B      59    -1.01    NaN
       72    -1.24  -0.23
       92     2.43   3.67
C      22    -1.29    NaN
       63     1.65   2.94
       88    -1.93  -3.58
</code></pre>
";;7;;2013-12-18T02:08:49.843;;20648462;2015-02-06T01:30:05.163;2015-02-06T01:30:05.163;;625914.0;;625914.0;20648346.0;2;17;;;
8754;8754;;;"<p>This error occurs among other things when you have NaN values in the Series. Could that be the case?</p>

<p>These NaN's are not handled well by the <code>hist</code> function of matplotlib. For example:</p>

<pre><code>s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])
fig, ax = plt.subplots()
ax.hist(s, alpha=0.9, color='blue')
</code></pre>

<p>produces the same error <code>AttributeError: max must be larger than min in range parameter.</code> One option is eg to remove the NaN's before plotting. This will work:</p>

<pre><code>ax.hist(s.dropna(), alpha=0.9, color='blue')
</code></pre>

<p>Another option is to use pandas <code>hist</code> method on your series and providing the <code>axes[0]</code> to the <code>ax</code> keyword:</p>

<pre><code>dfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')
</code></pre>
";;1;;2013-12-18T11:58:14.123;;20657592;2013-12-18T12:05:00.910;2013-12-18T12:05:00.910;;653364.0;;653364.0;20656663.0;2;66;;;
8768;8768;;;"<p>Seems your roundtripping IS causing some unicode. Not sure why that is, but easy to fix.
You cannot store unicode in a HDFStore Table in python 2, (this works correctly in python 3 however). You could do it as a Fixed format if you want though (it would be pickled). See <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#fixed-format"" rel=""noreferrer"">here</a>.</p>

<pre><code>In [33]: df = pd.read_json(s)

In [25]: df
Out[25]: 
  args                date            host kwargs     operation  status   thingy      time
0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf    -101  a13yy38  0.000801
1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}     x_initobj       1  a19kyy1  0.003244
2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}  x_gobjParams    -101  a14yy10  0.002247
3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull    -101  a14yy24  0.002787
4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf    -101  a14yy24  0.001067
5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}       gxyzinf    -101  a12yy34  0.002652
6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}     deletemfg       1  a15yy15  0.004371
7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}       gxyzinf    -101  a15yy15  0.000602

[8 rows x 8 columns]

In [26]: df.dtypes
Out[26]: 
args                 object
date         datetime64[ns]
host                 object
kwargs               object
operation            object
status                int64
thingy               object
time                float64
dtype: object
</code></pre>

<p>This is inferring the actual type of the <code>object</code> dtyped Series. They will come out as unicode only if at least 1 string is unicode (otherwise they would be inferred as string)</p>

<pre><code>In [27]: df.apply(lambda x: pd.lib.infer_dtype(x.values))
Out[27]: 
args            unicode
date         datetime64
host            unicode
kwargs          unicode
operation       unicode
status          integer
thingy          unicode
time           floating
dtype: object
</code></pre>

<p>Here's how to 'fix' it</p>

<pre><code>In [28]: types = df.apply(lambda x: pd.lib.infer_dtype(x.values))

In [29]: types[types=='unicode']
Out[29]: 
args         unicode
host         unicode
kwargs       unicode
operation    unicode
thingy       unicode
dtype: object

In [30]: for col in types[types=='unicode'].index:
   ....:     df[col] = df[col].astype(str)
   ....:     
</code></pre>

<p>Looks the same</p>

<pre><code>In [31]: df
Out[31]: 
  args                date            host kwargs     operation  status   thingy      time
0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf    -101  a13yy38  0.000801
1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}     x_initobj       1  a19kyy1  0.003244
2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}  x_gobjParams    -101  a14yy10  0.002247
3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull    -101  a14yy24  0.002787
4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf    -101  a14yy24  0.001067
5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}       gxyzinf    -101  a12yy34  0.002652
6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}     deletemfg       1  a15yy15  0.004371
7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}       gxyzinf    -101  a15yy15  0.000602

[8 rows x 8 columns]
</code></pre>

<p>But now infers correctly.</p>

<pre><code>In [32]: df.apply(lambda x: pd.lib.infer_dtype(x.values))
Out[32]: 
args             string
date         datetime64
host             string
kwargs           string
operation        string
status          integer
thingy           string
time           floating
dtype: object
</code></pre>
";;10;;2013-12-18T23:48:52.147;;20670901;2013-12-18T23:48:52.147;;;;;644898.0;20670370.0;2;22;;;
8769;8769;;;"<p>Nice easy to reproduce example!! more questions should be like this!</p>

<p>Just pass a lambda to transform (this is tantamount to passing afuncton object, e.g. np.diff (or Series.diff) directly. So this equivalent to data1/data2</p>

<pre><code>In [32]: data3['diffs'] = data3.groupby('ticker')['value'].transform(Series.diff)

In [34]: data3.sort_index(inplace=True)

In [25]: data3
Out[25]: 
         date    ticker     value     diffs
0  2013-10-03  ticker_2  0.435995  0.015627
1  2013-10-04  ticker_2  0.025926 -0.410069
2  2013-10-02  ticker_1  0.549662       NaN
3  2013-10-01  ticker_0  0.435322       NaN
4  2013-10-02  ticker_2  0.420368  0.120713
5  2013-10-03  ticker_0  0.330335 -0.288936
6  2013-10-04  ticker_1  0.204649 -0.345014
7  2013-10-02  ticker_0  0.619271  0.183949
8  2013-10-01  ticker_2  0.299655       NaN

[9 rows x 4 columns]
</code></pre>

<p>I believe that <code>np.diff</code> doesn't follow numpy's own unfunc guidelines to process array inputs (whereby it tries various methods to coerce input and send output, e.g. <code>__array__</code> on input <code>__array_wrap__</code> on output). I am not really sure why, see a bit more info <a href=""http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#internal-refactoring"">here</a>. So bottom line is that <code>np.diff</code> is not dealing with the index properly and doing its own calculation (which in this case is wrong).</p>

<p>Pandas has a lot of methods where they don't just call the numpy function, mainly because they handle different dtypes, handle nans, and in this case, handle 'special' diffs. e.g. you can pass a time frequency to a datelike-index where it calculates how many n to actually diff.</p>
";;15;;2013-12-19T00:00:36.287;;20671047;2013-12-19T00:08:54.960;2013-12-19T00:08:54.960;;644898.0;;644898.0;20670726.0;2;15;;;
8791;8791;;;"<p>@jeremiahbuddha mentioned that apply works on row/columns, while applymap works element-wise. But it seems you can still use apply for element-wise computation.... </p>

<pre><code>    frame.apply(np.sqrt)
    Out[102]: 
                   b         d         e
    Utah         NaN  1.435159       NaN
    Ohio    1.098164  0.510594  0.729748
    Texas        NaN  0.456436  0.697337
    Oregon  0.359079       NaN       NaN

    frame.applymap(np.sqrt)
    Out[103]: 
                   b         d         e
    Utah         NaN  1.435159       NaN
    Ohio    1.098164  0.510594  0.729748
    Texas        NaN  0.456436  0.697337
    Oregon  0.359079       NaN       NaN
</code></pre>
";;1;;2013-12-19T17:21:38.040;;20687887;2013-12-19T17:21:38.040;;;;;2921752.0;19798153.0;2;9;;;
8792;8792;;;"<p>The following method is about 30 times faster than <code>scipy.spatial.distance.pdist</code>. It works pretty quickly on large matrices (assuming you have enough RAM)</p>

<p>See below for a discussion of how to optimize for sparsity.</p>

<pre><code># base similarity matrix (all dot products)
# replace this with A.dot(A.T).toarray() for sparse representation
similarity = numpy.dot(A, A.T)


# squared magnitude of preference vectors (number of occurrences)
square_mag = numpy.diag(similarity)

# inverse squared magnitude
inv_square_mag = 1 / square_mag

# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)
inv_square_mag[numpy.isinf(inv_square_mag)] = 0

# inverse of the magnitude
inv_mag = numpy.sqrt(inv_square_mag)

# cosine similarity (elementwise multiply by inverse magnitudes)
cosine = similarity * inv_mag
cosine = cosine.T * inv_mag
</code></pre>

<p>If your problem is typical for large scale binary preference problems, you have a lot more entries in one dimension than the other. Also, the short dimension is the one whose entries you want to calculate similarities between. Let's call this dimension the 'item' dimension.</p>

<p>If this is the case, list your 'items' in rows and create <code>A</code> using <a href=""http://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""noreferrer""><code>scipy.sparse</code></a>.  Then replace the first line as indicated.</p>

<p>If your problem is atypical you'll need more modifications. Those should be pretty straightforward replacements of basic <code>numpy</code> operations with their <code>scipy.sparse</code> equivalents.</p>
";;0;;2013-12-19T17:26:53.960;;20687984;2016-08-11T14:14:13.393;2016-08-11T14:14:13.393;;74291.0;;74291.0;17627219.0;2;26;;;
8807;8807;;;"<p>I think the answers above are missing a simple approach that I've found very useful. </p>

<p>When I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)</p>

<p>Example: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the end</p>

<p>One of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)</p>

<p>The other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formats</p>

<p>This approach doesn't cover all scenarios, but is very useful in a lot of them</p>
";;1;;2013-12-19T19:46:48.987;;20690383;2013-12-23T15:21:27.433;2013-12-23T15:21:27.433;;1827356.0;;1827356.0;14262433.0;2;78;;;
8820;8820;;;"<p><a href=""https://bitbucket.org/djcbeach/monary/wiki/Home""><code>Monary</code></a> does exactly that, and it's <em>super fast</em>. (<a href=""http://djcinnovations.com/index.php/archives/164"">another link</a>)</p>

<p>See <a href=""http://alexgaudio.com/2012/07/07/monarymongopandas.html"">this cool post</a> which includes a quick tutorial and some timings.</p>
";;1;;2013-12-19T22:33:39.947;;20693013;2013-12-19T22:33:39.947;;;;;2096752.0;16249736.0;2;15;;;
8828;8828;;;"<p>try this:</p>

<pre><code>X = sm.add_constant(X)
sm.OLS(y,X)
</code></pre>

<p>as in the <a href=""http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html"" rel=""noreferrer"">documentations</a>:</p>

<blockquote>
  <p>An interecept is not included by default and should be added by the user</p>
</blockquote>

<p><a href=""http://statsmodels.sourceforge.net/devel/generated/statsmodels.tools.tools.add_constant.html"" rel=""noreferrer""><code>statsmodels.tools.tools.add_constant</code></a> </p>
";;3;;2013-12-20T10:30:29.287;;20701559;2013-12-20T10:35:32.307;2013-12-20T10:35:32.307;;625914.0;;625914.0;20701484.0;2;24;;;
8877;8877;;;"<p>Your own answer is correct and good. Slightly different way is to specify scale constants with <code>timedelta</code> expression. </p>

<p>For example, to scale to seconds:</p>

<pre><code>&gt;&gt;&gt; np.diff(index)/np.timedelta64(1, 's')
array([ 3.6139351 ,  3.39279693,  1.87199821])
</code></pre>

<p>To minutes:</p>

<pre><code>&gt;&gt;&gt; np.diff(index)/np.timedelta64(1, 'm')
array([ 0.06023225,  0.05654662,  0.03119997])
</code></pre>
";;1;;2013-12-23T08:46:22.330;;20739897;2013-12-23T08:46:22.330;;;;;1265154.0;20738357.0;2;16;;;
8892;8892;;;"<p>You need to specify <code>data</code>, <code>index</code> and <code>columns</code> to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame"" rel=""noreferrer""><code>DataFrame</code></a> constructor, as in:</p>

<pre><code>&gt;&gt;&gt; pd.DataFrame(data=data[1:,1:],    # values
...              index=data[1:,0],    # 1st column as index
...              columns=data[0,1:])  # 1st row as the column names
</code></pre>

<p><strong>edit</strong>: as in the @joris comment, you may need to change above to <code>np.int_(data[1:,1:])</code> to have correct data type.</p>
";;0;;2013-12-24T15:50:14.250;;20763459;2016-03-06T12:28:40.973;2016-03-06T12:28:40.973;;625914.0;;625914.0;20763012.0;2;88;;;
8946;8946;;;"<p>Converting data frames back and forth between <code>rpy2</code> and <code>pandas</code> will be in the rpy2 release 2.4.0, and should be largely automated (no need to convert explicitly, it will be done on the fly)</p>

<pre><code>from rpy2.robjects import pandas2ri
pandas2ri.activate()
</code></pre>

<p>If wishing to convert explicitly for any reason, the functions are <code>pandas2ri.py2ri()</code> and <code>pandas2ri.ri2py()</code> (they were <code>pandas2ri.pandas2ri()</code> and <code>pandas2ri.ri2pandas()</code>).</p>
";;0;;2013-12-27T22:06:03.150;;20808449;2015-05-07T02:40:57.753;2015-05-07T02:40:57.753;;294017.0;;294017.0;20630121.0;2;12;;;
8971;8971;;;"<p>For a <code>Series</code>, <code>.ix</code> is equivalent of <code>[]</code>, the <code>getitem</code> syntax. <code>.ix/.loc</code> support multi-axis indexing, which for a Series does not matter (only has 1 axis), and hence is there for compatibility.</p>

<p>e.g.</p>

<pre><code>DataFrame(...).ix[row_indexer,column_indexer]
Series(...).ix[row_indexer]
</code></pre>

<p><code>.ix</code> itself is an 'older' method that tries to figure out what you want when presented with label or positional (integer) indexing. This is why <code>.loc/.iloc</code> were introduced in 0.11 to provide indexing choice by the user.</p>
";;2;;2013-12-30T15:18:14.093;;20842283;2014-05-08T22:43:02.987;2014-05-08T22:43:02.987;;2744342.0;;644898.0;20838395.0;2;19;;;
9007;9007;;;"<p>A one liner does exist:</p>

<pre><code>In [27]: df=df.rename(columns = {'two':'new_name'})

In [28]: df
Out[28]: 
  one three  new_name
0    1     a         9
1    2     b         8
2    3     c         7
3    4     d         6
4    5     e         5
</code></pre>

<p>Following is the docstring for the <code>rename</code> method.</p>

<pre>
Definition: df.rename(self, index=None, columns=None, copy=True, inplace=False)
Docstring:
Alter index and / or columns using input function or
functions. Function / dict values must be unique (1-to-1). Labels not
contained in a dict / Series will be left as-is.

Parameters
----------
index : dict-like or function, optional
    Transformation to apply to index values
columns : dict-like or function, optional
    Transformation to apply to column values
copy : boolean, default True
    Also copy underlying data
inplace : boolean, default False
    Whether to return a new DataFrame. If True then value of copy is
    ignored.

See also
--------
Series.rename

Returns
-------
renamed : DataFrame (new object)
</pre>
";;0;;2014-01-01T12:06:46.947;;20868446;2014-01-01T12:06:46.947;;;;;743775.0;20868394.0;2;185;;;
9010;9010;;;"<p>Simply check the shape of your <code>Y</code> variable, it should be a one-dimensional object, and you are probably passing something with more (possibly trivial) dimensions. Reshape it to the form of list/1d array.</p>
";;0;;2014-01-01T13:49:18.937;;20869270;2014-01-01T13:49:18.937;;;;;2658050.0;20868664.0;2;10;;;
9011;9011;;;"<p>You can use <code>df.single_column.values</code> or <code>df['single_column'].values</code> to get the underlying numpy array of your series (which, in this case, should also have the correct 1D-shape as mentioned by lejlot).</p>
";;0;;2014-01-01T16:44:09.030;;20870801;2014-01-01T22:33:33.180;2014-01-01T22:33:33.180;;18601.0;;18601.0;20868664.0;2;8;;;
9092;9092;;;"<pre><code>import pandas as pd
pd.options.display.float_format = '${:,.2f}'.format
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
print(df)
</code></pre>

<p>yields</p>

<pre><code>        cost
foo  $123.46
bar  $234.57
baz  $345.68
quux $456.79
</code></pre>

<p>but this only works if you want <em>every</em> float to be formatted with a dollar sign.</p>

<p>Otherwise, if you want dollar formatting for some floats only, then I think you'll have to pre-modify the dataframe (converting those floats to strings):</p>

<pre><code>import pandas as pd
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
df['foo'] = df['cost']
df['cost'] = df['cost'].map('${:,.2f}'.format)
print(df)
</code></pre>

<p>yields</p>

<pre><code>         cost       foo
foo   $123.46  123.4567
bar   $234.57  234.5678
baz   $345.68  345.6789
quux  $456.79  456.7890
</code></pre>
";;1;;2014-01-05T18:44:21.490;;20937592;2014-01-05T19:19:36.123;2014-01-05T19:19:36.123;;190597.0;;190597.0;20937538.0;2;101;;;
9127;9127;;;"<pre><code>df['cum_sum'] = df.val1.cumsum()
df['cum_perc'] = 100*df.cum_sum/df.val1.sum()
</code></pre>

<p>This will add the columns to <code>df</code>.  If you want a copy, copy <code>df</code> first and then do these operations on the copy.</p>
";;4;;2014-01-07T06:15:34.113;;20965090;2014-01-07T06:15:34.113;;;;;1427416.0;20965046.0;2;44;;;
9131;9131;;;"<p>First two letters for each value in a column:</p>

<pre><code>&gt;&gt;&gt; df['StateInitial'] = df['state'].str[:2]
&gt;&gt;&gt; df
   pop       state  year StateInitial
0  1.5    Auckland  2000           Au
1  1.7       Otago  2001           Ot
2  3.6  Wellington  2002           We
3  2.4     Dunedin  2001           Du
4  2.9    Hamilton  2002           Ha
</code></pre>

<p>For last two that would be <code>df['state'].str[-2:]</code>. Don't know what exactly you want for middle, but you can apply arbitrary function to a column with <code>apply</code> method:</p>

<pre><code>&gt;&gt;&gt; df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])
0    kl
1    ta
2    in
3    ne
4    il
</code></pre>
";;8;;2014-01-07T11:22:28.893;;20970328;2014-01-07T11:27:50.217;2014-01-07T11:27:50.217;;1265154.0;;1265154.0;20970279.0;2;34;;;
9182;9182;;;"<p>You didn't mention the fancy indexing capabilities of dataframes, e.g.:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""class"":[1,1,1,2,2], ""value"":[1,2,3,4,5]})
&gt;&gt;&gt; df[df[""class""]==1].sum()
class    3
value    6
dtype: int64
&gt;&gt;&gt; df[df[""class""]==1].sum()[""value""]
6
&gt;&gt;&gt; df[df[""class""]==1].count()[""value""]
3
</code></pre>

<p>You could replace <code>df[""class""]==1</code>by another condition.</p>
";;2;;2014-01-08T12:12:11.140;;20995313;2016-08-07T00:29:35.467;2016-08-07T00:29:35.467;;2666859.0;;1156006.0;20995196.0;2;14;;;
9183;9183;;;"<p>You can first make a conditional selection, and sum up the results of the selection using the <code>sum</code> function.</p>

<pre><code>&gt;&gt; df = pd.DataFrame({'a': [1, 2, 3]})
&gt;&gt; df[df.a &gt; 1].sum()   
a    5
dtype: int64
</code></pre>

<p>Having more than one condition:</p>

<pre><code>&gt;&gt; df[(df.a &gt; 1) &amp; (df.a &lt; 3)].sum()
a    2
dtype: int64
</code></pre>
";;4;;2014-01-08T12:16:40.527;;20995428;2014-01-08T12:26:26.253;2014-01-08T12:26:26.253;;1672420.0;;1672420.0;20995196.0;2;24;;;
9194;9194;;;"<p>The most common intent is handled like this:</p>

<pre><code>def assertFrameEqual(df1, df2, **kwds ):
    """""" Assert that two dataframes are equal, ignoring ordering of columns""""""
    from pandas.util.testing import assert_frame_equal
    return assert_frame_equal(df1.sort(axis=1), df2.sort(axis=1), check_names=True, **kwds )
</code></pre>

<p>Of course see <code>pandas.util.testing.assert_frame_equal</code> for other parameters you can pass</p>
";;1;;2014-01-08T16:04:29.927;;21000675;2014-07-30T09:27:53.723;2014-07-30T09:27:53.723;;419338.0;;2868577.0;14224172.0;2;28;;;
9209;9209;;;"<p>If the number of ""obj_id""s is very high you'll want to sort the entire dataframe and then drop duplicates to get the last element.  </p>

<pre><code>sorted = df.sort_index(by='data_date')
result = sorted.drop_duplicates('obj_id', take_last=True).values
</code></pre>

<p>This should be faster (sorry I didn't test it) because you don't have to do a custom agg function, which is slow when there is a large number of keys.  You might think it's worse to sort the entire dataframe, but in practice in python sorts are fast and native loops are slow.</p>
";;2;;2014-01-08T21:35:50.247;;21007047;2014-01-08T21:35:50.247;;;;;1655788.0;9850954.0;2;6;;;
9227;9227;;;"<p>The dtype object comes from NumPy, it describes the type of element in a ndarray. Every element in a ndarray must has the same size in byte. For int64 and float64, they are 8 bytes. But for strings, the length of the string is not fixed. So instead of save the bytes of strings in the ndarray directly, Pandas use object ndarray, which save pointers to objects, because of this the dtype of this kind ndarray is object.</p>

<p>Here is an example:</p>

<ul>
<li>the int64 array contains 4 int64 value.</li>
<li>the object array contains 4 pointers to 3 string objects.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/pFF44.png"" alt=""enter image description here""></p>
";;1;;2014-01-09T12:33:58.977;;21020411;2014-01-09T12:41:35.160;2014-01-09T12:41:35.160;;772649.0;;772649.0;21018654.0;2;89;;;
9238;9238;;;"<pre><code>In [161]: pd.DataFrame(df.values*df2.values, columns=df.columns, index=df.index)
Out[161]: 
   col1  col2  col3
1    10   200  3000
2    10   200  3000
3    10   200  3000
4    10   200  3000
5    10   200  3000
</code></pre>
";;2;;2014-01-09T14:36:34.033;;21023125;2014-01-09T14:42:12.340;2014-01-09T14:42:12.340;;190597.0;;190597.0;21022865.0;2;20;;;
9264;9264;;;"<p>Differently from Dan, I consider his answer quite elegant... but unfortunately it is also very very inefficient. So, since the question mentioned <em>""a large csv file""</em>, let me suggest to try in a shell Dan's solution:</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print df['col'].apply(lambda x : pd.Series(x.split(' '))).head()""
</code></pre>

<p>... compared to this alternative:</p>

<pre><code>time python -c ""import pandas as pd;
from scipy import array, concatenate;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(concatenate(df['col'].apply( lambda x : [x.split(' ')]))).head()""
</code></pre>

<p>... and this:</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))).head()""
</code></pre>

<p>The second simply refrains from allocating 100 000 Series, and this is enough to make it around 10 times faster. But the third solution, which somewhat ironically wastes a lot of calls to str.split() (it is called once per column per row, so three times more than for the others two solutions), is around <em>40 times</em> faster than the first, because it even avoids to instance the 100 000 lists. And yes, it is certainly a little ugly...</p>

<p><strong>EDIT:</strong> <a href=""https://stackoverflow.com/a/12505089/2858145"">this answer</a> suggests how to use ""to_list()"" and to avoid the need for a lambda. The result is something like</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(df.col.str.split().tolist()).head()""
</code></pre>

<p>which is even more efficient than the third solution, and certainly much more elegant.</p>

<p><strong>EDIT:</strong> the even simpler</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(list(df.col.str.split())).head()""
</code></pre>

<p>works too, and is <strong>almost</strong> as efficient.</p>
";;5;;2014-01-09T22:25:55.987;;21032532;2015-02-07T10:35:56.093;2017-05-23T11:47:30.917;;-1.0;;2858145.0;17116814.0;2;42;;;
9269;9269;;;"<p>Hard to diagnose without any data. The following works for me:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas
series = pandas.Series(np.random.normal(size=2000))
fig, ax = plt.subplots()
series.hist(ax=ax, bins=100, bottom=0.1)
ax.set_yscale('log')
</code></pre>

<p><img src=""https://i.stack.imgur.com/WEL08.png"" alt=""enter image description here""></p>

<p>The key here is that you pass <code>ax</code> to the histogram function and you specify the <code>bottom</code> since there is no zero value on a log scale.</p>
";;2;;2014-01-10T00:05:26.557;;21033789;2014-01-10T00:05:26.557;;;;;1552748.0;21033720.0;2;30;;;
9310;9310;;;"<p>The <code>.contains()</code> method uses regular expressions, so you can use a <a href=""https://stackoverflow.com/questions/406230/regular-expression-to-match-string-not-containing-a-word"">negative lookahead test</a> to determine that a word is <em>not</em> contained:</p>

<pre><code>df['A'].str.contains(r'^(?:(?!Hello|World).)*$')
</code></pre>

<p>This expression matches any string where the words <code>Hello</code> and <code>World</code> are <em>not</em> found anywhere in the string.</p>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [""Hello"", ""this"", ""World"", ""apple""]})
&gt;&gt;&gt; df['A'].str.contains(r'^(?:(?!Hello|World).)*$')
0    False
1     True
2    False
3     True
Name: A, dtype: bool
&gt;&gt;&gt; df[df['A'].str.contains(r'^(?:(?!Hello|World).)*$')]
       A
1   this
3  apple
</code></pre>
";;2;;2014-01-10T21:56:27.083;;21055161;2014-01-10T22:11:03.440;2017-05-23T12:26:12.810;;-1.0;;100297.0;21055068.0;2;6;;;
9311;9311;;;"<p>You can use the tilde <code>~</code> to flip the bool values:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [""Hello"", ""this"", ""World"", ""apple""]})
&gt;&gt;&gt; df.A.str.contains(""Hello|World"")
0     True
1    False
2     True
3    False
Name: A, dtype: bool
&gt;&gt;&gt; ~df.A.str.contains(""Hello|World"")
0    False
1     True
2    False
3     True
Name: A, dtype: bool
&gt;&gt;&gt; df[~df.A.str.contains(""Hello|World"")]
       A
1   this
3  apple

[2 rows x 1 columns]
</code></pre>

<p>Whether this is the most efficient way, I don't know; you'd have to time it against your other options.  Sometimes using a regular expression is slower than things like <code>df[~(df.A.str.contains(""Hello"") | (df.A.str.contains(""World"")))]</code>, but I'm bad at guessing where the crossovers are.</p>
";;3;;2014-01-10T21:57:30.423;;21055176;2014-01-10T21:57:30.423;;;;;487339.0;21055068.0;2;23;;;
9326;9326;;;"<p>Here's a numpy version of the rolling maximum drawdown function.  <code>windowed_view</code> is a wrapper of a one-line function that uses <code>numpy.lib.stride_tricks.as_strided</code> to make a memory efficient 2d windowed view of the 1d array (full code below).  Once we have this windowed view, the calculation is basically the same as your <code>max_dd</code>, but written for a numpy array, and applied along the second axis (i.e. <code>axis=1</code>).</p>

<pre><code>def rolling_max_dd(x, window_size, min_periods=1):
    """"""Compute the rolling maximum drawdown of `x`.

    `x` must be a 1d numpy array.
    `min_periods` should satisfy `1 &lt;= min_periods &lt;= window_size`.

    Returns an 1d array with length `len(x) - min_periods + 1`.
    """"""
    if min_periods &lt; window_size:
        pad = np.empty(window_size - min_periods)
        pad.fill(x[0])
        x = np.concatenate((pad, x))
    y = windowed_view(x, window_size)
    running_max_y = np.maximum.accumulate(y, axis=1)
    dd = y - running_max_y
    return dd.min(axis=1)
</code></pre>

<p>Here's a complete script that demonstrates the function:</p>

<pre><code>import numpy as np
from numpy.lib.stride_tricks import as_strided
import pandas as pd
import matplotlib.pyplot as plt


def windowed_view(x, window_size):
    """"""Creat a 2d windowed view of a 1d array.

    `x` must be a 1d numpy array.

    `numpy.lib.stride_tricks.as_strided` is used to create the view.
    The data is not copied.

    Example:

    &gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5, 6])
    &gt;&gt;&gt; windowed_view(x, 3)
    array([[1, 2, 3],
           [2, 3, 4],
           [3, 4, 5],
           [4, 5, 6]])
    """"""
    y = as_strided(x, shape=(x.size - window_size + 1, window_size),
                   strides=(x.strides[0], x.strides[0]))
    return y


def rolling_max_dd(x, window_size, min_periods=1):
    """"""Compute the rolling maximum drawdown of `x`.

    `x` must be a 1d numpy array.
    `min_periods` should satisfy `1 &lt;= min_periods &lt;= window_size`.

    Returns an 1d array with length `len(x) - min_periods + 1`.
    """"""
    if min_periods &lt; window_size:
        pad = np.empty(window_size - min_periods)
        pad.fill(x[0])
        x = np.concatenate((pad, x))
    y = windowed_view(x, window_size)
    running_max_y = np.maximum.accumulate(y, axis=1)
    dd = y - running_max_y
    return dd.min(axis=1)


def max_dd(ser):
    max2here = pd.expanding_max(ser)
    dd2here = ser - max2here
    return dd2here.min()


if __name__ == ""__main__"":
    np.random.seed(0)
    n = 100
    s = pd.Series(np.random.randn(n).cumsum())

    window_length = 10

    rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)
    df = pd.concat([s, rolling_dd], axis=1)
    df.columns = ['s', 'rol_dd_%d' % window_length]
    df.plot(linewidth=3, alpha=0.4)

    my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)
    plt.plot(my_rmdd, 'g.')

    plt.show()
</code></pre>

<p>The plot shows the curves generated by your code.  The green dots are computed by <code>rolling_max_dd</code>.</p>

<p><img src=""https://i.stack.imgur.com/kPxVc.png"" alt=""rolling drawdown plot""></p>

<p>Timing comparison, with <code>n = 10000</code> and <code>window_length = 500</code>:</p>

<pre><code>In [2]: %timeit rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)
1 loops, best of 3: 247 ms per loop

In [3]: %timeit my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)
10 loops, best of 3: 38.2 ms per loop
</code></pre>

<p><code>rolling_max_dd</code> is about 6.5 times faster.  The speedup is better for smaller window lengths.  For example, with <code>window_length = 200</code>, it is almost 13 times faster.</p>

<p>To handle NA's, you could preprocess the <code>Series</code> using the <code>fillna</code> method before passing the array to <code>rolling_max_dd</code>.</p>
";;9;;2014-01-11T06:26:15.070;;21059308;2014-01-11T19:37:48.910;2014-01-11T19:37:48.910;;1217358.0;;1217358.0;21058333.0;2;14;;;
9439;9439;;;"<p>Granted, the answer I linked in the comments is not very helpful. You can specify your own string converter like so.</p>

<pre><code>In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)

In [28]: Series(np.random.randn(3))*1000000000
Out[28]: 
0    -757322420.605
1   -1436160588.997
2   -1235116117.064
dtype: float64
</code></pre>

<p>I'm not sure if that's the preferred way to do this, but it works.</p>

<p>Converting numbers to strings purely for aesthetic purposes seems like a bad idea, but if you have a good reason, this is one way:</p>

<pre><code>In [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)
Out[6]: 
0     0.026
1    -0.482
2    -0.694
dtype: object
</code></pre>
";;3;;2014-01-15T14:40:32.950;;21140339;2014-01-15T14:40:32.950;;;;;1221924.0;21137150.0;2;73;;;
9479;9479;;;"<pre><code>df.loc[:, (df != 0).any(axis=0)]
</code></pre>

<hr>

<p>Here is a break-down of how it works:</p>

<pre><code>In [74]: import pandas as pd

In [75]: df = pd.DataFrame([[1,0,0,0], [0,0,1,0]])

In [76]: df
Out[76]: 
   0  1  2  3
0  1  0  0  0
1  0  0  1  0

[2 rows x 4 columns]
</code></pre>

<p><code>df != 0</code> creates a boolean DataFrame which is True where <code>df</code> is nonzero:</p>

<pre><code>In [77]: df != 0
Out[77]: 
       0      1      2      3
0   True  False  False  False
1  False  False   True  False

[2 rows x 4 columns]
</code></pre>

<p><code>(df != 0).any(axis=0)</code> returns a boolean Series indicating which columns  have nonzero entries. (The <code>any</code> operation aggregates values along the 0-axis -- i.e. along the rows -- into a single boolean value. Hence the result is one boolean value for each column.)</p>

<pre><code>In [78]: (df != 0).any(axis=0)
Out[78]: 
0     True
1    False
2     True
3    False
dtype: bool
</code></pre>

<p>And <code>df.loc</code> can be used to select those columns:</p>

<pre><code>In [79]: df.loc[:, (df != 0).any(axis=0)]
Out[79]: 
   0  2
0  1  0
1  0  1

[2 rows x 2 columns]
</code></pre>

<hr>

<p>To ""delete"" the zero-columns, reassign <code>df</code>:</p>

<pre><code>df = df.loc[:, (df != 0).any(axis=0)]
</code></pre>
";;9;;2014-01-16T14:52:51.217;;21165116;2016-01-03T15:47:11.047;2016-01-03T15:47:11.047;;190597.0;;190597.0;21164910.0;2;45;;;
9499;9499;;;"<p>Setup:</p>

<pre><code>s1 = pd.Series([4,5,6,20,42])
s2 = pd.Series([1,2,3,5,42])
</code></pre>

<p>Timings:</p>

<pre><code>%%timeit
pd.Series(list(set(s1).intersection(set(s2))))
10000 loops, best of 3: 57.7 s per loop

%%timeit
pd.Series(np.intersect1d(s1,s2))
1000 loops, best of 3: 659 s per loop

%%timeit
pd.Series(np.intersect1d(s1.values,s2.values))
10000 loops, best of 3: 64.7 s per loop
</code></pre>

<p>So the numpy solution can be comparable to the set solution even for small series, if one uses the <code>values</code> explicitely.</p>
";;2;;2014-01-16T23:33:17.067;;21175114;2014-01-19T14:06:01.960;2014-01-19T14:06:01.960;;1204331.0;;1204331.0;18079563.0;2;12;;;
9527;9527;;;"<p>Based on BrenBarns's answer, but speeded up by using label based indexing rather than boolean based indexing:</p>

<p></p>

<pre><code>def rollBy(what,basis,window,func,*args,**kwargs):
    #note that basis must be sorted in order for this to work properly     
    indexed_what = pd.Series(what.values,index=basis.values)
    def applyToWindow(val):
        # using slice_indexer rather that what.loc [val:val+window] allows
        # window limits that are not specifically in the index
        indexer = indexed_what.index.slice_indexer(val,val+window,1)
        chunk = indexed_what[indexer]
        return func(chunk,*args,**kwargs)
    rolled = basis.apply(applyToWindow)
    return rolled
</code></pre>

<p>This is <strong>much</strong> faster than not using an indexed column:</p>

<pre><code>In [46]: df = pd.DataFrame({""RollBasis"":np.random.uniform(0,1000000,100000), ""ToRoll"": np.random.uniform(0,10,100000)})

In [47]: df = df.sort(""RollBasis"")

In [48]: timeit(""rollBy_Ian(df.ToRoll,df.RollBasis,10,sum)"",setup=""from __main__ import rollBy_Ian,df"", number =3)
Out[48]: 67.6615059375763

In [49]: timeit(""rollBy_Bren(df.ToRoll,df.RollBasis,10,sum)"",setup=""from __main__ import rollBy_Bren,df"", number =3)
Out[49]: 515.0221037864685
</code></pre>

<p>Its worth noting that the index based solution is O(n), while the logical slicing version is O(n^2) in the average case (I think). </p>

<p>I find it more useful to do this over evenly spaced windows from the min value of Basis to the max value of Basis, rather than at every value of basis. This means altering the function thus:</p>

<pre><code>def rollBy(what,basis,window,func,*args,**kwargs):
    #note that basis must be sorted in order for this to work properly
    windows_min = basis.min()
    windows_max = basis.max()
    window_starts = np.arange(windows_min, windows_max, window)
    window_starts = pd.Series(window_starts, index = window_starts)
    indexed_what = pd.Series(what.values,index=basis.values)
    def applyToWindow(val):
        # using slice_indexer rather that what.loc [val:val+window] allows
        # window limits that are not specifically in the index
        indexer = indexed_what.index.slice_indexer(val,val+window,1)
        chunk = indexed_what[indexer]
        return func(chunk,*args,**kwargs)
    rolled = window_starts.apply(applyToWindow)
    return rolled
</code></pre>
";;1;;2014-01-17T15:22:13.390;;21189441;2014-01-17T15:22:13.390;;;;;1883154.0;14300768.0;2;9;;;
9546;9546;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html""><code>convert_objects</code></a> to infer better dtypes:</p>

<pre><code>In [11]: df
Out[11]: 
   x  y
0  a  1
1  b  2

In [12]: df.dtypes
Out[12]: 
x    object
y    object
dtype: object

In [13]: df.convert_objects(convert_numeric=True)
Out[13]: 
   x  y
0  a  1
1  b  2

In [14]: df.convert_objects(convert_numeric=True).dtypes
Out[14]: 
x    object
y     int64
dtype: object
</code></pre>

<p><em>Magic!</em></p>
";;6;;2014-01-17T23:26:04.013;;21197863;2014-01-17T23:26:04.013;;;;;1240268.0;21197774.0;2;28;;;
9552;9552;;;"<p><code>merge()</code> can't do this kind of join, but you can use <code>searchsorted()</code>:</p>

<p>Create some random timestamps: <code>t1</code>, <code>t2</code>, there are in ascending order:</p>

<pre><code>import pandas as pd
import numpy as np
np.random.seed(0)

base = np.array([""2013-01-01 00:00:00""], ""datetime64[ns]"")

a = (np.random.rand(30)*1000000*1000).astype(np.int64)*1000000
t1 = base + a
t1.sort()

b = (np.random.rand(10)*1000000*1000).astype(np.int64)*1000000
t2 = base + b
t2.sort()
</code></pre>

<p>call <code>searchsorted()</code> to find index in <code>t1</code> for every value in <code>t2</code>:</p>

<pre><code>idx = np.searchsorted(t1, t2) - 1
mask = idx &gt;= 0

df = pd.DataFrame({""t1"":t1[idx][mask], ""t2"":t2[mask]})
</code></pre>

<p>here is the output:</p>

<pre><code>                         t1                         t2
0 2013-01-02 06:49:13.287000 2013-01-03 16:29:15.612000
1 2013-01-05 16:33:07.211000 2013-01-05 21:42:30.332000
2 2013-01-07 04:47:24.561000 2013-01-07 04:53:53.948000
3 2013-01-07 14:26:03.376000 2013-01-07 17:01:35.722000
4 2013-01-07 14:26:03.376000 2013-01-07 18:22:13.996000
5 2013-01-07 14:26:03.376000 2013-01-07 18:33:55.497000
6 2013-01-08 02:24:54.113000 2013-01-08 12:23:40.299000
7 2013-01-08 21:39:49.366000 2013-01-09 14:03:53.689000
8 2013-01-11 08:06:36.638000 2013-01-11 13:09:08.078000
</code></pre>

<p>To view this result by graph:</p>

<pre><code>import pylab as pl
pl.figure(figsize=(18, 4))
pl.vlines(pd.Series(t1), 0, 1, colors=""g"", lw=1)
pl.vlines(df.t1, 0.3, 0.7, colors=""r"", lw=2)
pl.vlines(df.t2, 0.3, 0.7, colors=""b"", lw=2)
pl.margins(0.02)
</code></pre>

<p>output:</p>

<p><img src=""https://i.stack.imgur.com/H5dbR.png"" alt=""enter image description here""></p>

<p>The green lines are <code>t1</code>, blue lines are <code>t2</code>, red lines are selected from <code>t1</code> for every <code>t2</code>.</p>
";;0;;2014-01-18T12:57:52.067;;21204417;2014-01-18T12:57:52.067;;;;;772649.0;21201618.0;2;23;;;
9570;9570;;;"<p>matplotlib >= 1.4 suports <a href=""https://github.com/matplotlib/matplotlib/blob/master/doc/users/style_sheets.rst"" rel=""noreferrer"">styles</a> (and ggplot-style is build in):</p>

<pre><code>In [1]: import matplotlib as mpl

In [2]: import matplotlib.pyplot as plt

In [3]: import numpy as np

In [4]: mpl.style.available
Out[4]: [u'dark_background', u'grayscale', u'ggplot']

In [5]: mpl.style.use('ggplot')

In [6]: plt.hist(np.random.randn(100000))
Out[6]: 
...
</code></pre>

<p><img src=""https://i.stack.imgur.com/arnXC.png"" alt=""enter image description here""></p>
";;2;;2014-01-19T18:56:42.090;;21221138;2014-01-19T18:56:42.090;;;;;1301710.0;14349055.0;2;29;;;
9587;9587;;;"<p>If you have same columns in all your <code>csv</code> files then you can try the code below.
I have added <code>header=0</code> so that after reading <code>csv</code> first row can be assigned as the column names.</p>

<pre><code>path =r'C:\DRO\DCL_rawdata_files' # use your path
allFiles = glob.glob(path + ""/*.csv"")
frame = pd.DataFrame()
list_ = []
for file_ in allFiles:
    df = pd.read_csv(file_,index_col=None, header=0)
    list_.append(df)
frame = pd.concat(list_)
</code></pre>
";;9;;2014-01-20T11:29:19.410;;21232849;2015-04-23T14:13:52.167;2015-04-23T14:13:52.167;;417415.0;;1983512.0;20906474.0;2;107;;;
9597;9597;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html"">concat</a>:</p>

<pre><code>In [11]: pd.concat([df1['c'], df2['c']], axis=1, keys=['df1', 'df2'])
Out[11]: 
                 df1       df2
2014-01-01       NaN -0.978535
2014-01-02 -0.106510 -0.519239
2014-01-03 -0.846100 -0.313153
2014-01-04 -0.014253 -1.040702
2014-01-05  0.315156 -0.329967
2014-01-06 -0.510577 -0.940901
2014-01-07       NaN -0.024608
2014-01-08       NaN -1.791899

[8 rows x 2 columns]
</code></pre>
";;8;;2014-01-20T19:08:12.033;;21242140;2014-01-20T19:08:12.033;;;;;1240268.0;21231834.0;2;31;;;
9601;9601;;;"<p>Similar to Andy Hayden answer with check if min equal to max (then row elements are all duplicates):</p>

<pre><code>df[df.apply(lambda x: min(x) == max(x), 1)]
</code></pre>
";;0;;2014-01-20T21:10:49.463;;21244212;2014-01-20T21:10:49.463;;;;;1181482.0;21231478.0;2;7;;;
9602;9602;;;"<p>My entry:</p>

<pre><code>&gt;&gt;&gt; df
        0       1       2
0   apple  banana  banana
1  orange  orange  orange
2  banana   apple  orange
3     NaN     NaN     NaN
4   apple   apple   apple

[5 rows x 3 columns]
&gt;&gt;&gt; df[df.apply(pd.Series.nunique, axis=1) == 1]
        0       1       2
1  orange  orange  orange
4   apple   apple   apple

[2 rows x 3 columns]
</code></pre>

<p>This works because calling <code>pd.Series.nunique</code> on the rows gives:</p>

<pre><code>&gt;&gt;&gt; df.apply(pd.Series.nunique, axis=1)
0    2
1    1
2    3
3    0
4    1
dtype: int64
</code></pre>

<p><strong>Note:</strong> this would, however, keep rows which look like <code>[nan, nan, apple]</code> or <code>[nan, apple, apple]</code>.  Usually I want that, but that might be the wrong answer for your use case.</p>
";;1;;2014-01-20T21:18:51.860;;21244355;2014-01-20T21:27:38.620;2014-01-20T21:27:38.620;;487339.0;;487339.0;21231478.0;2;9;;;
9610;9610;;;"<pre><code>pd.crosstab(df.A, df.B).apply(lambda r: r/r.sum(), axis=1)
</code></pre>

<p>Basically you just have the function that does <code>row/row.sum()</code>, and you use <code>apply</code> with <code>axis=1</code> to apply it by row.</p>

<p>(If doing this in Python 2, you should use <code>from __future__ import division</code> to make sure division always returns a float.)</p>
";;2;;2014-01-21T01:10:14.510;;21247312;2014-01-21T01:25:40.590;2014-01-21T01:25:40.590;;1427416.0;;1427416.0;21247203.0;2;17;;;
9612;9612;;;"<p>Use <code>reset_index</code></p>

<pre><code>In [9]: mydf.groupby(['cat', ""class""]).val.sum().reset_index()
Out[9]: 
      cat class  val
0   first     A    7
1  second     B    3
2   third     C   10
</code></pre>

<h2>EDIT</h2>

<p>set level=1 if you want to set <code>cat</code> as index</p>

<pre><code>In [10]: mydf.groupby(['cat', ""class""]).val.sum().reset_index(level=1)
Out[10]: 
       class  val
cat              
first      A    7
second     B    3
third      C   10
</code></pre>

<p>You can also set <code>as_index=False</code> to get the same output</p>

<pre><code>In [29]: mydf.groupby(['cat', ""class""], as_index=False).val.sum()
Out[29]: 
      cat class  val
0   first     A    7
1  second     B    3
2   third     C   10
</code></pre>
";;5;;2014-01-21T02:33:12.673;;21248050;2014-01-21T03:01:14.697;2014-01-21T03:01:14.697;;1426056.0;;1426056.0;21247992.0;2;13;;;
9637;9637;;;"<p>MySQL example:</p>

<pre><code>import MySQLdb as db
from pandas import DataFrame
from pandas.io.sql import frame_query

database = db.connect('localhost','username','password','database')
data     = frame_query(""SELECT * FROM data"", database)
</code></pre>
";;1;;2014-01-21T14:03:07.133;;21260328;2014-01-21T14:03:07.133;;;;;3210609.0;10065051.0;2;9;;;
9647;9647;;;"<p>You could try this instead:</p>

<pre><code>df[ (df.A=='blue') &amp; (df.B=='red') &amp; (df.C=='square') ]['D'] = 'succeed'
</code></pre>
";;4;;2014-01-21T16:02:29.293;;21263149;2014-01-21T16:02:29.293;;;;;819718.0;21263020.0;2;12;;;
9656;9656;;;"<p>I found a quick and easy solution to what I wanted using json_normalize function included in the latest release of pandas 0.13.   </p>

<pre><code>from urllib2 import Request, urlopen
import json
from pandas.io.json import json_normalize

path1 = '42.974049,-81.205203|42.974298,-81.195755'
request=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&amp;sensor=false')
response = urlopen(request)
elevations = response.read()
data = json.loads(elevations)
json_normalize(data['results'])
</code></pre>

<p>This gives a nice flattened dataframe with the json data that I got from the google maps API.</p>
";;1;;2014-01-21T18:17:22.187;;21266043;2014-01-21T18:17:22.187;;;;;2593236.0;21104592.0;2;61;;;
9670;9670;;;"<p>You might try passing actual types instead of strings.</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime, datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>But it's going to be really hard to diagnose this without any of your data to tinker with.</p>

<p>And really, you probably want pandas to parse the the dates into TimeStamps, so that might be:</p>

<pre><code>pd.read_csv(file, sep='\t', header=None, names=headers, parse_dates=True)
</code></pre>
";;0;;2014-01-21T23:19:41.040;;21271103;2014-01-21T23:19:41.040;;;;;1552748.0;21269399.0;2;7;;;
9676;9676;;;"<pre><code>df.loc[:, df.dtypes == np.float64]
</code></pre>
";;0;;2014-01-22T01:42:30.660;;21272615;2014-01-22T01:42:30.660;;;;;1221924.0;21271581.0;2;18;;;
9679;9679;;;"<p>Just for completeness (I'll add my comment as an answer), you missed out:</p>

<pre><code>pd.options.display.max_colwidth  # default is 50
</code></pre>

<p><em>this restricted the maximum length of a single column.</em></p>

<p>There are quite a few options to configure here, if you're using ipython then tab complete to find the <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#working-with-package-options"">full set of display options</a>: </p>

<pre><code>pd.options.display.&lt;tab&gt;
</code></pre>
";;3;;2014-01-22T06:41:55.940;;21275962;2014-01-22T06:41:55.940;;;;;1240268.0;21249206.0;2;23;;;
9703;9703;;;"<p>Just iterate over <code>DataFrame.columns</code>, now this is an example in which you will end up with a list of column names that match:</p>

<pre><code>import pandas as pd

data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}
df = pd.DataFrame(data)

spike_cols = [col for col in df.columns if 'spike' in col]
print(list(df.columns))
print(spike_cols)
</code></pre>

<p>Output:</p>

<pre><code>['hey spke', 'no', 'spike-2', 'spiked-in']
['spike-2', 'spiked-in']
</code></pre>

<p>Explanation:</p>

<ol>
<li><code>df.columns</code> returns a list of column names</li>
<li><code>[col for col in df.columns if 'spike' in col]</code> iterates over the list <code>df.columns</code> with the variable <code>col</code> and adds it to the resulting list if <code>col</code> contains <code>'spike'</code>. This syntax is <a href=""http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions"" rel=""noreferrer"">list comprehension</a>. </li>
</ol>

<p>If you only want the resulting data set with the columns that match you can do this:</p>

<pre><code>df2 = df.filter(regex='spike')
print(df2)
</code></pre>

<p>Output:</p>

<pre><code>   spike-2  spiked-in
0        1          7
1        2          8
2        3          9
</code></pre>
";;11;;2014-01-22T14:25:56.963;;21285575;2014-01-22T14:59:06.623;2014-01-22T14:59:06.623;;2997574.0;;2997574.0;21285380.0;2;60;;;
9707;9707;;;"<p>The recommended way (according to the maintainers) to set a value is:</p>

<pre><code>df.ix['x','C']=10
</code></pre>

<p>Using 'chained indexing' (<code>df['x']['C']</code>) may lead to problems.</p>

<p>See:</p>

<ul>
<li><a href=""https://stackoverflow.com/a/21287235/1579844"">https://stackoverflow.com/a/21287235/1579844</a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-view-versus-copy</a></li>
<li><a href=""https://github.com/pydata/pandas/pull/6031"" rel=""nofollow noreferrer"">https://github.com/pydata/pandas/pull/6031</a></li>
</ul>
";;0;;2014-01-22T15:48:25.217;;21287539;2014-01-22T15:48:25.217;2017-05-23T12:26:36.443;;-1.0;;1579844.0;13842088.0;2;25;;;
9711;9711;;;"<p>The lack of NaN rep in integer columns is a <a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"" rel=""noreferrer"">pandas ""gotcha""</a>.</p>

<p>The usual workaround is to simply use floats.</p>
";;5;2014-01-22T17:42:28.767;2014-01-22T17:42:28.767;;21290084;2017-05-26T16:15:51.753;2017-05-26T16:15:51.753;;1240268.0;;1240268.0;21287624.0;2;48;;;
9713;9713;;;"<p>Use the .astype() function to manipulate column dtypes.</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3,4), columns=list(""ABCD""))
&gt;&gt;&gt; df
          A         B         C         D
0  0.542447  0.949988  0.669239  0.879887
1  0.068542  0.757775  0.891903  0.384542
2  0.021274  0.587504  0.180426  0.574300
&gt;&gt;&gt; df[list(""ABCD"")] = df[list(""ABCD"")].astype(int)
&gt;&gt;&gt; df
   A  B  C  D
0  0  0  0  0
1  0  0  0  0
2  0  0  0  0
</code></pre>

<p>EDIT:</p>

<p>To handle missing values:</p>

<pre><code>&gt;&gt;&gt; df
          A         B     C         D
0  0.475103  0.355453  0.66  0.869336
1  0.260395  0.200287   NaN  0.617024
2  0.517692  0.735613  0.18  0.657106
&gt;&gt;&gt; df[list(""ABCD"")] = df[list(""ABCD"")].fillna(0.0).astype(int)
&gt;&gt;&gt; df
   A  B  C  D
0  0  0  0  0
1  0  0  0  0
2  0  0  0  0
&gt;&gt;&gt;
</code></pre>
";;9;;2014-01-22T18:49:11.553;;21291383;2014-01-22T18:56:35.027;2014-01-22T18:56:35.027;;1339024.0;;1339024.0;21291259.0;2;60;;;
9715;9715;;;"<p>To modify the float output do this:</p>

<pre><code>df= pd.DataFrame(range(5), columns=['a'])
df.a = df.a.astype(float)
df

Out[33]:

          a
0 0.0000000
1 1.0000000
2 2.0000000
3 3.0000000
4 4.0000000

pd.options.display.float_format = '{:,.0f}'.format
df

Out[35]:

   a
0  0
1  1
2  2
3  3
4  4
</code></pre>
";;4;;2014-01-22T19:01:18.130;;21291622;2014-01-22T19:07:02.770;2014-01-22T19:07:02.770;;704848.0;;704848.0;21291259.0;2;55;;;
9730;9730;;;"<p>DSM's suggestion seems to be about the best you're going to get without doing some manual microoptimization:</p>

<pre><code>%timeit -n 100 df.col1.str.len().max()
100 loops, best of 3: 11.7 ms per loop

%timeit -n 100 df.col1.map(lambda x: len(x)).max()
100 loops, best of 3: 16.4 ms per loop

%timeit -n 100 df.col1.map(len).max()
100 loops, best of 3: 10.1 ms per loop
</code></pre>

<p>Note that explicitly using the <code>str.len()</code> method doesn't seem to be much of an improvement. If you're not familiar with IPython, which is where that very convenient <code>%timeit</code> syntax comes from, I'd definitely suggest giving it a shot for quick testing of things like this.</p>
";;2;;2014-01-22T22:44:12.017;;21295630;2014-01-22T22:44:12.017;;;;;1222578.0;21295334.0;2;23;;;
9738;9738;;;"<p>You can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html"">extract</a> the different parts out quite neatly using a regex pattern:</p>

<pre><code>In [11]: df.row.str.extract('(?P&lt;fips&gt;\d{5})((?P&lt;state&gt;[A-Z ]*$)|(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))')
Out[11]: 
    fips                    1           state           county state_code
0  00000        UNITED STATES   UNITED STATES              NaN        NaN
1  01000              ALABAMA         ALABAMA              NaN        NaN
2  01001   Autauga County, AL             NaN   Autauga County         AL
3  01003   Baldwin County, AL             NaN   Baldwin County         AL
4  01005   Barbour County, AL             NaN   Barbour County         AL

[5 rows x 5 columns]
</code></pre>

<hr>

<p>To explain the somewhat long regex:</p>

<pre><code>(?P&lt;fips&gt;\d{5})
</code></pre>

<ul>
<li>Matches the five digits (<code>\d</code>) and names them <code>""fips""</code>.</li>
</ul>

<p>The next part:</p>

<pre><code>((?P&lt;state&gt;[A-Z ]*$)|(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))
</code></pre>

<p>Does either (<code>|</code>) one of two things:</p>

<pre><code>(?P&lt;state&gt;[A-Z ]*$)
</code></pre>

<ul>
<li>Matches any number (<code>*</code>) of capital letters or spaces (<code>[A-Z ]</code>) and names this <code>""state""</code> before the end of the string (<code>$</code>),</li>
</ul>

<p>or</p>

<pre><code>(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))
</code></pre>

<ul>
<li>matches anything else (<code>.*</code>) then</li>
<li>a comma and a space then  </li>
<li>matches the two digit <code>state_code</code> before the end of the string (<code>$</code>).</li>
</ul>

<p><em>In the example:</em><br>
<em>Note that the first two rows hit the ""state"" (leaving NaN in  the county and state_code columns), whilst the last three hit the county, state_code (leaving NaN in the state column).</em></p>
";;3;;2014-01-23T00:20:07.503;;21296915;2016-04-13T01:27:49.803;2016-04-13T01:27:49.803;;1240268.0;;1240268.0;14745022.0;2;24;;;
9755;9755;;;"<pre><code>[column for column in my_dataframe]
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#iteration"" rel=""nofollow"">pandas docs</a>: Iteration over dataframes return column labels</p>
";;0;;2014-01-23T17:23:40.807;;21315199;2015-03-13T09:41:55.880;2015-03-13T09:41:55.880;;1126386.0;;1126386.0;19482970.0;2;6;;;
9766;9766;;;"<p>The simplest way is to just do the concatenation, and then dedupe.</p>

<pre><code>&gt;&gt;&gt; df1
   A  B
0  1  2
1  3  1
&gt;&gt;&gt; df2
   A  B
0  5  6
1  3  1
&gt;&gt;&gt; pandas.concat([df1,df2]).drop_duplicates().reset_index(drop=True)
   A  B
0  1  2
1  3  1
2  5  6
</code></pre>

<p>The reset_index(drop=True) is to fix up the index after the concat and dedupe. Without it you will have an index of [0,1,0] instead of [0,1,2]. This could cause problems for further operations on this dataframe down the road if it isn't reset right away.</p>
";;1;;2014-01-23T19:27:33.300;;21317570;2014-01-23T19:27:33.300;;;;;1339024.0;21317384.0;2;17;;;
9767;9767;;;"<p>This is a very similar construction (but using <code>cartesian_product</code> which for larger arrays is faster than <code>itertools.product</code>)</p>

<pre><code>In [2]: from pandas.tools.util import cartesian_product

In [3]: MultiIndex.from_arrays(cartesian_product([range(3),list('ab')]))
Out[3]: 
MultiIndex(levels=[[0, 1, 2], [u'a', u'b']],
           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])
</code></pre>

<p>could be added as a convience method, maybe <code>MultiIndex.from_iterables(...)</code></p>

<p>pls open an issue (and PR if you'd like)</p>

<p>FYI I very rarely actually construct a multi-index 'manually', almost always easier to actually construct a frame and just <code>set_index</code>.</p>

<pre><code>In [10]: df = DataFrame(dict(A = np.arange(6), 
                             B = ['foo'] * 3 + ['bar'] * 3, 
                             C = np.ones(6)+np.arange(6)%2)
                       ).set_index(['C','B']).sortlevel()

In [11]: df
Out[11]: 
       A
C B     
1 bar  4
  foo  0
  foo  2
2 bar  3
  bar  5
  foo  1

[6 rows x 1 columns]
</code></pre>
";;6;;2014-01-23T19:34:35.720;;21317700;2014-01-23T19:57:00.513;2014-01-23T19:57:00.513;;644898.0;;644898.0;21316628.0;2;11;;;
9775;9775;;;"<p><code>in</code> of a Series checks whether the value is in the index:</p>

<pre><code>In [11]: s = pd.Series(list('abc'))

In [12]: s
Out[12]: 
0    a
1    b
2    c
dtype: object

In [13]: 1 in s
Out[13]: True

In [14]: 'a' in s
Out[14]: False
</code></pre>

<p>One option is to see if it's in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html"">unique</a> values:</p>

<pre><code>In [21]: s.unique()
Out[21]: array(['a', 'b', 'c'], dtype=object)

In [22]: 'a' in s.unique()
Out[22]: True
</code></pre>

<p>or a python set:</p>

<pre><code>In [23]: set(s)
Out[23]: {'a', 'b', 'c'}

In [24]: 'a' in set(s)
Out[24]: True
</code></pre>

<p>As pointed out by @DSM, it may be more efficient (especially if you're just doing this for one value) to just use in directly on the values:</p>

<pre><code>In [31]: s.values
Out[31]: array(['a', 'b', 'c'], dtype=object)

In [32]: 'a' in s.values
Out[32]: True
</code></pre>
";;4;;2014-01-23T21:45:44.373;;21320011;2014-01-23T21:56:17.513;2014-01-23T21:56:17.513;;1240268.0;;1240268.0;21319929.0;2;35;;;
9784;9784;;;"<p>I think the problem is that you have duplicated columns: two ( Female, R). </p>

<p>Not sure whether it's a bug or the duplicated columns are unacceptable. Here's a workaround for you:</p>

<h2>First read the csv with tupleize_cols=True</h2>

<pre><code>In [61]: df = pd.read_csv('test.csv', header=[0, 1], skipinitialspace=True, tupleize_cols=True)

In [62]: df
Out[62]: 
   (Male, R)  (Male, R)  (Male, L)  (Female, R)  (Female, R)
0       0.67       0.67       0.88         0.81         0.81

[1 rows x 5 columns]
</code></pre>

<h2>Then convert the type of the column from Index to MultiIndex</h2>

<pre><code>In [63]: df.columns = pd.MultiIndex.from_tuples(df.columns)

In [64]: df
Out[64]: 
   Male              Female      
      R     R     L       R     R
0  0.67  0.67  0.88    0.81  0.81

[1 rows x 5 columns]
</code></pre>
";;3;;2014-01-24T04:04:49.687;;21324222;2014-01-24T04:04:49.687;;;;;1426056.0;21318865.0;2;15;;;
9842;9842;;;"<p>use <code>IPython.display</code> module:</p>

<pre><code>%matplotlib inline
import time
import pylab as pl
from IPython import display
for i in range(10):
    pl.plot(pl.randn(100))
    display.clear_output(wait=True)
    display.display(pl.gcf())
    time.sleep(1.0)
</code></pre>
";;4;;2014-01-26T09:51:06.590;;21361994;2014-10-02T18:38:11.130;2014-10-02T18:38:11.130;;1666349.0;;772649.0;21360361.0;2;61;;;
9914;9914;;;"<p>When you say</p>

<pre><code>(a['x']==1) and (a['y']==10)
</code></pre>

<p>You are implicitly asking Python to convert <code>(a['x']==1)</code> and <code>(a['y']==10)</code> to boolean values. </p>

<p>NumPy arrays (of length greater than 1) and Pandas objects such as Series do not have a boolean value -- in other words, they raise </p>

<pre><code>ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().
</code></pre>

<p>when used as a boolean value. That's because its <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#using-if-truth-statements-with-pandas"" rel=""nofollow noreferrer"">unclear when it should be True or False</a>. Some users might assume they are True if they have non-zero length, like a Python list. Others might desire for it to be True only if <strong>all</strong> its elements are True. Others might want it to be True if <strong>any</strong> of its elements are True. </p>

<p>Because there are so many conflicting expectations, the designers of NumPy and Pandas refuse to guess, and instead raise a ValueError.</p>

<p>Instead, you must be explicit, by calling the <code>empty()</code>, <code>all()</code> or <code>any()</code> method to indicate which behavior you desire.</p>

<p>In this case, however, it looks like you do not want boolean evaluation, you want <strong>element-wise</strong> logical-and. That is what the <code>&amp;</code> binary operator performs:</p>

<pre><code>(a['x']==1) &amp; (a['y']==10)
</code></pre>

<p>returns a boolean array. </p>

<hr>

<p>By the way, as <a href=""https://stackoverflow.com/questions/21415661/logic-operator-for-boolean-indexing-in-pandas/21415990?noredirect=1#comment77317569_21415990"">alexpmil notes</a>, 
the parentheses are mandatory since <code>&amp;</code> has a higher <a href=""https://docs.python.org/3/reference/expressions.html#operator-precedence"" rel=""nofollow noreferrer"">operator precedence</a> than <code>==</code>.
Without the parentheses, <code>a['x']==1 &amp; a['y']==10</code> would be evaluated as <code>a['x'] == (1 &amp; a['y']) == 10</code> which would in turn be equivalent to the chained comparison <code>(a['x'] == (1 &amp; a['y'])) and ((1 &amp; a['y']) == 10)</code>. That is an expression of the form <code>Series and Series</code>.
The use of <code>and</code> with two Series would again trigger the same <code>ValueError</code> as above. That's why the parentheses are mandatory.</p>
";;6;;2014-01-28T20:22:56.873;;21415990;2017-07-18T19:00:02.870;2017-07-18T19:00:02.870;;190597.0;;190597.0;21415661.0;2;61;;;
9959;9959;;;"<p>You might be interested in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html"" rel=""noreferrer""><code>pd.cut</code></a>:</p>

<pre><code>&gt;&gt;&gt; df.groupby(pd.cut(df[""B""], np.arange(0, 1.0+0.155, 0.155))).sum()
                      A         B
B                                
(0, 0.155]     2.775458  0.246394
(0.155, 0.31]  1.123989  0.471618
(0.31, 0.465]  2.051814  1.882763
(0.465, 0.62]  2.277960  1.528492
(0.62, 0.775]  1.577419  2.810723
(0.775, 0.93]  0.535100  1.694955
(0.93, 1.085]       NaN       NaN

[7 rows x 2 columns]
</code></pre>
";;1;;2014-01-29T20:12:29.790;;21441621;2016-04-25T08:58:27.890;2016-04-25T08:58:27.890;;2781698.0;;487339.0;21441259.0;2;47;;;
9992;9992;;;"<p>The point of the <code>SettingWithCopy</code> is to warn the user that you <em>may</em> be doing something that will not update the original data frame as one might expect.</p>

<p>Here, <code>data</code> is a dataframe, possibly of a single dtype (or not). You are then taking a reference to this <code>data['amount']</code> which is a Series, and updating it. This probably works in your case because you are returning the same dtype of data as existed.</p>

<p>However it <em>could</em> create a copy which updates a copy of <code>data['amount']</code> which you would not see; Then you would be wondering why it is not updating.</p>

<p>Pandas returns a copy of an object in almost all method calls. The <code>inplace</code> operations are a convience operation which work, but in general are not clear that data is being modified and could potentially work on copies.</p>

<p>Much more clear to do this:</p>

<pre><code>data['amount'] = data[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""))

data[""amount""] = data['amount'].fillna(mean_avg)
</code></pre>

<p>One further plus to working on copies. You can chain operations, this is not possible with <code>inplace</code> ones.</p>

<p>e.g.</p>

<pre><code>data['amount'] = data['amount'].fillna(mean_avg)*2
</code></pre>

<p>And just an FYI. <code>inplace</code> operations are neither faster nor more memory efficient. my2c they should be banned. But too late on that API.</p>

<p>You can of course turn this off:</p>

<pre><code>pd.set_option('chained_assignment',None)
</code></pre>

<p>Pandas runs with the entire test suite with this set to <code>raise</code> (so we know if chaining is happening) on, FYI.</p>
";;5;;2014-01-30T17:49:49.350;;21463854;2014-01-30T19:49:48.077;2014-01-30T19:49:48.077;;644898.0;;644898.0;21463589.0;2;18;;;
10034;10034;;;"<p>The <code>df.plot()</code> function returns a <code>matplotlib.axes.AxesSubplot</code> object. You can set the labels on that object.</p>

<pre><code>In [4]: ax = df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')

In [6]: ax.set_xlabel(""x label"")
Out[6]: &lt;matplotlib.text.Text at 0x10e0af2d0&gt;

In [7]: ax.set_ylabel(""y label"")
Out[7]: &lt;matplotlib.text.Text at 0x10e0ba1d0&gt;
</code></pre>

<p><img src=""https://i.stack.imgur.com/ITpm2.png"" alt=""enter image description here""></p>

<p>Or, more succinctly: <code>ax.set(xlabel=""x label"", ylabel=""y label"")</code>.</p>

<p>Alternatively, the index x-axis label is automatically set to the Index name, if it has one. so <code>df2.index.name = 'x label'</code> would work too.</p>
";;2;;2014-01-31T18:35:34.720;;21487560;2016-02-26T21:17:28.217;2016-02-26T21:17:28.217;;1533576.0;;1889400.0;21487329.0;2;116;;;
10036;10036;;;"<p>You can use do it like this:</p>

<pre><code>import matplotlib.pyplot as plt 
import pandas as pd

plt.figure()
values = [[1,2], [2,5]]
df2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')
plt.xlabel('xlabel')
plt.ylabel('ylabel')
plt.show()
</code></pre>

<p>Obviously you have to replace the strings 'xlabel' and 'ylabel' with what you want them to be.</p>
";;0;;2014-01-31T18:52:55.753;;21487868;2014-01-31T18:52:55.753;;;;;2957554.0;21487329.0;2;14;;;
10038;10038;;;"<p>Thanks to @TomAugspurger</p>

<p>The solution is to get your axes back, and then use <code>ax.vlines</code>.</p>

<pre><code>ax = cum_edits.plot()
ymin, ymax = ax.get_ylim()
ax.vlines(x=dates, ymin=ymin, ymax=ymax-1, color='r')
</code></pre>

<p><img src=""https://i.stack.imgur.com/ta6H1.png"" alt=""Solutions with vlines""></p>

<p>One last niggle is that if the vlines are <code>ymax</code> long, then matplotlib adds extra space to the top of my plot, so I just slightly reduce the length to be less than the original axes, that is why you see the <code>ymax=ymax-1</code>.</p>
";;2;;2014-01-31T20:37:21.727;;21489607;2014-01-31T20:37:21.727;;;;;1704279.0;21488085.0;2;16;;;
10052;10052;;;"<p>I think organizing your data in way that yields repeating column names is only going to create headaches for you later on down the road. A better approach IMHO is to create a column for each of <code>pivots</code>, <code>interval_id</code>, and <code>p_value</code>. This will make extremely easy to query your data after loading it into pandas.</p>

<p>Also, your JSON has some errors in it. I ran it through <a href=""http://www.jsoneditoronline.org/"">this</a> to find the errors.</p>

<p><a href=""http://stedolan.github.io/jq/manual/""><code>jq</code></a> helps here</p>

<pre><code>import sh
jq = sh.jq.bake('-M')  # disable colorizing
json_data = ""from above""
rule = """"""[{pivots: .intervals[].pivots, 
            interval_id: .intervals[].series[].interval_id,
            p_value: .intervals[].series[].p_value}]""""""
out = jq(rule, _in=json_data).stdout
res = pd.DataFrame(json.loads(out))
</code></pre>

<p>This will yield output similar to</p>

<pre><code>    interval_id       p_value      pivots
32            2  2.867501e-06  Jane Smith
33            2  1.000000e+00  Jane Smith
34            2  1.116279e-08  Jane Smith
35            2  2.867501e-06  Jane Smith
36            0  1.000000e+00   Bob Smith
37            0  1.116279e-08   Bob Smith
38            0  2.867501e-06   Bob Smith
39            0  1.000000e+00   Bob Smith
40            0  1.116279e-08   Bob Smith
41            0  2.867501e-06   Bob Smith
42            1  1.000000e+00   Bob Smith
43            1  1.116279e-08   Bob Smith
</code></pre>

<p>Adapted from <a href=""https://github.com/pydata/pandas/pull/4007#issuecomment-21375565"">this comment</a></p>

<p>Of course, you can always call <code>res.drop_duplicates()</code> to remove the duplicate rows. This gives</p>

<pre><code>In [175]: res.drop_duplicates()
Out[175]:
    interval_id       p_value      pivots
0             0  1.000000e+00  Jane Smith
1             0  1.116279e-08  Jane Smith
2             0  2.867501e-06  Jane Smith
6             1  1.000000e+00  Jane Smith
7             1  1.116279e-08  Jane Smith
8             1  2.867501e-06  Jane Smith
12            2  1.000000e+00  Jane Smith
13            2  1.116279e-08  Jane Smith
14            2  2.867501e-06  Jane Smith
36            0  1.000000e+00   Bob Smith
37            0  1.116279e-08   Bob Smith
38            0  2.867501e-06   Bob Smith
42            1  1.000000e+00   Bob Smith
43            1  1.116279e-08   Bob Smith
44            1  2.867501e-06   Bob Smith
48            2  1.000000e+00   Bob Smith
49            2  1.116279e-08   Bob Smith
50            2  2.867501e-06   Bob Smith

[18 rows x 3 columns]
</code></pre>
";;8;;2014-02-01T16:16:15.213;;21500413;2014-02-01T16:21:54.393;2014-02-01T16:21:54.393;;564538.0;;564538.0;21494030.0;2;13;;;
10108;10108;;;"<p>You can use:</p>

<pre><code>data = pd.read_csv('output_list.txt', sep="" "", header=None)
data.columns = [""a"", ""b"", ""c"", ""etc.""]
</code></pre>

<p>Add <code>sep="" ""</code> in your code, leaving a blank space between the quotes. So pandas can detect spaces between values and sort in columns. Data columns is for naming your columns.</p>
";;4;;2014-02-04T07:53:58.393;;21546823;2017-08-03T10:07:28.887;2017-08-03T10:07:28.887;;2422648.0;;3142367.0;21546739.0;2;25;;;
10194;10194;;;"<p>You can give functions to the <code>rename</code> method. The <code>str.strip()</code> method should do what you want.</p>

<pre><code>In [5]: df
Out[5]: 
   Year  Month   Value
0     1       2      3

[1 rows x 3 columns]

In [6]: df.rename(columns=lambda x: x.strip())
Out[6]: 
   Year  Month  Value
0     1      2      3

[1 rows x 3 columns]
</code></pre>
";;1;;2014-02-06T15:49:04.963;;21607530;2014-02-06T15:49:04.963;;;;;1889400.0;21606987.0;2;48;;;
10197;10197;;;"<p>Try</p>

<pre><code>df.ix[df.my_channel &gt; 20000, 'my_channel'] = 0
</code></pre>
";;2;;2014-02-06T16:24:57.903;;21608417;2014-02-06T19:08:38.477;2014-02-06T19:08:38.477;;1181482.0;;1181482.0;21608228.0;2;52;;;
10263;10263;;;"<p>Install <code>pip</code>.</p>

<p>Then install <code>pandas</code> with <code>pip</code>:</p>

<pre><code>pip install pandas
</code></pre>
";;4;;2014-02-08T17:30:51.643;;21649359;2014-02-08T17:47:58.260;2014-02-08T17:47:58.260;;682480.0;;2335754.0;13249135.0;2;21;;;
10277;10277;;;"<p>With <code>plt.scatter</code>, I can only think of one: to use a proxy artist:</p>

<pre><code>df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))
df['key1'] = (4,4,4,6,6,6,8,8,8,8)
fig1 = plt.figure(1)
ax1 = fig1.add_subplot(111)
x=ax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)

ccm=x.get_cmap()
circles=[Line2D(range(1), range(1), color='w', marker='o', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]
leg = plt.legend(circles, ['4','6','8'], loc = ""center left"", bbox_to_anchor = (1, 0.5), numpoints = 1)
</code></pre>

<p>And the result is:</p>

<p><img src=""https://i.stack.imgur.com/7KzM5.png"" alt=""enter image description here""></p>
";;1;;2014-02-09T04:19:30.487;;21655221;2014-02-09T04:43:37.130;2014-02-09T04:43:37.130;;2487184.0;;2487184.0;21654635.0;2;14;;;
10278;10278;;;"<p>You can use <code>scatter</code> for this, but that requires having numerical values for your <code>key1</code>, and you won't have a legend, as you noticed.</p>

<p>It's better to just use <code>plot</code> for discrete categories like this.  For example:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
np.random.seed(1974)

# Generate Data
num = 20
x, y = np.random.random((2, num))
labels = np.random.choice(['a', 'b', 'c'], num)
df = pd.DataFrame(dict(x=x, y=y, label=labels))

groups = df.groupby('label')

# Plot
fig, ax = plt.subplots()
ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling
for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)
ax.legend()

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/Svrkn.png"" alt=""enter image description here""></p>

<p>If you'd like things to look like the default <code>pandas</code> style, then just update the <code>rcParams</code> with the pandas stylesheet and use its color generator. (I'm also tweaking the legend slightly):</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
np.random.seed(1974)

# Generate Data
num = 20
x, y = np.random.random((2, num))
labels = np.random.choice(['a', 'b', 'c'], num)
df = pd.DataFrame(dict(x=x, y=y, label=labels))

groups = df.groupby('label')

# Plot
plt.rcParams.update(pd.tools.plotting.mpl_stylesheet)
colors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')

fig, ax = plt.subplots()
ax.set_color_cycle(colors)
ax.margins(0.05)
for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)
ax.legend(numpoints=1, loc='upper left')

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/VuZeq.png"" alt=""enter image description here""></p>
";;4;;2014-02-09T04:23:06.483;;21655256;2014-02-09T04:23:06.483;;;;;325565.0;21654635.0;2;55;;;
10320;10320;;;"<p>In newer versions of pandas, the <code>Factor</code> is called <code>Categorical</code> instead. Change your line to:</p>

<pre><code>df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
</code></pre>
";;0;;2014-02-10T22:50:31.283;;21689542;2015-05-06T16:47:43.817;2015-05-06T16:47:43.817;;31676.0;;712603.0;21689423.0;2;42;;;
10341;10341;;;"<p>Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.</p>

<pre><code>df = pd.DataFrame({
    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],
    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],
    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],
    'count' : [3,2,5,8,10,1,2,2,7]
    })

df_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})

df_grouped = df_grouped.reset_index()

df_grouped = df_grouped.rename(columns={'count':'count_max'})

df = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])

df = df[df['count'] == df['count_max']]
</code></pre>
";;5;;2014-02-11T17:54:50.960;;21709413;2014-02-11T18:06:24.633;2014-02-11T18:06:24.633;;1889456.0;;1889456.0;15705630.0;2;18;;;
10355;10355;;;"<p>You can see what the dtype is for all the columns using the dtypes attribute:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 'a', 2.]])

In [12]: df
Out[12]: 
   0  1  2
0  1  a  2

In [13]: df.dtypes
Out[13]: 
0      int64
1     object
2    float64
dtype: object

In [14]: df.dtypes == object
Out[14]: 
0    False
1     True
2    False
dtype: bool
</code></pre>

<p>To access the object columns:</p>

<pre><code>In [15]: df.loc[:, df.dtypes == object]
Out[15]: 
   1
0  a
</code></pre>

<p>I think it's most explicit to use (I'm not <em>sure</em> that inplace would work here):</p>

<pre><code>In [16]: df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].fillna('')
</code></pre>

<p>Saying that, I recommend you use <a href=""https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none/17534682#17534682"">NaN for missing data</a>.</p>
";;1;;2014-02-12T06:17:25.210;;21720133;2014-02-12T06:17:25.210;2017-05-23T10:30:57.620;;-1.0;;1240268.0;21720022.0;2;13;;;
10377;10377;;;"<p>The pandas DataFrame provides a nice querying ability.</p>

<p>What you are trying to do can be done simply with:</p>

<pre><code># Set a default value
df['Age_Group'] = '&lt;40'
# Set Age_Group value for all row indexes which Age are greater than 40
df['Age_Group'][df['Age'] &gt; 40] = '&gt;40'
# Set Age_Group value for all row indexes which Age are greater than 18 and &lt; 40
df['Age_Group'][(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40)] = '&gt;18'
# Set Age_Group value for all row indexes which Age are less than 18
df['Age_Group'][df['Age'] &lt; 18] = '&lt;18'
</code></pre>

<p>The querying here is a powerful tool of the dataframe and will allow you to manipulate the DataFrame as you need.</p>

<p>For more complex conditionals, you can specify multiple conditions by encapsulating each condition in parenthesis and separating them with a boolean operator ( eg. '&amp;' or '|')</p>

<p>You can see this in work here for the second conditional statement for setting >18.</p>

<p>Edit:</p>

<p>You can read more about indexing of DataFrame and conditionals:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#index-objects"">http://pandas.pydata.org/pandas-docs/dev/indexing.html#index-objects</a></p>

<p>Edit:</p>

<p>To see how it works:</p>

<pre><code>&gt;&gt;&gt; d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }
&gt;&gt;&gt; df = pd.DataFrame(d)
&gt;&gt;&gt; df
   Age
0   36
1   42
2    6
3   66
4   38
&gt;&gt;&gt; df['Age_Group'] = '&lt;40'
&gt;&gt;&gt; df['Age_Group'][df['Age'] &gt; 40] = '&gt;40'
&gt;&gt;&gt; df['Age_Group'][(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40)] = '&gt;18'
&gt;&gt;&gt; df['Age_Group'][df['Age'] &lt; 18] = '&lt;18'
&gt;&gt;&gt; df
   Age Age_Group
0   36       &gt;18
1   42       &gt;40
2    6       &lt;18
3   66       &gt;40
4   38       &gt;18
</code></pre>

<p>Edit:</p>

<p>To see how to do this without the chaining [using EdChums approach].</p>

<pre><code>&gt;&gt;&gt; df['Age_Group'] = '&lt;40'
&gt;&gt;&gt; df.loc[df['Age'] &lt; 40,'Age_Group'] = '&lt;40'
&gt;&gt;&gt; df.loc[(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40), 'Age_Group'] = '&gt;18'
&gt;&gt;&gt; df.loc[df['Age'] &lt; 18,'Age_Group'] = '&lt;18'
&gt;&gt;&gt; df
   Age Age_Group
0   36       &gt;18
1   42       &lt;40
2    6       &lt;18
3   66       &lt;40
4   38       &gt;18
</code></pre>
";;3;;2014-02-12T16:49:27.047;;21734254;2014-02-12T17:03:51.153;2014-02-12T17:03:51.153;;1339024.0;;1339024.0;21733893.0;2;40;;;
10385;10385;;;"<p>If you want a string <code>mm/dd/yyyy</code> instead of the <code>datetime</code> object, you can use <code>strftime</code> (string format time):</p>

<pre><code>&gt;&gt;&gt; dt.datetime.today().strftime(""%m/%d/%Y"")
                   # ^ note parentheses
'02/12/2014'
</code></pre>
";;6;;2014-02-12T20:16:42.133;;21738682;2014-02-12T20:16:42.133;;;;;3001761.0;21738566.0;2;31;;;
10399;10399;;;"<p>you can predict the future values using Pandas Exponentially-weighted moving average <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.stats.moments.ewma.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.stats.moments.ewma.html</a>  as</p>

<pre><code>from pandas.stats.moments import ewma
import numpy as np

pred_period = 12

def predict(x,span,periods = pred_period):     
    x_predict = np.zeros((span+periods,))
    x_predict[:span] = x[-span:]
    pred =  ewma(x_predict,span)[span:]

    return pred
</code></pre>
";;0;;2014-02-13T10:29:15.343;;21751529;2014-02-13T10:29:15.343;;;;;2374707.0;12726432.0;2;6;;;
10405;10405;;;"<p>Append a totals row with</p>

<pre><code>df.append(df.sum(numeric_only=True), ignore_index=True)
</code></pre>

<p>The conversion is necessary only if you have a column of strings or objects.</p>

<p>It's a bit of a fragile solution so I'd recommend sticking to operations on the dataframe, though. eg.</p>

<pre><code>baz = 2*df['qux'].sum() + 3*df['bar'].sum()
</code></pre>
";;4;;2014-02-13T13:32:43.817;;21755752;2014-02-14T09:50:43.680;2014-02-14T09:50:43.680;;751572.0;;751572.0;21752399.0;2;15;;;
10438;10438;;;"<p>Method #1: <code>reset_index()</code></p>

<pre><code>&gt;&gt;&gt; g
              uses  books
               sum    sum
token   year             
xanthos 1830     3      3
        1840     3      3
        1868     2      2
        1875     1      1

[4 rows x 2 columns]
&gt;&gt;&gt; g = g.reset_index()
&gt;&gt;&gt; g
     token  year  uses  books
                   sum    sum
0  xanthos  1830     3      3
1  xanthos  1840     3      3
2  xanthos  1868     2      2
3  xanthos  1875     1      1

[4 rows x 4 columns]
</code></pre>

<p>Method #2: don't make the index in the first place, using <code>as_index=False</code></p>

<pre><code>&gt;&gt;&gt; g = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year'], as_index=False).sum()
&gt;&gt;&gt; g
     token  year  uses  books
0  xanthos  1830     3      3
1  xanthos  1840     3      3
2  xanthos  1868     2      2
3  xanthos  1875     1      1

[4 rows x 4 columns]
</code></pre>
";;1;;2014-02-13T23:42:13.130;;21768034;2014-02-13T23:42:13.130;;;;;487339.0;21767900.0;2;18;;;
10448;10448;;;"<p>Sorry about the confusion, this should be the correct approach. Do you want only to capture <code>'bad'</code> only, not things like <code>'good'</code>; Or just any non-numerical values?</p>

<pre><code>In[15]:
np.where(np.any(np.isnan(df.convert_objects(convert_numeric=True)), axis=1))
Out[15]:
(array([3]),)
</code></pre>
";;3;;2014-02-14T05:22:59.577;;21771438;2014-02-14T05:22:59.577;;;;;2487184.0;21771133.0;2;6;;;
10450;10450;;;"<p>You could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.isreal.html"" rel=""nofollow noreferrer""><code>np.isreal</code></a> to check the type of each element (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html"" rel=""nofollow noreferrer"">applymap</a> applies a function to each element in the DataFrame):</p>

<pre><code>In [11]: df.applymap(np.isreal)
Out[11]:
          a     b
item
a      True  True
b      True  True
c      True  True
d     False  True
e      True  True
</code></pre>

<p>If all in the row are True then they are all numeric:</p>

<pre><code>In [12]: df.applymap(np.isreal).all(1)
Out[12]:
item
a        True
b        True
c        True
d       False
e        True
dtype: bool
</code></pre>

<p>So to get the subDataFrame of rouges,  (Note: the negation, ~,  of the above finds the ones which have at least one rogue non-numeric):</p>

<pre><code>In [13]: df[~df.applymap(np.isreal).all(1)]
Out[13]:
        a    b
item
d     bad  0.4
</code></pre>

<p>You could also find the location of the <em>first</em> offender you could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html"" rel=""nofollow noreferrer"">argmin</a>:</p>

<pre><code>In [14]: np.argmin(df.applymap(np.isreal).all(1))
Out[14]: 'd'
</code></pre>

<p>As <a href=""https://stackoverflow.com/users/2487184/ct-zhu"">@CTZhu</a> points out, it may be slightly faster to <a href=""http://docs.python.org/2/library/functions.html#isinstance"" rel=""nofollow noreferrer"">check whether it's an instance of</a> either int or float (there is some additional overhead with np.isreal):</p>

<pre><code>df.applymap(lambda x: isinstance(x, (int, float)))
</code></pre>
";;2;;2014-02-14T06:13:00.133;;21772078;2014-02-14T06:34:17.833;2017-05-23T11:47:26.130;;-1.0;;1240268.0;21771133.0;2;27;;;
10467;10467;;;"<p>Merge them in two steps, <code>df1</code> and <code>df2</code> first, and then the result of that to <code>df3</code>.</p>

<pre><code>In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])
</code></pre>

<p>I dropped year from df3 since you don't need it for the last join.</p>

<pre><code>In [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],
                       how='left', on=['Week', 'Colour'])

In [40]: df
Out[40]: 
   Year Week Colour  Val1  Val2 Val3
0  2014    A    Red    50   NaN  NaN
1  2014    B    Red    60   NaN   60
2  2014    B  Black    70   100   10
3  2014    C    Red    10    20  NaN
4  2014    D  Green    20   NaN   20

[5 rows x 6 columns]
</code></pre>
";;2;;2014-02-14T18:52:57.897;;21787325;2014-02-14T20:22:34.023;2014-02-14T20:22:34.023;;1889400.0;;1889400.0;21786490.0;2;53;;;
10488;10488;;;"<p><code>df.iloc[i]</code> returns the <code>ith</code> row of <code>df</code>. <code>i</code> does not refer to the index label, <code>i</code> is a 0-based index.</p>

<p>In contrast, <strong>the attribute <code>index</code> returns actual index labels</strong>, not numeric row-indices:</p>

<pre><code>df[df['BoolCol'] == True].index.tolist()
</code></pre>

<p>or equivalently,</p>

<pre><code>df[df['BoolCol']].index.tolist()
</code></pre>

<p>You can see the difference quite clearly by playing with a DataFrame with
an ""unusual"" index:</p>

<pre><code>df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},
       index=[10,20,30,40,50])

In [53]: df
Out[53]: 
   BoolCol
10    True
20   False
30   False
40    True
50    True

[5 rows x 1 columns]

In [54]: df[df['BoolCol']].index.tolist()
Out[54]: [10, 40, 50]
</code></pre>

<hr>

<p><strong>If you want to use the index labels</strong>, </p>

<pre><code>In [56]: idx = df[df['BoolCol']].index.tolist()

In [57]: idx
Out[57]: [10, 40, 50]
</code></pre>

<p><strong>then you can select the rows using <code>loc</code> instead of <code>iloc</code></strong>:</p>

<pre><code>In [58]: df.loc[idx]
Out[58]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]
</code></pre>

<hr>

<p>Note that <strong><code>loc</code> can also accept boolean arrays</strong>:</p>

<pre><code>In [55]: df.loc[df['BoolCol']]
Out[55]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]
</code></pre>

<hr>

<p><strong>If you have a boolean array, <code>mask</code>, and need ordinal index values, you can compute them using <code>np.flatnonzero</code></strong>:</p>

<pre><code>In [110]: np.flatnonzero(df['BoolCol'])
Out[112]: array([0, 3, 4])
</code></pre>

<p>Use <code>df.iloc</code> to select rows by ordinal index:</p>

<pre><code>In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]
Out[113]: 
   BoolCol
10    True
40    True
50    True
</code></pre>
";;4;;2014-02-15T16:28:46.313;;21800319;2017-04-14T19:05:13.930;2017-04-14T19:05:13.930;;190597.0;;190597.0;21800169.0;2;120;;;
10575;10575;;;"<p>A quicker option might be to set the pandas dataframe so it doesn't line wrap by putting this line of code:</p>

<pre><code>import pandas
pandas.set_option('expand_frame_repr', False)
</code></pre>

<p>I'm using Sublime Text 2 and this is how it looks:</p>

<p><strong>Before putting in option</strong> (Notice how the output wraps the text around)
<img src=""https://i.stack.imgur.com/AEThF.png"" alt=""Before""></p>

<p><strong>After putting in option</strong> (Notice how the output continues)
<img src=""https://i.stack.imgur.com/vBXSt.png"" alt=""After""></p>

<p>Also make sure that 'View' > 'Word Wrap' is not checked.</p>

<p>Additionally, you can print out more or less as you need by using <em>head(#)</em> like this:</p>

<pre><code>mydf = pandas.DataFrame.from_csv('myfile.csv', header=1)
print mydf.head(20) # Prints first 20 lines
</code></pre>

<p>Here's some other pandas options:</p>

<pre><code>pandas.set_option('display.max_columns', 0) # Display any number of columns
pandas.set_option('display.max_rows', 0) # Display any number of rows
</code></pre>
";;4;;2014-02-19T01:21:44.510;;21869063;2014-04-13T02:44:42.573;2014-04-13T02:44:42.573;;464923.0;;464923.0;21834676.0;2;8;;;
10603;10603;;;"<p>Judging by your data it looks like the delimiter you're using is a <code></code>. </p>

<p>Try the following:</p>

<pre><code>a = pandas.DataFrame.from_csv('st1.csv', sep=' ')
</code></pre>

<p>The other issue is that it's assuming your first column is an index, which we can also disable:</p>

<pre><code>a = pandas.DataFrame.from_csv('st1.csv', index_col=None)
</code></pre>
";;5;;2014-02-20T08:30:39.673;;21902162;2017-05-10T15:06:09.123;2017-05-10T15:06:09.123;;1401034.0;;1401034.0;21902080.0;2;31;;;
10616;10616;;;"<p>You can just use the pd.Timestamp constructor. The following diagram may be useful for this and related questions. </p>

<p><img src=""https://i.stack.imgur.com/uiXQd.png"" alt=""Conversions between time representations""></p>
";;7;;2014-02-20T18:17:06.327;;21916253;2014-02-20T18:17:06.327;;;;;2868577.0;13703720.0;2;109;;;
10657;10657;;;"<p>You need to reshape your array first, try this:</p>

<pre><code>px2 = px.reshape((-1,3))
df = pd.DataFrame({'R':px2[:,0],'G':px2[:,1],'B':px2[:,2]})
</code></pre>
";;0;;2014-02-21T16:41:56.587;;21940107;2014-02-21T16:41:56.587;;;;;2997574.0;21938932.0;2;20;;;
10663;10663;;;"<p>I think <code>df.replace()</code> does the job:</p>

<pre><code>df = pd.DataFrame([
    [-0.532681, 'foo', 0],
    [1.490752, 'bar', 1],
    [-1.387326, 'foo', 2],
    [0.814772, 'baz', ' '],     
    [-0.222552, '   ', 4],
    [-1.176781,  'qux', '  '],         
], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))

print df.replace(r'\s+', np.nan, regex=True)
</code></pre>

<p>Produces:</p>

<pre><code>                   A    B   C
2000-01-01 -0.532681  foo   0
2000-01-02  1.490752  bar   1
2000-01-03 -1.387326  foo   2
2000-01-04  0.814772  baz NaN
2000-01-05 -0.222552  NaN   4
2000-01-06 -1.176781  qux NaN
</code></pre>
";;7;;2014-02-21T18:48:53.760;;21942746;2014-02-21T23:43:41.240;2014-02-21T23:43:41.240;;2336373.0;;2336373.0;13445241.0;2;60;;;
10694;10694;;;"<p>If I remember correctly, the plotting code only considers numeric columns. Internally it selects just the numeric columns, so that's why you get the key error. </p>

<p>What's the dtype of <code>date</code>? If it's a <code>datetime64</code>, you can recast it as an <code>np.int64</code>:</p>

<pre><code>df['date_int'] = df.date.astype(np.int64)
</code></pre>

<p>And then you're plot.</p>

<p>For the color part, make a dictionary of <code>{account number: color}</code>. For example:</p>

<pre><code>color_d = {1: 'k', 2: 'b', 3: 'r'}
</code></pre>

<p>Then when you plot:</p>

<pre><code>training.plot(kind='scatter',x='date',y='rate', color=df.account.map(color_d))
</code></pre>
";;3;;2014-02-22T22:30:24.990;;21961491;2014-02-22T22:30:24.990;;;;;1889400.0;21961360.0;2;11;;;
10734;10734;;;"<p>This is indeed a bit confusing. I think it boils down to how Matplotlib handles the secondary axes. Pandas probably calls <code>ax.twinx()</code> somewhere which superimposes a secondary axes on the first one, but this is actually a separate axes. Therefore also with separate lines &amp; labels and a separate legend. Calling <code>plt.legend()</code> only applies to one of the axes (the active one) which in your example is the second axes.</p>

<p>Pandas fortunately does store both axes, so you can grab all line objects from both of them and pass them to the <code>.legend()</code> command yourself. Given your example data:</p>

<p>You can plot exactly as you did:</p>

<pre><code>ax = var.total.plot(label='Variance')
ax = shares.average.plot(secondary_y=True, label='Average Age')

ax.set_ylabel('Variance of log wages')
ax.right_ax.set_ylabel('Average age')
</code></pre>

<p>Both axes objects are available with <code>ax</code> (left axe) and <code>ax.right_ax</code>, so you can grab the line objects from them. Matplotlib's <code>.get_lines()</code> return a list so you can merge them by simple addition.</p>

<pre><code>lines = ax.get_lines() + ax.right_ax.get_lines()
</code></pre>

<p>The line objects have a label property which can be used to read and pass the label to the <code>.legend()</code> command.</p>

<pre><code>ax.legend(lines, [l.get_label() for l in lines], loc='upper center')
</code></pre>

<p>And the rest of the plotting:</p>

<pre><code>ax.set_title('Wage Variance and Mean Age')
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/0C4Uj.png"" alt=""enter image description here""></p>

<h3>edit:</h3>

<p>It might be less confusing if you separate the Pandas (data) and the Matplotlib (plotting) parts more strictly, so avoid using the Pandas build-in plotting (which only wraps Matplotlib anyway):</p>

<pre><code>fig, ax = plt.subplots()

ax.plot(var.index.to_datetime(), var.total, 'b', label='Variance')
ax.set_ylabel('Variance of log wages')

ax2 = ax.twinx()
ax2.plot(shares.index.to_datetime(), shares.average, 'g' , label='Average Age')
ax2.set_ylabel('Average age')

lines = ax.get_lines() + ax2.get_lines()
ax.legend(lines, [line.get_label() for line in lines], loc='upper center')

ax.set_title('Wage Variance and Mean Age')
plt.show()
</code></pre>
";;0;;2014-02-24T13:28:19.510;;21989204;2016-10-14T08:32:36.303;2016-10-14T08:32:36.303;;3805131.0;;1755432.0;21988196.0;2;21;;;
10768;10768;;;"<p>One way to convert to string is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""noreferrer"">astype</a>:</p>

<pre><code>total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)
</code></pre>

<p>However, perhaps you are looking for the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html"" rel=""noreferrer""><code>to_json</code></a> function, which will convert keys to valid json (and therefore your keys to strings):</p>

<pre><code>In [11]: df = pd.DataFrame([['A', 2], ['A', 4], ['B', 6]])

In [12]: df.to_json()
Out[12]: '{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}'

In [13]: df[0].to_json()
Out[13]: '{""0"":""A"",""1"":""A"",""2"":""B""}'
</code></pre>

<p><em>Note: you can pass in a buffer/file to save this to, along with some other options...</em></p>
";;1;;2014-02-25T06:38:55.250;;22006514;2014-02-25T06:38:55.250;;;;;1240268.0;22005911.0;2;80;;;
10788;10788;;;"<p>Also, Given a DataFrame </p>

<blockquote>
  <p>data</p>
</blockquote>

<p>as in your example, if you would like to extract column a and d only (e.i. the 1st and the 4th column), iloc mothod from the pandas dataframe is what you need and could be used very effectively. All you need to know is the index of the columns you would like to extract. For example:</p>

<pre><code>&gt;&gt;&gt; data.iloc[:,[0,3]]
</code></pre>

<p>will give you</p>

<pre><code>          a         d
0  0.883283  0.100975
1  0.614313  0.221731
2  0.438963  0.224361
3  0.466078  0.703347
4  0.955285  0.114033
5  0.268443  0.416996
6  0.613241  0.327548
7  0.370784  0.359159
8  0.692708  0.659410
9  0.806624  0.875476
</code></pre>
";;0;;2014-02-25T15:33:29.497;;22018873;2014-02-25T15:33:29.497;;;;;959876.0;10665889.0;2;28;;;
10791;10791;;;"<p>The way to select specific columns is this -</p>

<pre><code>header = [""InviteTime (Oracle)"", ""Orig Number"", ""Orig IP Address"", ""Dest Number""]
df.to_csv('output.csv', columns = header)
</code></pre>
";;4;;2014-02-25T16:11:05.657;;22019831;2015-04-25T13:05:24.960;2015-04-25T13:05:24.960;;434337.0;;1827356.0;22019763.0;2;27;;;
10817;10817;;;"<p>This will remove all rows which are all zeros, or all nans:</p>

<pre><code>mask = np.all(np.isnan(arr), axis=1) | np.all(arr == 0, axis=1)
arr = arr[~mask]
</code></pre>

<p>And this will remove all rows which are all either zeros or nans:</p>

<pre><code>mask = np.all(np.isnan(arr) | arr == 0, axis=1)
arr = arr[~mask]
</code></pre>
";;0;;2014-02-26T06:16:16.747;;22033314;2014-02-26T06:16:16.747;;;;;110026.0;22032668.0;2;6;;;
10818;10818;;;"<pre><code>import numpy as np

a = np.array([
    [1, 0, 0],
    [0, np.nan, 0],
    [0, 0, 0],
    [np.nan, np.nan, np.nan],
    [2, 3, 4]
])

mask = np.all(np.isnan(a) | np.equal(a, 0), axis=1)
a[~mask]
</code></pre>
";;1;;2014-02-26T06:18:48.067;;22033364;2014-02-26T06:18:48.067;;;;;772649.0;22032668.0;2;13;;;
10869;10869;;;"<p>I think what you want to do is be able to display a legend for a subset of the lines on your plot. This should do it:</p>

<pre><code>df = pd.DataFrame(np.random.randn(400, 4), columns=['one', 'two', 'three', 'four'])
ax1 = df.cumsum().plot()
lines, labels = ax1.get_legend_handles_labels()
ax1.legend(lines[:2], labels[:2], loc='best')  # legend for first two lines only
</code></pre>

<p>Giving</p>

<p><img src=""https://i.stack.imgur.com/m4l4j.png"" alt=""enter image description here""></p>
";;3;;2014-02-27T14:06:13.413;;22070926;2014-02-27T14:06:13.413;;;;;751572.0;22070263.0;2;30;;;
10889;10889;;;"<p>The way to get the previous is using the shift method:</p>

<pre><code>In [11]: df1.change.shift(1)
Out[11]:
0          NaT
1   2014-03-08
2   2014-04-08
3   2014-05-08
4   2014-06-08
Name: change, dtype: datetime64[ns]
</code></pre>

<p>Now you can subtract these columns. <em>Note: This is with 0.13.1 (datetime stuff has had a lot of work recently, so YMMV with older versions).</em></p>

<pre><code>In [12]: df1.change.shift(1) - df1.change
Out[12]:
0        NaT
1   -31 days
2   -30 days
3   -31 days
4     0 days
Name: change, dtype: timedelta64[ns]
</code></pre>

<p>You can just apply this to each case/group:</p>

<pre><code>In [13]: df.groupby('case')['change'].apply(lambda x: x.shift(1) - x)
Out[13]:
0        NaT
1   -31 days
2   -30 days
3   -31 days
4        NaT
dtype: timedelta64[ns]
</code></pre>
";;2;;2014-02-27T23:04:08.550;;22082596;2014-02-27T23:04:08.550;;;;;1240268.0;22081878.0;2;29;;;
10898;10898;;;"<p>A dict is to a DataFrame as a bicycle is to a car.
You can pedal 10 feet on a bicycle faster than you can start a car, get it in gear, etc, etc. But if you need to go a mile, the car wins.</p>

<p>For certain small, targeted purposes, a dict may be faster.
And if that is all you need, then use a dict, for sure! But if you need/want the power and luxury of a DataFrame, then a dict is no substitute. It is meaningless to compare speed if the data structure does not first satisfy your needs.</p>

<p>Now for example -- to be more concrete -- a dict is good for accessing columns, but it is not so convenient for accessing rows. </p>

<pre><code>import timeit

setup = '''
import numpy, pandas
df = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))
dictionary = df.to_dict()
'''

# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']
f = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']

for func in f:
    print(func)
    print(min(timeit.Timer(func, setup).repeat(3, 100000)))
</code></pre>

<p>yields</p>

<pre><code>value = [val[5] for col,val in dictionary.iteritems()]
25.5416321754
value = df.loc[5]
5.68071913719
value = df.iloc[5]
4.56006002426
</code></pre>

<p>So the dict of lists is 5 times slower at retrieving rows than <code>df.iloc</code>. The speed deficit becomes greater as the number of columns grows. (The number of columns is like the number of feet in the bicycle analogy. The longer the distance, the more convenient the car becomes...)</p>

<p>This is just one example of when a dict of lists would be less convenient/slower than a DataFrame.</p>

<p>Another example would be when you have a DatetimeIndex for the rows and wish to select all rows between certain dates. With a DataFrame you can use</p>

<pre><code>df.loc['2000-1-1':'2000-3-31']
</code></pre>

<p>There is no easy analogue for that if you were to use a dict of lists. And the Python loops you would need to use to select the right rows would again be terribly slow compared to the DataFrame.</p>
";;2;;2014-02-28T02:02:17.790;;22084742;2017-04-19T13:31:44.150;2017-04-19T13:31:44.150;;2254228.0;;190597.0;22084338.0;2;41;;;
10902;10902;;;"<p>Using <code>&amp;</code> operator, don't forget to wrap the sub-statements with <code>()</code>:</p>

<pre><code>males = df[(df[Gender]=='Male') &amp; (df[Year]==2014)]
</code></pre>

<p>To store your dataframes in a <code>dict</code> using a for loop:</p>

<pre><code>from collections import defaultdict
dic={}
for g in ['male', 'female']:
  dic[g]=defaultdict(dict)
  for y in [2013, 2014]:
    dic[g][y]=df[(df[Gender]==g) &amp; (df[Year]==y)] #store the DataFrames to a dict of dict
</code></pre>

<h2>EDIT:</h2>

<p>A demo for your <code>getDF</code>:</p>

<pre><code>def getDF(dic, gender, year):
  return dic[gender][year]

print genDF(dic, 'male', 2014)
</code></pre>
";;10;;2014-02-28T04:40:28.680;;22086347;2014-02-28T05:37:15.293;2014-02-28T05:37:15.293;;1150712.0;;1150712.0;22086116.0;2;40;;;
10909;10909;;;"<p>You need to set <code>index=False</code> in <code>to_excel</code> in order for it to not write the index column out, this semantic is followed in other Pandas IO tools, see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/io.html</a></p>
";;1;;2014-02-28T08:26:51.970;;22089870;2014-02-28T08:26:51.970;;;;;704848.0;22089317.0;2;17;;;
10945;10945;;;"<p>""pip install --upgrade pandas""  did not work for me on a fresh Ubuntu: 12.04.2 LTS Desktop instance. Within Python, pandas was still showing version  0.7.0. </p>

<p>Instead, I was able to get the update through by using easy install:</p>

<pre><code>sudo easy_install -U pandas
</code></pre>
";;2;;2014-02-28T19:20:58.597;;22104034;2014-02-28T19:20:58.597;;;;;3062149.0;17759128.0;2;7;;;
10987;10987;;;"<p>there are a few ways;<br>
using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot"" rel=""noreferrer""><code>.pivot</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.pivot(index='label', columns='type')['value']
type   a  b  c
label         
x      1  2  3
y      4  5  6
z      7  8  9

[3 rows x 3 columns]
</code></pre>

<p>using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html#pandas.pivot_table"" rel=""noreferrer""><code>pivot_table</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.pivot_table(values='value', index='label', columns='type')
       value      
type       a  b  c
label             
x          1  2  3
y          4  5  6
z          7  8  9

[3 rows x 3 columns]
</code></pre>

<p>or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""noreferrer""><code>.groupby</code></a> followed by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack"" rel=""noreferrer""><code>.unstack</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()
type   a  b  c
label         
x      1  2  3
y      4  5  6
z      7  8  9

[3 rows x 3 columns]
</code></pre>
";;0;;2014-03-02T12:44:16.630;;22127685;2015-10-10T02:22:58.427;2015-10-10T02:22:58.427;;625914.0;;625914.0;22127569.0;2;30;;;
10995;10995;;;"<p>Assuming these were datetime columns (if they're not apply <code>to_datetime</code>) you can just subtract them:</p>

<pre><code>df['A'] = pd.to_datetime(df['A'])
df['B'] = pd.to_datetime(df['B'])

In [11]: df.dtypes  # if already datetime64 you don't need to use to_datetime
Out[11]:
A    datetime64[ns]
B    datetime64[ns]
dtype: object

In [12]: df['A'] - df['B']
Out[12]:
one   -58 days
two   -26 days
dtype: timedelta64[ns]

In [13]: df['C'] = df['A'] - df['B']

In [14]: df
Out[14]:
             A          B        C
one 2014-01-01 2014-02-28 -58 days
two 2014-02-03 2014-03-01 -26 days
</code></pre>

<p><em>Note: ensure you're using a new of pandas (e.g. 0.13.1), this may not work in older versions.</em></p>
";;6;;2014-03-02T19:59:06.983;;22132649;2014-03-02T19:59:06.983;;;;;1240268.0;22132525.0;2;30;;;
10999;10999;;;"<p>If you are trying to save memory, then reading the file a line at a time will be much more memory efficient:</p>

<pre><code>with open('test.json') as f:
    data = pd.DataFrame(json.loads(line) for line in f)
</code></pre>

<p>Also, if you <code>import simplejson as json</code>, the compiled C extensions included with <code>simplejson</code> are much faster than the pure-Python <code>json</code> module.</p>
";;2;;2014-03-02T23:47:40.937;;22135309;2014-03-02T23:47:40.937;;;;;171236.0;20037430.0;2;16;;;
11003;11003;;;"<p>You need to <a href=""https://stackoverflow.com/a/2308488/1240268"">set the locale</a> first:</p>

<pre><code>In [ 9]: import locale

In [10]: from locale import atof

In [11]: locale.setlocale(locale.LC_NUMERIC, '')
Out[11]: 'en_GB.UTF-8'

In [12]: df.applymap(atof)
Out[12]:
      0        1
0  1200  4200.00
1  7000    -0.03
2     5     0.00
</code></pre>

<p>If you're <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""nofollow noreferrer"">reading in from csv</a> then you can use the <a href=""https://stackoverflow.com/a/20667788/1240268"">thousands arg</a>:</p>

<pre><code>df.read_csv('foo.tsv', sep='\t', thousands=',')
</code></pre>
";;7;;2014-03-03T02:54:41.863;;22137890;2016-11-24T18:44:40.140;2017-05-23T12:18:21.853;;-1.0;;1240268.0;22137723.0;2;38;;;
11033;11033;;;"<p>It specifies the axis <strong>along which</strong> the means are computed. By default <code>axis=0</code>. This is consistent with the <code>numpy.mean</code> usage when <code>axis</code> is specified <em>explicitly</em> (in <code>numpy.mean</code>, axis==None by default, which computes the mean value over the flattened array) , in which <code>axis=0</code> along the <em>rows</em> (namely, <em>index</em> in pandas), and <code>axis=1</code> along the <em>columns</em>.</p>

<pre><code>+------------+---------+--------+
|            |  A      |  B     |
+------------+---------+---------
|      0     | 0.626386| 1.52325|----axis=1-----&gt;
+------------+---------+--------+
                |         |
                | axis=0  |
                ?         ?
</code></pre>
";;4;;2014-03-03T14:55:35.040;;22149930;2014-03-03T15:43:36.843;2014-03-03T15:43:36.843;;1150712.0;;1150712.0;22149584.0;2;130;;;
11049;11049;;;"<p>For Requirement 1, just define <code>_constructor</code>:</p>

<pre><code>import pandas as pd
import numpy as np

class MyDF(pd.DataFrame):
    @property
    def _constructor(self):
        return MyDF


mydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])
print type(mydf)

mydf_sub = mydf[['A','C']]
print type(mydf_sub)
</code></pre>

<p>I think there is no simple solution for Requirement 2, I think you need define <code>__init__</code>, <code>copy</code>, or do something in <code>_constructor</code>, for example:</p>

<pre><code>import pandas as pd
import numpy as np

class MyDF(pd.DataFrame):
    _attributes_ = ""myattr1,myattr2""

    def __init__(self, *args, **kw):
        super(MyDF, self).__init__(*args, **kw)
        if len(args) == 1 and isinstance(args[0], MyDF):
            args[0]._copy_attrs(self)

    def _copy_attrs(self, df):
        for attr in self._attributes_.split("",""):
            df.__dict__[attr] = getattr(self, attr, None)

    @property
    def _constructor(self):
        def f(*args, **kw):
            df = MyDF(*args, **kw)
            self._copy_attrs(df)
            return df
        return f

mydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])
print type(mydf)

mydf_sub = mydf[['A','C']]
print type(mydf_sub)

mydf.myattr1 = 1
mydf_cp1 = MyDF(mydf)
mydf_cp2 = mydf.copy()
print mydf_cp1.myattr1, mydf_cp2.myattr1
</code></pre>
";;1;;2014-03-04T01:22:35.410;;22161058;2014-03-04T01:22:35.410;;;;;772649.0;22155951.0;2;7;;;
11057;11057;;;"<p>Pandas 0.13 provides as an experimental feature:</p>

<p>PySide support for the qtpandas <code>DataFrameModel</code> and <code>DataFrameWidget</code></p>

<p>see <a href=""https://github.com/pydata/pandas/blob/master/doc/source/faq.rst"" rel=""noreferrer"">https://github.com/pydata/pandas/blob/master/doc/source/faq.rst</a></p>

<p>you can add this feature using</p>

<pre><code>from pandas.sandbox.qtpandas import DataFrameModel, DataFrameWidget
</code></pre>
";;2;;2014-03-04T08:07:48.387;;22166224;2014-03-04T08:07:48.387;;;;;2051311.0;10636024.0;2;6;;;
11086;11086;;;"<p>The following should work:</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x.to_html())
                                                                # ^^^^^^^^^
</code></pre>

<p>Check <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer"">the documentation</a> for additional options like CSS styling.</p>

<p>Additionally, you need to adjust your template like so:</p>

<pre class=""lang-html prettyprint-override""><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{data | safe}}
{% endblock %}
</code></pre>

<p>in order to tell Jinja you're passing in markup. Thanks to <a href=""https://stackoverflow.com/questions/22180993/pandas-dataframe-display-on-a-webpage/22181298?noredirect=1#comment33671934_22181298"">@SeanVieira</a> for the tip.</p>
";;10;;2014-03-04T19:37:04.907;;22181298;2014-03-04T22:04:30.187;2017-05-23T12:25:03.837;;-1.0;;1426065.0;22180993.0;2;28;;;
11133;11133;;;"<p>Dataframes have a <code>sort_index</code> method which returns a copy by default. Pass <code>inplace=True</code> to operate in place.</p>

<pre><code>import pandas as pd
df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])
df.sort_index(inplace=True)
print(df.to_string())
</code></pre>

<p>Gives me:</p>

<pre><code>     A
1    4
29   2
100  1
150  5
234  3
</code></pre>
";;1;;2014-03-05T23:41:50.340;;22211821;2014-03-05T23:41:50.340;;;;;1552748.0;22211737.0;2;40;;;
11152;11152;;;"<p>As you were saying the <code>groupby</code> method of a <code>pd.DataFrame</code> object can do the job.</p>

<p>Example</p>

<pre><code> L = ['A','A','B','B','B','C']
 N = [1,2,5,5,4,6]

 import pandas as pd
 df = pd.DataFrame(zip(L,N),columns = list('LN'))


 groups = df.groupby(df.L)

 groups.groups
      {'A': [0, 1], 'B': [2, 3, 4], 'C': [5]}
</code></pre>

<p>which gives and index-wise description of the groups.</p>

<p>To get elements of single groups, you can do, for instance</p>

<pre><code> groups.get_group('A')

     L  N
  0  A  1
  1  A  2

  groups.get_group('B')

     L  N
  2  B  5
  3  B  5
  4  B  4
</code></pre>
";;0;;2014-03-06T10:12:46.040;;22221272;2014-03-06T10:17:52.173;2014-03-06T10:17:52.173;;1714661.0;;1714661.0;22219004.0;2;6;;;
11153;11153;;;"<p>You can do this using <code>groupby</code> to group on the column of interest and then <code>apply</code> <code>list</code> to every group:</p>

<pre><code>In [1]:
# create the dataframe    
df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})
df
Out[1]:
   a  b
0  A  1
1  A  2
2  B  5
3  B  5
4  B  4
5  C  6

[6 rows x 2 columns]

In [76]:
df.groupby('a')['b'].apply(list)

Out[76]:
a
A       [1, 2]
B    [5, 5, 4]
C          [6]
Name: b, dtype: object
</code></pre>
";;9;;2014-03-06T10:28:32.803;;22221675;2016-09-28T12:09:22.400;2016-09-28T12:09:22.400;;704848.0;;704848.0;22219004.0;2;62;;;
11173;11173;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.MultiIndex.droplevel.html"" rel=""noreferrer""><code>MultiIndex.droplevel</code></a>:</p>

<pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
&gt;&gt;&gt; df = pd.DataFrame([[1,2], [3,4]], columns=cols)
&gt;&gt;&gt; df
   a   
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]
&gt;&gt;&gt; df.columns = df.columns.droplevel()
&gt;&gt;&gt; df
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]
</code></pre>
";;1;;2014-03-06T19:08:47.153;;22233719;2016-03-28T16:32:58.657;2016-03-28T16:32:58.657;;2074981.0;;487339.0;22233488.0;2;97;;;
11174;11174;;;"<p>Ok, I have managed to get some very nice results by now combining the hints I got here. In the actual Python viewer I use</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x)
</code></pre>

<p>e.g. I send the complete dataframe to the html template. My html template is based on bootstrap. Hence I can simply write</p>

<pre><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{ data.to_html(classes=""table table-striped"") | safe}}
{% endblock %}
</code></pre>

<p>There are numerous other options with bootstrap, check out here:
<a href=""http://getbootstrap.com/css/#tables"" rel=""noreferrer"">http://getbootstrap.com/css/#tables</a></p>

<p>Base.html is essentially copied from here
<a href=""http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-xii-facelift"" rel=""noreferrer"">http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-xii-facelift</a></p>

<p>The next question is obviously how to plot such a frame. Anyone any experience with Bokeh?</p>

<p>Thank you both to Matt and Sean.</p>

<p>thomas</p>
";;1;;2014-03-06T19:15:57.710;;22233851;2014-03-06T19:15:57.710;;;;;1695486.0;22180993.0;2;15;;;
11182;11182;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html#pandas.DataFrame.describe""><code>describe</code></a> may give you everything you want otherwise you can perform aggregations using groupby and pass a list of agg functions: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once"">http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once</a></p>

<pre><code>In [43]:

df.describe()

Out[43]:

       shopper_num is_martian  number_of_items  count_pineapples
count      14.0000         14        14.000000                14
mean        7.5000          0         3.357143                 0
std         4.1833          0         6.452276                 0
min         1.0000      False         0.000000                 0
25%         4.2500          0         0.000000                 0
50%         7.5000          0         0.000000                 0
75%        10.7500          0         3.500000                 0
max        14.0000      False        22.000000                 0

[8 rows x 4 columns]
</code></pre>

<p>Note that some columns cannot be summarised as there is no logical way to summarise them, for instance columns containing string data</p>

<p>As you prefer you can transpose the result if you prefer:</p>

<pre><code>In [47]:

df.describe().transpose()

Out[47]:

                 count      mean       std    min   25%  50%    75%    max
shopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14
is_martian          14         0         0  False     0    0      0  False
number_of_items     14  3.357143  6.452276      0     0    0    3.5     22
count_pineapples    14         0         0      0     0    0      0      0

[4 rows x 8 columns]
</code></pre>
";;0;;2014-03-06T20:36:18.490;;22235393;2014-03-06T21:06:08.960;2014-03-06T21:06:08.960;;704848.0;;704848.0;22235245.0;2;42;;;
11187;11187;;;"<p>In the particular case where you know the number of positions that you want to remove from the dataframe column, you can use string indexing inside a lambda function to get rid of that parts:</p>

<p>Last character:</p>

<pre><code>data['result'] = data['result'].map(lambda x: str(x)[:-1])
</code></pre>

<p>First two characters:</p>

<pre><code>data['result'] = data['result'].map(lambda x: str(x)[2:])
</code></pre>
";;3;;2014-03-06T23:27:54.170;;22238380;2014-03-06T23:27:54.170;;;;;1825593.0;13682044.0;2;14;;;
11208;11208;;;"<p>use pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"">vectorized string methods</a>; as in the documentation:</p>

<blockquote>
  <p>these methods exclude missing/NA values automatically</p>
</blockquote>

<p><code>.str.lower()</code> is the very first example there;</p>

<pre><code>&gt;&gt;&gt; df['x'].str.lower()
0    one
1    two
2    NaN
Name: x, dtype: object
</code></pre>
";;2;;2014-03-07T10:30:48.400;;22247593;2014-12-01T23:56:21.020;2014-12-01T23:56:21.020;;625914.0;;625914.0;22245171.0;2;37;;;
11231;11231;;;"<p>Both <code>describe</code> and <code>info</code> report the count of non-missing values.</p>

<pre><code>In [1]: df = DataFrame(np.random.randn(10,2))

In [2]: df.iloc[3:6,0] = np.nan

In [3]: df
Out[3]: 
          0         1
0 -0.560342  1.862640
1 -1.237742  0.596384
2  0.603539 -1.561594
3       NaN  3.018954
4       NaN -0.046759
5       NaN  0.480158
6  0.113200 -0.911159
7  0.990895  0.612990
8  0.668534 -0.701769
9 -0.607247 -0.489427

[10 rows x 2 columns]

In [4]: df.describe()
Out[4]: 
              0          1
count  7.000000  10.000000
mean  -0.004166   0.286042
std    0.818586   1.363422
min   -1.237742  -1.561594
25%   -0.583795  -0.648684
50%    0.113200   0.216699
75%    0.636036   0.608839
max    0.990895   3.018954

[8 rows x 2 columns]


In [5]: df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 10 entries, 0 to 9
Data columns (total 2 columns):
0    7 non-null float64
1    10 non-null float64
dtypes: float64(2)
</code></pre>

<p>To get a count of missing, your soln is correct</p>

<pre><code>In [20]: len(df.index)-df.count()
Out[20]: 
0    3
1    0
dtype: int64
</code></pre>

<p>You could do this too</p>

<pre><code>In [23]: df.isnull().sum()
Out[23]: 
0    3
1    0
dtype: int64
</code></pre>
";;5;;2014-03-07T18:13:08.047;;22257615;2014-03-07T18:23:10.583;2014-03-07T18:23:10.583;;644898.0;;644898.0;22257527.0;2;20;;;
11233;11233;;;"<p>Use <code>.column_stack</code>. Like so:</p>

<pre><code>X = np.column_stack((X, AllAlexaAndGoogleInfo))
</code></pre>

<p>From the <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html"" rel=""noreferrer"">docs</a>:</p>

<blockquote>
  <p>Take a sequence of 1-D arrays and stack them as columns to make a
  single 2-D array. 2-D arrays are stacked as-is, just like with hstack.</p>
</blockquote>
";;5;;2014-03-07T18:37:35.853;;22258061;2014-03-07T18:37:35.853;;;;;1272394.0;22257836.0;2;8;;;
11236;11236;;;"<p>Assuming no header in the CSV file:</p>

<pre><code>import pandas
import random

n = 1000000 #number of records in file
s = 10000 #desired sample size
filename = ""data.txt""
skip = sorted(random.sample(xrange(n),n-s))
df = pandas.read_csv(filename, skiprows=skip)
</code></pre>

<p>would be better if read_csv had a keeprows, or if skiprows took a callback func instead of a list.</p>

<p>With header and unknown file length:</p>

<pre><code>import pandas
import random

filename = ""data.txt""
n = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)
s = 10000 #desired sample size
skip = sorted(random.sample(xrange(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list
df = pandas.read_csv(filename, skiprows=skip)
</code></pre>
";;2;;2014-03-07T19:29:08.653;;22259008;2016-01-15T22:06:31.050;2016-01-15T22:06:31.050;;748925.0;;748925.0;22258491.0;2;27;;;
11247;11247;;;"<p>As <code>X</code> is a sparse array, instead of <code>numpy.hstack</code>, use <code>scipy.sparse.hstack</code> to join the arrays. In my opinion the error message is kind of misleading here.</p>

<p>This minimal example illustrates the situation:</p>

<pre><code>import numpy as np
from scipy import sparse

X = sparse.rand(10, 10000)
xt = np.random.random((10, 1))
print 'X shape:', X.shape
print 'xt shape:', xt.shape
print 'Stacked shape:', np.hstack((X,xt)).shape
#print 'Stacked shape:', sparse.hstack((X,xt)).shape #This works
</code></pre>

<p>Based on the following output</p>

<pre><code>X shape: (10, 10000)
xt shape: (10, 1)
</code></pre>

<p>one may expect that the <code>hstack</code> in the following line will work, but the fact is that it throws this error:</p>

<pre><code>ValueError: all the input arrays must have same number of dimensions
</code></pre>

<p>So, use <code>scipy.sparse.hstack</code> when you have a sparse array to stack.</p>

<hr>

<p>In fact I have answered this as a comment in your another questions, and you mentioned that another error message pops up:</p>

<pre><code>TypeError: no supported conversion for types: (dtype('float64'), dtype('O'))
</code></pre>

<p>First of all, <code>AllAlexaAndGoogleInfo</code> does not have a <code>dtype</code> as it is a <code>DataFrame</code>. To get it's underlying numpy array, simply use <code>AllAlexaAndGoogleInfo.values</code>. Check its <code>dtype</code>. Based on the error message, it has a <code>dtype</code> of <code>object</code>, which means that it might contain non-numerical elements like strings.</p>

<p>This is a minimal example that reproduces this situation:</p>

<pre><code>X = sparse.rand(100, 10000)
xt = np.random.random((100, 1))
xt = xt.astype('object') # Comment this to fix the error
print 'X:', X.shape, X.dtype
print 'xt:', xt.shape, xt.dtype
print 'Stacked shape:', sparse.hstack((X,xt)).shape
</code></pre>

<p>The error message:</p>

<pre><code>TypeError: no supported conversion for types: (dtype('float64'), dtype('O'))
</code></pre>

<p>So, check if there is any non-numerical values in <code>AllAlexaAndGoogleInfo</code> and repair them, before doing the stacking.</p>
";;1;;2014-03-08T02:46:12.243;;22264337;2014-03-08T02:46:12.243;;;;;1672128.0;22257836.0;2;8;;;
11271;11271;;;"<p>I'm unable to reproduce your problem but have you tried converting it to an integer first?</p>

<p><code>image_name_data['id'] = image_name_data['id'].astype(int).astype('str')</code></p>

<p>Then, regarding your more general question you could use <code>map</code> (<a href=""https://stackoverflow.com/questions/20937538/how-to-display-pandas-dataframe-using-a-format-string-for-columns"">as in this answer</a>). In your case:
<code>image_name_data['id'] = image_name_data['id'].map('{:.0f}'.format)</code></p>
";;4;;2014-03-09T00:08:09.330;;22276757;2014-03-09T00:08:09.330;2017-05-23T12:02:17.460;;-1.0;;3393459.0;22276503.0;2;37;;;
11380;11380;;;"<p>Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call <code>.tolist()</code> on to turn them into a Python list</p>

<pre><code>from pandas import *

d = {'one' : Series([1., 2., 3.], index=['a', 'b', 'c']),
    'two' : Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}

df = DataFrame(d)

#print df

print ""DF"", type(df['one']), ""\n"", df['one']

dfList = df['one'].tolist()

print ""DF list"", dfList, type(dfList)
</code></pre>

<p><a href=""https://stackoverflow.com/questions/14822680/convert-python-dataframe-to-list"">This question</a> might be helpful. And the <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html"" rel=""noreferrer"">Pandas docs</a> are actually quite good once you get your head around their style. </p>

<p>So in your case you could:</p>

<p><code>my_list = df[""cluster""].tolist()</code></p>

<p>and then go from there.</p>
";;8;;2014-03-12T03:24:24.527;;22341390;2017-03-09T19:09:11.823;2017-05-23T12:18:01.347;;-1.0;;1835727.0;22341271.0;2;109;;;
11425;11425;;;"<p>Yes, since pandas 0.14.0, it is now possible to merge a singly-indexed DataFrame with a level of a multi-indexed DataFrame using <code>.join</code>.</p>

<pre><code>df1.join(df2, how='inner')
</code></pre>

<p><a href=""http://pandas-docs.github.io/pandas-docs-travis/merging.html#merging-join-on-mi"" rel=""noreferrer"">The 0.14 pandas docs</a> describes this as equivalent but more memory efficient and faster than:</p>

<pre><code>merge(df1.reset_index(),
      df2.reset_index(),
      on=['index1'],
      how='inner'
     ).set_index(['index1','index2'])
</code></pre>

<p>The docs also mention that <code>.join</code> can not be used to merge two multiindexed DataFrames on a single level and from the GitHub tracker discussion for the previous issue, it seems like this might not of priority to implement:</p>

<blockquote>
  <p>so I merged in the single join, see #6363; along with some docs on
  how to do a multi-multi join. THat's fairly complicated to actually
  implement. and IMHO not worth the effort as it really doesn't change
  the memory usage/speed that much at all.</p>
</blockquote>

<p>However, there is a GitHub conversation regarding this, where there has been some recent development <a href=""https://github.com/pydata/pandas/issues/6360"" rel=""noreferrer"">https://github.com/pydata/pandas/issues/6360</a>. It is also possible achieve this by resetting the indices as mentioned earlier and described in the docs as well.</p>
";;3;;2014-03-12T22:31:00.293;;22365284;2014-03-12T22:31:00.293;;;;;2166823.0;16650945.0;2;9;;;
11428;11428;;;"<p>Example with aggregation:</p>

<p>I wanted to do something like the following, if pandas had a colour aesthetic like ggplot:</p>

<pre><code>aggregated = df.groupby(['model', 'training_examples']).aggregate(np.mean)
aggregated.plot(x='training_examples', y='accuracy', label='model')
</code></pre>

<p>(columns: model is a string, training_examples is an integer, accuracy is a decimal)</p>

<p>But that just produces a mess.</p>

<p>Thanks to joris's answer, I ended up with:</p>

<pre><code>for index, group in df.groupby(['model']):
    group_agg = group.groupby(['training_examples']).aggregate(np.mean)
    group_agg.plot(y='accuracy', label=index)
</code></pre>

<p>I found that <code>title=</code> was just replacing the single title of the plot on each loop iteration, but <code>label=</code> does what you'd expect -- <em>after</em> running <code>plt.legend()</code>, of course.</p>
";;0;;2014-03-13T03:49:28.663;;22368682;2014-03-13T03:49:28.663;;;;;424651.0;16376159.0;2;10;;;
11464;11464;;;"<p>Use <code>groupby</code> and <code>count</code>:</p>

<pre><code>In [37]:
df = pd.DataFrame({'a':list('abssbab')})
df.groupby('a').count()

Out[37]:

   a
a   
a  2
b  3
s  2

[3 rows x 1 columns]
</code></pre>

<p>See the online docs: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/groupby.html</a></p>

<p>Also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts()</code></a> as @DSM has commented, many ways to skin a cat here</p>

<pre><code>In [38]:
df['a'].value_counts()

Out[38]:

b    3
a    2
s    2
dtype: int64
</code></pre>

<p>If you wanted to add frequency back to the original dataframe use <code>transform</code> to return an aligned index:</p>

<pre><code>In [41]:
df['freq'] = df.groupby('a')['a'].transform('count')
df

Out[41]:

   a freq
0  a    2
1  b    3
2  s    2
3  s    2
4  b    3
5  a    2
6  b    3

[7 rows x 2 columns]
</code></pre>
";;6;;2014-03-13T21:41:34.147;;22391554;2017-03-31T14:29:07.377;2017-03-31T14:29:07.377;;4048278.0;;704848.0;22391433.0;2;73;;;
11549;11549;;;"<p>@behzad.nouri's solution worked perfectly to return the first and last non-<code>NaN values</code> using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.first_valid_index.html#pandas.Series.first_valid_index"" rel=""noreferrer"">Series.first_valid_index</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.last_valid_index.html#pandas.Series.last_valid_index"" rel=""noreferrer"">Series.last_valid_index</a>, respectively.</p>
";;0;;2014-03-17T13:16:22.133;;22455322;2016-08-10T16:12:06.190;2016-08-10T16:12:06.190;;2271127.0;;2694260.0;22403469.0;2;21;;;
11571;11571;;;"<p>You can use boolean mask on the dtypes attribute:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2.3456, 'c']])

In [12]: df.dtypes
Out[12]: 
0      int64
1    float64
2     object
dtype: object

In [13]: msk = df.dtypes == np.float64  # or object, etc.

In [14]: msk
Out[14]: 
0    False
1     True
2    False
dtype: bool
</code></pre>

<p>You can look at just those columns with the desired dtype:</p>

<pre><code>In [15]: df.loc[:, msk]
Out[15]: 
        1
0  2.3456
</code></pre>

<p>Now you can use round (or whatever) and assign it back:</p>

<pre><code>In [16]: np.round(df.loc[:, msk], 2)
Out[16]: 
      1
0  2.35

In [17]: df.loc[:, msk] = np.round(df.loc[:, msk], 2)

In [18]: df
Out[18]: 
   0     1  2
0  1  2.35  c
</code></pre>
";;1;;2014-03-18T05:38:28.767;;22471217;2014-03-18T05:38:28.767;;;;;1240268.0;22470690.0;2;13;;;
11575;11575;;;"<p>If you want a list of columns of a certain type, you can use <code>groupby</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 2.3456, 'c', 'd', 78]], columns=list(""ABCDE""))
&gt;&gt;&gt; df
   A       B  C  D   E
0  1  2.3456  c  d  78

[1 rows x 5 columns]
&gt;&gt;&gt; df.dtypes
A      int64
B    float64
C     object
D     object
E      int64
dtype: object
&gt;&gt;&gt; g = df.columns.to_series().groupby(df.dtypes).groups
&gt;&gt;&gt; g
{dtype('int64'): ['A', 'E'], dtype('float64'): ['B'], dtype('O'): ['C', 'D']}
&gt;&gt;&gt; {k.name: v for k, v in g.items()}
{'object': ['C', 'D'], 'int64': ['A', 'E'], 'float64': ['B']}
</code></pre>
";;2;;2014-03-18T09:29:34.047;;22475141;2014-03-18T09:29:34.047;;;;;487339.0;22470690.0;2;126;;;
11585;11585;;;"<p>You can manually create the subplots with matplotlib, and then plot the dataframes on a specific subplot using the <code>ax</code> keyword. For example for 4 subplots (2x2):</p>

<pre><code>import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=2, ncols=2)

df1.plot(ax=axes[0,0])
df2.plot(ax=axes[0,1])
...
</code></pre>

<p>Here <code>axes</code> is an array which holds the different subplot axes, and you can access one just by indexing <code>axes</code>.<br>
If you want a shared x-axis, then you can provide <code>sharex=True</code> to <code>plt.subplots</code>.</p>
";;4;;2014-03-18T15:45:26.467;;22484249;2014-03-18T15:54:06.340;2014-03-18T15:54:06.340;;653364.0;;653364.0;22483588.0;2;86;;;
11589;11589;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"">isin</a> Series method:</p>

<pre><code>In [11]: df['Name'].isin(['Alice', 'Bob'])
Out[11]: 
0     True
1     True
2    False
3     True
4    False
Name: Name, dtype: bool

In [12]: df[df.Name.isin(['Alice', 'Bob'])]
Out[12]: 
    Name  Amount
0  Alice     100
1    Bob      50
3  Alice      30
</code></pre>
";;6;;2014-03-18T16:37:36.490;;22485573;2014-03-18T16:37:36.490;;;;;1240268.0;22485375.0;2;34;;;
11595;11595;;;"<p>The first argument to <code>Value</code> is <em>typecode_or_type</em>.  That is defined as:</p>

<blockquote>
  <p>typecode_or_type determines the type of the returned object: <strong>it is
  either a ctypes type or a one character typecode of the kind used by
  the array module.</strong> *args is passed on to the constructor for the type.</p>
</blockquote>

<p>Emphasis mine.  So, you simply cannot put a pandas dataframe in a <code>Value</code>, it has to be <a href=""http://docs.python.org/2/library/ctypes.html#fundamental-data-types"" rel=""noreferrer"">a ctypes type</a>.</p>

<p>You could instead use a <code>multiprocessing.Manager</code> to serve your singleton dataframe instance to all of your processes.  There's a few different ways to end up in the same place - probably the easiest is to just plop your dataframe into the manager's <code>Namespace</code>.</p>

<pre><code>from multiprocessing import Manager

mgr = Manager()
ns = mgr.Namespace()
ns.df = my_dataframe

# now just give your processes access to ns, i.e. most simply
# p = Process(target=worker, args=(ns, work_unit))
</code></pre>

<p>Now your dataframe instance is accessible to any process that gets passed a reference to the Manager.  Or just pass a reference to the <code>Namespace</code>, it's cleaner.</p>

<p>One thing I didn't/won't cover is events and signaling - if your processes need to wait for others to finish executing, you'll need to add that in.  <a href=""http://broadcast.oreilly.com/2009/04/pymotw-multiprocessing-part-2.html"" rel=""noreferrer"">Here is a page</a> with some <code>Event</code> examples which also cover with a bit more detail how to use the manager's <code>Namespace</code>.</p>

<p>(note that none of this addresses whether <code>multiprocessing</code> is going to result in tangible performance benefits, this is just giving you the tools to explore that question)</p>
";;2;;2014-03-18T18:24:27.630;;22487898;2014-03-18T18:24:27.630;;;;;2581969.0;22487296.0;2;11;;;
11617;11617;;;"<p>I imagine a lot of data comes into Pandas from CSV files, in which case you can simply convert the date during the initial CSV read:</p>

<p><code>dfcsv = pd.read_csv('xyz.csv', parse_dates=[0])</code> where the 0 refers to the column the date is in.<br>
You could also add <code>, index_col=0</code> in there if you want the date to be your index.</p>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>
";;1;;2014-03-19T04:15:07.860;;22496075;2014-03-19T04:15:07.860;;;;;3427777.0;16852911.0;2;12;;;
11717;11717;;;"<p><strong>Update:</strong> If you have matplotlib >= 1.4, there is a new <code>style</code> module which has a <code>ggplot</code> style by default. To activate this, use:</p>

<pre><code>from matplotlib import pyplot as plt
plt.style.use('ggplot')
</code></pre>

<p>This is recommended above the styling through the pandas options as explained below (and is also used in the pandas docs now).</p>

<hr>

<p>For pandas, use:</p>

<pre><code>pd.options.display.mpl_style = 'default'
</code></pre>

<p>and this will give you the 'ggplot-like' style for matplotlib figures (Note that the name is a bit confusing as this is not enabled by default, and this should actually be added to the docs).  </p>

<p>For seaborn, as Paul H commented, it is enough to <code>import seaborn</code></p>

<p>By the way, if you really want something like ggplot in python with ggplot syntax (and not only ggplot-like style), there is also a <strong>python ggplot library</strong> based on pandas: <a href=""https://github.com/yhat/ggplot/"">https://github.com/yhat/ggplot/</a></p>
";;0;;2014-03-20T19:34:47.833;;22543333;2015-09-23T13:17:47.193;2015-09-23T13:17:47.193;;396967.0;;653364.0;22543208.0;2;21;;;
11732;11732;;;"<p>You need to enclose multiple conditions in braces due to operator precedence and use the bitwise and (<code>&amp;</code>) and or (<code>|</code>) operators:</p>

<pre><code>foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]
</code></pre>

<p>If you use <code>and</code> or <code>or</code>, then pandas is likely to moan that the comparison is ambiguous. In that case, it is unclear whether we are comparing every value in a series in the condition, and what does it mean if only 1 or all but 1 match the condition. That is why you should use the bitwise operators or the numpy <code>np.all</code> or <code>np.any</code> to specify the matching criteria.</p>

<p>There is also the query method: <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.query.html</a></p>

<p>but there are some limitations mainly to do with issues where there could be ambiguity between column names and index values.</p>
";;6;;2014-03-20T22:25:58.767;;22546459;2017-02-24T14:12:36.583;2017-02-24T14:12:36.583;;6049173.0;;704848.0;22546425.0;2;40;;;
11736;11736;;;"<p>A more concise--but not necessarily faster--method is to use <code>DataFrame.isin()</code> and <code>DataFrame.any()</code></p>

<pre><code>In [27]: n = 10

In [28]: df = DataFrame(randint(4, size=(n, 2)), columns=list('ab'))

In [29]: df
Out[29]:
   a  b
0  0  0
1  1  1
2  1  1
3  2  3
4  2  3
5  0  2
6  1  2
7  3  0
8  1  1
9  2  2

[10 rows x 2 columns]

In [30]: df.isin([1, 2])
Out[30]:
       a      b
0  False  False
1   True   True
2   True   True
3   True  False
4   True  False
5  False   True
6   True   True
7  False  False
8   True   True
9   True   True

[10 rows x 2 columns]

In [31]: df.isin([1, 2]).any(1)
Out[31]:
0    False
1     True
2     True
3     True
4     True
5     True
6     True
7    False
8     True
9     True
dtype: bool

In [32]: df.loc[df.isin([1, 2]).any(1)]
Out[32]:
   a  b
1  1  1
2  1  1
3  2  3
4  2  3
5  0  2
6  1  2
8  1  1
9  2  2

[8 rows x 2 columns]
</code></pre>
";;0;;2014-03-20T23:33:05.903;;22547347;2014-03-20T23:33:05.903;;;;;564538.0;22546425.0;2;11;;;
11739;11739;;;"<p>Just drop them:</p>

<pre><code>nms.dropna(thresh=2)
</code></pre>

<p>this will drop all rows where there are at least two <code>NaN</code></p>

<p>then you could then drop where name is <code>NaN</code>:</p>

<pre><code>In [87]:

nms
Out[87]:
  movie    name  rating
0   thg    John       3
1   thg     NaN       4
3   mol  Graham     NaN
4   lob     NaN     NaN
5   lob     NaN     NaN

[5 rows x 3 columns]
In [89]:

nms = nms.dropna(thresh=2)
In [90]:

nms[nms.name.notnull()]
Out[90]:
  movie    name  rating
0   thg    John       3
3   mol  Graham     NaN

[2 rows x 3 columns]
</code></pre>

<p><strong>EDIT</strong></p>

<p>Actually looking at what you originally want you can do just this without the <code>dropna</code> call:</p>

<pre><code>nms[nms.name.notnull()]
</code></pre>
";;4;;2014-03-21T08:33:05.050;;22553757;2014-03-21T09:46:20.713;2014-03-21T09:46:20.713;;704848.0;;704848.0;22551403.0;2;73;;;
11791;11791;;;"<p>You could remove all the non-digits using <code>re.sub()</code>:</p>

<pre><code>value = re.sub(r""[^0-9]+"", """", value)
</code></pre>

<p><a href=""http://regex101.com/r/yS7lG7"">regex101 demo</a></p>
";;7;;2014-03-23T07:51:48.720;;22588340;2014-03-23T07:51:48.720;;;;;1578604.0;22588316.0;2;10;;;
11796;11796;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"" rel=""noreferrer""><code>Series.str.replace</code></a>:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(['$40,000*','$40000 conditions attached'], columns=['P'])
print(df)
#                             P
# 0                    $40,000*
# 1  $40000 conditions attached

df['P'] = df['P'].str.replace(r'\D+', '').astype('int')
print(df)
</code></pre>

<p>yields</p>

<pre><code>       P
0  40000
1  40000
</code></pre>

<p>since <code>\D</code> matches any <a href=""https://docs.python.org/3/library/re.html#regular-expression-syntax"" rel=""noreferrer"">non-decimal digit</a>.</p>
";;1;;2014-03-23T12:39:48.570;;22591024;2017-04-03T01:45:13.327;2017-04-03T01:45:13.327;;190597.0;;190597.0;22588316.0;2;53;;;
11798;11798;;;"<blockquote>
  <p>As you can see, the AND operator drops every row in which at least one
  value equals -1. On the other hand, the OR operator requires both
  values to be equal to -1 to drop them.</p>
</blockquote>

<p>That's right.  Remember that you're writing the condition in terms of what you want to <em>keep</em>, not in terms of what you want to drop.  For <code>df1</code>:</p>

<pre><code>df1 = df[(df.a != -1) &amp; (df.b != -1)]
</code></pre>

<p>You're saying ""keep the rows in which <code>df.a</code> isn't -1 and <code>df.b</code> isn't -1"", which is the same as dropping every row in which at least one value is -1.</p>

<p>For <code>df2</code>:</p>

<pre><code>df2 = df[(df.a != -1) | (df.b != -1)]
</code></pre>

<p>You're saying ""keep the rows in which either <code>df.a</code> or <code>df.b</code> is not -1"", which is the same as dropping rows where both values are -1.</p>

<p>PS: chained access like <code>df['a'][1] = -1</code> can get you into trouble.  It's better to get into the habit of using <code>.loc</code> and <code>.iloc</code>.</p>
";;3;;2014-03-23T13:02:05.893;;22591267;2014-03-23T13:02:05.893;;;;;487339.0;22591174.0;2;49;;;
11809;11809;;;"<pre><code>df.drop([Column Name or list],inplace=True,axis=1)
</code></pre>

<p>will delete one or more columns inplace.</p>
";;3;;2014-03-23T20:57:57.500;;22596982;2014-11-27T03:39:42.910;2014-11-27T03:39:42.910;;3114046.0;;501379.0;13411544.0;2;135;;;
11815;11815;;;"<p>Simple way to do this was to use <a href=""https://docs.python.org/2/library/io.html#io.StringIO"" rel=""noreferrer""><code>StringIO</code></a> and pass that to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv"" rel=""noreferrer""><code>pandas.read_csv</code></a> function. E.g:</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3: 
    from StringIO import StringIO
else:
    from io import StringIO

import pandas as pd

TESTDATA=StringIO(""""""col1;col2;col3
    1;4.4;99
    2;4.5;200
    3;4.7;65
    4;3.2;140
    """""")

df = pd.read_csv(TESTDATA, sep="";"")
</code></pre>
";;1;;2014-03-24T09:21:27.440;;22605281;2016-05-23T06:00:23.380;2016-05-23T06:00:23.380;;355499.0;;355499.0;22604564.0;2;138;;;
11879;11879;;;"<p>You can divide by the <em>series</em> i.e. the first row of df2:</p>

<pre><code>In [11]: df = pd.DataFrame([[1., 2.], [3., 4.]], columns=['A', 'B'])

In [12]: df2 = pd.DataFrame([[5., 10.]], columns=['A', 'B'])

In [13]: df.div(df2)
Out[13]: 
     A    B
0  0.2  0.2
1  NaN  NaN

In [14]: df.div(df2.iloc[0])
Out[14]: 
     A    B
0  0.2  0.2
1  0.6  0.4
</code></pre>
";;0;;2014-03-25T17:50:11.480;;22642484;2014-03-25T17:50:11.480;;;;;1240268.0;22642162.0;2;9;;;
11880;11880;;;"<p>In <code>df.divide(df2, axis='index')</code>, you need to provide the axis/row of df2 (ex. <code>df2.ix[0]</code>). </p>

<pre><code>import pandas as pd

data1 = {""a"":[1.,3.,5.,2.],
         ""b"":[4.,8.,3.,7.],
         ""c"":[5.,45.,67.,34]}
data2 = {""a"":[4.],
         ""b"":[2.],
         ""c"":[11.]}

df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2) 

df1.div(df2.ix[0],axis='columns')
</code></pre>

<p>or you can use <code>df1/df2.values[0,:]</code></p>
";;2;;2014-03-25T18:16:30.283;;22643040;2014-03-25T18:16:30.283;;;;;2406579.0;22642162.0;2;14;;;
11900;11900;;;"<p>It turns out this can be nicely expressed in a vectorized fashion:</p>

<pre><code>&gt; df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})
&gt; df = df[(df.T != 0).any()]
&gt; df
   a  b
1  0  1
2  1  0
3  1  1
</code></pre>
";;6;;2014-03-26T01:59:04.623;;22650075;2014-03-26T03:03:54.573;2014-03-26T03:03:54.573;;507762.0;;507762.0;22649693.0;2;34;;;
11902;11902;;;"<p>One-liner.  No transpose needed:</p>

<pre><code>df.loc[~(df==0).all(axis=1)]
</code></pre>

<p>And for those who like symmetry, this also works...</p>

<pre><code>df.loc[(df!=0).any(axis=1)]
</code></pre>
";;3;;2014-03-26T02:07:25.923;;22650162;2014-03-26T03:04:11.630;2014-03-26T03:04:11.630;;2501018.0;;2501018.0;22649693.0;2;32;;;
11905;11905;;;"<p>This should do it, need <code>groupby()</code> twice.</p>

<pre><code>In [52]:

print df
   name        day   no
0  Jack     Monday   10
1  Jack    Tuesday   20
2  Jack    Tuesday   10
3  Jack  Wednesday   50
4  Jill     Monday   40
5  Jill  Wednesday  110
In [53]:

print df.groupby(by=['name','day']).sum().groupby(level=[0]).cumsum()
                 no
name day           
Jack Monday      10
     Tuesday     40
     Wednesday   90
Jill Monday      40
     Wednesday  150
</code></pre>

<p>Note, the resulting <code>DataFrame</code> has <code>MultiIndex</code>.</p>
";;3;;2014-03-26T03:56:25.017;;22651188;2014-03-26T04:02:50.697;2014-03-26T04:02:50.697;;2487184.0;;2487184.0;22650833.0;2;22;;;
11907;11907;;;"<p>I would just chain the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"">DataFrame.reset_index()</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html"">DataFrame.values</a> functions to get the Numpy representation of the dataframe, including the index:</p>

<pre><code>In [8]: df
Out[8]: 
          A         B         C
0 -0.982726  0.150726  0.691625
1  0.617297 -0.471879  0.505547
2  0.417123 -1.356803 -1.013499
3 -0.166363 -0.957758  1.178659
4 -0.164103  0.074516 -0.674325
5 -0.340169 -0.293698  1.231791
6 -1.062825  0.556273  1.508058
7  0.959610  0.247539  0.091333

[8 rows x 3 columns]

In [9]: df.reset_index().values
Out[9]:
array([[ 0.        , -0.98272574,  0.150726  ,  0.69162512],
       [ 1.        ,  0.61729734, -0.47187926,  0.50554728],
       [ 2.        ,  0.4171228 , -1.35680324, -1.01349922],
       [ 3.        , -0.16636303, -0.95775849,  1.17865945],
       [ 4.        , -0.16410334,  0.0745164 , -0.67432474],
       [ 5.        , -0.34016865, -0.29369841,  1.23179064],
       [ 6.        , -1.06282542,  0.55627285,  1.50805754],
       [ 7.        ,  0.95961001,  0.24753911,  0.09133339]])
</code></pre>

<p>To get the dtypes we'd need to transform this ndarray into a structured array using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.view.html"">view</a>:</p>

<pre><code>In [10]: df.reset_index().values.ravel().view(dtype=[('index', int), ('A', float), ('B', float), ('C', float)])
Out[10]:
array([( 0, -0.98272574,  0.150726  ,  0.69162512),
       ( 1,  0.61729734, -0.47187926,  0.50554728),
       ( 2,  0.4171228 , -1.35680324, -1.01349922),
       ( 3, -0.16636303, -0.95775849,  1.17865945),
       ( 4, -0.16410334,  0.0745164 , -0.67432474),
       ( 5, -0.34016865, -0.29369841,  1.23179064),
       ( 6, -1.06282542,  0.55627285,  1.50805754),
       ( 7,  0.95961001,  0.24753911,  0.09133339),
       dtype=[('index', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>
";;2;;2014-03-26T06:23:21.847;;22653050;2014-03-26T07:35:16.677;2014-03-26T07:35:16.677;;1825593.0;;1825593.0;13187778.0;2;22;;;
11917;11917;;;"<p>Since you only want to remove the $ sign in all column names, you could just do:</p>

<pre><code>df = df.rename(columns=lambda x: x.replace('$', ''))
</code></pre>

<p>OR</p>

<pre><code>df.rename(columns=lambda x: x.replace('$', ''), inplace=True)
</code></pre>
";;0;;2014-03-26T10:20:45.720;;22657894;2014-03-26T10:20:45.720;;;;;2700072.0;11346283.0;2;83;;;
11952;11952;;;"<p>Just for someone looking for a solution more similar to R:</p>

<pre><code>df[(df.Product == p_id) &amp; (df.Time&gt; start_time) &amp; (df.Time &lt; end_time)][['Time','Product']]
</code></pre>

<p>No need for <code>data.loc</code> or <code>query</code>, but I do think it is a bit long.</p>
";;0;;2014-03-26T22:22:13.283;;22674279;2015-05-12T10:21:19.373;2015-05-12T10:21:19.373;;1461210.0;;1811688.0;19237878.0;2;14;;;
11965;11965;;;"<p>I always use <code>join</code> on indices:</p>

<pre><code>import pandas as pd
left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')
right = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')
left.join(right, lsuffix='_l', rsuffix='_r')

     val_l  val_r
key            
foo      1      4
bar      2      5
</code></pre>

<p>The same functionality can be had by using <code>merge</code> on the columns follows:</p>

<pre><code>left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})
right = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})
left.merge(right, on=('key'), suffixes=('_l', '_r'))

   key  val_l  val_r
0  foo      1      4
1  bar      2      5
</code></pre>
";;2;;2014-03-27T00:55:03.857;;22676213;2017-01-17T21:54:15.537;2017-01-17T21:54:15.537;;1552748.0;;1552748.0;22676081.0;2;27;;;
11968;11968;;;"<p>Plots are not displayed until you run</p>

<blockquote>
  <p>plt.show()</p>
</blockquote>
";;0;;2014-03-27T02:40:58.297;;22677264;2014-03-27T02:40:58.297;;;;;792036.0;18388870.0;2;24;;;
12000;12000;;;"<p>You can access the data-type of a column with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dtype.html""><code>dtype</code></a>:</p>

<pre><code>for y in agg.columns:
    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):
          treat_numeric(agg[y])
    else:
          treat_str(agg[y])
</code></pre>
";;3;;2014-03-27T19:56:32.040;;22697903;2017-01-02T14:54:44.267;2017-01-02T14:54:44.267;;2314737.0;;712603.0;22697773.0;2;41;;;
12020;12020;;;"<p>use <code>multiply</code> method and set <code>axis=""index""</code>:</p>

<pre><code>df[[""A"", ""B""]].multiply(df[""C""], axis=""index"")
</code></pre>
";;1;;2014-03-28T02:01:40.883;;22702814;2014-03-28T02:01:40.883;;;;;772649.0;22702760.0;2;26;;;
12040;12040;;;"<p>You could also use</p>

<pre><code>df['bar'] = df['bar'].str.cat(df['foo'].values.astype(str), sep=' is ')
</code></pre>
";;0;;2014-03-28T17:56:36.183;;22719983;2014-03-28T17:56:36.183;;;;;2376000.0;11858472.0;2;9;;;
12047;12047;;;"<p>You can expect this to increase if keys match more than one row in the other DataFrame:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 3], [2, 4]], columns=['A', 'B'])

In [12]: df2 = pd.DataFrame([[1, 5], [1, 6]], columns=['A', 'C'])

In [13]: df.merge(df2, how='left')  # merges on columns A
Out[13]: 
   A  B   C
0  1  3   5
1  1  3   6
2  2  4 NaN
</code></pre>

<p>To avoid this behaviour <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"">drop the duplicates</a> in df2:</p>

<pre><code>In [21]: df2.drop_duplicates(subset=['A'])  # you can use take_last=True
Out[21]: 
   A  C
0  1  5

In [22]: df.merge(df2.drop_duplicates(subset=['A']), how='left')
Out[22]: 
   A  B   C
0  1  3   5
1  2  4 NaN
</code></pre>
";;6;;2014-03-28T18:44:48.803;;22720823;2015-08-07T05:36:02.820;2015-08-07T05:36:02.820;;1240268.0;;1240268.0;22720739.0;2;23;;;
12184;12184;;;"<p>With the new version of Pandas you can use the date_format parameter of the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"">to_csv</a> method:</p>

<pre><code>df.to_csv(filename, date_format='%Y%m%d')
</code></pre>
";;1;;2014-04-01T23:30:20.643;;22798849;2014-04-01T23:30:20.643;;;;;1827947.0;13999850.0;2;45;;;
12186;12186;;;"<p>I don't think <code>tsplot</code> is going to work with the data you have. The assumptions it makes about the input data are that you've sampled the same units at each timepoint (although you can have missing timepoints for some units).</p>

<p>For example, say you measured blood pressure from the same people every day for a month, and then you wanted to plot the average blood pressure by condition (where maybe the ""condition"" variable is the diet they are on). <code>tsplot</code> could do this, with a call that would look something like <code>sns.tsplot(df, time=""day"", unit=""person"", condition=""diet"", value=""blood_pressure"")</code></p>

<p>That scenario is different from having large groups of people on different diets and each day randomly sampling some from each group and measuring their blood pressure. From the example you gave, it seems like your data are structured like the this.</p>

<p>However, it's not that hard to come up with a mix of matplotlib and pandas that will do what I think you want:</p>

<pre><code># Read in the data from the stackoverflow question
df = pd.read_clipboard().iloc[1:]

# Convert it to ""long-form"" or ""tidy"" representation
df = pd.melt(df, id_vars=[""date""], var_name=""condition"")

# Plot the average value by condition and date
ax = df.groupby([""condition"", ""date""]).mean().unstack(""condition"").plot()

# Get a reference to the x-points corresponding to the dates and the the colors
x = np.arange(len(df.date.unique()))
palette = sns.color_palette()

# Calculate the 25th and 75th percentiles of the data
# and plot a translucent band between them
for cond, cond_df in df.groupby(""condition""):
    low = cond_df.groupby(""date"").value.apply(np.percentile, 25)
    high = cond_df.groupby(""date"").value.apply(np.percentile, 75)
    ax.fill_between(x, low, high, alpha=.2, color=palette.pop(0))
</code></pre>

<p>This code produces:</p>

<p><img src=""https://i.stack.imgur.com/Zm6Yz.png"" alt=""enter image description here""></p>
";;5;;2014-04-01T23:35:58.773;;22798911;2014-04-01T23:57:16.043;2014-04-01T23:57:16.043;;1533576.0;;1533576.0;22795348.0;2;30;;;
12193;12193;;;"<p>maybe:</p>

<pre><code>&gt;&gt;&gt; pd.DataFrame(out.tolist(), columns=['out-1','out-2'], index=out.index)
                  out-1     out-2
y                                
a   -1.9153853424536496  0.067433
b     1.277561889173181  0.213624
c  0.062021492729736116  0.951059
d    0.3036745009819999  0.763993

[4 rows x 2 columns]
</code></pre>
";;0;;2014-04-02T00:22:11.113;;22799358;2014-04-02T00:22:11.113;;;;;625914.0;22799300.0;2;9;;;
12197;12197;;;"<pre><code>pivoted = df.pivot('salesman', 'product', 'price')
</code></pre>

<p>pg. 192 Python for Data Analysis</p>
";;1;;2014-04-02T01:19:21.553;;22799830;2014-04-02T01:19:21.553;;;;;666246.0;22798934.0;2;7;;;
12198;12198;;;"<p>A simple pivot might be sufficient for your needs but this is what I did to reproduce your desired output:</p>

<pre><code>df['idx'] = df.groupby('Salesman').cumcount()
</code></pre>

<p>Just adding a within group counter/index will get you most of the way there but the column labels will not be as you desired:</p>

<pre><code>print df.pivot(index='Salesman',columns='idx')[['product','price']]

        product              price        
idx            0     1     2      0   1   2
Salesman                                   
Knut         bat  ball  wand      5   1   3
Steve        pen   NaN   NaN      2 NaN NaN
</code></pre>

<p>To get closer to your desired output I added the following:</p>

<pre><code>df['prod_idx'] = 'product_' + df.idx.astype(str)
df['prc_idx'] = 'price_' + df.idx.astype(str)

product = df.pivot(index='Salesman',columns='prod_idx',values='product')
prc = df.pivot(index='Salesman',columns='prc_idx',values='price')

reshape = pd.concat([product,prc],axis=1)
reshape['Height'] = df.set_index('Salesman')['Height'].drop_duplicates()
print reshape

         product_0 product_1 product_2  price_0  price_1  price_2  Height
Salesman                                                                 
Knut           bat      ball      wand        5        1        3       6
Steve          pen       NaN       NaN        2      NaN      NaN       5
</code></pre>

<p>Edit: if you want to generalize the procedure to more variables I think you could do something like the following (although it might not be efficient enough):</p>

<pre><code>df['idx'] = df.groupby('Salesman').cumcount()

tmp = []
for var in ['product','price']:
    df['tmp_idx'] = var + '_' + df.idx.astype(str)
    tmp.append(df.pivot(index='Salesman',columns='tmp_idx',values=var))

reshape = pd.concat(tmp,axis=1)
</code></pre>

<blockquote>
  <p>@Luke said:</p>
  
  <p>I think Stata can do something like this with the reshape command.</p>
</blockquote>

<p>You can but I think you also need a within group counter to get the reshape in stata to get your desired output:</p>

<pre><code>     +-------------------------------------------+
     | salesman   idx   height   product   price |
     |-------------------------------------------|
  1. |     Knut     0        6       bat       5 |
  2. |     Knut     1        6      ball       1 |
  3. |     Knut     2        6      wand       3 |
  4. |    Steve     0        5       pen       2 |
     +-------------------------------------------+
</code></pre>

<p>If you add <code>idx</code> then you could do reshape in <code>stata</code>:</p>

<pre><code>reshape wide product price, i(salesman) j(idx)
</code></pre>
";;3;;2014-04-02T01:30:23.343;;22799916;2014-04-02T23:48:10.007;2014-04-02T23:48:10.007;;3435183.0;;3435183.0;22798934.0;2;13;;;
12248;12248;;;"<p>You can use the to_pydatetime method to be more explicit:</p>

<pre><code>In [11]: ts = pd.Timestamp('2014-01-23 00:00:00', tz=None)

In [12]: ts.to_pydatetime()
Out[12]: datetime.datetime(2014, 1, 23, 0, 0)
</code></pre>

<p>It's also available on a DatetimeIndex:</p>

<pre><code>In [13]: rng = pd.date_range('1/10/2011', periods=3, freq='D')

In [14]: rng.to_pydatetime()
Out[14]:
array([datetime.datetime(2011, 1, 10, 0, 0),
       datetime.datetime(2011, 1, 11, 0, 0),
       datetime.datetime(2011, 1, 12, 0, 0)], dtype=object)
</code></pre>
";;2;;2014-04-03T01:03:52.123;;22825954;2014-04-03T01:03:52.123;;;;;1240268.0;22825349.0;2;12;;;
12256;12256;;;"<p>This is kind of hacky but it works:</p>

<pre><code>df = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])
ax = plt.figure(figsize=(10, 6)).add_subplot(111)
df.plot(ax=ax, kind='bar', legend=False)

bars = ax.patches
hatches = ''.join(h*len(df) for h in 'x/O.')

for bar, hatch in zip(bars, hatches):
    bar.set_hatch(hatch)

ax.legend(loc='center right', bbox_to_anchor=(1, 1), ncol=4)
</code></pre>

<p><img src=""https://i.stack.imgur.com/hBC8t.png"" alt=""bar""></p>
";;3;;2014-04-03T11:33:46.473;;22836353;2014-07-08T22:51:21.387;2014-07-08T22:51:21.387;;544059.0;;625914.0;22833404.0;2;11;;;
12265;12265;;;"<p>Simply type <code>conda update pandas</code> in your preferred shell (on Windows, use cmd). You can of course use Eclipse together with Anaconda, but you need to specify the Python-Path (the one in the Anaconda-Directory). 
See this <a href=""http://docs.continuum.io/anaconda/ide_integration.html"" rel=""noreferrer"">document</a> for a detailed instruction.</p>
";;6;;2014-04-03T14:27:50.363;;22840737;2014-04-03T14:27:50.363;;;;;872535.0;22840449.0;2;23;;;
12275;12275;;;"<p>So, I eventually found a trick (edit: see below for using seaborn and longform dataframe):</p>

<h3>Solution with pandas and matplotlib</h3>

<p>Here it is with a more complete example :</p>

<pre><code>import pandas as pd
import matplotlib.cm as cm
import numpy as np
import matplotlib.pyplot as plt

def plot_clustered_stacked(dfall, labels=None, title=""multiple stacked bar plot"",  H=""/"", **kwargs):
    """"""Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. 
labels is a list of the names of the dataframe, used for the legend
title is a string for the title of the plot
H is the hatch used for identification of the different dataframe""""""

    n_df = len(dfall)
    n_col = len(dfall[0].columns) 
    n_ind = len(dfall[0].index)
    axe = plt.subplot(111)

    for df in dfall : # for each data frame
        axe = df.plot(kind=""bar"",
                      linewidth=0,
                      stacked=True,
                      ax=axe,
                      legend=False,
                      grid=False,
                      **kwargs)  # make bar plots

    h,l = axe.get_legend_handles_labels() # get the handles we want to modify
    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df
        for j, pa in enumerate(h[i:i+n_col]):
            for rect in pa.patches: # for each index
                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))
                rect.set_hatch(H * int(i / n_col)) #edited part     
                rect.set_width(1 / float(n_df + 1))

    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)
    axe.set_xticklabels(df.index, rotation = 0)
    axe.set_title(title)

    # Add invisible data to add another legend
    n=[]        
    for i in range(n_df):
        n.append(axe.bar(0, 0, color=""gray"", hatch=H * i))

    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])
    if labels is not None:
        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) 
    axe.add_artist(l1)
    return axe

# create fake dataframes
df1 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""],
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])
df2 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""],
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])
df3 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""], 
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])

# Then, just call :
plot_clustered_stacked([df1, df2, df3],[""df1"", ""df2"", ""df3""])
</code></pre>

<p>And it gives that :</p>

<p><img src=""https://i.stack.imgur.com/3ZdAH.png"" alt=""multiple stacked bar plot""></p>

<p>You can change the colors of the bar by passing a <code>cmap</code> argument: </p>

<pre><code>plot_clustered_stacked([df1, df2, df3],
                       [""df1"", ""df2"", ""df3""],
                       cmap=plt.cm.viridis)
</code></pre>

<hr>

<h3>Solution with seaborn:</h3>

<p>Given the same df1, df2, df3, below, I convert them in a long form:</p>

<pre><code>df1[""Name""] = ""df1""
df2[""Name""] = ""df2""
df3[""Name""] = ""df3""
dfall = pd.concat([pd.melt(i.reset_index(),
                           id_vars=[""Name"", ""index""]) # transform in tidy format each df
                   for i in [df1, df2, df3]],
                   ignore_index=True)
</code></pre>

<p>The problem with seaborn is that it doesn't stack bars natively, so the trick is to plot the cumulative sum of each bar on top of each other:</p>

<pre><code>dfall.set_index([""Name"", ""index"", ""variable""], inplace=1)
dfall[""vcs""] = dfall.groupby(level=[""Name"", ""index""]).cumsum()
dfall.reset_index(inplace=True) 

&gt;&gt;&gt; dfall.head(6)
  Name index variable     value       vcs
0  df1     A        I  0.717286  0.717286
1  df1     B        I  0.236867  0.236867
2  df1     C        I  0.952557  0.952557
3  df1     D        I  0.487995  0.487995
4  df1     A        J  0.174489  0.891775
5  df1     B        J  0.332001  0.568868
</code></pre>

<p>Then loop over each group of <code>variable</code> and plot the cumulative sum:</p>

<pre><code>c = [""blue"", ""purple"", ""red"", ""green"", ""pink""]
for i, g in enumerate(dfall.groupby(""variable"")):
    ax = sns.barplot(data=g[1],
                     x=""index"",
                     y=""vcs"",
                     hue=""Name"",
                     color=c[i],
                     zorder=-i, # so first bars stay on top
                     edgecolor=""k"")
ax.legend_.remove() # remove the redundant legends 
</code></pre>

<p><a href=""https://i.stack.imgur.com/mVUc1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVUc1.png"" alt=""multiple stack bar plot seaborn""></a></p>

<p>It lacks the legend that can be added easily I think. The problem is that instead of hatches (which can be added easily) to differentiate the dataframes we have a gradient of lightness, and it's a bit too light for the first one, and I don't really know how to change that without changing each rectangle one by one (as in the first solution).</p>

<p>Tell me if you don't understand something in the code.</p>

<p>Feel free to re-use this code which is under CC0.</p>
";;8;;2014-04-03T18:22:46.383;;22845857;2017-06-09T16:45:56.817;2017-06-09T16:45:56.817;;3297428.0;;3297428.0;22787209.0;2;31;;;
12287;12287;;;"<p>You can install python with homebrew:</p>

<pre><code>brew install python
</code></pre>

<p>Make sure that OSX uses the correct path:</p>

<pre><code>which python
</code></pre>

<p>Then you can use the pip tool to install pandas: </p>

<pre><code>pip install pandas
</code></pre>

<p>Make sure that all dependencies are installed. I followed this tutorial: <a href=""http://penandpants.com/2013/04/04/install-scientific-python-on-mac-os-x/"">http://penandpants.com/2013/04/04/install-scientific-python-on-mac-os-x/</a> </p>

<p>Works fine on my machine. </p>
";;0;;2014-04-03T20:36:36.473;;22848472;2014-04-03T20:36:36.473;;;;;3495502.0;13249135.0;2;11;;;
12364;12364;;;"<p>If it's the index, you should use the <code>.ix</code> or <code>.loc</code> selector.</p>

<p>For example:</p>

<pre><code>df.ix['2014-01-01':'2014-02-01']
</code></pre>

<p>See details here <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selection"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selection</a></p>

<p>I guess it's smart to read up on pandas quite extensively before you start, as it's a very powerful library. Otherwise it will be hard to get anything meaningful done.</p>

<p><strong>UPDATE:</strong></p>

<p>If the column is not the index you have two choices:</p>

<ol>
<li>Make it the index (either temporarily or permanently if it's time-series data)</li>
<li><code>df[(df['date'] &gt; '2013-01-01') &amp; (df['date'] &lt; '2013-02-01')]</code></li>
</ol>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">here</a> for the general explanation</p>
";;5;;2014-04-06T19:32:50.863;;22898920;2016-07-09T09:07:44.290;2016-07-09T09:07:44.290;;2074981.0;;1705798.0;22898824.0;2;49;;;
12405;12405;;;"<p>Simply: <code>DF.groupby(DF.index).first()</code></p>
";;4;;2014-04-07T17:02:57.543;;22918691;2014-04-07T17:02:57.543;;;;;2487184.0;22918212.0;2;18;;;
12409;12409;;;"<p>On Ubuntu you may need to install the package manager <code>pip</code> first:</p>

<pre><code>sudo apt-get install python-pip
</code></pre>

<p>Then install the <code>python-dateutil</code> package with:</p>

<pre><code>sudo pip install python-dateutil
</code></pre>
";;2;;2014-04-07T19:01:50.280;;22920808;2014-04-30T09:33:10.230;2014-04-30T09:33:10.230;;2521769.0;;2026317.0;20853474.0;2;120;;;
12422;12422;;;"<p>Pandas timestamp differences returns a datetime.timedelta object. This can easily be converted into hours by using the *as_type* method, like so</p>

<pre><code>import pandas
df = pandas.DataFrame(columns=['to','fr','ans'])
df.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]
df.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]
(df.fr-df.to).astype('timedelta64[h]')
</code></pre>

<p>to yield,</p>

<pre><code>0    58
1     3
2     8
dtype: float64
</code></pre>
";;2;;2014-04-07T23:01:08.143;;22924683;2014-04-07T23:01:08.143;;;;;1257953.0;22923775.0;2;29;;;
12443;12443;;;"<p>This is another possible solution. I believe it's is the fastest.</p>

<pre><code>df.loc[df.groupby('obj_id').data_date.idxmax(),:]
</code></pre>
";;2;;2014-04-08T14:58:02.850;;22940775;2017-08-13T23:58:06.307;2017-08-13T23:58:06.307;;2285236.0;;3506405.0;9850954.0;2;7;;;
12472;12472;;;"<p>You can try this:</p>

<pre><code>d = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)
</code></pre>
";;1;;2014-04-09T13:49:36.700;;22964673;2016-02-09T05:16:06.797;2016-02-09T05:16:06.797;;1979914.0;;1979914.0;22963263.0;2;38;;;
12497;12497;;;"<p><code>where</code> is probably what you're looking for. So</p>

<pre><code>data=data.where(data=='-', None) 
</code></pre>

<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html"" rel=""noreferrer"" title=""docs"">panda docs</a>: </p>

<blockquote>
  <p><code>where</code> [returns] an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other).</p>
</blockquote>
";;2;;2014-04-09T21:38:12.773;;22974440;2014-04-09T21:38:12.773;;;;;2966041.0;17097236.0;2;9;;;
12537;12537;;;"<p>Quick note: if you want to do selection based on a partial string contained in the index, try the following:</p>

<pre><code>df['stridx']=df.index
df[df['stridx'].str.contains(""Hello|Britain"")]
</code></pre>
";;1;;2014-04-10T15:36:14.217;;22992568;2014-04-10T15:36:14.217;;;;;3450632.0;11350770.0;2;17;;;
12573;12573;;;"<p>As suggested in the error message, you should use loc to do this:</p>

<pre><code>sve2_all.loc[sve2_all['Hgtot ng/l'] &gt; 100] = np.nan
</code></pre>

<p><em>The warning is here to stop you modifying a copy (here <code>sve2_all[sve2_all[' Hgtot ng/l'] &gt; 100]</code> is <strong>potentially</strong> a copy, and if it is then any modifications would not change the original frame. It could be that it works correctly in some cases but pandas cannot guarantee it will work in all cases... use at your own risk (consider yourself warned! ;) ).</em></p>
";;0;;2014-04-11T06:56:40.990;;23005564;2014-04-11T06:56:40.990;;;;;1240268.0;23002762.0;2;21;;;
12701;12701;;;"<p>I have faced this issue, but found an answer before finding this post :</p>

<p>Based on unutbu's answer, load your data...</p>

<pre><code>import pandas as pd
import io

texts = ['''\
id   Name   score                    isEnrolled                       Date
111  Jack                            True              2013-05-01 12:00:00
112  Nick   1.11                     False             2013-05-12 15:05:23
     Zoe    4.12                     True                                  ''',

         '''\
id   Name   score                    isEnrolled                       Date
111  Jack   2.17                     True              2013-05-01 12:00:00
112  Nick   1.21                     False                                
     Zoe    4.12                     False             2013-05-01 12:00:00''']


df1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,17,20], parse_dates=[4])
df2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,17,20], parse_dates=[4])
</code></pre>

<p>...define your <em>diff</em> function...</p>

<pre><code>def report_diff(x):
    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)
</code></pre>

<p>Then you can simply use a Panel to conclude :</p>

<pre><code>my_panel = pd.Panel(dict(df1=df1,df2=df2))
print my_panel.apply(report_diff, axis=0)

#          id  Name        score    isEnrolled                       Date
#0        111  Jack   nan | 2.17          True        2013-05-01 12:00:00
#1        112  Nick  1.11 | 1.21         False  2013-05-12 15:05:23 | NaT
#2  nan | nan   Zoe         4.12  True | False  NaT | 2013-05-01 12:00:00
</code></pre>

<p>By the way, if you're in IPython Notebook, you may like to use a colored <em>diff</em> function
to give colors depending whether cells are different, equal or left/right null :</p>

<pre><code>from IPython.display import HTML
pd.options.display.max_colwidth = 500  # You need this, otherwise pandas
#                          will limit your HTML strings to 50 characters

def report_diff(x):
    if x[0]==x[1]:
        return unicode(x[0].__str__())
    elif pd.isnull(x[0]) and pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#00ff00;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % ('nan', 'nan')
    elif pd.isnull(x[0]) and ~pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#ffff00;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % ('nan', x[1])
    elif ~pd.isnull(x[0]) and pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#0000ff;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % (x[0],'nan')
    else:
        return u'&lt;table style=""background-color:#ff0000;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % (x[0], x[1])

HTML(my_panel.apply(report_diff, axis=0).to_html(escape=False))
</code></pre>
";;3;;2014-04-15T15:57:24.027;;23088780;2014-04-15T15:57:24.027;;;;;3536754.0;17095101.0;2;15;;;
12722;12722;;;"<p>Starting from pandas 0.14 (released end of May 2014), postgresql is supported. The <code>sql</code> module now uses <code>sqlalchemy</code> to support different database flavors. You can pass a sqlalchemy engine for a postgresql database (see <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#io-sql"">docs</a>). E.g.:</p>

<pre><code>from sqlalchemy import create_engine
engine = create_engine('postgresql://scott:tiger@localhost:5432/mydatabase')
df.to_sql('table_name', engine)
</code></pre>

<hr>

<p>You are correct that in pandas up to version 0.13.1 postgresql was not supported. If you need to use an older version of pandas, here is a patched version of <code>pandas.io.sql</code>: <a href=""https://gist.github.com/jorisvandenbossche/10841234"">https://gist.github.com/jorisvandenbossche/10841234</a>.<br>
I wrote this a time ago, so cannot fully guarantee that it always works, buth the basis should be there). If you put that file in your working directory and import it, then you should be able to do (where <code>con</code> is a postgresql connection):</p>

<pre><code>import sql  # the patched version (file is named sql.py)
sql.write_frame(df, 'table_name', con, flavor='postgresql')
</code></pre>
";;3;;2014-04-16T08:52:13.810;;23104436;2014-10-08T19:38:01.217;2014-10-08T19:38:01.217;;653364.0;;653364.0;23103962.0;2;33;;;
12738;12738;;;"<p>As you pointed out, this can commonly happen when saving and loading pandas DataFrames as <code>.csv</code> files, which is a text format.</p>

<p>In your case this happened because list objects have a string representation, allowing them to be stored as <code>.csv</code> files. Loading the <code>.csv</code> will then yield that string representation.</p>

<p>If you want to store the actual objects, you should you use <code>DataFrame.to_pickle()</code> (note: objects must be picklable!).</p>

<p>To answer your second question, you can convert it back with <a href=""https://docs.python.org/3.4/library/ast.html#ast.literal_eval"" rel=""noreferrer""><code>ast.literal_eval</code></a>:</p>

<pre><code>&gt;&gt;&gt; from ast import literal_eval
&gt;&gt;&gt; literal_eval('[1.23, 2.34]')
[1.23, 2.34]
</code></pre>
";;2;;2014-04-16T14:13:31.373;;23112008;2016-06-16T17:13:17.453;2016-06-16T17:13:17.453;;2261276.0;;2080262.0;23111990.0;2;18;;;
12796;12796;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/version/0.7.0/generated/pandas.Series.shift.html"">shift</a>.</p>

<pre><code>df['dA'] = df['A'] - df['A'].shift(-1)
</code></pre>
";;6;;2014-04-17T20:43:31.797;;23143081;2014-04-17T20:43:31.797;;;;;3393459.0;23142967.0;2;48;;;
12797;12797;;;"<p>You could use <code>diff</code> and pass <code>-1</code> as the <code>periods</code> argument:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [9, 4, 2, 1], ""B"": [12, 7, 5, 4]})
&gt;&gt;&gt; df[""dA""] = df[""A""].diff(-1)
&gt;&gt;&gt; df
   A   B  dA
0  9  12   5
1  4   7   2
2  2   5   1
3  1   4 NaN

[4 rows x 3 columns]
</code></pre>
";;1;;2014-04-17T20:45:00.443;;23143110;2014-04-17T20:45:00.443;;;;;487339.0;23142967.0;2;17;;;
12805;12805;;;"<p><code>iterrows</code> gives you <code>(index, row)</code> tuples rather than just the rows, so you should be able to access the columns in basically the same way you were thinking if you just do:</p>

<pre><code>for index, row in df.iterrows():
    print row['Date']
</code></pre>
";;1;;2014-04-18T01:26:10.067;;23146038;2014-04-18T01:26:10.067;;;;;1222578.0;23145928.0;2;27;;;
12810;12810;;;"<p>Firstly, your ""messy way"" is ok, there's nothing wrong with using indices into the dataframe, and this will not be too slow. iterrows() itself isn't terribly fast.</p>

<p>A version of your first idea that would work would be:</p>

<pre><code>row_iterator = df.iterrows()
_, last = row_iterator.next()  # take first item from row_iterator
for i, row in row_iterator:
    print(row['value'])
    print(last['value'])
    last = row
</code></pre>

<p>The second method could do something similar, to save one index into the dataframe:</p>

<pre><code>last = df.irow(0)
for i in range(1, df.shape[0]):
    print(last)
    print(df.irow(i))
    last = df.irow(i)
</code></pre>

<p>When speed is critical you can always try both and time the code.</p>
";;1;;2014-04-18T09:57:41.707;;23151722;2015-06-19T14:07:17.827;2015-06-19T14:07:17.827;;3548427.0;;3548427.0;23151246.0;2;11;;;
12852;12852;;;"<p>'Date' is your index so you want to do,</p>

<pre><code>print df.index.min()
print df.index.max()

2014-03-13 00:00:00
2014-03-31 00:00:00
</code></pre>
";;1;;2014-04-20T03:43:59.833;;23178185;2014-04-20T03:43:59.833;;;;;3435183.0;23178129.0;2;19;;;
12879;12879;;;"<p>Use <code>groupby/shift</code> to apply the shift to each group individually: (Thanks to Jeff for pointing out this simplification.)</p>

<pre><code>In [60]: df['beyer_shifted'] = df.groupby(level=0)['beyer'].shift(1); df
Out[61]: 
                  line_date  line_race  beyer  beyer_shifted
Last Gunfighter  2013-09-28         10     99            NaN
Last Gunfighter  2013-08-18         10    102             99
Last Gunfighter  2013-07-06          8    103            102
Paynter          2013-09-28         10    103            NaN
Paynter          2013-08-31         10     88            103
Paynter          2013-07-27          8    100             88
</code></pre>

<p>If you have a multiindex, you can group by more than one level by passing a sequence of <code>ints</code> or level names to <code>groupby's</code> <code>level</code> parameter.</p>
";;5;;2014-04-21T13:11:18.003;;23198160;2016-04-28T12:29:06.033;2016-04-28T12:29:06.033;;190597.0;;190597.0;23198053.0;2;21;;;
12883;12883;;;"<p>Use <code>boolean</code> indexing as you would do in <code>numpy.array</code></p>

<pre><code>df=pd.DataFrame({'Data':np.random.normal(size=200)})  #example dataset of normally distributed data. 
df[np.abs(df.Data-df.Data.mean())&lt;=(3*df.Data.std())] #keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.
df[~(np.abs(df.Data-df.Data.mean())&gt;(3*df.Data.std()))] #or if you prefer the other way around
</code></pre>

<p>For a series it is similar:</p>

<pre><code>S=pd.Series(np.random.normal(size=200))
S[~((S-S.mean()).abs()&gt;3*S.std())]
</code></pre>
";;5;;2014-04-21T15:44:04.580;;23200666;2014-04-21T19:53:15.853;2014-04-21T19:53:15.853;;2487184.0;;2487184.0;23199796.0;2;60;;;
12886;12886;;;"<p>If you have multiple columns in your dataframe and would like to remove all rows that have outliers in at least one column, the following expression would do that in one shot.</p>

<pre><code>df = pd.DataFrame(np.random.randn(100, 3))

from scipy import stats
df[(np.abs(stats.zscore(df)) &lt; 3).all(axis=1)]
</code></pre>
";;2;;2014-04-21T17:20:35.350;;23202269;2014-04-21T17:40:58.203;2014-04-21T17:40:58.203;;3553133.0;;3553133.0;23199796.0;2;51;;;
12935;12935;;;"<p>I know this has already been answered, but just for the sake of a purely pandas solution to this specific question as opposed to the general description from Aman (which was wonderful) and in case anyone else happens upon this:</p>

<pre><code>import pandas as pd
df = df[pd.notnull(df['EPS'])]
</code></pre>
";;3;;2014-04-23T05:37:45.533;;23235618;2014-04-23T05:37:45.533;;;;;2970214.0;13413590.0;2;60;;;
12994;12994;;;"<p>Most <a href=""http://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">sklearn</a> objects work with <code>pandas</code> dataframes just fine, would something like this work for you?</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

df = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))

pca = PCA(n_components=5)
pca.fit(df)
</code></pre>

<p>You can access the components themselves with</p>

<pre><code>pca.components_ 
</code></pre>
";;1;;2014-04-25T00:42:14.113;;23282290;2017-08-03T03:13:33.923;2017-08-03T03:13:33.923;;1078084.0;;1078084.0;23282130.0;2;47;;;
13012;13012;;;"<p>Here's the rules, subsequent override:</p>

<ul>
<li><p>All operations generate a copy</p></li>
<li><p>If <code>inplace=True</code> is provided, it will modify in-place; only some operations support this</p></li>
<li><p>An indexer that sets, e.g. <code>.loc/.ix/.iloc/.iat/.at</code> will set inplace.</p></li>
<li><p>An indexer that gets on a single-dtyped object is almost always a view (depending on the memory layout it may not be that's why this is not reliable). This is mainly for efficiency. (the example from above is for <code>.query</code>; this will <strong>always</strong> return a copy as its evaluated by <code>numexpr</code>)</p></li>
<li><p>An indexer that gets on a multiple-dtyped object is always a copy.</p></li>
</ul>

<p>Your example of <code>chained indexing</code></p>

<pre><code>df[df.C &lt;= df.B].ix[:,'B':'E']
</code></pre>

<p>is not guaranteed to work (and thus you shoulld <strong>never</strong> do this). </p>

<p>Instead do:</p>

<pre><code>df.ix[df.C &lt;= df.B, 'B':'E']
</code></pre>

<p>as this is <em>faster</em> and will always work</p>

<p>The chained indexing is 2 separate python operations and thus cannot be reliably intercepted by pandas (you will oftentimes get a <code>SettingWithCopyWarning</code>, but that is not 100% detectable either). The <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-view-versus-copy"">dev docs</a>, which you pointed, offer a much more full explanation.</p>
";;23;;2014-04-25T14:57:14.193;;23296545;2014-04-25T15:20:05.700;2014-04-25T15:20:05.700;;644898.0;;644898.0;23296282.0;2;36;;;
13030;13030;;;"<p>If I understand right, you want something like this:</p>

<pre><code>w['female'] = w['female'].map({'female': 1, 'male': 0})
</code></pre>

<p>(Here I convert the values to numbers instead of strings containing numbers.  You can convert them to <code>""1""</code> and <code>""0""</code>, if you really want, but I'm not sure why you'd want that.)</p>

<p>The reason your code doesn't work is because using <code>['female']</code> on a column (the second <code>'female'</code> in your <code>w['female']['female']</code>) doesn't mean ""select rows where the value is 'female'"".  It means to select rows where the <em>index</em> is 'female', of which there may not be any in your DataFrame.</p>
";;2;;2014-04-26T06:12:02.527;;23307361;2014-04-26T06:12:02.527;;;;;1427416.0;23307301.0;2;72;;;
13063;13063;;;"<p>I'd do something like the following:</p>

<pre><code>foo = lambda x: pd.Series([i for i in reversed(x.split(','))])
rev = df['City, State, Country'].apply(foo)
print rev

      0    1        2
0   HUN  NaN      NaN
1   ESP  NaN      NaN
2   GBR  NaN      NaN
3   ESP  NaN      NaN
4   FRA  NaN      NaN
5   USA   ID      NaN
6   USA   GA      NaN
7   USA   NJ  Hoboken
8   USA   NJ      NaN
9   AUS  NaN      NaN
</code></pre>

<p>I think that gets you what you want but if you also want to pretty things up and get a City, State, Country column order, you could add the following:</p>

<pre><code>rev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)
rev = rev[['City','State','Country']]
print rev

     City State Country
0      NaN   NaN     HUN
1      NaN   NaN     ESP
2      NaN   NaN     GBR
3      NaN   NaN     ESP
4      NaN   NaN     FRA
5      NaN    ID     USA
6      NaN    GA     USA
7  Hoboken    NJ     USA
8      NaN    NJ     USA
9      NaN   NaN     AUS
</code></pre>
";;0;;2014-04-26T23:23:50.320;;23317595;2014-04-26T23:34:03.130;2014-04-26T23:34:03.130;;3435183.0;;3435183.0;23317342.0;2;33;;;
13079;13079;;;"<p>You should assign value by <code>df.ix[i, 'exp']=X</code> or <code>df.loc[i, 'exp']=X</code> instead of <code>df.ix[i]['ifor'] = x</code>. </p>

<p>Otherwise you are working on a view, and should get a warming:</p>

<p><code>-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead</code></p>

<p>But certainly, loop probably should better be replaced by some vectorized algorithm to make the full use of <code>DataFrame</code> as @Phillip Cloud suggested.</p>
";;0;;2014-04-28T02:36:51.847;;23331659;2014-04-28T02:36:51.847;;;;;2487184.0;23330654.0;2;6;;;
13083;13083;;;"<p>I prefer to create queries with <a href=""http://www.sqlalchemy.org/""><strong>SQLAlchemy</strong></a>, and then make a DataFrame from it. <strong>SQLAlchemy</strong> makes it easier to combine <strong>SQL</strong> conditions Pythonically if you intend to mix and match things over and over.</p>

<pre><code>from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Table
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from pandas import DataFrame
import datetime

# We are connecting to an existing service
engine = create_engine('dialect://user:pwd@host:port/db', echo=False)
Session = sessionmaker(bind=engine)
session = Session()
Base = declarative_base()

# And we want to query an existing table
tablename = Table('tablename', 
    Base.metadata, 
    autoload=True, 
    autoload_with=engine, 
    schema='ownername')

# These are the ""Where"" parameters, but I could as easily 
# create joins and limit results
us = tablename.c.country_code.in_(['US','MX'])
dc = tablename.c.locn_name.like('%DC%')
dt = tablename.c.arr_date &gt;= datetime.date.today() # Give me convenience or...

q = session.query(tablename).\
            filter(us &amp; dc &amp; dt) # That's where the magic happens!!!

def querydb(query):
    """"""
    Function to execute query and return DataFrame.
    """"""
    df = DataFrame(query.all());
    df.columns = [x['name'] for x in query.column_descriptions]
    return df

querydb(q)
</code></pre>
";;0;;2014-04-28T03:08:09.287;;23331896;2014-04-28T03:21:59.873;2014-04-28T03:21:59.873;;1479269.0;;1479269.0;10065051.0;2;15;;;
13128;13128;;;"<p>This can be done with <code>pd.melt()</code>:</p>

<pre><code># value_name is 'value' by default, but setting it here to make it clear
pd.melt(x, id_vars=['farm', 'fruit'], var_name='year', value_name='value')
</code></pre>

<p>Result:</p>

<pre><code>  farm  fruit  year  value
0    A  apple  2014     10
1    B  apple  2014     12
2    A   pear  2014      6
3    B   pear  2014      8
4    A  apple  2015     11
5    B  apple  2015     13
6    A   pear  2015      7
7    B   pear  2015      9

[8 rows x 4 columns]
</code></pre>

<p>I'm not sure how common ""melt"" is as the name for this kind of operation, but that's what it's called in R's <code>reshape2</code> package, which probably inspired the name here.</p>
";;1;;2014-04-29T02:05:49.027;;23354240;2014-04-29T02:05:49.027;;;;;1222578.0;23354124.0;2;15;;;
13159;13159;;;"<p>You need to make a second groupby object that groups by the states, and then use the <code>div</code> method:</p>

<pre><code>import numpy as np
import pandas as pd
np.random.seed(0)
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
               'office_id': list(range(1, 7)) * 2,
               'sales': [np.random.randint(100000, 999999) for _ in range(12)]})

state_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
state = df.groupby(['state']).agg({'sales': 'sum'})
state_office.div(state, level='state') * 100


                     sales
state office_id           
AZ    2          16.981365
      4          19.250033
      6          63.768601
CA    1          19.331879
      3          33.858747
      5          46.809373
CO    1          36.851857
      3          19.874290
      5          43.273852
WA    2          34.707233
      4          35.511259
      6          29.781508
</code></pre>

<p>the <code>level='state'</code> kwarg in <code>div</code> tells pandas to broadcast/join the dataframes base on the values in the <code>state</code> level of the index.</p>
";;2;;2014-04-29T23:35:30.953;;23377155;2014-04-29T23:35:30.953;;;;;1552748.0;23377108.0;2;24;;;
13160;13160;;;"<p><a href=""https://stackoverflow.com/a/23377155/3393459"">Paul H's answer</a> is right that you will have to make a second <code>groupby</code> object, but you can calculate the percentage in a simpler way -- just <code>groupby</code> the <code>state_office</code> and divide the <code>sales</code> column by its sum. Copying the beginning of Paul H's answer:</p>

<pre><code># From Paul H
import numpy as np
import pandas as pd
np.random.seed(0)
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': list(range(1, 7)) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})
state_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
# Change: groupby state_office and divide by sum
state_pcts = state_office.groupby(level=0).apply(lambda x:
                                                 100 * x / float(x.sum()))
</code></pre>

<p>Returns:</p>

<pre><code>                     sales
state office_id           
AZ    2          16.981365
      4          19.250033
      6          63.768601
CA    1          19.331879
      3          33.858747
      5          46.809373
CO    1          36.851857
      3          19.874290
      5          43.273852
WA    2          34.707233
      4          35.511259
      6          29.781508
</code></pre>
";;9;;2014-04-29T23:45:31.563;;23377232;2017-01-25T18:25:03.953;2017-05-23T11:55:06.673;;-1.0;;3393459.0;23377108.0;2;52;;;
13197;13197;;;"<p>For efficient appending see <a href=""https://stackoverflow.com/questions/19365513/how-to-add-an-extra-row-to-a-pandas-dataframe/19368360#19368360"">How to add an extra row to a pandas dataframe</a> and <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#setting-with-enlargement"" rel=""nofollow noreferrer""><em>Setting With Enlargement</em></a>.</p>

<p>Add rows through <code>loc/ix</code> on <strong>non existing</strong> key index data. e.g. :</p>

<pre><code>In [1]: se = pd.Series([1,2,3])

In [2]: se
Out[2]: 
0    1
1    2
2    3
dtype: int64

In [3]: se[5] = 5.

In [4]: se
Out[4]: 
0    1.0
1    2.0
2    3.0
5    5.0
dtype: float64
</code></pre>

<p>Or:</p>

<pre><code>In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),
   .....:                 columns=['A','B'])
   .....: 

In [2]: dfi
Out[2]: 
   A  B
0  0  1
1  2  3
2  4  5

In [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']

In [4]: dfi
Out[4]: 
   A  B  C
0  0  1  0
1  2  3  2
2  4  5  4
In [5]: dfi.loc[3] = 5

In [6]: dfi
Out[6]: 
   A  B  C
0  0  1  0
1  2  3  2
2  4  5  4
3  5  5  5
</code></pre>
";;2;;2014-04-30T17:31:04.917;;23394497;2017-02-27T01:47:07.940;2017-05-23T12:26:33.380;;-1.0;;1085495.0;10715965.0;2;37;;;
13198;13198;;;"<p>Method #1: use <code>idxmin()</code> to get the <em>indices</em> of the elements of minimum <code>diff</code>, and then select those:</p>

<pre><code>&gt;&gt;&gt; df.loc[df.groupby(""item"")[""diff""].idxmin()]
   item  diff  otherstuff
1     1     1           2
6     2    -6           2
7     3     0           0

[3 rows x 3 columns]
</code></pre>

<p>Method #2: sort by <code>diff</code>, and then take the first element in each <code>item</code> group:</p>

<pre><code>&gt;&gt;&gt; df.sort(""diff"").groupby(""item"", as_index=False).first()
   item  diff  otherstuff
0     1     1           2
1     2    -6           2
2     3     0           0

[3 rows x 3 columns]
</code></pre>

<p>Note that the resulting indices are different even though the row content is the same.</p>
";;3;;2014-04-30T17:43:00.297;;23394706;2014-04-30T17:43:00.297;;;;;487339.0;23394476.0;2;35;;;
13266;13266;;;"<p>Are you getting errors, or just not sure where to start?</p>

<pre><code>%pylab inline
import pandas as pd
import matplotlib.pyplot as plt

df2 = df.groupby(['Name', 'Abuse/NFF'])['Name'].count().unstack('Abuse/NFF').fillna(0)
df2[['abuse','nff']].plot(kind='bar', stacked=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/vykVK.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vykVK.jpg"" alt=""stacked bar plot""></a></p>
";;6;;2014-05-02T13:00:33.583;;23428804;2016-12-08T04:45:43.617;2016-12-08T04:45:43.617;;210945.0;;666246.0;23415500.0;2;26;;;
13319;13319;;;"<p>Well the <code>pandas'</code> versions of <code>mean</code> and <code>std</code> will hand the <code>Nan</code> so you could just compute that way (to get the same as scipy zscore I think you need to use ddof=0 on <code>std</code>):</p>

<pre><code>df['zscore'] = (df.a - df.a.mean())/df.a.std(ddof=0)
print df

        a    zscore
0     NaN       NaN
1  0.0767 -1.148329
2  0.4383  0.071478
3  0.7866  1.246419
4  0.8091  1.322320
5  0.1954 -0.747912
6  0.6307  0.720512
7  0.6599  0.819014
8  0.1065 -1.047803
9  0.0508 -1.235699
</code></pre>
";;0;;2014-05-04T00:33:50.717;;23451304;2014-05-04T00:50:57.237;2014-05-04T00:50:57.237;;3435183.0;;3435183.0;23451244.0;2;17;;;
13340;13340;;;"<p>Interestingly enough, very often <code>len(unique())</code> is a few times (3x-15x) faster than <code>nunique()</code>.</p>
";;2;;2014-05-05T02:59:28.173;;23464103;2015-12-23T10:55:33.657;2015-12-23T10:55:33.657;;2901002.0;;2370124.0;15411158.0;2;21;;;
13365;13365;;;"<p>In Pandas version 0.13 and greater the index level names are immutable (type <code>FrozenList</code>) and can no longer be set directly.  You must first use <code>Index.rename()</code> to apply the new index level names to the Index and then use <code>DataFrame.reindex()</code> to apply the new index to the DataFrame.  Examples:</p>

<p><strong>For Pandas version &lt; 0.13</strong></p>

<pre><code>df.index.names = ['Date']
</code></pre>

<p><strong>For Pandas version >= 0.13</strong></p>

<pre><code>df = df.reindex(df.index.rename(['Date']))
</code></pre>
";;3;;2014-05-05T17:38:45.837;;23478395;2015-04-30T16:09:48.497;2015-04-30T16:09:48.497;;419338.0;;3556656.0;19851005.0;2;11;;;
13369;13369;;;"<p>Here is my first version that seems to be working fine, feel free to copy or make suggestions on how it could be more efficient (I have quite a long experience with programming in general but not that long with python or numpy)</p>

<p>This function creates single random balanced subsample.</p>

<p>edit: The subsample size now samples down minority classes, this should probably be changed.</p>

<pre><code>def balanced_subsample(x,y,subsample_size=1.0):

    class_xs = []
    min_elems = None

    for yi in np.unique(y):
        elems = x[(y == yi)]
        class_xs.append((yi, elems))
        if min_elems == None or elems.shape[0] &lt; min_elems:
            min_elems = elems.shape[0]

    use_elems = min_elems
    if subsample_size &lt; 1:
        use_elems = int(min_elems*subsample_size)

    xs = []
    ys = []

    for ci,this_xs in class_xs:
        if len(this_xs) &gt; use_elems:
            np.random.shuffle(this_xs)

        x_ = this_xs[:use_elems]
        y_ = np.empty(use_elems)
        y_.fill(ci)

        xs.append(x_)
        ys.append(y_)

    xs = np.concatenate(xs)
    ys = np.concatenate(ys)

    return xs,ys
</code></pre>

<p>For anyone trying to make the above work with a Pandas DataFrame, you need to make a couple of changes: </p>

<ol>
<li><p>Replace the <code>np.random.shuffle</code> line with </p>

<p><code>this_xs = this_xs.reindex(np.random.permutation(this_xs.index))</code></p></li>
<li><p>Replace the <code>np.concatenate</code> lines with </p>

<p><code>xs = pd.concat(xs)
ys = pd.Series(data=np.concatenate(ys),name='target')</code></p></li>
</ol>
";;1;;2014-05-05T19:11:46.463;;23479973;2016-02-23T23:20:49.293;2016-02-23T23:20:49.293;;4667484.0;;615985.0;23455728.0;2;16;;;
13383;13383;;;"<p>Below is an example that performs lexicographic sort on a dataframe.
The idea is to create an numerical index based on the specific sort.
Then to perform a numerical sort based on the index.
A column is added to the dataframe to do so, and is then removed.</p>

<pre><code>import pandas as pd

# Create DataFrame
df = pd.DataFrame(
{'id':[2967, 5335, 13950, 6141, 6169],\
 'Player': ['Cedric Hunter', 'Maurice Baker' ,\
            'Ratko Varda' ,'Ryan Bowen' ,'Adrian Caldwell'],\
 'Year': [1991 ,2004 ,2001 ,2009 ,1997],\
 'Age': [27 ,25 ,22 ,34 ,31],\
 'Tm':['CHH' ,'VAN' ,'TOT' ,'OKC' ,'DAL'],\
 'G':[6 ,7 ,60 ,52 ,81]})

# Define the sorter
sorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL','DEN',\
          'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',\
          'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',\
          'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',\
          'WAS', 'WSB']

# Create the dictionary that defines the order for sorting
sorterIndex = dict(zip(sorter,range(len(sorter))))

# Generate a rank column that will be used to sort
# the dataframe numerically
df['Tm_Rank'] = df['Tm'].map(sorterIndex)

# Here is the result asked with the lexicographic sort
# Result may be hard to analyze, so a second sorting is
# proposed next
df.sort(['Player', 'Year', 'Tm_Rank'], \
        ascending = [True, True, True], inplace = True)
df.drop('Tm_Rank', 1, inplace = True)
print(df)

# Here is an example where 'Tm' is sorted first, that will 
# give the first row of the DataFrame df to contain TOT as 'Tm'
df['Tm_Rank'] = df['Tm'].map(sorterIndex)
df.sort(['Tm_Rank', 'Player', 'Year'], \
        ascending = [True , True, True], inplace = True)
df.drop('Tm_Rank', 1, inplace = True)
print(df)
</code></pre>
";;2;;2014-05-05T22:50:38.337;;23483221;2016-01-09T16:51:12.157;2016-01-09T16:51:12.157;;1478537.0;;1478537.0;23482668.0;2;6;;;
13423;13423;;;"<p>Well, I can think of a few ways. (1) essentially blow up the dataframe by merging on <code>company</code> and then filter on the 30 day windows after the merge. This should be fast but could use lots of memory. (2) Move the merging and filtering on the 30 day window into a groupby. This results in a merge for each group so it would be slower but it should use less memory</p>

<p><strong>Option #1</strong></p>

<p>Suppose your data looks like the following (I expanded your sample data):</p>

<pre><code>print df

    company       date  measure
0         0 2010-01-01       10
1         0 2010-01-15       10
2         0 2010-02-01       10
3         0 2010-02-15       10
4         0 2010-03-01       10
5         0 2010-03-15       10
6         0 2010-04-01       10
7         1 2010-03-01        5
8         1 2010-03-15        5
9         1 2010-04-01        5
10        1 2010-04-15        5
11        1 2010-05-01        5
12        1 2010-05-15        5

print windows

   company   end_date
0        0 2010-02-01
1        0 2010-03-15
2        1 2010-04-01
3        1 2010-05-15
</code></pre>

<p>Create a beginning date for the 30 day windows:</p>

<pre><code>windows['beg_date'] = (windows['end_date'].values.astype('datetime64[D]') -
                       np.timedelta64(30,'D'))
print windows

   company   end_date   beg_date
0        0 2010-02-01 2010-01-02
1        0 2010-03-15 2010-02-13
2        1 2010-04-01 2010-03-02
3        1 2010-05-15 2010-04-15
</code></pre>

<p>Now do a merge and then select based on if <code>date</code> falls within <code>beg_date</code> and <code>end_date</code>:</p>

<pre><code>df = df.merge(windows,on='company',how='left')
df = df[(df.date &gt;= df.beg_date) &amp; (df.date &lt;= df.end_date)]
print df

    company       date  measure   end_date   beg_date
2         0 2010-01-15       10 2010-02-01 2010-01-02
4         0 2010-02-01       10 2010-02-01 2010-01-02
7         0 2010-02-15       10 2010-03-15 2010-02-13
9         0 2010-03-01       10 2010-03-15 2010-02-13
11        0 2010-03-15       10 2010-03-15 2010-02-13
16        1 2010-03-15        5 2010-04-01 2010-03-02
18        1 2010-04-01        5 2010-04-01 2010-03-02
21        1 2010-04-15        5 2010-05-15 2010-04-15
23        1 2010-05-01        5 2010-05-15 2010-04-15
25        1 2010-05-15        5 2010-05-15 2010-04-15
</code></pre>

<p>You can compute the 30 day window sums by grouping on <code>company</code> and <code>end_date</code>:</p>

<pre><code>print df.groupby(['company','end_date']).sum()

                    measure
company end_date           
0       2010-02-01       20
        2010-03-15       30
1       2010-04-01       10
        2010-05-15       15
</code></pre>

<p><strong>Option #2</strong> Move all merging into a groupby. This should be better on memory but I would think much slower:</p>

<pre><code>windows['beg_date'] = (windows['end_date'].values.astype('datetime64[D]') -
                       np.timedelta64(30,'D'))

def cond_merge(g,windows):
    g = g.merge(windows,on='company',how='left')
    g = g[(g.date &gt;= g.beg_date) &amp; (g.date &lt;= g.end_date)]
    return g.groupby('end_date')['measure'].sum()

print df.groupby('company').apply(cond_merge,windows)

company  end_date  
0        2010-02-01    20
         2010-03-15    30
1        2010-04-01    10
         2010-05-15    15
</code></pre>

<p><strong>Another option</strong> Now if your windows never overlap (like in the example data), you could do something like the following as an alternative that doesn't blow up a dataframe but is pretty fast:</p>

<pre><code>windows['date'] = windows['end_date']

df = df.merge(windows,on=['company','date'],how='outer')
print df

    company       date  measure   end_date
0         0 2010-01-01       10        NaT
1         0 2010-01-15       10        NaT
2         0 2010-02-01       10 2010-02-01
3         0 2010-02-15       10        NaT
4         0 2010-03-01       10        NaT
5         0 2010-03-15       10 2010-03-15
6         0 2010-04-01       10        NaT
7         1 2010-03-01        5        NaT
8         1 2010-03-15        5        NaT
9         1 2010-04-01        5 2010-04-01
10        1 2010-04-15        5        NaT
11        1 2010-05-01        5        NaT
12        1 2010-05-15        5 2010-05-15
</code></pre>

<p>This merge essentially inserts your window end dates into the dataframe and then backfilling the end dates (by group) will give you a structure to easily create you summation windows:</p>

<pre><code>df['end_date'] = df.groupby('company')['end_date'].apply(lambda x: x.bfill())

print df

    company       date  measure   end_date
0         0 2010-01-01       10 2010-02-01
1         0 2010-01-15       10 2010-02-01
2         0 2010-02-01       10 2010-02-01
3         0 2010-02-15       10 2010-03-15
4         0 2010-03-01       10 2010-03-15
5         0 2010-03-15       10 2010-03-15
6         0 2010-04-01       10        NaT
7         1 2010-03-01        5 2010-04-01
8         1 2010-03-15        5 2010-04-01
9         1 2010-04-01        5 2010-04-01
10        1 2010-04-15        5 2010-05-15
11        1 2010-05-01        5 2010-05-15
12        1 2010-05-15        5 2010-05-15

df = df[df.end_date.notnull()]
df['beg_date'] = (df['end_date'].values.astype('datetime64[D]') -
                   np.timedelta64(30,'D'))

print df

   company       date  measure   end_date   beg_date
0         0 2010-01-01       10 2010-02-01 2010-01-02
1         0 2010-01-15       10 2010-02-01 2010-01-02
2         0 2010-02-01       10 2010-02-01 2010-01-02
3         0 2010-02-15       10 2010-03-15 2010-02-13
4         0 2010-03-01       10 2010-03-15 2010-02-13
5         0 2010-03-15       10 2010-03-15 2010-02-13
7         1 2010-03-01        5 2010-04-01 2010-03-02
8         1 2010-03-15        5 2010-04-01 2010-03-02
9         1 2010-04-01        5 2010-04-01 2010-03-02
10        1 2010-04-15        5 2010-05-15 2010-04-15
11        1 2010-05-01        5 2010-05-15 2010-04-15
12        1 2010-05-15        5 2010-05-15 2010-04-15

df = df[(df.date &gt;= df.beg_date) &amp; (df.date &lt;= df.end_date)]
print df.groupby(['company','end_date']).sum()

                    measure
company end_date           
0       2010-02-01       20
        2010-03-15       30
1       2010-04-01       10
        2010-05-15       15
</code></pre>

<p>Another alternative is to resample your first dataframe to daily data and then compute rolling_sums with a 30 day window; and select the dates at the end that you are interested in. This could be quite memory intensive too.</p>
";;2;;2014-05-07T05:42:39.883;;23509622;2014-05-07T19:34:36.923;2014-05-07T19:34:36.923;;3435183.0;;3435183.0;23508351.0;2;18;;;
13441;13441;;;"<p>No need to initialize an empty DataFrame (you weren't even doing that, you'd need <code>pd.DataFrame()</code> with the parens). Instead make a list of Series and concat those together with <code>df = pd.concat(series, axis=1)</code></p>

<p>Something like:</p>

<pre><code>series = [pd.Series(mat[name][:, 1]) for name in Variables]
df = pd.concat(series, axis=1)
</code></pre>
";;2;;2014-05-07T15:28:42.990;;23522030;2014-05-07T15:28:42.990;;;;;1889400.0;23521511.0;2;8;;;
13469;13469;;;"<p>A general approach is to tell matplotlib the desired number of ticks:</p>

<pre><code>plt.locator_params(nbins=10)
</code></pre>

<p>Edit by comments from @Daniel Power: to change for a single axis (e.g. <code>'x'</code>) on an axis, use:</p>

<pre><code>ax.locator_params(nbins=10, axis='x')
</code></pre>
";;5;;2014-05-08T06:39:07.190;;23534505;2016-09-01T09:43:38.473;2016-09-01T09:43:38.473;;832621.0;;832621.0;13052844.0;2;21;;;
13491;13491;;;"<p>Here are ways to convert timedeltas, docs are <a href=""http://pandas-docs.github.io/pandas-docs-travis/timeseries.html#time-deltas"">here</a></p>

<pre><code>In [2]: pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')
Out[2]: 
0   0 days, 00:00:01
1   1 days, 00:00:01
2   2 days, 00:00:01
3   3 days, 00:00:01
4   4 days, 00:00:01
dtype: timedelta64[ns]
</code></pre>

<p>Convert to seconds (is an exact conversion)</p>

<pre><code>In [3]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')).astype('timedelta64[s]')
Out[3]: 
0         1
1     86401
2    172801
3    259201
4    345601
dtype: float64
</code></pre>

<p>Convert using astype will round to that unit</p>

<pre><code>In [4]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')).astype('timedelta64[D]')
Out[4]: 
0    0
1    1
2    2
3    3
4    4
dtype: float64
</code></pre>

<p>Division will give an exact repr</p>

<pre><code>In [5]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')) / np.timedelta64(1,'D')
Out[5]: 
0    0.000012
1    1.000012
2    2.000012
3    3.000012
4    4.000012
dtype: float64
</code></pre>
";;1;;2014-05-08T14:01:24.823;;23544011;2014-05-08T14:01:24.823;;;;;644898.0;23543909.0;2;25;;;
13515;13515;;;"<p>This should do the trick</p>

<pre><code>'g' in df.index
</code></pre>
";;2;;2014-05-08T18:19:30.533;;23549599;2014-05-08T18:19:30.533;;;;;1478537.0;23549231.0;2;76;;;
13600;13600;;;"<p>You can do it with <code>concat</code> (the <code>keys</code> argument will create the hierarchical columns index):</p>

<pre><code>dict = {'ABC' : df1, 'XYZ' : df2}
print pd.concat(dict.values(),axis=1,keys=dict.keys())


                XYZ                                          ABC           \
               Open     High      Low    Close   Volume     Open     High   
Date                                                                        
2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  0.18077  0.18800   
2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  0.18439  0.21331   
2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  0.19523  0.20970   


                Low    Close   Volume  
Date                                   
2002-01-17  0.16993  0.18439  1720833  
2002-01-18  0.18077  0.19523  2027866  
2002-01-21  0.19162  0.20608   771149
</code></pre>

<p>Really <code>concat</code> wants lists so the following is equivalent:</p>

<pre><code>print pd.concat([df1,df2],axis=1,keys=['ABC','XYZ'])
</code></pre>

<p>and in Python >3 you will need to explicitly create the lists using:</p>

<pre><code>print(pd.concat( list(dict.values()), axis=1, list(keys=dict.keys()) ))
</code></pre>
";;0;;2014-05-12T04:00:20.827;;23600844;2014-05-12T05:39:56.327;2014-05-12T05:39:56.327;;3435183.0;;3435183.0;23600582.0;2;32;;;
13718;13718;;;"<p>You could try this if you have 3 dataframes</p>

<pre><code># Merge multiple dataframes
df1 = pd.DataFrame(np.array([
    ['a', 5, 9],
    ['b', 4, 61],
    ['c', 24, 9]]),
    columns=['name', 'attr11', 'attr12'])
df2 = pd.DataFrame(np.array([
    ['a', 5, 19],
    ['b', 14, 16],
    ['c', 4, 9]]),
    columns=['name', 'attr21', 'attr22'])
df3 = pd.DataFrame(np.array([
    ['a', 15, 49],
    ['b', 4, 36],
    ['c', 14, 9]]),
    columns=['name', 'attr31', 'attr32'])

pd.merge(pd.merge(df1,df2,on='name'),df3,on='name')
</code></pre>

<p>alternatively, as mentioned by cwharland </p>

<pre><code>df1.merge(df2,on='name').merge(df3,on='name')
</code></pre>
";;1;;2014-05-15T07:04:25.750;;23671390;2014-05-15T16:35:08.653;2014-05-15T16:35:08.653;;2137255.0;;2137255.0;23668427.0;2;42;;;
13752;13752;;;"<pre><code>gb = df.groupby('ZZ')    
[gb.get_group(x) for x in gb.groups]
</code></pre>
";;0;;2014-05-16T01:15:12.757;;23691168;2014-05-16T01:15:12.757;;;;;1583239.0;23691133.0;2;32;;;
13754;13754;;;"<p>Joe Kington's answer was very helpful, however, I noticed that it does not bin all of the data. It actually leaves out the row with a = a.min(). Summing up <code>groups.size()</code> gave 99 instead of 100.</p>

<p>To guarantee that all data is binned, just pass in the number of bins to cut() and that function will automatically pad the first[last] bin by 0.1% to ensure all data is included.</p>

<pre><code>df = pandas.DataFrame({""a"": np.random.random(100), 
                    ""b"": np.random.random(100) + 10})

# Bin the data frame by ""a"" with 10 bins...
groups = df.groupby(pandas.cut(df.a, 10))

# Get the mean of b, binned by the values in a
print(groups.mean().b)
</code></pre>

<p>In this case, summing up groups.size() gave 100.</p>

<p>I know this is a picky point for this particular problem, but for a similar problem I was trying to solve, it was crucial to obtain the correct answer.</p>
";;0;;2014-05-16T02:26:51.107;;23691692;2014-05-16T02:26:51.107;;;;;3643112.0;16947336.0;2;13;;;
13758;13758;;;"<p>For a more precise answer related to OP's question (with Pandas):</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.DataFrame({ ""A"":np.random.normal(0.8,0.2,20),
                      ""B"":np.random.normal(0.8,0.1,20), 
                      ""C"":np.random.normal(0.9,0.1,20)} )

data.boxplot()

for i,d in enumerate(data):
    y = data[d]
    x = np.random.normal(i+1, 0.04, len(y))
    plt.plot(x, y, mfc = [""orange"",""blue"",""yellow""][i], mec='k', ms=7, marker=""o"", linestyle=""None"")

plt.hlines(1,0,4,linestyle=""--"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/wpeZ7.png"" alt=""boxplot""></p>

<hr>

<p>Old version (more generic) :</p>

<p>With matplotlib :</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

a = np.random.normal(0,2,1000)
b = np.random.normal(-2,7,100)
data = [a,b]

plt.boxplot(data) # Or you can use the boxplot from Pandas

for i in [1,2]:
    y = data[i-1]
    x = np.random.normal(i, 0.02, len(y))
    plt.plot(x, y, 'r.', alpha=0.2)
</code></pre>

<p>Which gives that :
<img src=""https://i.stack.imgur.com/6KAK9.png"" alt=""dot-boxplot""></p>

<p>Inspired from <a href=""http://nbviewer.ipython.org/urls/gist.github.com/fonnesbeck/5850463/raw/a29d9ffb863bfab09ff6c1fc853e1d5bf69fe3e4/3.+Plotting+and+Visualization.ipynb"" rel=""noreferrer"">this tutorial</a></p>

<p>Hope this helps !</p>
";;0;;2014-05-16T13:22:24.880;;23696169;2014-06-04T16:07:27.593;2014-06-04T16:07:27.593;;3297428.0;;3297428.0;23519135.0;2;12;;;
13773;13773;;;"<p>use <code>groupby</code> and <code>filter</code></p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"":[""foo"", ""foo"", ""foo"", ""bar""], ""B"":[0,1,1,1], ""C"":[""A"",""A"",""B"",""A""]})
df.groupby([""A"", ""C""]).filter(lambda df:df.shape[0] == 1)
</code></pre>
";;0;;2014-05-17T08:27:41.443;;23709208;2014-05-17T08:27:41.443;;;;;772649.0;23667369.0;2;7;;;
13804;13804;;;"<p>You most likely have an extra character at the beginning of your file, that is prepended to your first column name, <code>'Date'</code>. Simply Copy / Paste your output to a non-unicode console produces.</p>

<pre><code>Index([u'?Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')
</code></pre>
";;0;;2014-05-19T08:16:16.920;;23732825;2016-07-22T02:20:13.060;2016-07-22T02:20:13.060;;1580216.0;;1265154.0;23731564.0;2;11;;;
13805;13805;;;"<p>As mentioned by alko, it is probably extra character at the beginning of your file.
When using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>read_csv</code></a>, you can specify <code>encoding</code> to deal with encoding and heading character, known as <a href=""http://en.wikipedia.org/wiki/Byte_order_mark"" rel=""nofollow noreferrer"">BOM (Byte order mark)</a></p>

<pre><code>df = pd.read_csv('values.csv', delimiter=',', encoding=""utf-8-sig"")
</code></pre>

<p>This question finds some echoes on Stackoverflow: 
<a href=""https://stackoverflow.com/questions/21806496/pandas-seems-to-ignore-first-column-name-when-reading-tab-delimited-data-gives"">Pandas seems to ignore first column name when reading tab-delimited data, gives KeyError</a></p>
";;2;;2014-05-19T08:52:58.900;;23733522;2017-01-17T22:08:03.143;2017-05-23T10:31:15.950;;-1.0;;1478537.0;23731564.0;2;20;;;
13806;13806;;;"<p>I don't have reputation to comment yet, but I want to add to <a href=""https://stackoverflow.com/a/20637559/1896459"">alko</a> answer for further reference.</p>

<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>skiprows: A collection of numbers for rows in the file to skip. Can also be an integer to skip the first n rows</p>
</blockquote>
";;0;;2014-05-19T13:35:52.990;;23739252;2014-05-19T13:35:52.990;2017-05-23T12:26:32.797;;-1.0;;1896459.0;20637439.0;2;11;;;
13809;13809;;;"<p>You could also do something like this:</p>

<pre><code>df = df[['mean', '0', '1', '2', '3']]
</code></pre>

<p>You can get the list of columns with:</p>

<pre><code>cols = list(df.columns.values)
</code></pre>

<p>The output will produce:</p>

<pre><code>['0', '1', '2', '3', 'mean']
</code></pre>

<p>...which is then easy to rearrange manually before dropping it into the first function</p>
";;2;;2014-05-19T15:20:33.217;;23741480;2014-05-19T15:20:33.217;;;;;2518486.0;13148429.0;2;101;;;
13810;13810;;;"<p>You could also do something like this:</p>

<pre><code>df = df[['x', 'y', 'a', 'b']]
</code></pre>

<p>You can get the list of columns with:</p>

<pre><code>cols = list(df.columns.values)
</code></pre>

<p>The output will produce something like this:</p>

<pre><code>['a', 'b', 'x', 'y']
</code></pre>

<p>...which is then easy to rearrange manually before dropping it into the first function</p>
";;0;;2014-05-19T15:32:06.533;;23741704;2014-05-19T15:32:06.533;;;;;2518486.0;12329853.0;2;20;;;
13813;13813;;;"<p>You should use method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer"">fillna</a></p>

<pre><code>from pandas import DataFrame
from numpy import nan
DataFrame().fillna(value=nan, inplace=True)
</code></pre>

<p>Example</p>

<pre><code>my_dataframe.fillna(value=nan, inplace=True)
</code></pre>
";;0;;2014-05-19T17:18:01.040;;23743582;2017-07-07T05:55:29.480;2017-07-07T05:55:29.480;;1478537.0;;1478537.0;23743460.0;2;28;;;
13821;13821;;;"<p><code>isnull</code> and <code>notnull</code> work with <code>NaT</code> so you can handle them much the same way you handle <code>NaNs</code>:</p>

<pre><code>&gt;&gt;&gt; df

   a          b  c
0  1        NaT  w
1  2 2014-02-01  g
2  3        NaT  x

&gt;&gt;&gt; df.dtypes

a             int64
b    datetime64[ns]
c            object
</code></pre>

<p>just use <code>isnull</code> to select:</p>

<pre><code>df[df.b.isnull()]

   a   b  c
0  1 NaT  w
2  3 NaT  x
</code></pre>
";;0;;2014-05-19T21:34:16.797;;23747587;2014-05-19T21:34:16.797;;;;;3435183.0;23747451.0;2;22;;;
13825;13825;;;"<p>Use <code>.values</code> to get a <code>numpy.array</code> and then <code>.tolist()</code> to get a list.</p>

<p>For example:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],
                   'b':[3,5,6,2,4,6,7,8,7,8,9]})
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; df['a'].values.tolist()
[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]
</code></pre>

<p>or you can just use</p>

<pre><code>&gt;&gt;&gt; df['a'].tolist()
[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]
</code></pre>

<p>To drop duplicates you can do one of the following:</p>

<pre><code>&gt;&gt;&gt; df['a'].drop_duplicates().values.tolist()
[1, 3, 5, 7, 4, 6, 8, 9]
&gt;&gt;&gt; list(set(df['a'])) # as pointed out by EdChum
[1, 3, 4, 5, 6, 7, 8, 9]
</code></pre>
";;6;;2014-05-20T00:09:04.943;;23749057;2016-08-20T14:46:09.487;2016-08-20T14:46:09.487;;5642766.0;;1078084.0;23748995.0;2;88;;;
13907;13907;;;"<p>So let's dissect this:</p>

<pre><code>df_train_csv = pd.read_csv('./train.csv',parse_dates=['Date'],index_col='Date')
</code></pre>

<p>OK first problem here is you have specified that the index column should be 'Date' this means that you will not have a 'Date' column anymore.</p>

<pre><code>start = datetime(2010, 2, 5)
end = datetime(2012, 10, 26)

df_train_fly = pd.date_range(start, end, freq=""W-FRI"")
df_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=['Date'])

merged = df_train_csv.join(df_train_fly.set_index(['Date']), on = ['Date'], how = 'right', lsuffix='_x')
</code></pre>

<p>So the above join will not work as the error reported so in order to fix this:</p>

<pre><code># remove the index_col param
df_train_csv = pd.read_csv('./train.csv',parse_dates=['Date'])
# don't set the index on df_train_fly
merged = df_train_csv.join(df_train_fly, on = ['Date'], how = 'right', lsuffix='_x')
</code></pre>

<p>OR don't set the 'on' param:</p>

<pre><code>merged = df_train_csv.join(df_train_fly, how = 'right', lsuffix='_x')
</code></pre>

<p>the above will use the index of both df's to join on</p>

<p>You can also achieve the same result by performing a merge instead:</p>

<pre><code>merged = df_train_csv.merge(df_train_fly.set_index(['Date']), left_index=True, right_index=True, how = 'right', lsuffix='_x')
</code></pre>
";;3;;2014-05-21T15:12:51.450;;23787275;2014-05-21T15:18:51.893;2014-05-21T15:18:51.893;;704848.0;;704848.0;23787072.0;2;19;;;
13908;13908;;;"<p>you can use directly fillna and assigning the result to the column 'bar'</p>

<pre><code>df['bar'].fillna(df['foo'], inplace=True)
del df['foo']
</code></pre>

<p>general example: </p>

<pre><code>import pandas as pd
#creating the table with two missing values
df1 = pd.DataFrame({'a':[1,2],'b':[3,4]}, index = [1,2])
df2 = pd.DataFrame({'b':[5,6]}, index = [3,4])
dftot = pd.concat((df1, df2))
print dftot
#creating the dataframe to fill the missing values
filldf = pd.DataFrame({'a':[7,7,7,7]})

#filling 
print dftot.fillna(filldf)
</code></pre>
";;1;;2014-05-21T15:38:41.700;;23787861;2014-05-21T15:56:05.220;2014-05-21T15:56:05.220;;1883737.0;;1883737.0;10972410.0;2;16;;;
13960;13960;;;"<p>Use this:</p>

<pre><code>ix = pd.DatetimeIndex(start=date(2012, 1, 1), end=date(2012, 1, 31), freq='D')
df2.reindex(ix)
</code></pre>

<p>Which gives:</p>

<pre><code>               b
2012-01-01  0.22
2012-01-02   NaN
2012-01-03  0.30
2012-01-04   NaN
2012-01-05   NaN
[...]
2012-01-29   NaN
2012-01-30   NaN
2012-01-31   NaN
</code></pre>
";;0;;2014-05-22T13:07:41.933;;23807740;2014-05-22T13:07:41.933;;;;;2606953.0;19119039.0;2;13;;;
14003;14003;;;"<p>Try:</p>

<pre><code>df['ID'] = df['ID'].apply(lambda x: '{0:0&gt;15}'.format(x))
</code></pre>

<p>or even</p>

<pre><code>df['ID'] = df['ID'].apply(lambda x: x.zfill(15))
</code></pre>
";;2;;2014-05-23T18:42:07.813;;23836353;2014-05-23T18:42:07.813;;;;;1392458.0;23836277.0;2;19;;;
14045;14045;;;"<p>If you only want to read the first 999,999 (non-header) rows:</p>

<pre><code>read_csv(..., nrows=999999)
</code></pre>

<p>If you only want to read rows 1,000,000 ... 1,999,999</p>

<pre><code>read_csv(..., skiprows=1000000, nrows=999999)
</code></pre>

<p><strong><em>nrows</em></strong> : int, default None Number of rows of file to read. Useful for
reading pieces of large files*</p>

<p><strong><em>skiprows</em></strong> : list-like or integer
Row numbers to skip (0-indexed) or number of rows to skip (int) at the start of the file</p>

<p>and for large files, you'll probably also want to use chunksize:</p>

<p><strong><em>chunksize</em></strong> : int, default None
Return TextFileReader object for iteration</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.io.parsers.read_csv.html"">pandas.io.parsers.read_csv documentation</a></p>
";;4;;2014-05-25T08:52:50.213;;23853569;2014-05-26T08:41:52.210;2014-05-26T08:41:52.210;;202229.0;;202229.0;23853553.0;2;34;;;
14102;14102;;;"<p>You can use the <code>concat</code> function:</p>

<pre><code>In [13]: pd.concat([x]*5)
Out[13]: 
   a  b
0  1  2
0  1  2
0  1  2
0  1  2
0  1  2
</code></pre>

<p>If you only want to repeat the values and not the index, you can do:</p>

<pre><code>In [14]: pd.concat([x]*5, ignore_index=True)
Out[14]: 
   a  b
0  1  2
1  1  2
2  1  2
3  1  2
4  1  2
</code></pre>
";;1;;2014-05-27T11:13:48.410;;23887956;2014-05-27T11:13:48.410;;;;;653364.0;23887881.0;2;25;;;
14126;14126;;;"<p>Note that --pylab is deprecated and has been removed from newer builds of IPython, so the accepted answer will no longer work. The recommended way to enable inline plotting in the IPython Notebook is now to run:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
</code></pre>

<p>See <a href=""http://mail.scipy.org/pipermail/ipython-dev/2014-March/013411.html"">this post</a> from the ipython-dev mailing list for more details.</p>
";;5;;2014-05-28T01:47:44.803;;23901625;2014-05-28T01:47:44.803;;;;;2445984.0;10511024.0;2;121;;;
14155;14155;;;"<p>If you don't want to modify the dataframe, you could use a custom formatter for that column.</p>

<pre><code>import pandas as pd
pd.options.display.float_format = '${:,.2f}'.format
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])


print df.to_string(formatters={'cost':'${:,.2f}'.format})
</code></pre>

<p>yields</p>

<pre><code>        cost
foo  $123.46
bar  $234.57
baz  $345.68
quux $456.79
</code></pre>
";;1;;2014-05-28T21:24:04.973;;23922119;2014-05-28T21:24:04.973;;;;;1777286.0;20937538.0;2;39;;;
14170;14170;;;"<p>When I think of dummy variables I think of using them in the context of OLS regression, and I would do something like this:</p>

<pre><code>import numpy as np
import pandas as pd
import statsmodels.api as sm

my_data = np.array([[5, 'a', 1],
                    [3, 'b', 3],
                    [1, 'b', 2],
                    [3, 'a', 1],
                    [4, 'b', 2],
                    [7, 'c', 1],
                    [7, 'c', 1]])                


df = pd.DataFrame(data=my_data, columns=['y', 'dummy', 'x'])
just_dummies = pd.get_dummies(df['dummy'])

step_1 = pd.concat([df, just_dummies], axis=1)      
step_1.drop(['dummy', 'c'], inplace=True, axis=1)
# to run the regression we want to get rid of the strings 'a', 'b', 'c' (obviously)
# and we want to get rid of one dummy variable to avoid the dummy variable trap
# arbitrarily chose ""c"", coefficients on ""a"" an ""b"" would show effect of ""a"" and ""b""
# relative to ""c""
step_1 = step_1.applymap(np.int) 

result = sm.OLS(step_1['y'], sm.add_constant(step_1[['x', 'a', 'b']])).fit()
print result.summary()
</code></pre>
";;4;;2014-05-29T03:26:56.530;;23925229;2015-09-30T14:49:03.163;2015-09-30T14:49:03.163;;1078084.0;;1078084.0;11587782.0;2;24;;;
14233;14233;;;"<p>You can use the <code>TimeGrouper</code> function in a <code>groupy/apply</code>. With a <code>TimeGrouper</code> you don't need to create your period column. I know you're not trying to compute the mean but I will use it as an example:</p>

<pre><code>&gt;&gt;&gt; df.groupby(pd.TimeGrouper('5Min'))['val'].mean()

time
2014-04-03 16:00:00    14390.000000
2014-04-03 16:05:00    14394.333333
2014-04-03 16:10:00    14396.500000
</code></pre>

<p>Or an example with an explicit <code>apply</code>:</p>

<pre><code>&gt;&gt;&gt; df.groupby(pd.TimeGrouper('5Min'))['val'].apply(lambda x: len(x) &gt; 3)

time
2014-04-03 16:00:00    False
2014-04-03 16:05:00    False
2014-04-03 16:10:00     True
</code></pre>

<p>Doctstring for <code>TimeGrouper</code>:</p>

<pre><code>Docstring for resample:class TimeGrouper@21

TimeGrouper(self, freq = 'Min', closed = None, label = None,
how = 'mean', nperiods = None, axis = 0, fill_method = None,
limit = None, loffset = None, kind = None, convention = None, base = 0,
**kwargs)

Custom groupby class for time-interval grouping

Parameters
----------
freq : pandas date offset or offset alias for identifying bin edges
closed : closed end of interval; left or right
label : interval boundary to use for labeling; left or right
nperiods : optional, integer
convention : {'start', 'end', 'e', 's'}
    If axis is PeriodIndex

Notes
-----
Use begin, end, nperiods to generate intervals that cannot be derived
directly from the associated object
</code></pre>

<p><strong>Edit</strong></p>

<p>I don't know of an elegant way to create the period column, but the following will work:</p>

<pre><code>&gt;&gt;&gt; new = df.groupby(pd.TimeGrouper('5Min'),as_index=False).apply(lambda x: x['val'])
&gt;&gt;&gt; df['period'] = new.index.get_level_values(0)
&gt;&gt;&gt; df

                     id    val  period
time
2014-04-03 16:01:53  23  14389       0
2014-04-03 16:01:54  28  14391       0 
2014-04-03 16:05:55  24  14393       1
2014-04-03 16:06:25  23  14395       1
2014-04-03 16:07:01  23  14395       1
2014-04-03 16:10:09  23  14395       2
2014-04-03 16:10:23  26  14397       2
2014-04-03 16:10:57  26  14397       2
2014-04-03 16:11:10  26  14397       2
</code></pre>

<p>It works because the groupby here with as_index=False actually returns the period column you want as the part of the multiindex and I just grab that part of the multiindex and assign to a new column in the orginal dataframe. You could do anything in the apply, I just want the index:</p>

<pre><code>&gt;&gt;&gt; new

   time
0  2014-04-03 16:01:53    14389
   2014-04-03 16:01:54    14391
1  2014-04-03 16:05:55    14393
   2014-04-03 16:06:25    14395
   2014-04-03 16:07:01    14395
2  2014-04-03 16:10:09    14395
   2014-04-03 16:10:23    14397
   2014-04-03 16:10:57    14397
   2014-04-03 16:11:10    14397

&gt;&gt;&gt;  new.index.get_level_values(0)

Int64Index([0, 0, 1, 1, 1, 2, 2, 2, 2], dtype='int64')
</code></pre>
";;12;;2014-05-31T03:55:26.870;;23966229;2014-05-31T05:13:09.387;2014-05-31T05:13:09.387;;3435183.0;;3435183.0;23966152.0;2;7;;;
14337;14337;;;"<p>You can put <code>df_try</code> inside a list and then do what you have in mind:</p>

<pre><code>&gt;&gt;&gt; df.append([df_try]*5,ignore_index=True)

    Store  Dept       Date  Weekly_Sales IsHoliday
0       1     1 2010-02-05      24924.50     False
1       1     1 2010-02-12      46039.49      True
2       1     1 2010-02-19      41595.55     False
3       1     1 2010-02-26      19403.54     False
4       1     1 2010-03-05      21827.90     False
5       1     1 2010-03-12      21043.39     False
6       1     1 2010-03-19      22136.64     False
7       1     1 2010-03-26      26229.21     False
8       1     1 2010-04-02      57258.43     False
9       1     1 2010-02-12      46039.49      True
10      1     1 2010-02-12      46039.49      True
11      1     1 2010-02-12      46039.49      True
12      1     1 2010-02-12      46039.49      True
13      1     1 2010-02-12      46039.49      True
</code></pre>
";;2;;2014-06-04T05:47:21.543;;24029921;2014-06-04T07:08:56.490;2014-06-04T07:08:56.490;;3435183.0;;3435183.0;24029659.0;2;15;;;
14357;14357;;;"<p>OK, first problem is you have embedded spaces causing the function to incorrectly apply:</p>

<p>fix this using vectorised <code>str</code>:</p>

<pre><code>mydf['Cigarettes'] = mydf['Cigarettes'].str.replace(' ', '')
</code></pre>

<p>now create your new column should just work:</p>

<pre><code>mydf['CigarNum'] = mydf['Cigarettes'].apply(numcigar.get).astype(float)
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Thanks to @Jeff as always for pointing out superior ways to do things:</p>

<p>So you can call <code>replace</code> instead of calling <code>apply</code>:</p>

<pre><code>mydf['CigarNum'] = mydf['Cigarettes'].replace(numcigar)
# now convert the types
mydf['CigarNum'] = mydf['CigarNum'].convert_objects(convert_numeric=True)
</code></pre>

<p>you can also use <code>factorize</code> method also.</p>

<p>Thinking about it why not just set the dict values to be floats anyway and then you avoid the type conversion?</p>

<p>So:</p>

<pre><code>numcigar = {""Never"":0.0 ,""1-5 Cigarettes/day"" :1.0,""10-20 Cigarettes/day"":4.0}
</code></pre>

<p><strong>Version 0.17.0 or newer</strong></p>

<p><code>convert_objects</code> is deprecated since <code>0.17.0</code>, this has been replaced with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html#pandas.to_numeric"" rel=""noreferrer""><code>to_numeric</code></a></p>

<pre><code>mydf['CigarNum'] = pd.to_numeric(mydf['CigarNum'], errors='coerce')
</code></pre>

<p>Here <code>errors='coerce'</code> will return <code>NaN</code> where the values cannot be converted to a numeric value, without this it will raise an exception</p>
";;3;;2014-06-04T12:51:12.487;;24037972;2017-01-30T15:25:08.027;2017-01-30T15:25:08.027;;704848.0;;704848.0;24037507.0;2;26;;;
14362;14362;;;"<p>The reason this puts <code>NaN</code> into a column is because <code>df.index</code> and the <code>Index</code> of your right-hand-side object are different. @zach shows the proper way to assign a new column of zeros. In general, <code>pandas</code> tries to do as much alignment of indices as possible. One downside is that when indices are not aligned you get <code>NaN</code> wherever they <em>aren't</em> aligned. Play around with the <code>reindex</code> and <code>align</code> methods to gain some intuition for alignment works with objects that have partially, totally, and not-aligned-all aligned indices. For example here's how <code>DataFrame.align()</code> works with partially aligned indices:</p>

<pre><code>In [7]: from pandas import DataFrame

In [8]: from numpy.random import randint

In [9]: df = DataFrame({'a': randint(3, size=10)})

In [10]:

In [10]: df
Out[10]:
   a
0  0
1  2
2  0
3  1
4  0
5  0
6  0
7  0
8  0
9  0

In [11]: s = df.a[:5]

In [12]: dfa, sa = df.align(s, axis=0)

In [13]: dfa
Out[13]:
   a
0  0
1  2
2  0
3  1
4  0
5  0
6  0
7  0
8  0
9  0

In [14]: sa
Out[14]:
0     0
1     2
2     0
3     1
4     0
5   NaN
6   NaN
7   NaN
8   NaN
9   NaN
Name: a, dtype: float64
</code></pre>
";;3;;2014-06-04T14:29:49.663;;24040239;2017-02-09T10:57:08.027;2017-02-09T10:57:08.027;;1300833.0;;564538.0;24039023.0;2;10;;;
14364;14364;;;"<p>When you pass inplace in makes the changes on the original variable and returns None, and the function <strong>does not</strong> return the modified dataframe, it returns None.</p>

<pre><code>is_none = df.set_index(['Company', 'date'], inplace=True)
df  # the dataframe you want
is_none # has the value None
</code></pre>

<p>so when you have a line like:</p>

<pre><code>df = df.set_index(['Company', 'date'], inplace=True)
</code></pre>

<p>it first modifies <code>df</code>... but then it sets <code>df</code> to None!</p>

<p>That is, you should just use the line:</p>

<pre><code>df.set_index(['Company', 'date'], inplace=True)
</code></pre>
";;0;;2014-06-04T15:37:57.130;;24041761;2014-06-04T15:37:57.130;;;;;1240268.0;24041436.0;2;31;;;
14366;14366;;;"<p>So first of all, <strong>pandas updates using the index</strong>. When an update command does not update anything, check both left-hand side and right-hand side. If for some reason you are too lazy to update the indices to follow your identification logic, you can do something along the lines of </p>

<pre><code>&gt;&gt;&gt; df.loc[df.filename == 'test2.dat', 'n'] = df2[df2.filename == 'test2.dat'].loc[0]['n']
&gt;&gt;&gt; df
Out[331]: 
    filename   m     n
0  test0.dat  12  None
1  test2.dat  13    16
</code></pre>

<p>If you want to do this for the whole table, I suggest a method I believe is superior to the previously mentioned ones: since your identifier is <code>filename</code>, set <code>filename</code> as your index, and then use <code>update()</code> as you wanted to. Both <code>merge</code> and the <code>apply()</code> approach contain unnecessary overhead:</p>

<pre><code>&gt;&gt;&gt; df.set_index('filename', inplace=True)
&gt;&gt;&gt; df2.set_index('filename', inplace=True)
&gt;&gt;&gt; df.update(df2)
&gt;&gt;&gt; df
Out[292]: 
            m     n
filename           
test0.dat  12  None
test2.dat  13    16
</code></pre>
";;0;;2014-06-04T16:48:48.437;;24043138;2014-06-04T16:56:55.057;2014-06-04T16:56:55.057;;1082349.0;;1082349.0;24036911.0;2;23;;;
14442;14442;;;"<p>Another way in which this could be achieved is </p>

<pre><code>df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')
</code></pre>
";;0;;2014-06-06T04:43:52.083;;24074316;2014-06-06T04:43:52.083;;;;;678613.0;19913659.0;2;11;;;
14454;14454;;;"<p>You can use an approach like the following one. You can simply slice the dataframe according to the values of each template, and subsequently use the dates and scores for the plot.</p>

<pre><code>from pandas import *
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import datetime as dt

#The following part is just for generating something similar to your dataframe
date1 = ""20140605""
date2 = ""20140606""

d = {'date': Series([date1]*5 + [date2]*5), 'template': Series(range(5)*2),
'score': Series([random() for i in range(10)]) } 

data = DataFrame(d)
#end of dataset generation

fig, ax = plt.subplots()

for temp in range(5):
    dat = data[data['template']==temp]
    dates =  dat['date']
    dates_f = [dt.datetime.strptime(date,'%Y%m%d') for date in dates]
    ax.plot(dates_f, dat['score'], label = ""Template: {0}"".format(temp))

plt.xlabel(""Date"")
plt.ylabel(""Score"")
ax.legend()
plt.show()
</code></pre>
";;0;;2014-06-06T13:14:59.123;;24082767;2014-06-06T13:14:59.123;;;;;446185.0;24080275.0;2;9;;;
14456;14456;;;"<p>Managed to do it:</p>

<pre><code>pd.groupby(b,by=[b.index.month,b.index.year])
</code></pre>

<p>Or </p>

<pre><code>df.groupby(pd.TimeGrouper(freq='M'))
</code></pre>
";;2;;2014-06-06T13:38:42.773;;24083253;2017-02-17T12:31:05.727;2017-02-17T12:31:05.727;;843463.0;;1461850.0;24082784.0;2;53;;;
14497;14497;;;"<p>This works:</p>

<pre><code>df.groupby(level=[0,1]).apply(lambda x: x.set_index('Date').resample('2D', how='sum'))

                               value_a  value_b
State   City       Date
Alabama Mobile     2012-01-01       17       37
                   2012-01-03       21       41
        Montgomery 2012-01-01       25       45
                   2012-01-03       29       49
Georgia Atlanta    2012-01-01        1       21
                   2012-01-03        5       25
        Savanna    2012-01-01        9       29
                   2012-01-03       13       33
</code></pre>

<p>If the Date column is strings, then convert to datetime beforehand:</p>

<pre><code>df['Date'] = pd.to_datetime(df['Date'])
</code></pre>
";;1;;2014-06-07T14:44:14.840;;24098354;2016-01-22T03:36:52.390;2016-01-22T03:36:52.390;;690430.0;;690430.0;15799162.0;2;7;;;
14502;14502;;;"<p>You can use the groupby method:</p>

<pre><code>data.groupby(""template"").plot(x=""date"", y=""score"")
</code></pre>
";;7;;2014-06-07T15:47:06.353;;24098903;2014-06-07T15:47:06.353;;;;;3690711.0;24080275.0;2;47;;;
14533;14533;;;"<p>I am answering the question as stated in its title and first sentence: the following aggregates values to lists.</p>

<pre><code>import pandas as pd

df = pd.DataFrame( {'A' : [1, 1, 1, 1, 2, 2, 3], 'B' : [10, 12, 11, 10, 11, 12, 14], 'C' : [22, 20,     8, 10, 13, 10, 0]})
print df

df2=df.groupby(['A']).apply(lambda tdf: pd.Series(  dict([[vv,tdf[vv].unique().tolist()] for vv in tdf if vv not in ['A']])  )) 
print df2
</code></pre>

<p>The output is as follows:</p>

<pre><code>In [3]: run tmp
   A   B   C
0  1  10  22
1  1  12  20
2  1  11   8
3  1  10  10
4  2  11  13
5  2  12  10
6  3  14   0

[7 rows x 3 columns]
              B                C
A                               
1  [10, 12, 11]  [22, 20, 8, 10]
2      [11, 12]         [13, 10]
3          [14]              [0]

[3 rows x 2 columns]
</code></pre>
";;1;;2014-06-09T01:05:46.450;;24112443;2014-06-09T01:05:46.450;;;;;1159005.0;19530568.0;2;19;;;
14604;14604;;;"<p>I would just use numpy's <code>randn</code>:</p>

<pre><code>In [11]: df = pd.DataFrame(np.random.randn(100, 2))

In [12]: msk = np.random.rand(len(df)) &lt; 0.8

In [13]: train = df[msk]

In [14]: test = df[~msk]
</code></pre>

<p>And just to see this has worked:</p>

<pre><code>In [15]: len(test)
Out[15]: 21

In [16]: len(train)
Out[16]: 79
</code></pre>
";;11;;2014-06-10T17:29:25.983;;24147363;2014-06-11T00:30:42.967;2014-06-11T00:30:42.967;;1240268.0;;1240268.0;24147278.0;2;111;;;
14614;14614;;;"<p>SciKit Learn's <code>train_test_split</code>  is a good one.</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size = 0.2)
</code></pre>
";;8;;2014-06-10T22:19:31.937;;24151789;2016-11-07T10:27:46.963;2016-11-07T10:27:46.963;;3577054.0;;3222797.0;24147278.0;2;211;;;
14722;14722;;;"<p>An example with time fixed effects using pandas' <code>PanelOLS</code> (which is in the plm module). Notice, the import of <code>PanelOLS</code>:</p>

<pre><code>&gt;&gt;&gt; from pandas.stats.plm import PanelOLS
&gt;&gt;&gt; df

                y    x
date       id
2012-01-01 1   0.1  0.2
           2   0.3  0.5
           3   0.4  0.8
           4   0.0  0.2
2012-02-01 1   0.2  0.7 
           2   0.4  0.5
           3   0.2  0.3
           4   0.1  0.1
2012-03-01 1   0.6  0.9
           2   0.7  0.5
           3   0.9  0.6
           4   0.4  0.5
</code></pre>

<p>Note, the dataframe must have a multindex set ; <code>panelOLS</code> determines the <code>time</code> and <code>entity</code> effects based on the index:</p>

<pre><code>&gt;&gt;&gt; reg  = PanelOLS(y=df['y'],x=df[['x']],time_effects=True)
&gt;&gt;&gt; reg

-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~ &lt;x&gt;

Number of Observations:         12
Number of Degrees of Freedom:   4

R-squared:         0.2729
Adj R-squared:     0.0002

Rmse:              0.1588

F-stat (1, 8):     1.0007, p-value:     0.3464

Degrees of Freedom: model 3, resid 8

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%
--------------------------------------------------------------------------------
             x     0.3694     0.2132       1.73     0.1214    -0.0485     0.7872
---------------------------------End of Summary--------------------------------- 
</code></pre>

<p>Docstring:</p>

<pre><code>PanelOLS(self, y, x, weights = None, intercept = True, nw_lags = None,
entity_effects = False, time_effects = False, x_effects = None,
cluster = None, dropped_dummies = None, verbose = False,
nw_overlap = False)

Implements panel OLS.

See ols function docs
</code></pre>

<p>This is another function (like <code>fama_macbeth</code>) where I believe the plan is to move this functionality to <code>statsmodels</code>.</p>
";;10;;2014-06-13T01:12:37.160;;24196288;2014-06-13T01:12:37.160;;;;;3435183.0;24195432.0;2;9;;;
14765;14765;;;"<p>The right way of doing it will be <code>df[""B""] = df[""A""].map(equiv)</code>.</p>

<pre><code>In [55]:

import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001]} )
df[""B""] = df[""A""].map(equiv)
print(df)
      A  B
0  7001  1
1  8001  2
2  9001  3

[3 rows x 2 columns]
</code></pre>

<p>And it will handle the situation when the key does not exist very nicely, considering the following example:</p>

<pre><code>In [56]:

import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001, 10000]} )
df[""B""] = df[""A""].map(equiv)
print(df)
       A   B
0   7001   1
1   8001   2
2   9001   3
3  10000 NaN

[4 rows x 2 columns]
</code></pre>
";;2;;2014-06-14T04:07:18.153;;24216489;2016-03-04T22:38:11.840;2016-03-04T22:38:11.840;;2487184.0;;2487184.0;24216425.0;2;48;;;
14770;14770;;;"<p>I always use lambdas and the built-in <code>map()</code> function to create new rows by combining other rows:</p>

<pre><code>st['a'] = map(lambda path, row: path + 2 * row, st['path'], st['row'])
</code></pre>

<p>It might be slightly more complicated than necessary for doing linear combinations of numerical columns.  On the other hand, I feel it's good to adopt as a convention as it can be used with more complicated combinations of rows (e.g. working with strings) or filling missing data in a column using functions of the other columns.</p>

<p>For example, lets say you have a table with columns gender, and title, and some of the titles are missing.  You can fill them with a function as follows:</p>

<pre><code>title_dict = {'male': 'mr.', 'female': 'ms.'}
table['title'] = map(lambda title,
    gender: title if title != None else title_dict[gender],
    table['title'], table['gender'])
</code></pre>
";;1;;2014-06-14T18:14:40.657;;24222837;2014-06-14T18:44:19.780;2014-06-14T18:44:19.780;;3448.0;;3281048.0;15118111.0;2;8;;;
14823;14823;;;"<p>You can install previous version of pandas.</p>

<pre><code>pip uninstall numpy
pip uninstall pandas
pip install pandas==0.13.1
</code></pre>

<p>In my situation it solved problem...</p>
";;2;;2014-06-16T11:09:21.643;;24242333;2014-06-16T11:09:21.643;;;;;2285037.0;24122850.0;2;23;;;
14850;14850;;;"<p>Try:</p>

<pre><code>dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')
</code></pre>

<p>According to the pandas documentation:</p>

<blockquote>
  <p>dtype : Type name or dict of column -> type</p>
</blockquote>

<p>As for low_memory, it's True <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html"">by default</a> and isn't yet documented. I don't think its relevant though. The error message is generic, so you shouldn't need to mess with low_memory anyway. Hope this helps and let me know if you have further problems</p>
";;2;;2014-06-16T20:11:56.300;;24251426;2015-04-02T23:39:17.010;2015-04-02T23:39:17.010;;783412.0;;783412.0;24251219.0;2;16;;;
14893;14893;;;"<p>Use iloc to access by position (rather than label):</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2], [3, 4]], ['a', 'b'], ['A', 'B'])

In [12]: df
Out[12]: 
   A  B
a  1  2
b  3  4

In [13]: df.iloc[0]  # first row in a DataFrame
Out[13]: 
A    1
B    2
Name: a, dtype: int64

In [14]: df['A'].iloc[0]  # first item in a Series (Column)
Out[14]: 1
</code></pre>
";;0;;2014-06-17T21:27:45.957;;24273597;2014-06-17T21:27:45.957;;;;;1240268.0;24273130.0;2;28;;;
14905;14905;;;"<p>You can reset the colorcycle to the original with <a href=""http://matplotlib.org/api/axes_api.html?#matplotlib.axes.Axes.set_color_cycle"">Axes.set_color_cycle</a>. Looking at the code for this, there is a function to do the actual work:</p>

<pre><code>def set_color_cycle(self, clist=None):
    if clist is None:
        clist = rcParams['axes.color_cycle']
    self.color_cycle = itertools.cycle(clist
</code></pre>

<p>And a method on the Axes which uses it:</p>

<pre><code>def set_color_cycle(self, clist):
    """"""
    Set the color cycle for any future plot commands on this Axes.

    *clist* is a list of mpl color specifiers.
    """"""
    self._get_lines.set_color_cycle(clist)
    self._get_patches_for_fill.set_color_cycle(clist)
</code></pre>

<p>This basically means you can call the set_color_cycle with None as the only argument, and it will be replaced with the default cycle found in rcParams['axes.color_cycle'].</p>

<p>I tried this with the following code and got the expected result:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

for i in range(3):
    plt.plot(np.arange(10) + i)

plt.gca().set_color_cycle(None)

for i in range(3):
    plt.plot(np.arange(10, 1, -1) + i)

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/2k3hr.png"" alt=""Code output, showing the color cycling reset functionality""></p>
";;3;;2014-06-18T10:21:52.947;;24283087;2014-06-18T10:21:52.947;;;;;741316.0;24193174.0;2;39;;;
14910;14910;;;"<p>One way to achieve this is </p>

<pre><code>&gt;&gt;&gt; pd.DataFrame(np.array([[2, 3, 4]]), columns=['A', 'B', 'C']).append(df, ignore_index=True)
Out[330]: 
   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>

<p>Generally, it's easiest to append dataframes, not series. In your case, since you want the new row to be ""on top"" (with starting id), and there is no function <code>pd.prepend()</code>, I first create the new dataframe and then append your old one.</p>

<p><code>ignore_index</code> will ignore the old ongoing index in your dataframe and ensure that the first row actually starts with index <code>1</code> instead of restarting with index <code>0</code>.</p>

<p><em>Typical Disclaimer: Cetero censeo ... appending rows is a quite inefficient operation. If you care about performance and can somehow ensure to first create a dataframe with the correct (longer) index and then just <strong>inserting</strong> the additional row into the dataframe, you should definitely do that. See:</em></p>

<pre><code>&gt;&gt;&gt; index = np.array([0, 1, 2])
&gt;&gt;&gt; df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)
&gt;&gt;&gt; df2.loc[0:1] = [list(s1), list(s2)]
&gt;&gt;&gt; df2
Out[336]: 
     A    B    C
0    5    6    7
1    7    8    9
2  NaN  NaN  NaN
&gt;&gt;&gt; df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)
&gt;&gt;&gt; df2.loc[1:] = [list(s1), list(s2)]
</code></pre>

<p>So far, we have what you had as <code>df</code>:</p>

<pre><code>&gt;&gt;&gt; df2
Out[339]: 
     A    B    C
0  NaN  NaN  NaN
1    5    6    7
2    7    8    9
</code></pre>

<p>But now you can easily insert the row as follows. Since the space was preallocated, this is more efficient. </p>

<pre><code>&gt;&gt;&gt; df2.loc[0] = np.array([2, 3, 4])
&gt;&gt;&gt; df2
Out[341]: 
   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>
";;1;;2014-06-18T11:36:54.453;;24284515;2014-06-18T11:48:48.550;2014-06-18T11:48:48.550;;1082349.0;;1082349.0;24284342.0;2;12;;;
14911;14911;;;"<p>Just assign row to a particular index, using <code>loc</code>:</p>

<pre><code> df.loc[-1] = [2, 3, 4]  # adding a row
 df.index = df.index + 1  # shifting index
 df = df.sort()  # sorting by index
</code></pre>

<p>And you get, as desired:</p>

<pre><code>    A  B  C
 0  2  3  4
 1  5  6  7
 2  7  8  9
</code></pre>

<p>See in Pandas documentation <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#setting-with-enlargement"" rel=""noreferrer"">Indexing: Setting with enlargement</a>.</p>
";;2;;2014-06-18T11:44:29.707;;24284680;2016-11-04T20:15:22.633;2016-11-04T20:15:22.633;;3816583.0;;907575.0;24284342.0;2;36;;;
14914;14914;;;"<p>Not sure how you were calling concat() but it should work as long as both objects are of the same type. Maybe the issue is that you need to cast your second vector to a dataframe? Using the df that you defined the following works for me.</p>

<pre><code>&gt;&gt;&gt;df2 = pd.DataFrame([[2,3,4]],columns=['A','B','C'])
&gt;&gt;&gt;pd.concat([df2,df])
</code></pre>
";;0;;2014-06-18T13:42:46.503;;24287210;2014-06-18T13:42:46.503;;;;;1451311.0;24284342.0;2;6;;;
15082;15082;;;"<p>The answers by joris in this thread and by punchagan in the <a href=""https://stackoverflow.com/questions/18012505/python-pandas-dataframe-columns-convert-to-dict-key-and-value"">duplicated thread</a> are very elegant, however they will not give correct results if the column used for the keys contains any duplicated value. </p>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; ptest = p.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) 
&gt;&gt;&gt; ptest
  id  value
0  a      1
1  a      2
2  b      3

# note that in both cases the association a-&gt;1 is lost:
&gt;&gt;&gt; ptest.set_index('id')['value'].to_dict()
{'a': 2, 'b': 3}
&gt;&gt;&gt; dict(zip(ptest.id, ptest.value))
{'a': 2, 'b': 3}
</code></pre>

<p>If you have duplicated entries and do not want to lose them, you can use this ugly but working code:</p>

<pre><code>&gt;&gt;&gt; mydict = {}
&gt;&gt;&gt; for x in range(len(ptest)):
...     currentid = ptest.iloc[x,0]
...     currentvalue = ptest.iloc[x,1]
...     mydict.setdefault(currentid, [])
...     mydict[currentid].append(currentvalue)
&gt;&gt;&gt; mydict
{'a': [1, 2], 'b': [3]}
</code></pre>
";;2;;2014-06-23T14:35:48.740;;24368660;2014-06-23T14:35:48.740;2017-05-23T10:31:36.660;;-1.0;;95048.0;18695605.0;2;14;;;
15084;15084;;;"<p>If you want a simple way to preserve duplicates, you could use <code>groupby</code>:</p>

<pre><code>&gt;&gt;&gt; ptest = pd.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) 
&gt;&gt;&gt; ptest
  id  value
0  a      1
1  a      2
2  b      3
&gt;&gt;&gt; {k: g[""value""].tolist() for k,g in ptest.groupby(""id"")}
{'a': [1, 2], 'b': [3]}
</code></pre>
";;2;;2014-06-23T16:08:36.923;;24370510;2014-06-23T16:08:36.923;;;;;487339.0;18695605.0;2;27;;;
15123;15123;;;"<p>with fillna()</p>

<pre><code>frame['c'] = frame.fillna(0)['a'] + frame.fillna(0)['b']
</code></pre>

<p>or as suggested :</p>

<pre><code>frame['c'] = frame.a.fillna(0) + frame.b.fillna(0)
</code></pre>

<p>giving :</p>

<pre><code>    a   b  c
0   1   3  4
1   2 NaN  2
2 NaN   4  4
</code></pre>
";;2;;2014-06-24T12:32:46.857;;24386746;2014-06-24T12:49:23.017;2014-06-24T12:49:23.017;;3297428.0;;3297428.0;24386638.0;2;10;;;
15124;15124;;;"<p>Another approach:</p>

<pre><code>&gt;&gt;&gt; frame[""c""] = frame[[""a"", ""b""]].sum(axis=1)
&gt;&gt;&gt; frame
    a   b  c
0   1   3  4
1   2 NaN  2
2 NaN   4  4
</code></pre>
";;2;;2014-06-24T12:52:45.620;;24387164;2014-06-24T12:52:45.620;;;;;487339.0;24386638.0;2;16;;;
15137;15137;;;"<p>You can just do:</p>

<pre>
df[sorted(df.columns)]
</pre>
";;1;;2014-06-24T21:22:45.420;;24396554;2014-06-24T21:22:45.420;;;;;805030.0;11067027.0;2;17;;;
15172;15172;;;"<p>The <code>read_sql</code> docs say this <code>params</code> argument can be a list, tuple or dict (see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html"">docs</a>).</p>

<p>To pass the values in the sql query, there are different syntaxes possible: <code>?</code>, <code>:1</code>, <code>:name</code>, <code>%s</code>, <code>%(name)s</code> (see <a href=""http://legacy.python.org/dev/peps/pep-0249/#paramstyle"">PEP249</a>).<br>
But not all of these possibilities are supported by all database drivers, <strong>which syntax is supported depends on the driver you are using</strong> (<code>psycopg2</code> in your case I suppose).</p>

<p>In your second case, when using a dict, you are using 'named arguments', and according to the <code>psycopg2</code> documentation, they support the <code>%(name)s</code> style (and so not the <code>:name</code> I suppose), see <a href=""http://initd.org/psycopg/docs/usage.html#query-parameters"">http://initd.org/psycopg/docs/usage.html#query-parameters</a>.<br>
So using that style should work:</p>

<pre><code>df = psql.read_sql(('select ""Timestamp"",""Value"" from ""MyTable"" '
                     'where ""Timestamp"" BETWEEN %(dstart)s AND %(dfinish)s'),
                   db,params={""dstart"":datetime(2014,6,24,16,0),""dfinish"":datetime(2014,6,24,17,0)},
                   index_col=['Timestamp'])
</code></pre>
";;3;;2014-06-25T20:49:51.047;;24418294;2014-06-25T20:49:51.047;;;;;653364.0;24408557.0;2;26;;;
15203;15203;;;"<p>If you are on version 0.14, you can simply pass a tuple to <code>.loc</code> as below:</p>

<pre><code>df.loc[('at', [1,3,4]), 'Dwell']
</code></pre>
";;0;;2014-06-26T17:29:58.117;;24436783;2014-06-26T17:29:58.117;;;;;3657742.0;24435788.0;2;10;;;
15228;15228;;;"<p>Pandas is aware of the object datetime but when you use some of the import functions it is taken as a string. So what you need to do is make sure the column is set as the datetime type not as a string. Then you can make your query. </p>

<pre><code>df['date']  = pd.to_datetime(df['date'])
df_masked = df[(df['date'] &gt; datetime.date(2012,4,1)) &amp; (df['date'] &lt; datetime.date(2012,4,4))]
</code></pre>
";;0;;2014-06-27T07:53:45.180;;24446716;2014-06-27T14:27:37.627;2014-06-27T14:27:37.627;;759866.0;;3647167.0;16412099.0;2;40;;;
15283;15283;;;"<pre><code>In [21]: df.values[[np.arange(5)]*2] = 0

In [22]: df
Out[22]: 
          0         1         2         3         4
0  0.000000  0.931374  0.604412  0.863842  0.280339
1  0.531528  0.000000  0.641094  0.204686  0.997020
2  0.137725  0.037867  0.000000  0.983432  0.458053
3  0.594542  0.943542  0.826738  0.000000  0.753240
4  0.357736  0.689262  0.014773  0.446046  0.000000
</code></pre>

<p>Note that this will only work if <code>df</code> has the same number of rows as columns. Another way which will work for arbitrary shapes is to use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html"">np.fill_diagonal</a>:</p>

<pre><code>In [36]: np.fill_diagonal(df.values, 0)
</code></pre>
";;2;;2014-06-29T10:30:32.267;;24475214;2014-06-29T15:33:57.707;2014-06-29T15:33:57.707;;190597.0;;190597.0;24475094.0;2;22;;;
15304;15304;;;"<p>Andy Hayden's answer (<code>index.levels[blah]</code>) is great for some scenarios, but can lead to odd behavior in others.  My understanding is that Pandas goes to great lengths to ""reuse"" indices when possible to avoid having the indices of lots of similarly-indexed DataFrames taking up space in memory.  As a result, <a href=""https://stackoverflow.com/questions/24434724/dataframe-index-levels-shows-extra-values-after-paring-down-dataframe/"">I've found the following annoying behavior</a>:</p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed(0)

idx = pd.MultiIndex.from_product([['John', 'Josh', 'Alex'], list('abcde')], 
                                 names=['Person', 'Letter'])
large = pd.DataFrame(data=np.random.randn(15, 2), 
                     index=idx, 
                     columns=['one', 'two'])
small = large.loc[['Jo'==d[0:2] for d in large.index.get_level_values('Person')]]

print small.index.levels[0]
print large.index.levels[0]
</code></pre>

<p>Which outputs</p>

<pre><code>Index([u'Alex', u'John', u'Josh'], dtype='object')
Index([u'Alex', u'John', u'Josh'], dtype='object')
</code></pre>

<p>rather than the expected</p>

<pre><code>Index([u'John', u'Josh'], dtype='object')
Index([u'Alex', u'John', u'Josh'], dtype='object')
</code></pre>

<p>As one person pointed out on the other thread, one idiom that seems very natural and works properly would be:</p>

<pre><code>small.index.get_level_values('Person').unique()
large.index.get_level_values('Person').unique()
</code></pre>

<p>I hope this helps someone else dodge the super-unexpected behavior that I ran into.</p>
";;1;;2014-06-30T11:38:56.107;;24489283;2014-06-30T11:38:56.107;2017-05-23T12:02:47.340;;-1.0;;2501018.0;13888468.0;2;20;;;
15306;15306;;;"<p>But for any future bypassers you could mention that <code>df = df[df.line_race != 0]</code> doesn't do anything when trying to filter for <code>None</code>/missing values.</p>

<p>Does work:</p>

<pre><code>df = df[df.line_race != 0]
</code></pre>

<p>Doesn't do anything:</p>

<pre><code>df = df[df.line_race != None]
</code></pre>

<p>Does work:</p>

<pre><code>df = df[df.line_race.notnull()]
</code></pre>
";;4;;2014-06-30T11:56:43.917;;24489602;2015-12-23T10:45:58.250;2015-12-23T10:45:58.250;;2901002.0;;1597866.0;18172851.0;2;91;;;
15311;15311;;;"<p>I guess u want unique values in a certain level (and by level names) of a multiindex. I usually do the following, which is a bit long.</p>

<pre><code>In [11]: df.index.get_level_values('co').unique()
Out[11]: array(['DE', 'FR'], dtype=object)
</code></pre>
";;4;;2014-06-30T18:18:25.177;;24496435;2014-06-30T18:18:25.177;;;;;3599890.0;24495695.0;2;37;;;
15338;15338;;;"<p>The fastest way to do this is using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"">set_value</a>. This method is ~100 times faster than <code>.ix</code> method. For example: </p>

<p><code>df.set_value('C', 'x', 10)</code></p>
";;6;;2014-07-01T19:16:40.957;;24517695;2016-03-14T18:11:29.340;2016-03-14T18:11:29.340;;2662493.0;;2662493.0;13842088.0;2;146;;;
15379;15379;;;"<p>Not sure if you still need it, but in Pandas 0.14 i  usually use .astype('timedelta64[X]') method
<a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/timeseries.html</a> (frequency conversion)</p>

<pre><code>df = pd.DataFrame([ pd.Timestamp('20010101'), pd.Timestamp('20040605') ])
df.ix[0]-df.ix[1]
</code></pre>

<p>Returns:</p>

<blockquote>
<pre><code>0   -1251 days
dtype: timedelta64[ns]
</code></pre>
</blockquote>

<pre><code>(df.ix[0]-df.ix[1]).astype('timedelta64[Y]')
</code></pre>

<p>Returns:</p>

<pre><code>  0   -4
 dtype: float64
</code></pre>

<p>Hope that will help</p>
";;2;;2014-07-02T18:02:02.603;;24537997;2014-07-02T18:02:02.603;;;;;2968805.0;16103238.0;2;7;;;
15383;15383;;;"<p>You can apply a specific function to a specific column by passing in a dict.</p>

<pre><code>pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})
</code></pre>
";;1;;2014-07-02T23:13:24.023;;24542540;2014-07-02T23:13:24.023;;;;;3325052.0;20119414.0;2;30;;;
15501;15501;;;"<p>Here is a somewhat hackish solution but it gets the job done. You wanted a .pdf but you get a bonus .png. :) </p>

<pre><code>import numpy as np
import pandas as pd
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt

from PySide.QtGui import QImage
from PySide.QtGui import QPainter
from PySide.QtCore import QSize
from PySide.QtWebKit import QWebPage

arrays = [np.hstack([ ['one']*3, ['two']*3]), ['Dog', 'Bird', 'Cat']*2]
columns = pd.MultiIndex.from_arrays(arrays, names=['foo', 'bar'])
df =pd.DataFrame(np.zeros((3,6)),columns=columns,index=pd.date_range('20000103',periods=3))

h = ""&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;p&gt; "" + df.to_html() + "" &lt;/p&gt; &lt;/body&gt; &lt;/html&gt;"";
page = QWebPage()
page.setViewportSize(QSize(5000,5000))

frame = page.mainFrame()
frame.setHtml(h, ""text/html"")

img = QImage(1000,700, QImage.Format(5))
painter = QPainter(img)
frame.render(painter)
painter.end()
a = img.save(""html.png"")

pp = PdfPages('html.pdf')
fig = plt.figure(figsize=(8,6),dpi=1080) 
ax = fig.add_subplot(1, 1, 1)
img2 = plt.imread(""html.png"")
plt.axis('off')
ax.imshow(img2)
pp.savefig()
pp.close()
</code></pre>

<p>Edits welcome.</p>
";;0;;2014-07-07T11:49:11.703;;24609894;2014-07-07T14:38:27.333;2014-07-07T14:38:27.333;;3647167.0;;3647167.0;24574976.0;2;6;;;
15633;15633;;;"<p>The gcf method is depricated in V 0.14, The below code works for me:</p>

<pre><code>plot = dtf.plot()
fig = plot.get_figure()
fig.savefig(""output.png"")
</code></pre>
";;4;;2014-07-10T10:51:42.867;;24674675;2014-07-10T10:51:42.867;;;;;1945699.0;19555525.0;2;42;;;
15857;15857;;;"<p>The corresponding operator is <code>|</code>:</p>

<pre><code> df[(df &lt; 3) | (df == 5)]
</code></pre>

<p>would elementwise check if value is less than 3 or equal to 5.</p>
";;1;;2014-07-16T08:24:41.700;;24775756;2016-03-08T19:48:28.200;2016-03-08T19:48:28.200;;2901002.0;;1099682.0;24775648.0;2;42;;;
15883;15883;;;"<p>The quickest way is to use DatetimeIndex's normalize (you first need to make the column a DatetimeIndex):</p>

<pre><code>In [11]: df = pd.DataFrame({""t"": pd.date_range('2014-01-01', periods=5, freq='H')})

In [12]: df
Out[12]:
                    t
0 2014-01-01 00:00:00
1 2014-01-01 01:00:00
2 2014-01-01 02:00:00
3 2014-01-01 03:00:00
4 2014-01-01 04:00:00

In [13]: pd.DatetimeIndex(df.t).normalize()
Out[13]:
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2014-01-01, ..., 2014-01-01]
Length: 5, Freq: None, Timezone: None

In [14]: df['date'] = pd.DatetimeIndex(df.t).normalize()

In [15]: df
Out[15]:
                    t       date
0 2014-01-01 00:00:00 2014-01-01
1 2014-01-01 01:00:00 2014-01-01
2 2014-01-01 02:00:00 2014-01-01
3 2014-01-01 03:00:00 2014-01-01
4 2014-01-01 04:00:00 2014-01-01
</code></pre>

<p><em>DatetimeIndex also has some other useful attributes, e.g. .year, .month, .day.</em></p>

<hr>

<p>From 0.15 they'll be a dt attribute, so you can access this (and other methods) with:</p>

<pre><code>df.t.dt.normalize()
# equivalent to
pd.DatetimeIndex(df.t).normalize()
</code></pre>
";;7;;2014-07-16T22:45:42.887;;24792087;2014-08-11T16:17:52.047;2014-08-11T16:17:52.047;;1240268.0;;1240268.0;24786209.0;2;22;;;
15888;15888;;;"<p>Pandas has something built in...</p>

<pre><code>numpyMatrix = df.as_matrix()
</code></pre>
";;1;;2014-07-17T01:13:50.003;;24793359;2014-07-17T01:22:10.027;2014-07-17T01:22:10.027;;2600939.0;;2600939.0;13187778.0;2;58;;;
15909;15909;;;"<p>You can further improve this by adding <code>wait=True</code> to <code>clear_output</code>:</p>

<pre><code>display.clear_output(wait=True)
display.display(pl.gcf())
</code></pre>
";;2;;2014-07-17T13:09:07.913;;24804512;2014-07-17T13:09:07.913;;;;;3849295.0;21360361.0;2;23;;;
15960;15960;;;"<p>If you know you always want to aggregate over the first two levels, then this is pretty easy:</p>

<pre><code>In [27]: data.groupby(level=[0, 1]).sum()
Out[27]:
A  B    277
   b     37
a  B    159
   b     16
dtype: int64
</code></pre>
";;3;;2014-07-18T13:46:44.137;;24826569;2014-07-18T13:46:44.137;;;;;478288.0;24826368.0;2;24;;;
15965;15965;;;"<pre><code>df.select_dtypes(include=[np.float64])
</code></pre>
";;0;;2014-07-18T15:15:27.080;;24828425;2015-08-13T13:17:44.127;2015-08-13T13:17:44.127;;3877338.0;;3853312.0;21271581.0;2;9;;;
16042;16042;;;"<p>This will work:</p>

<pre><code>if 'A' in df:
</code></pre>

<p>But for clarity, I'd probably write it as:</p>

<pre><code>if 'A' in df.columns:
</code></pre>
";;0;;2014-07-21T16:48:49.260;;24870404;2014-07-21T16:48:49.260;;;;;3657742.0;24870306.0;2;134;;;
16046;16046;;;"<p>Generally, <code>iterrows</code> should only be used in very very specific cases. This is the general order of precedence for performance of various operations:</p>

<pre><code>1) vectorization
2) using a custom cython routine
3) apply
    a) reductions that can be performed in cython
    b) iteration in python space
4) itertuples
5) iterrows
6) updating an empty frame (e.g. using loc one-row-at-a-time)
</code></pre>

<p>Using a custom cython routine is usually too complicated, so let's skip that for now.</p>

<p>1) Vectorization is ALWAYS ALWAYS the first and best choice. However, their are a small set of cases which can not be vectorized in obvious ways (mostly involving a recurrence). Further on a smallish frame, it may be faster to do other methods.</p>

<p>3) Apply involves <em>can</em> usually be done by an iterator in cython space (this is done internally in pandas) (this is a) case.</p>

<p>This is dependent on what is going on inside the apply expression. e.g. <code>df.apply(lambda x: np.sum(x))</code> will be executed pretty swiftly (of course <code>df.sum(1)</code> is even better). However something like: <code>df.apply(lambda x: x['b'] + 1)</code> will be executed in python space, and consequently is slower.</p>

<p>4) <code>itertuples</code> does not box the data into a Series, just returns it as a tuple</p>

<p>5) <code>iterrows</code> DOES box the data into a Series. Unless you really need this, use another method.</p>

<p>6) updating an empty frame a-single-row-at-a-time. I have seen this method used WAY too much. It is by far the slowest. It is probably common place (and reasonably fast for some python structures), but a DataFrame does a fair number of checks on indexing, so this will always be very slow to update a row at a time. Much better to create new structures and <code>concat</code>.</p>
";;3;;2014-07-21T17:39:48.443;;24871316;2014-07-21T17:46:05.553;2014-07-21T17:46:05.553;;644898.0;;644898.0;24870953.0;2;48;;;
16079;16079;;;"<p>Example at @Nasser's answer:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(columns=('lib', 'qty1', 'qty2'))
&gt;&gt;&gt; for i in range(5):
&gt;&gt;&gt;     df.loc[i] = [randint(-1,1) for n in range(3)]
&gt;&gt;&gt;
&gt;&gt;&gt; print(df)
    lib  qty1  qty2
0    0     0    -1
1   -1    -1     1
2    1    -1     1
3    0     0     0
4    1    -1    -1

[5 rows x 3 columns]
</code></pre>
";;5;;2014-07-22T13:10:25.313;;24888331;2016-04-11T07:20:25.597;2016-04-11T07:20:25.597;;1209582.0;;1289107.0;10715965.0;2;180;;;
16106;16106;;;"<p>Someone will give you a better answe than this possibly, but one thing I tend to do is if all my numeric data are <code>int64</code> or <code>float64</code> objects, then you can create a dict of the column data types and then use the values to create your list of columns.</p>

<p>So for example, in a dataframe where I have columns of type <code>float64</code>, <code>int64</code> and <code>object</code> firstly you can look at the data types as so:</p>

<pre><code>DF.dtypes
</code></pre>

<p>and if they conform to the standard whereby the non-numeric columns of data are all <code>object</code> types (as they are in my dataframes), then you can do the following to get a list of the numeric columns:</p>

<pre><code>[key for key in dict(DF.dtypes) if dict(DF.dtypes)[key] in ['float64', 'int64']]
</code></pre>

<p>Its just a simple list comprehension. Nothing fancy. Again, though whether this works for you will depend upon how you set up you dataframe...</p>
";;1;;2014-07-23T05:09:47.197;;24902313;2014-07-23T05:09:47.197;;;;;2484720.0;24901766.0;2;15;;;
16109;16109;;;"<p>There's a new feature in 0.14.1, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes""><code>select_dtypes</code></a> to select columns by dtype, by providing a list of dtypes to include or exclude.</p>

<p>For example:</p>

<pre><code>df = pd.DataFrame({'a': np.random.randn(1000),
                   'b': range(1000),
                   'c': ['a'] * 1000,
                   'd': pd.date_range('2000-1-1', periods=1000)})


df.select_dtypes(['float64','int64'])

Out[129]: 
            a    b
0    0.153070    0
1    0.887256    1
2   -1.456037    2
3   -1.147014    3
...
</code></pre>
";;2;;2014-07-23T10:06:18.270;;24907560;2014-07-23T10:06:18.270;;;;;3657742.0;24901766.0;2;12;;;
16124;16124;;;"<p>If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer):</p>

<pre><code>import pandas as pd
import numpy as np
# we know we're gonna have 5 rows of data
numberOfRows = 5
# create dataframe
df = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )

# now fill it up row by row
for x in np.arange(0, numberOfRows):
    #loc or iloc both work here since the index is natural numbers
    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]
In[23]: df
Out[23]: 
   lib  qty1  qty2
0   -1    -1    -1
1    0     0     0
2   -1     0    -1
3    0    -1     0
4   -1     0     0
</code></pre>

<p><strong>Speed comparison</strong></p>

<pre><code>In[30]: %timeit tryThis() # function wrapper for this answer
In[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)
1000 loops, best of 3: 1.23 ms per loop
100 loops, best of 3: 2.31 ms per loop
</code></pre>

<p>And - as from the comments - with a size of 6000, the speed difference becomes even larger:</p>

<blockquote>
  <p>Increasing the size of the array (12) and the number of rows (500) makes
  the speed difference more striking: 313ms vs 2.29s</p>
</blockquote>
";;2;;2014-07-23T14:21:45.483;;24913075;2015-04-02T12:03:48.543;2015-04-02T12:03:48.543;;1082349.0;;1082349.0;10715965.0;2;42;;;
16179;16179;;;"<p>I use len function. It's much faster than empty(). len(df.index) is even faster.</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10000, 4), columns=list('ABCD'))

def empty(df):
    return df.empty

def lenz(df):
    return len(df) == 0

def lenzi(df):
    return len(df.index) == 0

'''
%timeit empty(df)
%timeit lenz(df)
%timeit lenzi(df)

10000 loops, best of 3: 13.9 s per loop
100000 loops, best of 3: 2.34 s per loop
1000000 loops, best of 3: 695 ns per loop

len on index seems to be faster
'''
</code></pre>
";;3;;2014-07-24T12:06:18.817;;24933234;2016-03-13T14:37:57.207;2016-03-13T14:37:57.207;;2137255.0;;2137255.0;19828822.0;2;25;;;
16238;16238;;;"<p>By default, <code>groupby</code> output has the grouping columns as indicies, not columns, which is why the merge is failing.</p>

<p>There are a couple different ways to handle it, probably the easiest is using the <code>as_index</code> parameter when you define the groupby object.</p>

<pre><code>po_grouped_df = poagg_df.groupby(['EID','PCODE'], as_index=False)
</code></pre>

<p>Then, your merge should work as expected.</p>

<pre><code>In [356]: pd.merge(acc_df, pol_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))
Out[356]: 
   EID PCODE  SC_Acc  EE_Acc        SI_Acc  PVALUE_Acc  EE_Po  PVALUE_Po  \
0  123    GR     236   40000  1.805222e+31         350  10000         50   
1  123    GR     236   40000  1.805222e+31         350  30000        300   
2  123    GU     443   12000  8.765549e+87         250  10000        100   
3  123    GU     443   12000  8.765549e+87         250   2000        150   

   SC_Po  SI_Po  
0     23     40  
1    213    140  
2    230    400  
3    213    140  
</code></pre>
";;0;;2014-07-27T12:10:29.183;;24980809;2014-07-27T12:10:29.183;;;;;3657742.0;24980437.0;2;25;;;
16248;16248;;;"<p>Pandas wants the MultiIndex values as tuples, not nested dicts.  The simplest thing is to convert your dictionary to the right format before trying to pass it to DataFrame:</p>

<pre><code>&gt;&gt;&gt; reform = {(outerKey, innerKey): values for outerKey, innerDict in dictionary.iteritems() for innerKey, values in innerDict.iteritems()}
&gt;&gt;&gt; reform
{('A', 'a'): [1, 2, 3, 4, 5],
 ('A', 'b'): [6, 7, 8, 9, 1],
 ('B', 'a'): [2, 3, 4, 5, 6],
 ('B', 'b'): [7, 8, 9, 1, 2]}
&gt;&gt;&gt; pandas.DataFrame(reform)
   A     B   
   a  b  a  b
0  1  6  2  7
1  2  7  3  8
2  3  8  4  9
3  4  9  5  1
4  5  1  6  2

[5 rows x 4 columns]
</code></pre>
";;12;;2014-07-28T03:58:45.413;;24988227;2014-07-28T03:58:45.413;;;;;1427416.0;24988131.0;2;26;;;
16317;16317;;;"<pre><code>def order(frame,var):
    varlist =[w for w in frame.columns if w not in var]
    frame = frame[var+varlist]
    return frame 
</code></pre>

<p>This function takes two arguments, the first is the dataset, the second are the columns in the data set that you want to bring to the front. </p>

<p>So in my case I have a data set called Frame with variables A1, A2, B1, B2, Total and Date. If I want to bring Total to the front then all I have to do is: </p>

<pre><code>frame = order(frame,['Total'])
</code></pre>

<p>If I want to bring Total and Date to the front then I do:</p>

<pre><code>frame = order(frame,['Total','Date'])
</code></pre>

<p>EDIT:</p>

<p>Another useful way to use this is, if you have an unfamiliar table and you're looking with variables with a particular term in them, like VAR1, VAR2,... you may execute something like: </p>

<pre><code>frame = order(frame,[v for v in frame.columns if ""VAR"" in v])
</code></pre>
";;0;;2014-07-29T19:30:18.293;;25023460;2016-08-12T20:08:03.233;2016-08-12T20:08:03.233;;1920550.0;;1920550.0;13148429.0;2;8;;;
16321;16321;;;"<p>You need to combine the functions that apply to the same column, like this:</p>

<pre><code>In [116]: gb.agg({'sum_col' : np.sum,
     ...:         'date' : [np.min, np.max]})
Out[116]: 
                      date             sum_col
                      amin       amax      sum
type weekofyear                               
A    25         2014-06-22 2014-06-22        1
     26         2014-06-25 2014-06-25        1
     27         2014-07-05 2014-07-05        2
B    26         2014-06-24 2014-06-24        2
     27         2014-07-02 2014-07-02        1
C    26         2014-06-25 2014-06-25        3
     27         2014-07-06 2014-07-06        3
     30         2014-07-27 2014-07-27        1
</code></pre>
";;1;;2014-07-29T21:12:10.107;;25025065;2014-07-29T21:12:10.107;;;;;3657742.0;25024797.0;2;22;;;
16336;16336;;;"<p>For recent readers of this question: pandas have the following warning in their <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html"" rel=""noreferrer"">docs for version 14.0</a>:</p>

<blockquote>
  <p>Warning: Some of the existing functions or function aliases have been
  deprecated and will be removed in future versions. This includes:
  tquery, uquery, read_frame, frame_query, write_frame. </p>
</blockquote>

<p>And:</p>

<blockquote>
  <p>Warning: The support for the mysql flavor when using DBAPI connection objects has
  been deprecated. MySQL will be further supported with SQLAlchemy
  engines (GH6900).</p>
</blockquote>

<p>This makes many of the answers here outdated. You should use <code>sqlalchemy</code>:</p>

<pre><code>from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('dialect://user:pass@host:port/schema', echo=False)
f = pd.read_sql_query('SELECT * FROM mytable', engine, index_col = 'ID')
</code></pre>
";;5;;2014-07-30T07:04:38.090;;25030617;2014-09-04T05:11:19.800;2014-09-04T05:11:19.800;;1809530.0;;1809530.0;10065051.0;2;41;;;
16372;16372;;;"<p>This doesn't work because <code>NaN</code> isn't equal to anything, including <code>NaN</code>. Use <code>pd.isnull(df.var2)</code> instead.</p>
";;2;;2014-07-31T03:02:10.893;;25050179;2015-03-13T00:53:06.667;2015-03-13T00:53:06.667;;2038264.0;;3479224.0;25050141.0;2;24;;;
16381;16381;;;"<p>I'd use <code>iloc</code>, which takes a row/column slice, both based on integer position and following normal python syntax.</p>

<pre><code>df.iloc[::5, :]
</code></pre>
";;1;;2014-07-31T11:25:16.297;;25057724;2014-07-31T11:25:16.297;;;;;3657742.0;25055712.0;2;41;;;
16384;16384;;;"<p>Assuming the two dataframes have the same columns, you could just concatenate them and compute your summary stats on the concatenated frames:</p>

<pre><code>import numpy as np
import pandas as pd

# some random data frames
df1 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))
df2 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))

# concatenate them
df_concat = pd.concat((df1, df2))

print df_concat.mean()
# x   -0.163044
# y    2.120000
# dtype: float64

print df_concat.median()
# x   -0.192037
# y    2.000000
# dtype: float64
</code></pre>

<h2>Update</h2>

<p>If you want to compute stats across each set of rows with the same index in the two datasets, you can use <code>.groupby()</code> to group the data by row index, then apply the mean, median etc.:</p>

<pre><code>by_row_index = df_concat.groupby(df_concat.index)
df_means = by_row_index.mean()

print df_means.head()
#           x    y
# 0 -0.850794  1.5
# 1  0.159038  1.5
# 2  0.083278  1.0
# 3 -0.540336  0.5
# 4  0.390954  3.5
</code></pre>

<p>This method will work even when your dataframes have unequal numbers of rows - if a particular row index is missing in one of the two dataframes, the mean/median will be computed on the single existing row.</p>
";;3;;2014-07-31T11:44:59.443;;25058102;2014-07-31T13:21:59.960;2014-07-31T13:21:59.960;;1461210.0;;1461210.0;25057835.0;2;15;;;
16387;16387;;;"<p>You can simply assign a label to each frame, call it <code>group</code> and then <code>concat</code> and <code>groupby</code> to do what you want:</p>

<pre><code>In [57]: df = DataFrame(np.random.randn(10, 4), columns=list('abcd'))

In [58]: df2 = df.copy()

In [59]: dfs = [df, df2]

In [60]: df
Out[60]:
        a       b       c       d
0  0.1959  0.1260  0.1464  0.1631
1  0.9344 -1.8154  1.4529 -0.6334
2  0.0390  0.4810  1.1779 -1.1799
3  0.3542  0.3819 -2.0895  0.8877
4 -2.2898 -1.0585  0.8083 -0.2126
5  0.3727 -0.6867 -1.3440 -1.4849
6 -1.1785  0.0885  1.0945 -1.6271
7 -1.7169  0.3760 -1.4078  0.8994
8  0.0508  0.4891  0.0274 -0.6369
9 -0.7019  1.0425 -0.5476 -0.5143

In [61]: for i, d in enumerate(dfs):
   ....:     d['group'] = i
   ....:

In [62]: dfs[0]
Out[62]:
        a       b       c       d  group
0  0.1959  0.1260  0.1464  0.1631      0
1  0.9344 -1.8154  1.4529 -0.6334      0
2  0.0390  0.4810  1.1779 -1.1799      0
3  0.3542  0.3819 -2.0895  0.8877      0
4 -2.2898 -1.0585  0.8083 -0.2126      0
5  0.3727 -0.6867 -1.3440 -1.4849      0
6 -1.1785  0.0885  1.0945 -1.6271      0
7 -1.7169  0.3760 -1.4078  0.8994      0
8  0.0508  0.4891  0.0274 -0.6369      0
9 -0.7019  1.0425 -0.5476 -0.5143      0

In [63]: final = pd.concat(dfs, ignore_index=True)

In [64]: final
Out[64]:
         a       b       c       d  group
0   0.1959  0.1260  0.1464  0.1631      0
1   0.9344 -1.8154  1.4529 -0.6334      0
2   0.0390  0.4810  1.1779 -1.1799      0
3   0.3542  0.3819 -2.0895  0.8877      0
4  -2.2898 -1.0585  0.8083 -0.2126      0
5   0.3727 -0.6867 -1.3440 -1.4849      0
6  -1.1785  0.0885  1.0945 -1.6271      0
..     ...     ...     ...     ...    ...
13  0.3542  0.3819 -2.0895  0.8877      1
14 -2.2898 -1.0585  0.8083 -0.2126      1
15  0.3727 -0.6867 -1.3440 -1.4849      1
16 -1.1785  0.0885  1.0945 -1.6271      1
17 -1.7169  0.3760 -1.4078  0.8994      1
18  0.0508  0.4891  0.0274 -0.6369      1
19 -0.7019  1.0425 -0.5476 -0.5143      1

[20 rows x 5 columns]

In [65]: final.groupby('group').mean()
Out[65]:
           a       b       c       d
group
0     -0.394 -0.0576 -0.0682 -0.4339
1     -0.394 -0.0576 -0.0682 -0.4339
</code></pre>

<p>Here, each <code>group</code> is the same, but that's only because <code>df == df2</code>.</p>

<p>Alternatively, you can throw the frames into a <code>Panel</code>:</p>

<pre><code>In [69]: df = DataFrame(np.random.randn(10, 4), columns=list('abcd'))

In [70]: df2 = DataFrame(np.random.randn(10, 4), columns=list('abcd'))

In [71]: panel = pd.Panel({0: df, 1: df2})

In [72]: panel
Out[72]:
&lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: 2 (items) x 10 (major_axis) x 4 (minor_axis)
Items axis: 0 to 1
Major_axis axis: 0 to 9
Minor_axis axis: a to d

In [73]: panel.mean()
Out[73]:
        0       1
a  0.3839  0.2956
b  0.1855 -0.3164
c -0.1167 -0.0627
d -0.2338 -0.0450
</code></pre>
";;1;;2014-07-31T12:57:05.800;;25059471;2014-07-31T12:57:05.800;;;;;564538.0;25057835.0;2;6;;;
16389;16389;;;"<p>I go similar as @ali_m, but since you want one mean per row-column combination, I conclude differently:</p>

<pre><code>df1 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))
df2 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))
df = pd.concat([df1, df2])
foo = df.groupby(level=0).mean()
foo.head()

          x    y
0  0.841282  2.5
1  0.716749  1.0
2 -0.551903  2.5
3  1.240736  1.5
4  1.227109  2.0
</code></pre>
";;2;;2014-07-31T13:04:46.927;;25059620;2014-07-31T13:04:46.927;;;;;1082349.0;25057835.0;2;10;;;
16391;16391;;;"<p>I found this error to be due to a combination of:</p>

<ul>
<li>using pandas plotting with a series or dataframe member method</li>
<li>plotting with a date index</li>
<li>using <code>%matplotlib inline</code> magic in ipython</li>
<li>importing the pylab module <strong>before</strong> the matplotlib magic</li>
</ul>

<p>So the following will fail on a <strong>newly started kernel</strong> in an ipython notebook:</p>

<pre><code># fails 
import matplotlib.pylab
%matplotlib inline

import pandas
ser = pandas.Series(range(10), pandas.date_range(end='2014-01-01', periods=10))
ser.plot()
</code></pre>

<p>The best way to solve this is to move the magic up to the top:</p>

<pre><code># succeeds
%matplotlib inline # moved up
import matplotlib.pylab

import pandas
ser = pandas.Series(range(10), pandas.date_range(end='2014-01-01', periods=10))
ser.plot()
</code></pre>

<p>However the problem also goes away if you pass the series to a <code>matplotlib</code> plotting method, don't use a date index, or simply don't import the <code>matplotlib.pylab</code> module.</p>
";;6;;2014-07-31T13:58:30.983;;25060811;2014-07-31T13:58:30.983;;;;;2402965.0;23550056.0;2;19;;;
16505;16505;;;"<p>We can use <code>ix</code> to reorder by passing a list:</p>

<pre><code>In [27]:
# get a list of columns
cols = list(df)
# move the column to head of list using index, pop and insert
cols.insert(0, cols.pop(cols.index('Mid')))
cols
Out[27]:
['Mid', 'Net', 'Upper', 'Lower', 'Zsore']
In [28]:
# use ix to reorder
df = df.ix[:, cols]
df
Out[28]:
                      Mid Net  Upper   Lower  Zsore
Answer_option                                      
More_than_once_a_day    2  0%  0.22%  -0.12%     65
Once_a_day              3  0%  0.32%  -0.19%     45
Several_times_a_week    4  2%  2.45%   1.10%     78
Once_a_week             6  1%  1.63%  -0.40%     65
</code></pre>

<p>Another method is to take a reference to the column and reinsert it at the front:</p>

<pre><code>In [39]:
mid = df['Mid']
df.drop(labels=['Mid'], axis=1,inplace = True)
df.insert(0, 'Mid', mid)
df
Out[39]:
                      Mid Net  Upper   Lower  Zsore
Answer_option                                      
More_than_once_a_day    2  0%  0.22%  -0.12%     65
Once_a_day              3  0%  0.32%  -0.19%     45
Several_times_a_week    4  2%  2.45%   1.10%     78
Once_a_week             6  1%  1.63%  -0.40%     65
</code></pre>

<p>You can also use <code>loc</code> to achieve the same result as <code>ix</code> will be deprecated in a future version of pandas from <a href=""https://pandas.pydata.org/pandas-docs/stable/whatsnew.html#deprecate-ix"" rel=""nofollow noreferrer""><code>0.20.0</code></a> onwards:</p>

<pre><code>df = df.loc[:, cols]
</code></pre>
";;8;;2014-08-04T15:30:09.930;;25122293;2017-06-27T11:09:46.847;2017-06-27T11:09:46.847;;704848.0;;704848.0;25122099.0;2;25;;;
16531;16531;;;"<p>You can use a <a href=""https://stackoverflow.com/questions/890128/why-are-python-lambdas-useful"">lambda expression</a>, e.g: </p>

<pre><code>sales['time_hour'] = sales.timestamp.apply(lambda x: x.hour)
</code></pre>
";;1;;2014-08-04T23:53:33.917;;25129265;2016-07-03T11:02:47.857;2017-05-23T12:18:19.107;;-1.0;;3908468.0;25129144.0;2;7;;;
16532;16532;;;"<p>Assuming timestamp is the index of the dataframe, you can just do</p>

<pre><code>    hours = sales.index.hour
</code></pre>

<p>If you want to add that to your sales dataframe, just do</p>

<pre><code>    import pandas as pd
    pd.concat([sales, pd.DataFrame(hours, index=sales.index)], axis = 1)
</code></pre>

<p>Edit:
If you have several columns of datetime objects, its the same process. If you have a column ['date'] in your dataframe, and assuming that 'date' has datetime values, you can access the hour from the 'date' as:</p>

<pre><code>    hours = sales['date'].hour
</code></pre>
";;3;;2014-08-05T00:44:41.673;;25129655;2014-08-07T02:15:44.283;2014-08-07T02:15:44.283;;3321542.0;;3321542.0;25129144.0;2;7;;;
16574;16574;;;"<p>You can directly access the <code>year</code> and <code>month</code> attributes, or request a <code>datetime.datetime</code>:</p>

<pre><code>In [15]: t = pandas.tslib.Timestamp.now()

In [16]: t
Out[16]: Timestamp('2014-08-05 14:49:39.643701', tz=None)

In [17]: t.to_datetime()
Out[17]: datetime.datetime(2014, 8, 5, 14, 49, 39, 643701)

In [18]: t.day
Out[18]: 5

In [19]: t.month
Out[19]: 8

In [20]: t.year
Out[20]: 2014
</code></pre>

<p>One way to combine year and month is to make an integer encoding them, such as: <code>201408</code> for August, 2014. Along a whole column, you could do this as:</p>

<pre><code>df['YearMonth'] = df['ArrivalDate'].map(lambda x: 1000*x.year + x.month)
</code></pre>

<p>or many variants thereof.</p>

<p>I'm not a big fan of doing this, though, since it makes date alignment and arithmetic painful later and especially painful for others who come upon your code or data without this same convention. A better way is to choose a day-of-month convention, such as final non-US-holiday weekday, or first day, etc., and leave the data in a date/time format with the chosen date convention.</p>

<p>The <code>calendar</code> module is useful for obtaining the number value of certain days such as the final weekday. Then you could do something like:</p>

<pre><code>import calendar
import datetime
df['AdjustedDateToEndOfMonth'] = df['ArrivalDate'].map(
    lambda x: datetime.datetime(
        x.year,
        x.month,
        max(calendar.monthcalendar(x.year, x.month)[-1][:5])
    )
)
</code></pre>

<p>If you happen to be looking for a way to solve the simpler problem of just formatting the datetime column into some stringified representation, for that you can just make use of the <a href=""http://strftime.org/""><code>strftime</code></a> function from the <code>datetime.datetime</code> class, like this:</p>

<pre><code>In [5]: df
Out[5]: 
            date_time
0 2014-10-17 22:00:03

In [6]: df.date_time
Out[6]: 
0   2014-10-17 22:00:03
Name: date_time, dtype: datetime64[ns]

In [7]: df.date_time.map(lambda x: x.strftime('%Y-%m-%d'))
Out[7]: 
0    2014-10-17
Name: date_time, dtype: object
</code></pre>
";;3;;2014-08-05T18:59:43.080;;25146337;2015-09-24T02:11:09.603;2015-09-24T02:11:09.603;;567620.0;;567620.0;25146121.0;2;42;;;
16587;16587;;;"<p>If you want new columns showing year and month separately you can do this:</p>

<pre><code>df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year
df['month'] = pd.DatetimeIndex(df['ArrivalDate']).month
</code></pre>

<p>or...</p>

<pre><code>df['year'] = df['ArrivalDate'].dt.year
df['month'] = df['ArrivalDate'].dt.month
</code></pre>

<p>Then you can combine them or work with them just as they are.</p>
";;3;;2014-08-05T22:18:11.913;;25149272;2015-05-03T20:17:59.537;2015-05-03T20:17:59.537;;3232824.0;;3232824.0;25146121.0;2;96;;;
16611;16611;;;"<p>In my practice, the strongest, easiest-to-see difference is that a Panel needs to be homogeneous in every dimension. If you look at a Panel as a stack of Dataframes, you cannot create it by stacking Dataframes of different sizes or with different indexes/columns. You can  indeed handle more non-homogeneous type of data with multiindex.</p>

<p>So the first choice has to be made based on how your data is to be organized.</p>
";;1;;2014-08-06T14:22:36.390;;25162895;2014-08-06T14:22:36.390;;;;;1223677.0;22156258.0;2;9;;;
16664;16664;;;"<p>There is a <code>get_level_values</code> method that you can use in conjunction with boolean indexing to get the the intended result.</p>

<pre><code>In [13]:

df = pd.DataFrame(np.random.random((4,4)))
df.columns = pd.MultiIndex.from_product([[1,2],['A','B']])
print df
          1                   2          
          A         B         A         B
0  0.543980  0.628078  0.756941  0.698824
1  0.633005  0.089604  0.198510  0.783556
2  0.662391  0.541182  0.544060  0.059381
3  0.841242  0.634603  0.815334  0.848120
In [14]:

print df.iloc[:, df.columns.get_level_values(1)=='A']
          1         2
          A         A
0  0.543980  0.756941
1  0.633005  0.198510
2  0.662391  0.544060
3  0.841242  0.815334
</code></pre>
";;0;;2014-08-07T18:56:28.847;;25190070;2014-08-07T18:56:28.847;;;;;2487184.0;25189575.0;2;17;;;
16694;16694;;;"<p>Latest version of Pandas does not have this issue anymore</p>

<pre><code>  import pandas as pd
  df = pd.DataFrame([[1], [2], [3]])

  df2 = df.copy(deep=True)

  id(df), id(df2)
  Out[3]: (136575472, 127792400)

  id(df.index), id(df2.index)
  Out[4]: (145820144, 127657008)
</code></pre>
";;1;;2014-08-08T14:52:55.733;;25206286;2014-08-08T14:52:55.733;;;;;1379678.0;17591104.0;2;19;;;
16707;16707;;;"<p>I know it's been a while since this question was asked, but there is (at least <em>now</em> there is) a one-liner that is supported by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get_dummies.html"" rel=""noreferrer"">the documentation</a>:</p>

<pre><code>In [4]: df
Out[4]:
      label
0  (a, c, e)
1     (a, d)
2       (b,)
3     (d, e)

In [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')
Out[5]:
   a  b  c  d  e
0  1  0  1  0  1
1  1  0  0  1  0
2  0  1  0  0  0
3  0  0  0  1  1
</code></pre>
";;5;;2014-08-08T17:25:47.957;;25208947;2016-09-13T00:38:52.143;2016-09-13T00:38:52.143;;2808975.0;;2808975.0;18889588.0;2;37;;;
16721;16721;;;"<p>Suppose you want to output the first and last 10 rows of the iris data set.</p>

<p>In R:</p>

<pre><code>data(iris)
head(iris, 10)
tail(iris, 10)
</code></pre>

<p>In Python (scikit-learn required to load the iris data set):</p>

<pre><code>import pandas as pd
from sklearn import datasets
iris = pd.DataFrame(datasets.load_iris().data)
iris.head(10)
iris.tail(10)
</code></pre>

<p>Now, as <a href=""https://stackoverflow.com/questions/13085709/df-head-sometimes-doesnt-work-in-pandas-python"">previously answered</a>, if your data frame is too large for the display you use in the terminal, a summary is output. To visualize your data in a terminal, you could either expend the terminal or reduce the number of columns to display, as follows.</p>

<pre><code>iris.ix[:,1:2].head(10)
</code></pre>
";;0;;2014-08-08T20:35:14.200;;25211834;2014-08-09T04:08:25.667;2017-05-23T12:34:42.623;;-1.0;;1298376.0;25211220.0;2;10;;;
16727;16727;;;"<p>You need to get hold of the axes themselves. Probably the cleanest way is to change your last row:</p>

<pre><code>lm = sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)
</code></pre>

<p>Then you can get hold of the axes objects (an array of axes):</p>

<pre><code>axes = lm.axes
</code></pre>

<p>After that you can tweak the axes properties</p>

<pre><code>axes[0,0].set_ylim(0,)
axes[0,1].set_ylim(0,)
</code></pre>

<p>creates:</p>

<p><img src=""https://i.stack.imgur.com/Cgfsu.png"" alt=""enter image description here""></p>
";;1;;2014-08-08T23:00:00.743;;25213438;2014-08-08T23:00:00.743;;;;;3735428.0;25212986.0;2;35;;;
16730;16730;;;"<p>The <code>lmplot</code> function returns a <code>FacetGrid</code> instance. This object has a method called <code>set</code>, to which you can pass <code>key=value</code> pairs and they will be set on each Axes object in the grid.</p>

<p>Secondly, you can set only one side of an Axes limit in matplotlib by passing <code>None</code> for the value you want to remain as the default.</p>

<p>Putting these together, we have:</p>

<pre><code>g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)
g.set(ylim=(0, None))
</code></pre>

<p><img src=""https://i.stack.imgur.com/45rjT.png"" alt=""enter image description here""></p>
";;2;;2014-08-08T23:19:53.093;;25213614;2014-08-08T23:19:53.093;;;;;1533576.0;25212986.0;2;48;;;
16735;16735;;;"<p>Here's a simple way to do that:</p>

<pre><code>In[20]: my_dict = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )
In[21]: df = pd.DataFrame.from_dict(my_dict, orient='index')
In[22]: df
Out[22]: 
   0  1   2   3
A  1  2 NaN NaN
B  1  2   3   4
In[23]: df.transpose()
Out[23]: 
    A  B
0   1  1
1   2  2
2 NaN  3
3 NaN  4
</code></pre>
";;1;;2014-08-09T10:06:16.477;;25217425;2014-08-09T10:06:16.477;;;;;3924801.0;19736080.0;2;28;;;
16758;16758;;;"<p>Use <code>index=False</code>.</p>

<pre><code>pd.to_csv('your.csv', index=False)
</code></pre>
";;0;;2014-08-10T16:17:52.547;;25230582;2014-08-10T16:17:52.547;;;;;3925504.0;20845213.0;2;164;;;
16814;16814;;;"<p>To select the <code>ith</code> row, <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing-loc-iloc-and-ix"">use <code>iloc</code></a>:</p>

<pre><code>In [31]: df_test.iloc[0]
Out[31]: 
ATime     1.2
X         2.0
Y        15.0
Z         2.0
Btime     1.2
C        12.0
D        25.0
E        12.0
Name: 0, dtype: float64
</code></pre>

<p>To select the ith value in the <code>Btime</code> column you could use:</p>

<pre><code>In [30]: df_test['Btime'].iloc[0]
Out[30]: 1.2
</code></pre>

<hr>

<p><strong>Warning</strong>: I had previously suggested <code>df_test.ix[i, 'Btime']</code>. But this is not guaranteed to give you the <code>ith</code> value since <code>ix</code> tries to index by <em>label</em> before trying to index by <em>position</em>. So if the DataFrame has an integer index which is not in sorted order starting at 0, then using <code>ix[i]</code> will return the row <em>labeled</em> <code>i</code> rather than the <code>ith</code> row. For example,</p>

<pre><code>In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])

In [2]: df
Out[2]: 
  foo
0   A
2   B
1   C

In [4]: df.ix[1, 'foo']
Out[4]: 'C'
</code></pre>
";;0;;2014-08-11T23:39:45.140;;25254087;2014-08-12T08:46:33.097;2014-08-12T08:46:33.097;;190597.0;;190597.0;25254016.0;2;115;;;
16894;16894;;;"<p>As the other answers note, the <code>boxplot</code> function is limited to plotting a single ""layer"" of boxplots, and the <code>groupby</code> parameter only has an effect when the input is a Series and you have a second variable you want to use to bin the observations into each box..</p>

<p>However, you can accomplish what I think you're hoping for with the <code>factorplot</code> function, using <code>kind=""box""</code>. But, you'll first have to ""melt"" the sample dataframe into what is called long-form or ""tidy"" format where each column is a variable and each row is an observation:</p>

<pre><code>df_long = pd.melt(df, ""b"", var_name=""a"", value_name=""c"")
</code></pre>

<p>Then it's very simple to plot:</p>

<pre><code>sns.factorplot(""a"", hue=""b"", y=""c"", data=df_long, kind=""box"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/KHJXh.png"" alt=""enter image description here""></p>
";;1;;2014-08-13T14:39:07.427;;25289109;2014-08-13T14:47:35.450;2014-08-13T14:47:35.450;;1533576.0;;1533576.0;25284859.0;2;21;;;
17035;17035;;;"<p>Set the <code>display.max_colwidth</code> option to <code>-1</code>:</p>

<pre><code>pd.set_option('display.max_colwidth', -1)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html#pandas-set-option""><code>set_option</code> docs</a></p>
";;3;;2014-08-17T18:15:57.337;;25352191;2014-08-17T19:14:51.747;2014-08-17T19:14:51.747;;625914.0;;625914.0;25351968.0;2;43;;;
17077;17077;;;"<p>Try this:</p>

<pre><code>df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] 
</code></pre>
";;4;;2014-08-19T06:33:20.680;;25376997;2017-06-01T13:37:55.310;2017-06-01T13:37:55.310;;3705840.0;;3955169.0;19365513.0;2;43;;;
17116;17116;;;"<p>Using the following DataFrame ...</p>

<p><img src=""https://i.stack.imgur.com/psVUU.png"" alt=""DataFrame""></p>

<pre><code># using pandas version 0.14.1
from pandas import DataFrame
import pandas as pd
import matplotlib.pyplot as plt

data = {'ColB': {('A', 4): 3.0,
('C', 2): 0.0,
('B', 4): 51.0,
('B', 1): 0.0,
('C', 3): 0.0,
('B', 2): 7.0,
('Code', 'Month'): '',
('A', 3): 5.0,
('C', 1): 0.0,
('C', 4): 0.0,
('B', 3): 12.0},
'ColA': {('A', 4): 66.0,
('C', 2): 5.0,
('B', 4): 125.0,
('B', 1): 5.0,
('C', 3): 41.0,
('B', 2): 52.0,
('Code', 'Month'): '',
('A', 3): 22.0,
('C', 1): 14.0,
('C', 4): 51.0,
('B', 3): 122.0}}

df = DataFrame(data)
</code></pre>

<p>... you can plot the following (using cross-section):</p>

<pre><code>f, a = plt.subplots(3,1)
df.xs('A').plot(kind='bar',ax=a[0])
df.xs('B').plot(kind='bar',ax=a[1])
df.xs('C').plot(kind='bar',ax=a[2])
</code></pre>

<p><img src=""https://i.stack.imgur.com/sIbtl.png"" alt=""enter image description here""></p>

<p>One for A, one for B and one for C, x-axis: 'Month', the bars are ColA and ColB.
Maybe this is what you are looking for.</p>
";;2;;2014-08-20T09:34:00.990;;25401328;2014-08-20T12:47:41.123;2014-08-20T12:47:41.123;;3953012.0;;3953012.0;25386870.0;2;6;;;
17133;17133;;;"<p>I found the unstack(level) method to work perfectly, which has the added benefit of not needing a priori knowledge about how many Codes there are.</p>

<pre><code>summed_group.unstack(level=0).plot(kind='bar', subplots=True)
</code></pre>
";;1;;2014-08-20T19:28:55.840;;25412939;2014-08-20T19:28:55.840;;;;;1183875.0;25386870.0;2;43;;;
17136;17136;;;"<p>Try this:</p>

<pre><code>pd.set_option('display.expand_frame_repr', False)
</code></pre>

<p>From the documentation:</p>

<blockquote>
  <p>display.expand_frame_repr : boolean</p>
  
  <p>Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple pages if its width exceeds display.width. [default: True] [currently: True]</p>
</blockquote>

<p>See: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html</a></p>
";;1;;2014-08-20T22:19:24.467;;25415404;2014-08-20T22:19:24.467;;;;;3962081.0;11707586.0;2;59;;;
17184;17184;;;"<p>You can just do:</p>

<pre><code>df.values.flatten()
</code></pre>

<p>and you can also add <code>.tolist()</code> if you want the result to be a Python <code>list</code>.</p>
";;0;;2014-08-22T06:02:14.753;;25440505;2014-08-22T06:02:14.753;;;;;832621.0;25440008.0;2;34;;;
17190;17190;;;"<pre><code>sudo pip install pandas
sudo easy_install --upgrade numpy
</code></pre>

<p>should also realign everything.</p>
";;1;;2014-08-22T07:57:20.517;;25442201;2014-08-22T07:57:20.517;;;;;340322.0;24122850.0;2;18;;;
17193;17193;;;"<p>Update: this functionality has been merged in pandas master and will be released in 0.15 (probably end of september), thanks to @artemyk! See <a href=""https://github.com/pydata/pandas/pull/8062"">https://github.com/pydata/pandas/pull/8062</a></p>

<p>So starting from 0.15, you can specify the <code>chunksize</code> argument and e.g. simply do:</p>

<pre><code>df.to_sql('table', engine, chunksize=20000)
</code></pre>
";;0;;2014-08-22T08:45:32.957;;25442986;2014-08-22T08:45:32.957;;;;;653364.0;24007762.0;2;11;;;
17206;17206;;;"<p>You get it directly from the axes' patches:</p>

<pre><code>In [35]: for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))
</code></pre>

<p>You'll want to tweak the string formatting and the offsets to get things centered, maybe use the width from <code>p.get_width()</code>, but that should get you started. May not worked with stacked barplots unless you track the offsets somewhere.</p>
";;6;;2014-08-22T14:19:45.450;;25449186;2014-08-22T14:19:45.450;;;;;1889400.0;25447700.0;2;37;;;
17218;17218;;;"<p>Here's what I've done to adapt RandomForestClassifier to work with RFECV:</p>

<pre><code>class RandomForestClassifierWithCoef(RandomForestClassifier):
    def fit(self, *args, **kwargs):
        super(RandomForestClassifierWithCoef, self).fit(*args, **kwargs)
        self.coef_ = self.feature_importances_
</code></pre>

<p>Just using this class does the trick if you use 'accuracy' or 'f1' score. For 'roc_auc', RFECV complains that multiclass format is not supported. Changing it to two-class classification with the code below, the 'roc_auc' scoring works. (Using Python 3.4.1 and scikit-learn 0.15.1)</p>

<pre><code>y=(pd.Series(iris.target, name='target')==2).astype(int)
</code></pre>

<p>Plugging into your code:</p>

<pre><code>from sklearn import datasets
import pandas as pd
from pandas import Series
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV

class RandomForestClassifierWithCoef(RandomForestClassifier):
    def fit(self, *args, **kwargs):
        super(RandomForestClassifierWithCoef, self).fit(*args, **kwargs)
        self.coef_ = self.feature_importances_

iris = datasets.load_iris()
x=pd.DataFrame(iris.data, columns=['var1','var2','var3', 'var4'])
y=(pd.Series(iris.target, name='target')==2).astype(int)
rf = RandomForestClassifierWithCoef(n_estimators=500, min_samples_leaf=5, n_jobs=-1)
rfecv = RFECV(estimator=rf, step=1, cv=2, scoring='roc_auc', verbose=2)
selector=rfecv.fit(x, y)
</code></pre>
";;0;;2014-08-22T19:14:15.207;;25454051;2014-08-22T19:14:15.207;;;;;3969102.0;24123498.0;2;17;;;
17272;17272;;;"<p>The rows you get back from <code>iterrows</code> are copies that are no longer connected to the original data frame, so edits don't change your dataframe. Thankfully, because each item you get back from <code>iterrows</code> contains the current index, you can use that to access and edit the relevant row of the dataframe:</p>

<pre><code>for index, row in rche_df.iterrows():
    if isinstance(row.wgs1984_latitude, float):
        row = row.copy()
        target = row.address_chi        
        dict_temp = geocoding(target)
        rche_df.loc[index, 'wgs1984_latitude'] = dict_temp['lat']
        rche_df.loc[index, 'wgs1984_longitude'] = dict_temp['long']
</code></pre>

<p>In my experience, this approach seems slower than using an approach like <code>apply</code> or <code>map</code>, but as always, it's up to you to decide how to make the performance/ease of coding tradeoff.</p>
";;1;;2014-08-25T04:04:16.833;;25478896;2014-08-25T04:04:16.833;;;;;1222578.0;25478528.0;2;26;;;
17275;17275;;;"<p>This is a one-liner, you just need to use the <code>axis</code> argument for <code>min</code> to tell it to work across the columns rather than down:</p>

<pre><code>df['Minimum'] = df.loc[:, ['B0', 'B1', 'B2']].min(axis=1)
</code></pre>

<p>If you need to use this solution for different numbers of columns, you can use a for loop or list comprehension to construct the list of columns:</p>

<pre><code>n_columns = 2
cols_to_use = ['B' + str(i) for i in range(n_columns)]
df['Minimum'] = df.loc[:, cols_to_use].min(axis=1)
</code></pre>
";;6;;2014-08-25T06:14:26.757;;25479955;2014-08-25T07:58:36.440;2014-08-25T07:58:36.440;;1222578.0;;1222578.0;25479607.0;2;16;;;
17292;17292;;;"<p>Perform a <code>left</code> merge, this will use <code>sku</code> column as the column to join on:</p>

<pre><code>In [26]:

df.merge(df1, on='sku', how='left')
Out[26]:
   sku  loc   flag dept
0  122   61   True    b
1  122   62   True    b
2  122   63  False    b
3  123   61   True    b
4  123   62  False    b
5  113   62   True    a
6  301   63   True    c
</code></pre>

<p>If <code>sku</code> is in fact your index then do this:</p>

<pre><code>In [28]:

df.merge(df1, left_index=True, right_index=True, how='left')
Out[28]:
     loc   flag dept
sku                 
113   62   True    a
122   61   True    b
122   62   True    b
122   63  False    b
123   61   True    b
123   62  False    b
301   63   True    c
</code></pre>

<p>Another method is to use <code>map</code>, if you set <code>sku</code> as the index on your second df, so in effect it becomes a Series then the code simplifies to this:</p>

<pre><code>In [19]:

df['dept']=df.sku.map(df1.dept)
df
Out[19]:
   sku  loc   flag dept
0  122   61   True    b
1  123   61   True    b
2  113   62   True    a
3  122   62   True    b
4  123   62  False    b
5  122   63  False    b
6  301   63   True    c
</code></pre>
";;6;;2014-08-25T20:22:13.397;;25493765;2014-08-26T08:03:49.210;2014-08-26T08:03:49.210;;704848.0;;704848.0;25493625.0;2;39;;;
17323;17323;;;"<p>As @chrisb said, pandas' <code>read_csv</code> is probably faster than <code>csv.reader/numpy.genfromtxt/loadtxt</code>. I don't think you will find something better to parse the csv (as a note, <code>read_csv</code> is not a 'pure python' solution, as the CSV parser is implemented in C). </p>

<p>But, if you have to load/query the data often, a solution would be to parse the CSV only once and then store it in another format, eg HDF5. You can use <code>pandas</code> (with <code>PyTables</code> in background) to query that efficiently (<a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"" rel=""nofollow noreferrer"">docs</a>).<br>
See here for a comparison of the io performance of HDF5, csv and SQL with pandas: <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#performance-considerations"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/io.html#performance-considerations</a></p>

<p>And a possibly relevant other question: <a href=""https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas/14268804#14268804"">&quot;Large data&quot; work flows using pandas</a></p>
";;1;;2014-08-26T15:36:57.603;;25509805;2014-08-26T16:20:06.497;2017-05-23T12:34:28.853;;-1.0;;653364.0;25508510.0;2;15;;;
17390;17390;;;"<p>I ran into a similar question myself, and just wanted to add what I settled on. I liked the reindex_axis( ) method for changing column order. This worked:</p>

<p><code>df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)</code></p>
";;0;;2014-08-27T19:49:31.997;;25535803;2014-08-27T19:49:31.997;;;;;3878013.0;13148429.0;2;6;;;
17451;17451;;;"<p>Faster way would be to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.weekday.html"" rel=""nofollow noreferrer""><code>DatetimeIndex.weekday</code></a>, like so:</p>

<pre><code>temp = pd.DatetimeIndex(data['my_dt'])
data['weekday'] = temp.weekday
</code></pre>

<p>Much much faster, especially for a large number of rows. For further info, check <a href=""https://stackoverflow.com/a/21811766/3765319"">this</a> answer.</p>
";;1;;2014-08-29T03:04:26.423;;25561094;2014-08-29T03:04:26.423;2017-05-23T12:17:55.503;;-1.0;;3765319.0;13740672.0;2;8;;;
17455;17455;;;"<p>To use mean values for numeric columns and the most frequent value for non-numeric columns you could do something like this. You could further distinguish between integers and floats. I guess it might make sense to use the median for integer columns instead.</p>

<pre><code>import pandas as pd
import numpy as np

from sklearn.base import TransformerMixin

class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """"""Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """"""
    def fit(self, X, y=None):

        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)

        return self

    def transform(self, X, y=None):
        return X.fillna(self.fill)

data = [
    ['a', 1, 2],
    ['b', 1, 1],
    ['b', 2, 2],
    [np.nan, np.nan, np.nan]
]

X = pd.DataFrame(data)
xt = DataFrameImputer().fit_transform(X)

print('before...')
print(X)
print('after...')
print(xt)
</code></pre>

<p>which prints,</p>

<pre><code>before...
     0   1   2
0    a   1   2
1    b   1   1
2    b   2   2
3  NaN NaN NaN
after...
   0         1         2
0  a  1.000000  2.000000
1  b  1.000000  1.000000
2  b  2.000000  2.000000
3  b  1.333333  1.666667
</code></pre>
";;4;;2014-08-29T06:44:58.287;;25562948;2014-08-29T15:29:17.337;2014-08-29T15:29:17.337;;469992.0;;469992.0;25239958.0;2;40;;;
17473;17473;;;"<p>Since <code>inplace</code> argument is available, you don't need to copy and assign the original data frame back to itself, but do as follows:</p>

<pre><code>df.rename(columns={'two':'new_name'}, inplace=True)
</code></pre>
";;0;;2014-08-29T18:23:55.447;;25574089;2014-09-25T19:54:13.747;2014-09-25T19:54:13.747;;3216742.0;;3216742.0;20868394.0;2;61;;;
17502;17502;;;"<p>You have to use the figure returned by the <code>DataFrame.plot()</code> command:</p>

<pre><code>ax = df.plot()
fig = ax.get_figure()
fig.savefig('asdf.png')
</code></pre>
";;2;;2014-08-31T02:31:58.650;;25588487;2014-08-31T02:31:58.650;;;;;158259.0;19726663.0;2;25;;;
17541;17541;;;"<p>Here is a alternative which uses <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html"" rel=""noreferrer"">NumPy datetime64 and timedelta64 arithmetic</a>. It appears to be a bit faster for small DataFrames and much faster for larger DataFrames:</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame({'M':[1,2,3,4], 'D':[6,7,8,9], 'Y':[1990,1991,1992,1993]})
#    D  M     Y
# 0  6  1  1990
# 1  7  2  1991
# 2  8  3  1992
# 3  9  4  1993

y = np.array(df['Y']-1970, dtype='&lt;M8[Y]')
m = np.array(df['M']-1, dtype='&lt;m8[M]')
d = np.array(df['D']-1, dtype='&lt;m8[D]')
dates2 = pd.Series(y+m+d)
# 0   1990-01-06
# 1   1991-02-07
# 2   1992-03-08
# 3   1993-04-09
# dtype: datetime64[ns]
</code></pre>

<hr>

<pre><code>In [214]: df = pd.concat([df]*1000)

In [215]: %timeit pd.to_datetime((df['Y']*10000+df['M']*100+df['D']).astype('int'), format='%Y%m%d')
100 loops, best of 3: 4.87 ms per loop

In [216]: %timeit pd.Series(np.array(df['Y']-1970, dtype='&lt;M8[Y]')+np.array(df['M']-1, dtype='&lt;m8[M]')+np.array(df['D']-1, dtype='&lt;m8[D]'))
1000 loops, best of 3: 839 s per loop
</code></pre>

<hr>

<p>Here's a helper function to make this easier to use:</p>

<pre><code>def combine64(years, months=1, days=1, weeks=None, hours=None, minutes=None,
              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):
    years = np.asarray(years) - 1970
    months = np.asarray(months) - 1
    days = np.asarray(days) - 1
    types = ('&lt;M8[Y]', '&lt;m8[M]', '&lt;m8[D]', '&lt;m8[W]', '&lt;m8[h]',
             '&lt;m8[m]', '&lt;m8[s]', '&lt;m8[ms]', '&lt;m8[us]', '&lt;m8[ns]')
    vals = (years, months, days, weeks, hours, minutes, seconds,
            milliseconds, microseconds, nanoseconds)
    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)
               if v is not None)

In [437]: combine64(df['Y'], df['M'], df['D'])
Out[437]: array(['1990-01-06', '1991-02-07', '1992-03-08', '1993-04-09'], dtype='datetime64[D]')
</code></pre>
";;2;;2014-09-01T19:39:47.627;;25612064;2014-09-02T03:35:03.480;2014-09-02T03:35:03.480;;190597.0;;190597.0;19350806.0;2;7;;;
17567;17567;;;"<p>It appears I was mistaken in thinking that the issue was one in PyCharm (that could be solved, for example, in a setting or preference.) It actually has to do with the console session itself. The console attempts to auto-detect the width of the display area, but when that fails it defaults to 80 characters. This behavior can be overridden with:</p>

<pre><code>desired_width = 320    
pd.set_option('display.width', desired_width)
</code></pre>

<p>Where you can of course set the <code>desired_width</code> to whatever your display will tolerate.
Thanks to @TidB for the suggestion that my initial concern wasn't focused in the right area.</p>
";;2;;2014-09-02T19:15:26.537;;25630681;2014-09-02T19:15:26.537;;;;;3866328.0;25628496.0;2;19;;;
17583;17583;;;"<p>You could provide a list of columns to be dropped and return back the DataFrame with only the columns needed using the <code>drop()</code> function on a Pandas DataFrame.</p>

<p>Just saying</p>

<pre><code>colsToDrop = ['a']
df.drop(colsToDrop, axis=1)
</code></pre>

<p>would return a DataFrame with just the columns <code>b</code> and <code>c</code>.</p>

<p>The <code>drop</code> method is documented <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a>.</p>
";;0;;2014-09-03T11:30:59.433;;25643178;2014-11-03T22:16:59.837;2014-11-03T22:16:59.837;;3923281.0;;590215.0;11285613.0;2;9;;;
17588;17588;;;"<p>You could do this, where <code>td</code> is your series of timedeltas.  The division converts the nanosecond deltas into day deltas, and the conversion to int drops to whole days.</p>

<pre><code>import numpy as np

(td / np.timedelta64(1, 'D')).astype(int)
</code></pre>
";;1;;2014-09-03T14:03:46.967;;25646414;2016-07-13T23:39:01.223;2016-07-13T23:39:01.223;;3657742.0;;3657742.0;25646200.0;2;35;;;
17719;17719;;;"<p>You can use <code>replace</code> and pass the strings to find/replace as dictionary keys/items:</p>

<pre><code>df.replace({'\n': '&lt;br&gt;'}, regex=True)
</code></pre>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': ['1\n', '2\n', '3'], 'b': ['4\n', '5', '6\n']})
&gt;&gt;&gt; df
   a    b
0  1\n  4\n
1  2\n  5
2  3    6\n

&gt;&gt;&gt; df.replace({'\n': '&lt;br&gt;'}, regex=True)
   a      b
0  1&lt;br&gt;  4&lt;br&gt;
1  2&lt;br&gt;  5
2  3      6&lt;br&gt;
</code></pre>
";;8;;2014-09-06T09:21:22.297;;25698756;2015-11-26T22:04:31.757;2015-11-26T22:04:31.757;;3923281.0;;3923281.0;25698710.0;2;46;;;
17725;17725;;;"<p>I'm not sure if this is exactly what you want, but I found these grouper functions on <a href=""https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks"">another SO thread</a> fairly useful for doing a multiprocessor pool.</p>

<p>Here's a short example from that thread, which might do something like what you want:</p>

<pre><code>import numpy as np
import pandas as pds

df = pds.DataFrame(np.random.rand(14,4), columns=['a', 'b', 'c', 'd'])

def chunker(seq, size):
    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))

for i in chunker(df,5):
    print i
</code></pre>

<p>Which gives you something like this:</p>

<pre><code>          a         b         c         d
0  0.860574  0.059326  0.339192  0.786399
1  0.029196  0.395613  0.524240  0.380265
2  0.235759  0.164282  0.350042  0.877004
3  0.545394  0.881960  0.994079  0.721279
4  0.584504  0.648308  0.655147  0.511390
          a         b         c         d
5  0.276160  0.982803  0.451825  0.845363
6  0.728453  0.246870  0.515770  0.343479
7  0.971947  0.278430  0.006910  0.888512
8  0.044888  0.875791  0.842361  0.890675
9  0.200563  0.246080  0.333202  0.574488
           a         b         c         d
10  0.971125  0.106790  0.274001  0.960579
11  0.722224  0.575325  0.465267  0.258976
12  0.574039  0.258625  0.469209  0.886768
13  0.915423  0.713076  0.073338  0.622967
</code></pre>

<p>I hope that helps.</p>

<p>EDIT</p>

<p>In this case, I used this function with <a href=""https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers"" rel=""nofollow noreferrer"">pool of processors</a> in (approximately) this manner:</p>

<pre><code>from multiprocessing import Pool

nprocs = 4

pool = Pool(nprocs)

for chunk in chunker(df, nprocs):
    data = pool.map(myfunction, chunk)
    data.domorestuff()
</code></pre>

<p>I assume this should be very similar to using the IPython distributed machinery, but I haven't tried it.</p>
";;1;;2014-09-06T15:03:35.830;;25701576;2014-09-06T15:15:06.067;2017-05-23T12:26:06.950;;-1.0;;2662077.0;25699439.0;2;14;;;
17731;17731;;;"<p>In practice, you can't guarantee equal-sized chunks: the number of rows might be prime, after all, in which case your only chunking options would be chunks of size 1 or one big chunk.  I tend to pass an array to <code>groupby</code>.  Starting from:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(15, 5), index=[0]*15)
&gt;&gt;&gt; df[0] = range(15)
&gt;&gt;&gt; df
    0         1         2         3         4
0   0  0.746300  0.346277  0.220362  0.172680
0   1  0.657324  0.687169  0.384196  0.214118
0   2  0.016062  0.858784  0.236364  0.963389
[...]
0  13  0.510273  0.051608  0.230402  0.756921
0  14  0.950544  0.576539  0.642602  0.907850

[15 rows x 5 columns]
</code></pre>

<p>where I've deliberately made the index uninformative by setting it to 0, we simply decide on our size (here 10) and integer-divide an array by it:</p>

<pre><code>&gt;&gt;&gt; df.groupby(np.arange(len(df))//10)
&lt;pandas.core.groupby.DataFrameGroupBy object at 0xb208492c&gt;
&gt;&gt;&gt; for k,g in df.groupby(np.arange(len(df))//10):
...     print(k,g)
...     
0    0         1         2         3         4
0  0  0.746300  0.346277  0.220362  0.172680
0  1  0.657324  0.687169  0.384196  0.214118
0  2  0.016062  0.858784  0.236364  0.963389
[...]
0  8  0.241049  0.246149  0.241935  0.563428
0  9  0.493819  0.918858  0.193236  0.266257

[10 rows x 5 columns]
1     0         1         2         3         4
0  10  0.037693  0.370789  0.369117  0.401041
0  11  0.721843  0.862295  0.671733  0.605006
[...]
0  14  0.950544  0.576539  0.642602  0.907850

[5 rows x 5 columns]
</code></pre>

<p>Methods based on slicing the DataFrame can fail when the index isn't compatible with that, although you can always use <code>.iloc[a:b]</code> to ignore the index values and access data by position.</p>
";;1;;2014-09-06T17:43:02.533;;25703030;2014-09-06T17:43:02.533;;;;;487339.0;25699439.0;2;15;;;
17745;17745;;;"<p>Just ran into this issue myself. As of pandas 0.13, DataFrames have a _metadata attribute on them that does persist through functions that return new DataFrames. Also seems to survive serialization just fine (I've only tried json, but I imagine hdf is covered as well).</p>
";;5;;2014-09-07T23:31:15.983;;25715719;2014-09-07T23:31:15.983;;;;;782063.0;14688306.0;2;8;;;
17747;17747;;;"<p>This is by design, as described <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"">here </a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply"">here</a></p>

<p>The <code>apply</code> function needs to know the shape of the returned data to intelligently figure out how it will be combined. To do this it calls the function (<code>checkit</code> in your case) twice to achieve this.</p>

<p>Depending on your actual use case, you can replace the call to <code>apply</code> with <code>aggregate</code>, <code>transform</code> or <code>filter</code>, as described in detail <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply"">here</a>. These functions require the return value to be a particular shape, and so don't call the function twice.</p>

<p>However - if the function you are calling does not have side-effects, it most likely does not matter that the function is being called twice on the first value.</p>
";;0;;2014-09-08T01:39:33.043;;25716383;2014-09-08T01:39:33.043;;;;;424153.0;21390035.0;2;12;;;
17776;17776;;;"<p>The <em>reset_index()</em> is a pandas DataFrame method that will transfer index values into the DataFrame as columns.  The default setting for the parameter is <em>drop=False</em> (which will keep the index values as columns).</p>

<p>All you have to do add <code>.reset_index(inplace=True)</code> after the name of the DataFrame:</p>

<pre><code>df.reset_index(inplace=True)  
</code></pre>
";;0;;2014-09-08T21:42:18.217;;25733562;2014-10-21T21:02:50.783;2014-10-21T21:02:50.783;;464923.0;;3000639.0;20110170.0;2;82;;;
17800;17800;;;"<p>If you have a just a few columns to sum, you can write: </p>

<pre><code>df['e'] = df.a + df.b + df.d
</code></pre>

<p>This creates new column <code>e</code> with the values:</p>

<pre><code>   a  b   c  d   e
0  1  2  dd  5   8
1  2  3  ee  9  14
2  3  4  ff  1   8
</code></pre>

<p>For longer lists of columns, EdChum's answer is preferred.</p>
";;4;;2014-09-09T15:38:24.337;;25748741;2015-02-26T13:07:37.607;2015-02-26T13:07:37.607;;3923281.0;;3923281.0;25748683.0;2;10;;;
17801;17801;;;"<p>You can just <code>sum</code> and set param <code>axis=1</code> to sum the rows, this will ignore none numeric columns:</p>

<pre><code>In [91]:

df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})
df['e'] = df.sum(axis=1)
df
Out[91]:
   a  b   c  d   e
0  1  2  dd  5   8
1  2  3  ee  9  14
2  3  4  ff  1   8
</code></pre>

<p>If you want to just sum specific columns then you can create a list of the columns and remove the ones you are not interested in:</p>

<pre><code>In [98]:

col_list= list(df)
col_list.remove('d')
col_list
Out[98]:
['a', 'b', 'c']
In [99]:

df['e'] = df[col_list].sum(axis=1)
df
Out[99]:
   a  b   c  d  e
0  1  2  dd  5  3
1  2  3  ee  9  5
2  3  4  ff  1  7
</code></pre>
";;4;;2014-09-09T15:42:55.143;;25748826;2014-09-09T15:42:55.143;;;;;704848.0;25748683.0;2;71;;;
17855;17855;;;"<p>It's perhaps simplest to remember it as <em>0=down</em> and <em>1=across</em>. </p>

<p>This means:</p>

<ul>
<li>Use <code>axis=0</code> to apply a method down each column, or to the row labels (the index).</li>
<li>Use <code>axis=1</code> to apply a method across each row, or to the column labels.</li>
</ul>

<p>Here's a picture to show the parts of a DataFrame that each axis refers to:</p>

<p><img src=""https://i.stack.imgur.com/DL0iQ.jpg"" width=""410"" height=""210""></p>

<p>It's also useful to remember that Pandas follows NumPy's use of the word <code>axis</code>. The usage is explained in NumPy's <a href=""http://docs.scipy.org/doc/numpy/glossary.html"">glossary of terms</a>:</p>

<blockquote>
  <p>Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically <strong>downwards across rows (axis 0)</strong>, and the second running <strong>horizontally across columns (axis 1)</strong>. [<em>my emphasis</em>] </p>
</blockquote>

<p>So, concerning the method in the question, <code>df.mean(axis=1)</code>, seems to be correctly defined. It takes the mean of entries <em>horizontally across columns</em>, that is, along each individual row. On the other hand, <code>df.mean(axis=0)</code> would be an operation acting vertically <em>downwards across rows</em>.</p>

<p>Similarly, <code>df.drop(name, axis=1)</code> refers to an action on column labels, because they intuitively go across the horizontal axis. Specifying <code>axis=0</code> would make the method act on rows instead.</p>
";;4;;2014-09-10T20:20:45.450;;25774395;2015-02-26T23:00:52.070;2015-02-26T23:00:52.070;;3923281.0;;3923281.0;25773245.0;2;107;;;
17856;17856;;;"<p>Both above answers would only return one index if there are multiple rows that take the maximum value. If you want all the rows, there does not seem to have a function.
But it is not hard to do. Below is an example for Series; the same can be done for DataFrame:</p>

<pre><code>In [1]: from pandas import Series, DataFrame

In [2]: s=Series([2,4,4,3],index=['a','b','c','d'])

In [3]: s.idxmax()
Out[3]: 'b'

In [4]: s[s==s.max()]
Out[4]: 
b    4
c    4
dtype: int64
</code></pre>
";;2;;2014-09-10T20:55:04.853;;25774932;2014-09-10T20:55:04.853;;;;;3870096.0;10202570.0;2;11;;;
17887;17887;;;"<p>You can call the <code>str</code> method and apply a slice, this will be much quicker than the other method as this is vectorised (thanks @unutbu):</p>

<pre><code>df['New_Sample'] = df.Sample.str[:1]
</code></pre>

<p>You can also call a lambda function on the df but this will be slower on larger dataframes:</p>

<pre><code>In [187]:

df['New_Sample'] = df.Sample.apply(lambda x: x[:1])
df
Out[187]:
  Sample  Value New_Sample
0    AAB     23          A
1    BAB     25          B
</code></pre>
";;2;;2014-09-11T14:02:02.597;;25789512;2014-09-11T14:21:13.137;2014-09-11T14:21:13.137;;704848.0;;704848.0;25789445.0;2;23;;;
17913;17913;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects"">pd.DateOffset</a>:</p>

<pre><code>test[1].index + pd.DateOffset(hours=16)
</code></pre>

<p><code>pd.DateOffset</code> accepts the same keyword arguments as <a href=""https://labix.org/python-dateutil#head-ba5ffd4df8111d1b83fc194b97ebecf837add454"">dateutil.relativedelta</a>.</p>

<hr>

<p>The problem you encountered was due to this bug which has been <a href=""https://github.com/pydata/pandas/issues/7611"">fixed in Pandas version 0.14.1</a>:</p>

<pre><code>In [242]: pd.to_timedelta(16, unit='h')
Out[242]: numpy.timedelta64(16,'ns')
</code></pre>

<p>If you upgrade, your original code should work.</p>
";;2;;2014-09-11T21:34:42.423;;25797313;2014-09-11T22:22:24.727;2014-09-11T22:22:24.727;;190597.0;;190597.0;25797245.0;2;17;;;
17921;17921;;;"<p>As the other answer points out, you're trying to collapse identical dates into single rows, whereas the cumsum function will return a series of the same length as the original DataFrame. Stated differently, you actually want to group by [Bool, Dir, Date], calculate a sum in each group, THEN return a cumsum on rows grouped by [Bool, Dir]. The other answer is a perfectly valid solution to your specific question, here's a one-liner variation:</p>

<pre><code>data1.groupby(['Bool', 'Dir', 'Date']).sum().groupby(level=[0, 1]).cumsum()
</code></pre>

<p>This returns output exactly in the requested format.</p>

<p>For those looking for a simple cumsum on a Pandas group, you can use:</p>

<pre><code>data1.groupby(['Bool', 'Dir']).apply(lambda x: x['Data'].cumsum())
</code></pre>

<p>The cumulative sum is calculated internal to each group. Here's what the output looks like:</p>

<pre><code>Bool  Dir            
N     E    2000-12-30     5
           2000-12-30    16
      W    2001-01-02     7
           2001-01-03    16
Y     E    2000-12-30     4
           2001-01-03    12
      W    2000-12-30     6
           2000-12-30    16
Name: Data, dtype: int64
</code></pre>

<p>Note the repeated dates, but this is doing a strict cumulative sum internal to the rows of each group identified by the Bool and Dir columns.</p>
";;3;;2014-09-11T23:09:13.970;;25798359;2015-09-30T13:23:39.147;2015-09-30T13:23:39.147;;3653274.0;;3653274.0;15755057.0;2;6;;;
17922;17922;;;"<p><code>plt.gca().xaxis.grid(True)</code> proved to be the solution for me</p>
";;2;;2014-09-12T02:23:47.900;;25799781;2014-09-12T04:40:55.983;2014-09-12T04:40:55.983;;2009616.0;;4033157.0;16074392.0;2;20;;;
18143;18143;;;"<p><code>set_index</code> and <code>reset_index</code> are your friends. </p>

<pre><code>df = DataFrame({""A"":[0,0.5,1.0,3.5,4.0,4.5], ""B"":[1,4,6,2,4,3], ""C"":[3,2,1,0,5,3]})
</code></pre>

<p>First move column A to the index:</p>

<pre><code>In [64]: df.set_index(""A"")
Out[64]: 
     B  C
 A        
0.0  1  3
0.5  4  2
1.0  6  1
3.5  2  0
4.0  4  5
4.5  3  3
</code></pre>

<p>Then reindex with a new index, here the missing data is filled in with nans. We use the <code>Index</code> object since we can name it; this will be used in the next step.</p>

<pre><code>In [66]: new_index = Index(arange(0,5,0.5), name=""A"")
In [67]: df.set_index(""A"").reindex(new_index)
Out[67]: 
      B   C
0.0   1   3
0.5   4   2
1.0   6   1
1.5 NaN NaN
2.0 NaN NaN
2.5 NaN NaN
3.0 NaN NaN
3.5   2   0
4.0   4   5
4.5   3   3
</code></pre>

<p>Finally move the index back to the columns with <code>reset_index</code>. Since we named the index, it all works magically:</p>

<pre><code>In [69]: df.set_index(""A"").reindex(new_index).reset_index()
Out[69]: 
       A   B   C
0    0.0   1   3
1    0.5   4   2
2    1.0   6   1
3    1.5 NaN NaN
4    2.0 NaN NaN
5    2.5 NaN NaN
6    3.0 NaN NaN
7    3.5   2   0
8    4.0   4   5
9    4.5   3   3
</code></pre>
";;1;;2014-09-18T14:58:28.663;;25916109;2014-09-18T14:58:28.663;;;;;1798300.0;25909984.0;2;11;;;
18201;18201;;;"<p>One easy way would be to groupby the first level of the index - iterating over the groupby object will return the group keys and a subframe containing each group.</p>

<pre><code>In [136]: for date, new_df in df.groupby(level=0):
     ...:     print(new_df)
     ...:     
                    observation1  observation2
date       Time                               
2012-11-02 9:15:00     79.373668           224
           9:16:00    130.841316           477

                    observation1  observation2
date       Time                               
2012-11-03 9:15:00     45.312814           835
           9:16:00    123.776946           623
           9:17:00    153.766460           624
           9:18:00    463.276946           626
           9:19:00    663.176934           622
           9:20:00    763.773330           621

                    observation1  observation2
date       Time                               
2012-11-04 9:15:00    115.449437           122
           9:16:00    123.776946           555
           9:17:00    153.766460           344
           9:18:00    463.276946           212
</code></pre>
";;1;;2014-09-19T13:23:17.607;;25935024;2014-09-19T13:23:17.607;;;;;3657742.0;25929319.0;2;27;;;
18251;18251;;;"<pre><code># Say you have a df object containing your dataframe
df.head(5) # will print out the first 5 rows
df.tail(5) # will print out the 5 last rows
# Note: it is similar to R
</code></pre>
";;0;;2014-09-21T13:09:56.510;;25959539;2014-09-21T13:09:56.510;;;;;1115377.0;15006298.0;2;26;;;
18257;18257;;;"<p>The error shows that the machine does not have enough memory to read the entire
CSV into a DataFrame at one time. Assuming you do not need the entire dataset in
memory all at one time, one way to avoid the problem would be to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">process the CSV in
chunks</a> (by specifying the <code>chunksize</code> parameter):</p>

<pre><code>chunksize = 10 ** 6
for chunk in pd.read_csv(filename, chunksize=chunksize):
    process(chunk)
</code></pre>
";;12;;2014-09-21T17:54:03.433;;25962187;2014-09-21T17:54:03.433;;;;;190597.0;25962114.0;2;60;;;
18315;18315;;;"<p>Here's a (very) slightly slicker version of <a href=""https://stackoverflow.com/a/15911372/2071807"">Dan Allan's answer</a>:</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import string

df = pd.DataFrame({'x':np.random.rand(10), 'y':np.random.rand(10)}, 
                  index=list(string.ascii_lowercase[:10]))
</code></pre>

<p>Which gives:</p>

<pre><code>          x         y
a  0.541974  0.042185
b  0.036188  0.775425
c  0.950099  0.888305
d  0.739367  0.638368
e  0.739910  0.596037
f  0.974529  0.111819
g  0.640637  0.161805
h  0.554600  0.172221
i  0.718941  0.192932
j  0.447242  0.172469
</code></pre>

<p>And then:</p>

<pre><code>fig, ax = plt.subplots()
df.plot('x', 'y', kind='scatter', ax=ax)

for k, v in df.iterrows():
    ax.annotate(k, v)
</code></pre>

<p>Finally, if you're in interactive mode you might need to refresh the plot:</p>

<pre><code>fig.canvas.draw()
</code></pre>

<p>Which produces:
<img src=""https://i.stack.imgur.com/UDrIj.png"" alt=""Boring scatter plot""></p>

<p>Or, since that looks incredibly ugly, you can beautify things a bit pretty easily:</p>

<pre><code>from matplotlib import cm
cmap = cm.get_cmap('Spectral')
df.plot('x', 'y', kind='scatter', ax=ax, s=120, linewidth=0, 
        c=range(len(df)), colormap=cmap)

for k, v in df.iterrows():
    ax.annotate(k, v,
                xytext=(10,-5), textcoords='offset points',
                family='sans-serif', fontsize=18, color='darkslategrey')
</code></pre>

<p>Which looks a lot nicer:
<img src=""https://i.stack.imgur.com/SiTb6.png"" alt=""Nice scatter plot""></p>
";;1;;2014-09-23T16:50:16.560;;26000515;2016-10-07T16:56:27.400;2017-05-23T10:30:49.480;;-1.0;;2071807.0;15910019.0;2;14;;;
18343;18343;;;"<p>I proceeded like this:</p>

<pre><code>chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\
       names=['lat','long','rf','date','slno'],index_col='slno',\
       header=None,parse_dates=['date'])

df=pd.DataFrame()
%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)
</code></pre>
";;2;;2014-09-24T12:46:23.757;;26017289;2017-08-03T18:56:53.103;2017-08-03T18:56:53.103;;4909087.0;;4064040.0;25962114.0;2;18;;;
18379;18379;;;"<p>In my case, the problem was that I was initially <em>reading</em> the CSV file with the wrong encoding (<code>ASCII</code> instead of <code>cp1252</code>). Therefore, when pandas tried to write it to an Excel file, it found some characters it couldn't decode.</p>

<p>I solved it by specifying the correct encoding when reading the CSV file.</p>

<pre><code>data = pd.read_csv(fname, encoding='cp1252')
</code></pre>
";;0;;2014-09-25T15:25:12.770;;26042312;2014-09-25T15:25:12.770;;;;;3718878.0;18645401.0;2;8;;;
18421;18421;;;"<p>An easy way to do this is using the <code>pandas</code> library like this.</p>

<pre><code>import pandas as pd
fields = ['star_name', 'ra']

df = pd.read_csv('data.csv', skipinitialspace=True, usecols=fields)
# See the keys
print df.keys()
# See content in 'star_name'
print df.star_name
</code></pre>

<p>The problem here was the <code>skipinitialspace</code> which remove the spaces in the header. So ' star_name' becomes 'star_name'</p>
";;0;;2014-09-26T17:14:44.877;;26064898;2014-09-26T17:14:44.877;;;;;1340702.0;26063231.0;2;25;;;
18477;18477;;;"<p>Rather than create 2 temporary dfs you can just pass these as params within a dict using the DataFrame constructor:</p>

<pre><code>pd.DataFrame({'email':sf.index, 'list':sf.values})
</code></pre>

<p>There are lots of ways to construct a df, see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame"" rel=""noreferrer"">docs</a></p>
";;1;;2014-09-29T10:59:01.407;;26098292;2014-09-29T11:20:04.303;2014-09-29T11:20:04.303;;704848.0;;704848.0;26097916.0;2;30;;;
18537;18537;;;"<p>You need to change this line: </p>

<pre><code>BabyDataSet = zip(names,births)
</code></pre>

<p>to:</p>

<pre><code>BabyDataSet = list(zip(names,births))
</code></pre>

<p>This is because <code>zip</code> now returns an iterator in python 3, hence your error message. For more details see: <a href=""http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html#zip"">http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html#zip</a> and <a href=""https://docs.python.org/3/library/functions.html#zip"">https://docs.python.org/3/library/functions.html#zip</a></p>
";;2;;2014-09-30T12:44:58.770;;26121238;2014-09-30T12:44:58.770;;;;;704848.0;26121009.0;2;14;;;
18576;18576;;;"<p>You are very close.
You applied the round the the series of values given by <code>df.value1</code>.
The return type is thus a Series.
You need to assign that series back to the dataframe (or another dataframe with the same Index)</p>

<p>Also there is a <code>Series.round</code> method which is basically a short hand for <code>Series.apply(np.round)</code></p>

<pre><code>In[2]: 
    df.value1 = df.value1.round()
    print df

Out[2]:
    item  value1  value2
    0    a       1     1.3
    1    a       2     2.5
    2    a       0     0.0
    3    b       3    -1.0
    4    b       5    -1.0
</code></pre>
";;2;;2014-10-01T03:24:10.343;;26133621;2014-10-01T03:24:10.343;;;;;179081.0;26133538.0;2;15;;;
18580;18580;;;"<p>You can pass <code>plt.scatter</code> a <code>c</code> argument which will allow you to select the colors. The code below defines a <code>colors</code> dictionary to map your diamond colors to the plotting colors.</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

carat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
price = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]
color =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]

df = pd.DataFrame(dict(carat=carat, price=price, color=color))

fig, ax = plt.subplots()

colors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}

ax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))

plt.show()
</code></pre>

<p><code>df['color'].apply(lambda x: colors[x])</code> effectively maps the colours from ""diamond"" to ""plotting"".</p>

<p><em>(Forgive me for not putting another example image up, I think 2 is enough :P)</em></p>

<h2>With <code>seaborn</code></h2>

<p>You can use <code>seaborn</code> which is a wrapper around <code>matplotlib</code> that makes it look prettier by default (rather opinion-based, I know :P) but also adds some plotting functions.</p>

<p>For this you could use <a href=""http://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.lmplot.html#seaborn-lmplot""><code>seaborn.lmplot</code></a> with <code>fit_reg=False</code> (which prevents it from automatically doing some regression).</p>

<p>The code below uses an example dataset. By selecting <code>hue='color'</code> you tell seaborn to split your dataframe up based on your colours and then plot each one.</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

carat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
price = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]
color =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]

df = pd.DataFrame(dict(carat=carat, price=price, color=color))

sns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/vOut1.png"" alt=""enter image description here""></p>

<h2>Without <code>seaborn</code> using <code>pandas.groupby</code></h2>

<p>If you don't want to use seaborn then you can use <code>pandas.groupby</code> to get the colors alone and then plot them using just matplotlib, but you'll have to manually assign colors as you go, I've added an example below:</p>

<pre><code>fig, ax = plt.subplots()

colors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}

grouped = df.groupby('color')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])

plt.show()
</code></pre>

<p>This code assumes the same DataFrame as above and then groups it based on <code>color</code>. It then iterates over these groups, plotting for each one. To select a color I've created a <code>colors</code> dictionary which can map the diamond color (for instance <code>D</code>) to a real color (for instance <code>red</code>).</p>

<p><img src=""https://i.stack.imgur.com/mWjhz.png"" alt=""enter image description here""></p>
";;5;;2014-10-01T10:50:55.357;;26139658;2014-10-01T11:11:56.227;2014-10-01T11:11:56.227;;3005188.0;;3005188.0;26139423.0;2;58;;;
18596;18596;;;"<pre><code>In [21]: df = pd.DataFrame([(1,2,3), ('foo','bar','baz'), (4,5,6)])

In [22]: df
Out[22]: 
     0    1    2
0    1    2    3
1  foo  bar  baz
2    4    5    6
</code></pre>

<p>Set the column labels to equal the values in the 2nd row (index location 1):</p>

<pre><code>In [23]: df.columns = df.iloc[1]
</code></pre>

<p>Drop the 2nd row:</p>

<pre><code>In [24]: df.reindex(df.index.drop(1))
Out[24]: 
1 foo bar baz
0   1   2   3
2   4   5   6
</code></pre>
";;3;;2014-10-01T17:42:09.403;;26147330;2014-10-01T17:42:09.403;;;;;190597.0;26147180.0;2;36;;;
18704;18704;;;"<p>I think I would do this with two groupbys.</p>

<p>First to calculate the ""weighted average"":</p>

<pre><code>In [11]: g = df.groupby('Date')

In [12]: df.value / g.value.transform(""sum"") * df.wt
Out[12]:
0    0.125000
1    0.250000
2    0.416667
3    0.277778
4    0.444444
dtype: float64
</code></pre>

<p>If you set this as a column, you can groupby over it:</p>

<pre><code>In [13]: df['wa'] = df.value / g.value.transform(""sum"") * df.wt
</code></pre>

<p>Now the sum of this column is the desired:</p>

<pre><code>In [14]: g.wa.sum()
Out[14]:
Date
01/01/2012    0.791667
01/02/2012    0.722222
Name: wa, dtype: float64
</code></pre>

<p>or potentially:</p>

<pre><code>In [15]: g.wa.transform(""sum"")
Out[15]:
0    0.791667
1    0.791667
2    0.791667
3    0.722222
4    0.722222
Name: wa, dtype: float64
</code></pre>
";;5;;2014-10-05T19:56:52.043;;26206622;2014-10-05T19:56:52.043;;;;;1240268.0;26205922.0;2;14;;;
18749;18749;;;"<p>Quoting the <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.html"">Pandas docs</a></p>

<blockquote>
  <p>pandas.<strong>DataFrame</strong>(<em>data=None, index=None, columns=None, dtype=None, copy=False</em>)</p>
  
  <p>Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes 
  (rows and columns). Arithmetic operations align on both row and column labels. 
  <strong>Can be thought of as a dict-like container for Series objects.</strong> The primary pandas data structure</p>
</blockquote>

<p>(Emphasis mine, sentence fragment not mine)</p>

<p>So the Series is the datastructure for a single column of a <code>DataFrame</code>, not only conceptually, but literally i.e. the data in a <code>DataFrame</code> is actually stored in memory as a collection of <code>Series</code>. </p>

<p>Analogously: <em>We need both lists and matrices, because matrices are built with lists. Single row matricies, while equivalent to lists in functionality still cannot exists without the list(s) they're composed of.</em></p>

<p>They both have extremely similar APIs, but you'll find that <code>DataFrame</code> methods always cater to the possibility that you have more than one column. And of course, you can always add another <code>Series</code> (or equivalent object) to a <code>DataFrame</code>, while adding a <code>Series</code> to another <code>Series</code> involves creating a <code>DataFrame</code>.</p>
";;3;;2014-10-07T15:55:09.320;;26240208;2014-10-09T19:39:32.820;2014-10-09T19:39:32.820;;2580133.0;;2580133.0;26047209.0;2;56;;;
18764;18764;;;"<p><strong>EDIT for <em>Pandas 0.17</em> or later:</strong></p>

<p>As the <code>take_last</code> argument of the <code>duplicated()</code> method was <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#whatsnew-0170-deprecations"" rel=""nofollow noreferrer"">deprecated</a> in favour of the new <code>keep</code> argument since <em>Pandas 0.17</em>, please refer to <a href=""https://stackoverflow.com/a/33381151/3903832"">this answer</a> for the correct approach:</p>

<ul>
<li>Invoke the <code>duplicated()</code> method with <code>keep=False</code>, i.e. <code>frame.duplicated(['key1', 'key2'], keep=False)</code>.</li>
</ul>

<p>Therefore, in order to extract the required data for this specific question, the following suffices:</p>

<pre><code>In [81]: frame[frame.duplicated(['key1', 'key2'], keep=False)].groupby(('key1', 'key2')).min()
Out[81]: 
           data
key1 key2      
1    2        5
2    2        1

[2 rows x 1 columns]
</code></pre>

<blockquote>
  <p>Interestingly enough, this change in <em>Pandas 0.17</em> may be partially attributed to this question, as referred to in <a href=""https://github.com/pydata/pandas/issues/8505"" rel=""nofollow noreferrer"">this issue</a>.</p>
</blockquote>

<hr>

<p><strong>For versions preceding <em>Pandas 0.17</em>:</strong></p>

<p>We can play with the <code>take_last</code> argument of the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"" rel=""nofollow noreferrer""><code>duplicated()</code></a> method:</p>

<blockquote>
  <p><code>take_last</code>: <code>boolean</code>, default <code>False</code>  </p>
  
  <blockquote>
    <p>For a set of distinct duplicate rows, flag all but the last row as duplicated. Default is for all but the first row to be flagged.</p>
  </blockquote>
</blockquote>

<p>If we set <code>take_last</code>'s value to <code>True</code>, we flag all but the last duplicate row. Combining this along with its default value of <code>False</code>, which flags all but the first duplicate row, allows us to flag all duplicated rows:</p>

<pre><code>In [76]: frame.duplicated(['key1', 'key2'])
Out[76]: 
0    False
1    False
2    False
3     True
4     True
5    False
6     True
7     True
dtype: bool

In [77]: frame.duplicated(['key1', 'key2'], take_last=True)
Out[77]: 
0     True
1     True
2    False
3    False
4     True
5    False
6     True
7    False
dtype: bool

In [78]: frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])
Out[78]: 
0     True
1     True
2    False
3     True
4     True
5    False
6     True
7     True
dtype: bool

In [79]: frame[frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])]
Out[79]: 
   key1  key2  data
0     1     2     5
1     2     2     6
3     1     2     6
4     2     2     1
6     2     2     2
7     2     2     8

[6 rows x 3 columns]
</code></pre>

<p>Now we just need to use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> and <code>min</code> methods, and I believe the output is in the required format:</p>

<pre><code>In [81]: frame[frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])].groupby(('key1', 'key2')).min()
Out[81]: 
           data
key1 key2      
1    2        5
2    2        1

[2 rows x 1 columns]
</code></pre>
";;1;;2014-10-07T20:42:19.990;;26244925;2016-02-25T11:09:37.227;2017-05-23T12:34:30.577;;-1.0;;3903832.0;26244309.0;2;24;;;
18802;18802;;;"<p>You could construct a dataframe from the series and then merge with the dataframe.
So you specify the data as the values but multiply them by the length, set the columns to the index and set params for left_index and right_index to True:</p>

<pre><code>In [27]:

df.merge(pd.DataFrame(data = [s.values] * len(s), columns = s.index), left_index=True, right_index=True)
Out[27]:
   a  b  s1  s2
0  1  3   5   6
1  2  4   5   6
</code></pre>

<p><strong>EDIT</strong> for the situation where you want the index of your constructed df from the series to use the index of the df then you can do the following:</p>

<pre><code>df.merge(pd.DataFrame(data = [s.values] * len(df), columns = s.index, index=df.index), left_index=True, right_index=True)
</code></pre>

<p>This assumes that the indices match the length.</p>
";;0;;2014-10-08T20:39:17.193;;26266031;2014-10-10T21:58:39.723;2014-10-10T21:58:39.723;;704848.0;;704848.0;26265819.0;2;6;;;
18805;18805;;;"<p>You could subtract the total length from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.count.html"">count</a> of non-nan values:</p>

<pre><code>count_nan = len(df) - df.count()
</code></pre>

<p>You should time it on your data. For small Series got a 3x speed up in comparison with the <code>isnull</code> solution.</p>
";;2;;2014-10-08T21:05:52.237;;26266439;2014-10-08T21:05:52.237;;;;;1330293.0;26266362.0;2;30;;;
18807;18807;;;"<p>You can use the <code>isnull()</code> method and then sum to count the nan values. For one column:</p>

<pre><code>In [1]: s = pd.Series([1,2,3, np.nan, np.nan])

In [4]: s.isnull().sum()
Out[4]: 2
</code></pre>

<p>For several columns, it also works:</p>

<pre><code>In [5]: df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})

In [6]: df.isnull().sum()
Out[6]:
a    1
b    2
dtype: int64
</code></pre>
";;4;;2014-10-08T21:06:28.037;;26266451;2014-10-08T21:06:28.037;;;;;653364.0;26266362.0;2;191;;;
18821;18821;;;"<p>Since pandas 0.14.1 my suggestion <a href=""https://github.com/pydata/pandas/issues/5569"">here</a> to have a keyword argument in the value_counts method has been implemented:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})
for col in df:
    print df[col].value_counts(dropna=False)

2     1
 1     1
NaN    1
dtype: int64
NaN    2
 1     1
dtype: int64
</code></pre>
";;0;;2014-10-09T07:14:27.363;;26272425;2014-10-09T07:14:27.363;;;;;680232.0;26266362.0;2;15;;;
18842;18842;;;"<p>Consider <a href=""http://www.ruffus.org.uk/"" rel=""noreferrer"">Ruffus</a> if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. </p>
";;0;;2014-10-09T19:07:16.193;;26286140;2014-10-09T19:07:16.193;;;;;3966500.0;14262433.0;2;7;;;
18865;18865;;;"<p>What you are seeing is pandas truncating the output for display purposes only. </p>

<p>The default <code>max_colwidth</code> value is 50 which is what you are seeing.</p>

<p>You can set this value to whatever you desire or you can set it to -1 which effectively turns this off:</p>

<pre><code>pd.set_option('display.max_colwidth', -1)
</code></pre>

<p>Although I would advise against this, it would be better to set it to something that can be displayed easily in your console or ipython.</p>

<p>A list of the options can be found here: <a href=""http://pandas.pydata.org/pandas-docs/stable/options.html"">http://pandas.pydata.org/pandas-docs/stable/options.html</a></p>
";;1;;2014-10-10T14:38:22.250;;26301947;2014-10-10T14:38:22.250;;;;;704848.0;26277757.0;2;49;;;
18888;18888;;;"<p>Could you do something like this?</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame(columns=['col1', 'col2'])
&gt;&gt;&gt; df = df.append(pd.Series(['a', 'b'], index=['col1','col2']), ignore_index=True)
&gt;&gt;&gt; df = df.append(pd.Series(['d', 'e'], index=['col1','col2']), ignore_index=True) 
&gt;&gt;&gt; df
  col1 col2
0    a    b
1    d    e
</code></pre>

<p>Does anyone have a more elegant solution?</p>
";;1;;2014-10-11T02:01:52.493;;26310294;2014-10-11T02:01:52.493;;;;;2626491.0;26309962.0;2;14;;;
18889;18889;;;"<p>Here's a simple and dumb solution:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame()
&gt;&gt;&gt; df = df.append({'foo':1, 'bar':2}, ignore_index=True)
</code></pre>
";;0;;2014-10-11T04:37:54.687;;26311118;2014-10-11T04:37:54.687;;;;;960370.0;26309962.0;2;22;;;
18910;18910;;;"<p>And if you came here looking for slicing two ranges of columns and combining them together (like me) you can do something like </p>

<pre><code>op = df[list(df.columns[0:899]) + list(df.columns[3593:])]
print op
</code></pre>

<p>This will create a new dataframe with first 900 columns and (all) columns > 3593 (assuming you have some 4000 columns in your data set). </p>
";;0;;2014-10-11T23:21:52.623;;26320276;2014-10-11T23:21:52.623;;;;;2023507.0;10665889.0;2;14;;;
18943;18943;;;"<p>Try this:</p>

<pre><code>df=pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})

columns=[('c','a'),('c','b')]

df.columns=pd.MultiIndex.from_tuples(columns)
</code></pre>
";;0;;2014-10-13T03:26:18.730;;26332512;2014-10-13T03:26:18.730;;;;;3377361.0;21443963.0;2;17;;;
18968;18968;;;"<p>You don't need to wrap it in a list with <code>[..]</code>, just provide the subselection of the columns index:</p>

<pre><code>df.drop(df.columns[[1, 69]], axis=1, inplace=True)
</code></pre>

<p>as the index object is already regarded as list-like.</p>
";;3;;2014-10-13T19:27:43.557;;26347456;2014-10-13T19:27:43.557;;;;;653364.0;26347412.0;2;60;;;
18989;18989;;;"<p>Also have a look into the built-in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html"" rel=""noreferrer""><code>DataFrame.filter</code></a> function.</p>

<p>Minimalistic but greedy approach (sufficient for the given df):
</p>

<pre><code>df.filter(regex=""[^BD]"")
</code></pre>

<p>Conservative/lazy approach (exact matches only):</p>

<pre class=""lang-py prettyprint-override""><code>df.filter(regex=""^(?!(B|D)$).*$"")
</code></pre>

<p>Conservative and generic:</p>

<pre class=""lang-py prettyprint-override""><code>exclude_cols = ['B','C']
df.filter(regex=""^(?!({0})$).*$"".format('|'.join(exclude_cols)))
</code></pre>
";;0;;2014-10-14T09:02:31.400;;26356675;2014-10-14T09:02:31.400;;;;;3054677.0;14940743.0;2;9;;;
19058;19058;;;"<p>I believe the functionality you're looking for is in the hist method of a Series object which wraps the hist() function in matplotlib</p>

<p>Here's the relevant documentation</p>

<pre><code>In [10]: import matplotlib.pyplot as plt

In [11]: plt.hist?
...
Plot a histogram.

Compute and draw the histogram of *x*. The return value is a
tuple (*n*, *bins*, *patches*) or ([*n0*, *n1*, ...], *bins*,
[*patches0*, *patches1*,...]) if the input contains multiple
data.
...
cumulative : boolean, optional, default : True
    If `True`, then a histogram is computed where each bin gives the
    counts in that bin plus all bins for smaller values. The last bin
    gives the total number of datapoints.  If `normed` is also `True`
    then the histogram is normalized such that the last bin equals 1.
    If `cumulative` evaluates to less than 0 (e.g., -1), the direction
    of accumulation is reversed.  In this case, if `normed` is also
    `True`, then the histogram is normalized such that the first bin
    equals 1.

...
</code></pre>

<p>For example</p>

<pre><code>In [12]: import pandas as pd

In [13]: import numpy as np

In [14]: ser = pd.Series(np.random.normal(size=1000))

In [15]: ser.hist(cumulative=True, normed=1, bins=100)
Out[15]: &lt;matplotlib.axes.AxesSubplot at 0x11469a590&gt;

In [16]: plt.show()
</code></pre>
";;5;;2014-10-15T23:57:58.150;;26394108;2014-10-17T00:50:40.997;2014-10-17T00:50:40.997;;3965256.0;;3965256.0;25577352.0;2;30;;;
19096;19096;;;"<p>You can use the package sklearn and its associated preprocessing utilities to normalize the data.</p>

<pre><code>from sklearn import preprocessing

x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pandas.DataFrame(x_scaled)
</code></pre>

<p>For more information look at the scikit-learn <a href=""http://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range"" rel=""noreferrer"">documentation</a> on preprocessing data: scaling features to a range.</p>
";;3;;2014-10-16T23:34:48.710;;26415620;2016-12-08T20:25:53.257;2016-12-08T20:25:53.257;;525169.0;;1292090.0;26414913.0;2;29;;;
19105;19105;;;"<p>Your problem is actually a simple transform acting on the columns:</p>

<pre><code>def f(s):
    return s/s.max()

frame.apply(f, axis=0)
</code></pre>

<p>Or even more terse:</p>

<pre><code>   frame.apply(lambda x: x/x.max(), axis=0)
</code></pre>
";;0;;2014-10-17T09:57:03.580;;26422495;2017-02-22T13:52:02.133;2017-02-22T13:52:02.133;;1695486.0;;1695486.0;26414913.0;2;6;;;
19152;19152;;;"<p>This works properly in the current version of Pandas (version 0.14):</p>

<pre><code>In [132]: df[:5]['duration'] / np.timedelta64(1, 's')
Out[132]: 
0    1232
1    1390
2    1495
3     797
4    1132
Name: duration, dtype: float64
</code></pre>

<p>Here is a workaround for older versions of Pandas/NumPy:</p>

<pre><code>In [131]: df[:5]['duration'].values.view('&lt;i8')/10**9
Out[131]: array([1232, 1390, 1495,  797, 1132], dtype=int64)
</code></pre>

<p>timedelta64 and datetime64 data are stored internally as 8-byte ints (dtype
<code>'&lt;i8'</code>). So the above views the timedelta64s as 8-byte ints and then does integer
division to convert nanoseconds to seconds.</p>

<p>Note that you <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#numpy-datetime64-dtype-and-1-6-dependency"" rel=""nofollow noreferrer"">need NumPy version 1.7 or newer</a> to work with datetime64/timedelta64s.    </p>
";;0;;2014-10-20T01:13:49.817;;26457238;2017-07-19T11:22:57.317;2017-07-19T11:22:57.317;;190597.0;;190597.0;26456825.0;2;24;;;
19163;19163;;;"<p>See this enhancement <a href=""https://github.com/pydata/pandas/issues/8588"" rel=""noreferrer"">issue</a></p>

<p>The short answer is yes, the groupby will preserve the orderings as passed in. You can prove this by using your example like this:</p>

<pre><code>In [20]: df.sort_index(ascending=False).groupby('A').agg([np.mean, lambda x: x.iloc[1] ])
Out[20]: 
           B             C         
        mean &lt;lambda&gt; mean &lt;lambda&gt;
A                                  
group1  11.0       10  101      100
group2  17.5       10  175      100
group3  11.0       10  101      100
</code></pre>

<p>This is NOT true for resample however as it requires a monotonic index (it WILL work with a non-monotonic index, but will sort it first).</p>

<p>Their is a <code>sort=</code> flag to groupby, but this relates to the sorting of the groups themselves and not the observations within a group.</p>

<p>FYI: <code>df.groupby('A').nth(1)</code> is a safe way to get the 2nd value of a group (as your method above will fail if a group has &lt; 2 elements)</p>
";;3;;2014-10-20T12:19:04.443;;26465555;2014-10-20T12:19:04.443;;;;;644898.0;26456125.0;2;12;;;
19182;19182;;;"<p>I had this exact problem recently and used</p>

<pre><code>apt-get install python-numpy
</code></pre>

<p>This adds numpy to your system python interpreter.  I may have had to do the same for matplotlib.  To use in a virtualenv, you have to create your environment using the</p>

<pre><code>--system-site-packages
</code></pre>

<p>option</p>

<p><a href=""http://www.scipy.org/install.html"">http://www.scipy.org/install.html</a></p>
";;6;;2014-10-20T20:11:22.417;;26474062;2014-10-20T20:11:22.417;;;;;3704260.0;26473681.0;2;42;;;
19236;19236;;;"<p>There is no real difference (both are based on the same underlying function), but as noted in the comments, they have some <em>different default values</em> (<code>index_col</code> is 0 or None, <code>parse_dates</code> is True or False for <code>read_csv</code> and <code>DataFrame.from_csv</code> respectively) and <em><code>read_csv</code> supports more arguments</em> (in <code>from_csv</code> they are just not passed through).</p>

<p>Apart from that, it is <strong>recommended to use <code>pd.read_csv</code></strong>.<br>
<code>DataFrame.from_csv</code> exists merely for historical reasons and to keep backwards compatibility (plans are to deprecate it, see <a href=""https://github.com/pydata/pandas/issues/4916"">here</a>), but all new features are only added to <code>read_csv</code> (as you can see in the much longer list of keyword arguments). Actually, this should be made more clear in the docs.</p>
";;1;;2014-10-21T20:35:43.050;;26495839;2014-10-21T20:43:18.843;2014-10-21T20:43:18.843;;653364.0;;653364.0;26495408.0;2;17;;;
19276;19276;;;"<p><code>df3.set_value(1, 'B', abc)</code> works for any dataframe. Take care of the data type of column 'B'. Eg. a list can not be inserted into a float column, at that case <code>df['B'] = df['B'].astype(object)</code> can help.</p>
";;0;;2014-10-22T14:44:05.657;;26510251;2014-10-22T14:44:05.657;;;;;2740380.0;26483254.0;2;23;;;
19304;19304;;;"<p>Try <a href=""https://github.com/pydata/pandas/blob/master/pandas/io/excel.py#L130"">this</a>:</p>

<pre><code>xls = pd.ExcelFile('path_to_file.xls')
df1 = xls.parse('Sheet1')
df2 = xls.parse('Sheet2')
</code></pre>

<p>Haven't looked closely to see how much is done in the first line and how much is done in subsequent lines, but it's at least a place to start playing around.</p>
";;1;;2014-10-23T05:16:38.650;;26521726;2014-10-23T05:16:38.650;;;;;28035.0;26521266.0;2;22;;;
19318;19318;;;"<p>In general, you could use <code>@local_variable_name</code>, so something like</p>

<pre><code>&gt;&gt;&gt; pi = np.pi; nan = np.nan
&gt;&gt;&gt; df = pd.DataFrame({""value"": [3,4,9,10,11,np.nan,12]})
&gt;&gt;&gt; df.query(""(value &lt; 10) and (value &gt; @pi)"")
   value
1      4
2      9
</code></pre>

<p>would work, but <code>nan</code> isn't equal to itself, so <code>value == NaN</code> will always be false.  One way to hack around this is to use that fact, and use <code>value != value</code> as an <code>isnan</code> check.  We have</p>

<pre><code>&gt;&gt;&gt; df.query(""(value &lt; 10) or (value == @nan)"")
   value
0      3
1      4
2      9
</code></pre>

<p>but</p>

<pre><code>&gt;&gt;&gt; df.query(""(value &lt; 10) or (value != value)"")
   value
0      3
1      4
2      9
5    NaN
</code></pre>
";;5;;2014-10-23T19:28:05.397;;26535881;2014-10-23T19:28:05.397;;;;;487339.0;26535563.0;2;34;;;
19331;19331;;;"<p>Try the following:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.read_csv(""test.csv"")

In [3]: df
Out[3]: 
  id  value1  value2  value3
0  A       1       2       3
1  B       4       5       6
2  C       7       8       9

In [4]: df[""sum""] = df.sum(axis=1)

In [5]: df
Out[5]: 
  id  value1  value2  value3  sum
0  A       1       2       3    6
1  B       4       5       6   15
2  C       7       8       9   24

In [6]: df_new = df.loc[:,""value1"":""value3""].div(df[""sum""], axis=0)

In [7]: df_new
Out[7]: 
     value1    value2  value3
0  0.166667  0.333333   0.500
1  0.266667  0.333333   0.400
2  0.291667  0.333333   0.375
</code></pre>

<p>Or you can do the following:</p>

<pre><code>In [8]: df.loc[:,""value1"":""value3""] = df.loc[:,""value1"":""value3""].div(df[""sum""], axis=0)

In [9]: df
Out[9]: 
  id    value1    value2  value3  sum
0  A  0.166667  0.333333   0.500    6
1  B  0.266667  0.333333   0.400   15
2  C  0.291667  0.333333   0.375   24
</code></pre>

<p>Or just straight up from the beginning:</p>

<pre><code>In [10]: df = pd.read_csv(""test.csv"")

In [11]: df
Out[11]: 
  id  value1  value2  value3
0  A       1       2       3
1  B       4       5       6
2  C       7       8       9

In [12]: df.loc[:,""value1"":""value3""] = df.loc[:,""value1"":""value3""].div(df.sum(axis=1), axis=0)

In [13]: df
Out[13]: 
  id    value1    value2  value3
0  A  0.166667  0.333333   0.500
1  B  0.266667  0.333333   0.400
2  C  0.291667  0.333333   0.375
</code></pre>

<p>Changing the column <code>value1</code> and the like to your headers should work similarly.</p>
";;0;;2014-10-23T22:13:03.580;;26538379;2014-10-23T22:13:03.580;;;;;2548721.0;26537878.0;2;18;;;
19346;19346;;;"<p>Had the same problem a while ago. Another solution, which I believe much more efficient, is to compute the distance only for the upper triangular matrix, and later copy to the lower part. </p>

<p>It can be done with scipy as follows:</p>

<pre><code>from scipy.spatial.distance import squareform,pdist                                                              
similarities = squareform(pdist(data,'speuclidean'))
</code></pre>
";;0;;2014-10-24T08:40:49.500;;26544349;2014-10-24T08:40:49.500;;;;;1013125.0;16990996.0;2;6;;;
19390;19390;;;"<p>One option is just to use the regex <code>|</code> character to try to match each of the substrings in the words in your Series <code>s</code> (still using <code>str.contains</code>). </p>

<p>You can construct the regex by joining the words in <code>searchfor</code> with <code>|</code>:</p>

<pre><code>&gt;&gt;&gt; searchfor = ['og', 'at']
&gt;&gt;&gt; s[s.str.contains('|'.join(searchfor))]
0    cat
1    hat
2    dog
3    fog
dtype: object
</code></pre>

<p>As @AndyHayden noted in the comments below, take care if your substrings have special characters such as <code>$</code> and <code>^</code> which you want to match literally. These characters have specific meanings in the context of regular expressions and will affect the matching.</p>

<p>You can make your list of substrings safer by escaping non-alphanumeric characters with <code>re.escape</code>:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; matches = ['$money', 'x^y']
&gt;&gt;&gt; safe_matches = [re.escape(m) for m in matches]
&gt;&gt;&gt; safe_matches
['\\$money', 'x\\^y']
</code></pre>

<p>The strings with in this new list will match each character literally when  used with <code>str.contains</code>.</p>
";;3;;2014-10-26T20:40:33.717;;26577689;2017-02-28T20:42:50.397;2017-02-28T20:42:50.397;;3923281.0;;3923281.0;26577516.0;2;27;;;
19436;19436;;;"<p>It might be an issue with </p>

<ul>
<li>the delimiters in your data</li>
<li>the first row, as @TomAugspurger assiduously noted</li>
</ul>

<p>To solve it, try specifying the <code>sep</code> and/or <code>header</code> arguments when calling <code>read_csv</code>. For instance, </p>

<pre><code>df = pandas.read_csv(fileName, sep='delimiter', header=None)
</code></pre>

<p>In the code above, <code>sep</code> defines your delimiter and <code>header=None</code> tells pandas that your source data has no row for headers / column titles. Thus saith <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">the docs</a>: ""If file contains no header row, then you should explicitly pass header=None"". In this instance, pandas automatically creates whole-number indeces for each field {0,1,2,...}. </p>

<p>According to the docs, the delimiter thing should <em>not</em> be an issue. The docs say that ""if sep is None [not specified], will try to automatically determine this."" I however have not had good luck with this, including instances with obvious delimiters. But, defining <code>sep</code> is not arduous. </p>
";;1;;2014-10-28T02:18:23.647;;26599892;2014-10-28T02:32:16.290;2014-10-28T02:32:16.290;;781938.0;;781938.0;18039057.0;2;24;;;
19517;19517;;;"<pre><code>df.index
</code></pre>

<p>will output the row names as pandas Index object.  For a pure list,</p>

<pre><code>list(df.index)
</code></pre>

<p>Lastly, the index supports label slicing similar to columns, </p>

<pre><code>df.index['Row 2':'Row 5'] 
</code></pre>
";;0;;2014-10-29T20:36:33.510;;26640189;2017-01-19T15:35:49.313;2017-01-19T15:35:49.313;;4076764.0;;4076764.0;26640145.0;2;37;;;
19535;19535;;;"<p>There are different ways to do that. </p>

<ul>
<li>I created the data frame to showcase the different techniques to filter your data. </li>
</ul>

<blockquote>
<pre><code>df = pd.DataFrame({'Date':['01-Jun-13','03-Jun-13', '15-Aug-13', '20-Jan-14', '21-Feb-14'],
</code></pre>
  
  <p>'abc':[100,-20,40,25,60],'xyz':[200,50,-5,15,80] })</p>
</blockquote>

<ul>
<li>I separated months/year/day and seperated month-year as you explained.</li>
</ul>

<blockquote>
<pre><code>def getMonth(s):
  return s.split(""-"")[1]

def getDay(s):
  return s.split(""-"")[0]

def getYear(s):
  return s.split(""-"")[2]

def getYearMonth(s):
  return s.split(""-"")[1]+""-""+s.split(""-"")[2]
</code></pre>
</blockquote>

<ul>
<li>I created new columns: <code>year</code>, <code>month</code>, <code>day</code> and '<code>yearMonth</code>'. In your case, you need one of both. You can group using two columns <code>'year','month'</code> or using one column <code>yearMonth</code></li>
</ul>

<blockquote>
<pre><code>df['year']= df['Date'].apply(lambda x: getYear(x))
df['month']= df['Date'].apply(lambda x: getMonth(x))
df['day']= df['Date'].apply(lambda x: getDay(x))
df['YearMonth']= df['Date'].apply(lambda x: getYearMonth(x))
</code></pre>
</blockquote>

<p>Output: </p>

<pre><code>        Date  abc  xyz year month day YearMonth
0  01-Jun-13  100  200   13   Jun  01    Jun-13
1  03-Jun-13  -20   50   13   Jun  03    Jun-13
2  15-Aug-13   40   -5   13   Aug  15    Aug-13
3  20-Jan-14   25   15   14   Jan  20    Jan-14
4  21-Feb-14   60   80   14   Feb  21    Feb-14
</code></pre>

<ul>
<li>You can go through the different groups in groupby(..) items. </li>
</ul>

<p>In this case, we are grouping by two columns: </p>

<blockquote>
<pre><code>for key,g in df.groupby(['year','month']):
    print key,g
</code></pre>
</blockquote>

<p>Output: </p>

<pre><code>('13', 'Jun')         Date  abc  xyz year month day YearMonth
0  01-Jun-13  100  200   13   Jun  01    Jun-13
1  03-Jun-13  -20   50   13   Jun  03    Jun-13
('13', 'Aug')         Date  abc  xyz year month day YearMonth
2  15-Aug-13   40   -5   13   Aug  15    Aug-13
('14', 'Jan')         Date  abc  xyz year month day YearMonth
3  20-Jan-14   25   15   14   Jan  20    Jan-14
('14', 'Feb')         Date  abc  xyz year month day YearMonth
</code></pre>

<p>In this case, we are grouping by one column: </p>

<blockquote>
<pre><code>for key,g in df.groupby(['YearMonth']):
    print key,g
</code></pre>
</blockquote>

<p>Output: </p>

<pre><code>Jun-13         Date  abc  xyz year month day YearMonth
0  01-Jun-13  100  200   13   Jun  01    Jun-13
1  03-Jun-13  -20   50   13   Jun  03    Jun-13
Aug-13         Date  abc  xyz year month day YearMonth
2  15-Aug-13   40   -5   13   Aug  15    Aug-13
Jan-14         Date  abc  xyz year month day YearMonth
3  20-Jan-14   25   15   14   Jan  20    Jan-14
Feb-14         Date  abc  xyz year month day YearMonth
4  21-Feb-14   60   80   14   Feb  21    Feb-14
</code></pre>

<ul>
<li>In case you wanna access to specific item, you can use <code>get_group</code></li>
</ul>

<blockquote>
  <p>print df.groupby(['YearMonth']).get_group('Jun-13')</p>
</blockquote>

<p>Output: </p>

<pre><code>        Date  abc  xyz year month day YearMonth
0  01-Jun-13  100  200   13   Jun  01    Jun-13
1  03-Jun-13  -20   50   13   Jun  03    Jun-13
</code></pre>

<ul>
<li>Similar to <code>get_group</code>. This hack would help to filter values and get the grouped values.</li>
</ul>

<p>This also would give the same result.</p>

<pre><code>print df[df['YearMonth']=='Jun-13'] 
</code></pre>

<p>Output:</p>

<pre><code>        Date  abc  xyz year month day YearMonth
0  01-Jun-13  100  200   13   Jun  01    Jun-13
1  03-Jun-13  -20   50   13   Jun  03    Jun-13
</code></pre>

<p>You can select list of <code>abc</code> or <code>xyz</code> values during <code>Jun-13</code> </p>

<pre><code>print df[df['YearMonth']=='Jun-13'].abc.values
print df[df['YearMonth']=='Jun-13'].xyz.values
</code></pre>

<p>Output: </p>

<pre><code>[100 -20]  #abc values
[200  50]  #xyz values
</code></pre>

<p>You can use this to go through the dates that you have classified as ""year-month"" and apply cretiria on it to get related data. </p>

<pre><code>for x in set(df.YearMonth): 
    print df[df['YearMonth']==x].abc.values
    print df[df['YearMonth']==x].xyz.values
</code></pre>

<p>I recommend also to check this <a href=""https://stackoverflow.com/questions/26558576/conditional-probability-density-estimation-in-python/26559206#26559206"">answer</a> as well.</p>
";;1;;2014-10-30T07:22:47.060;;26647211;2014-10-30T07:22:47.060;2017-05-23T11:54:50.260;;-1.0;user4179775;;26646191.0;2;7;;;
19537;19537;;;"<p>You can use either resample or TimeGrouper (which resample uses under the hood).</p>

<p>First make the datetime column is actually of datetimes (hit it with <code>pd.to_datetime</code>). It's easier if it'd a DatetimeIndex:</p>

<pre><code>In [11]: df1
Out[11]:
            abc  xyz
Date
2013-06-01  100  200
2013-06-03  -20   50
2013-08-15   40   -5
2014-01-20   25   15
2014-02-21   60   80

In [12]: g = df1.groupby(pd.TimeGrouper(""M"")) # DataFrameGroupBy (grouped by Month)

In [13]: g.sum()
Out[13]:
            abc  xyz
Date
2013-06-30   80  250
2013-07-31  NaN  NaN
2013-08-31   40   -5
2013-09-30  NaN  NaN
2013-10-31  NaN  NaN
2013-11-30  NaN  NaN
2013-12-31  NaN  NaN
2014-01-31   25   15
2014-02-28   60   80

In [14]: df1.resample(""M"", how='sum') # the same
Out[14]:
            abc  xyz
Date
2013-06-30   40  125
2013-07-31  NaN  NaN
2013-08-31   40   -5
2013-09-30  NaN  NaN
2013-10-31  NaN  NaN
2013-11-30  NaN  NaN
2013-12-31  NaN  NaN
2014-01-31   25   15
2014-02-28   60   80
</code></pre>

<hr>

<p>I had thought the following would work, but it doesn't (due to <code>as_index</code> not being respected? I'm not sure.), I'm including this for interests' sake.</p>

<p>If it's a column (it has to be a datetime64 column! as I say, hit it with <code>to_datetime</code>), you can use the PeriodIndex:</p>

<pre><code>In [21]: df
Out[21]:
        Date  abc  xyz
0 2013-06-01  100  200
1 2013-06-03  -20   50
2 2013-08-15   40   -5
3 2014-01-20   25   15
4 2014-02-21   60   80

In [22]: pd.DatetimeIndex(df.Date).to_period(""M"")  # old way
Out[22]:
&lt;class 'pandas.tseries.period.PeriodIndex'&gt;
[2013-06, ..., 2014-02]
Length: 5, Freq: M

In [23]: per = df.Date.dt.to_period(""M"")  # new way to get the same

In [24]: g = df.groupby(per)

In [25]: g.sum()  # dang not quite what we want (doesn't fill in the gaps)
Out[25]:
         abc  xyz
2013-06   80  250
2013-08   40   -5
2014-01   25   15
2014-02   60   80
</code></pre>

<p>To get the desired result we have to reindex...</p>
";;1;;2014-10-30T09:24:40.830;;26649199;2014-10-30T20:05:16.217;2014-10-30T20:05:16.217;;1240268.0;;1240268.0;26646191.0;2;31;;;
19545;19545;;;"<p>Your error on the snippet of data you posted is a little cryptic, in that because there are no common values, the join operation fails because the values don't overlap it requires you to supply a suffix for the left and right hand side:</p>

<pre><code>In [173]:

df_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')
Out[173]:
       mukey_left  DI  PI  mukey_right  niccdcd
index                                          
0          100000  35  14          NaN      NaN
1         1000005  44  14          NaN      NaN
2         1000006  44  14          NaN      NaN
3         1000007  43  13          NaN      NaN
4         1000008  43  13          NaN      NaN
</code></pre>

<p><code>merge</code> works because it doesn't have this restriction:</p>

<pre><code>In [176]:

df_a.merge(df_b, on='mukey', how='left')
Out[176]:
     mukey  DI  PI  niccdcd
0   100000  35  14      NaN
1  1000005  44  14      NaN
2  1000006  44  14      NaN
3  1000007  43  13      NaN
4  1000008  43  13      NaN
</code></pre>
";;4;;2014-10-30T13:24:22.470;;26654201;2014-10-30T13:24:22.470;;;;;704848.0;26645515.0;2;42;;;
19555;19555;;;"<p>To access the index in this case you access the <code>name</code> attribute:</p>

<pre><code>In [182]:

df = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
def rowFunc(row):
    return row['a'] + row['b'] * row['c']

def rowIndex(row):
    return row.name
df['d'] = df.apply(rowFunc, axis=1)
df['rowIndex'] = df.apply(rowIndex, axis=1)
df
Out[182]:
   a  b  c   d  rowIndex
0  1  2  3   7         0
1  4  5  6  34         1
</code></pre>

<p>Note that if this is really what you are trying to do that the following works and is much faster:</p>

<pre><code>In [198]:

df['d'] = df['a'] + df['b'] * df['c']
df
Out[198]:
   a  b  c   d
0  1  2  3   7
1  4  5  6  34

In [199]:

%timeit df['a'] + df['b'] * df['c']
%timeit df.apply(rowIndex, axis=1)
10000 loops, best of 3: 163 s per loop
1000 loops, best of 3: 286 s per loop
</code></pre>
";;2;;2014-10-30T16:25:18.800;;26658301;2014-10-30T16:33:39.140;2014-10-30T16:33:39.140;;704848.0;;704848.0;26658240.0;2;28;;;
19589;19589;;;"<p>IIUC, if you make your (unfortunately named) <code>List</code> into an <code>ndarray</code>, you can simply index into it naturally.</p>

<pre><code>&gt;&gt;&gt; m = np.arange(16)*10
&gt;&gt;&gt; m[df.A]
array([  0,  40,  50,  60, 150, 150, 140, 130])
&gt;&gt;&gt; df[""D""] = m[df.A]
&gt;&gt;&gt; df
    A   B   C    D
0   0 NaN NaN    0
1   4 NaN NaN   40
2   5 NaN NaN   50
3   6 NaN NaN   60
4  15 NaN NaN  150
5  15 NaN NaN  150
6  14 NaN NaN  140
7  13 NaN NaN  130
</code></pre>

<p>Here I built a new <code>m</code>, but if you use <code>m = np.asarray(List)</code>, the same thing should work: the values in <code>df.A</code> will pick out the appropriate elements of <code>m</code>.</p>

<hr>

<p>Note that if you're using an old version of <code>numpy</code>, you might have to use <code>m[df.A.values]</code> instead-- in the past, <code>numpy</code> didn't play well with others, and some refactoring in <code>pandas</code> caused some headaches.  Things have improved now.</p>
";;3;;2014-10-31T03:18:57.927;;26667043;2014-10-31T03:56:52.097;2014-10-31T03:56:52.097;;487339.0;;487339.0;26666919.0;2;11;;;
19608;19608;;;"<p>If you have pdflatex and imagemagick installed, you could export the DataFrame to tex, use pdflatex to convert it to a pdf file, and then convert the pdf to png using imagemagick:</p>

<pre><code>import pandas as pd
import numpy as np
import subprocess

df = pd.DataFrame({'d': [1., 1., 1., 2., 2., 2.],
                   'c': np.tile(['a', 'b', 'c'], 2),
                   'v': np.arange(1., 7.)})
filename = 'out.tex'
pdffile = 'out.pdf'
outname = 'out.png'

template = r'''\documentclass[preview]{{standalone}}
\usepackage{{booktabs}}
\begin{{document}}
{}
\end{{document}}
'''

with open(filename, 'wb') as f:
    f.write(template.format(df.to_latex()))

subprocess.call(['pdflatex', filename])
subprocess.call(['convert', '-density', '300', pdffile, '-quality', '90', outname])
</code></pre>

<p><img src=""https://i.stack.imgur.com/nd93b.png"" alt=""enter image description here""></p>

<p>If you install <a href=""http://phantomjs.org/"" rel=""noreferrer"">phantomjs</a> and imagemagick, you could
export the DataFrame to HTML and then use phantomjs to convert the HTML to png, and imagemagick to crop the result:</p>

<pre><code>import pandas as pd
import numpy as np
import subprocess

df = pd.DataFrame({'d': [1., 1., 1., 2., 2., 2.],
                   'c': np.tile(['a', 'b', 'c'], 2),
                   'v': np.arange(1., 7.)})
filename = '/tmp/out.html'
outname = '/tmp/out.png'
cropname = '/tmp/cropped.png'

with open(filename, 'wb') as f:
    f.write(df.to_html())
rasterize = '/path/to/phantomjs/examples/rasterize.js'
subprocess.call(['phantomjs', rasterize, filename, outname])
subprocess.call(['convert', outname, '-trim', cropname])
</code></pre>

<p><img src=""https://i.stack.imgur.com/Vuw3w.png"" alt=""enter image description here""></p>
";;2;;2014-10-31T19:04:19.427;;26681726;2014-10-31T19:57:40.283;2014-10-31T19:57:40.283;;190597.0;;190597.0;26678467.0;2;9;;;
19657;19657;;;"<p>Try to use <code>Zip</code></p>

<pre><code>df = pd.read_csv(""file"")
d= dict([(i,[a,b,c ]) for i, a,b,c in zip(df.ID, df.A,df.B,df.C)])
print d
</code></pre>

<p>Output: </p>

<pre><code>{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}
</code></pre>
";;0;;2014-11-03T14:55:08.823;;26716759;2014-11-03T14:55:08.823;;;;user4179775;;26716616.0;2;8;;;
19658;19658;;;"<p>The <code>to_dict()</code> method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.</p>

<p><code>to_dict()</code> also accepts an <code>outtype</code> keyword argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form <code>{index:value}</code> will be returned for each column.</p>

<p>These steps can be done with the following line:</p>

<pre><code>&gt;&gt;&gt; df.set_index('ID').T.to_dict('list')
{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}
</code></pre>
";;3;;2014-11-03T14:55:51.767;;26716774;2014-11-05T11:31:05.897;2014-11-05T11:31:05.897;;3923281.0;;3923281.0;26716616.0;2;56;;;
19671;19671;;;"<p>Just for reference as it was something I was looking for, you can test for presence within the values or the index by appending the "".values"" method, e.g.</p>

<pre><code>g in df.&lt;your selected field&gt;.values
g in df.index.values
</code></pre>

<p>I find that adding the "".values"" to get a simple list or ndarray out makes exist or ""in"" checks run more smoothly with the other python tools. Just thought I'd toss that out there for people.</p>
";;4;;2014-11-03T19:41:25.010;;26721808;2017-04-08T17:53:02.183;2017-04-08T17:53:02.183;;2285236.0;;1373804.0;23549231.0;2;15;;;
19677;19677;;;"<p>The issue here is that: <code>df.col1[df.col1 == 10]</code> returns a copy.</p>

<p>So I would say:</p>

<pre><code>row_index = df.col1 == 10
# then with the form .loc[row_indexer,col_indexer]
df.loc[row_index, 'col1'] = 100
</code></pre>
";;4;;2014-11-03T22:37:48.817;;26724581;2014-11-04T03:19:01.460;2014-11-04T03:19:01.460;;1552748.0;;1552748.0;26724378.0;2;33;;;
19679;19679;;;"<p><code>isin()</code> is ideal if you have a list of exact matches, but if you have a list of partial matches or substrings to look for, you can filter using the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.strings.StringMethods.contains.html""><code>str.contains</code></a> method and regular expressions.</p>

<p>For example, if we want to return a DataFrame where all of the stock IDs which begin with <code>'600'</code> and then are followed by any three digits:</p>

<pre><code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string
...   STK_ID   ...                                    # [0-9]{3} means any three digits
...  '600809'  ...                                    # $ means end of string
...  '600141'  ...
...  '600329'  ...
...      ...   ...
</code></pre>

<p>Suppose now we have a list of strings which we want the values in <code>'STK_ID'</code> to end with, e.g.</p>

<pre><code>endstrings = ['01$', '02$', '05$']
</code></pre>

<p>We can join these strings with the regex 'or' character <code>|</code> and pass the string to <code>str.contains</code> to filter the DataFrame: </p>

<pre><code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]
...   STK_ID   ...
...  '155905'  ...
...  '633101'  ...
...  '210302'  ...
...      ...   ...
</code></pre>

<p>Finally, <code>contains</code> can ignore case (by setting <code>case=False</code>), allowing you to be more general when specifying the strings you want to match.</p>

<p>For example,</p>

<pre><code>str.contains('pandas', case=False)
</code></pre>

<p>would match <code>PANDAS</code>, <code>PanDAs</code>, <code>paNdAs123</code>, and so on.</p>
";;0;;2014-11-03T22:49:27.230;;26724725;2015-04-12T18:29:27.530;2015-04-12T18:29:27.530;;3923281.0;;3923281.0;12065885.0;2;71;;;
19742;19742;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/dev/timeseries.html#converting-to-timestamps""><code>to_datetime</code></a> function, specifying a <a href=""http://strftime.org/"">format</a> to match your data.</p>

<pre><code>raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')
</code></pre>
";;1;;2014-11-05T17:50:27.787;;26763793;2014-11-05T17:50:27.787;;;;;3657742.0;26763344.0;2;93;;;
19743;19743;;;"<p>You can use the DataFrame method <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html"" rel=""noreferrer""><code>.apply()</code></a> to operate on the values in Mycol:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol'])
&gt;&gt;&gt; df
                    Mycol
0  05SEP2014:00:00:00.000
&gt;&gt;&gt; import datetime as dt
&gt;&gt;&gt; df['Mycol'] = df['Mycol'].apply(lambda x: 
                                    dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f'))
&gt;&gt;&gt; df
       Mycol
0 2014-09-05
</code></pre>
";;2;;2014-11-05T17:51:24.417;;26763810;2014-11-05T17:51:24.417;;;;;42346.0;26763344.0;2;19;;;
19785;19785;;;"<p>You could use <code>np.repeat</code> to get the repeated indices and then use that to index into the frame:</p>

<pre><code>&gt;&gt;&gt; df2 = df.loc[np.repeat(df.index.values,df.n)]
&gt;&gt;&gt; df2
  id  n   v
0  A  1  10
1  B  2  13
1  B  2  13
2  C  3   8
2  C  3   8
2  C  3   8
</code></pre>

<p>After which there's only a bit of cleaning up to do:</p>

<pre><code>&gt;&gt;&gt; df2 = df2.drop(""n"",axis=1).reset_index(drop=True)
&gt;&gt;&gt; df2
  id   v
0  A  10
1  B  13
2  B  13
3  C   8
4  C   8
5  C   8
</code></pre>
";;0;;2014-11-06T11:42:27.623;;26778637;2014-11-06T11:42:27.623;;;;;487339.0;26777832.0;2;17;;;
19811;19811;;;"<p>What you are seeing is the index column. Just set <code>index=False</code>:</p>

<pre><code>df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)
</code></pre>
";;3;;2014-11-06T18:43:00.030;;26787032;2016-11-08T16:02:31.487;2016-11-08T16:02:31.487;;2831353.0;;37751.0;26786960.0;2;42;;;
19853;19853;;;"<p>So I used to use a for loop for iterating through the dictionary as well, but one thing I've found that works much faster is to convert to a panel and then to a dataframe. 
Say you have a dictionary d</p>

<pre><code>import pandas as pd
d
{'RAY Index': {datetime.date(2014, 11, 3): {'PX_LAST': 1199.46,
'PX_OPEN': 1200.14},
datetime.date(2014, 11, 4): {'PX_LAST': 1195.323, 'PX_OPEN': 1197.69},
datetime.date(2014, 11, 5): {'PX_LAST': 1200.936, 'PX_OPEN': 1195.32},
datetime.date(2014, 11, 6): {'PX_LAST': 1206.061, 'PX_OPEN': 1200.62}},
'SPX Index': {datetime.date(2014, 11, 3): {'PX_LAST': 2017.81,
'PX_OPEN': 2018.21},
datetime.date(2014, 11, 4): {'PX_LAST': 2012.1, 'PX_OPEN': 2015.81},
datetime.date(2014, 11, 5): {'PX_LAST': 2023.57, 'PX_OPEN': 2015.29},
datetime.date(2014, 11, 6): {'PX_LAST': 2031.21, 'PX_OPEN': 2023.33}}}
</code></pre>

<p>The command</p>

<pre><code>pd.Panel(d)
&lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: 2 (items) x 2 (major_axis) x 4 (minor_axis)
Items axis: RAY Index to SPX Index
Major_axis axis: PX_LAST to PX_OPEN
Minor_axis axis: 2014-11-03 to 2014-11-06
</code></pre>

<p>where pd.Panel(d)[item] yields a dataframe</p>

<pre><code>pd.Panel(d)['SPX Index']
2014-11-03  2014-11-04  2014-11-05 2014-11-06
PX_LAST 2017.81 2012.10 2023.57 2031.21
PX_OPEN 2018.21 2015.81 2015.29 2023.33
</code></pre>

<p>You can then hit the command to_frame() to turn it into a dataframe. I use reset_index as well to turn the major and minor axis into columns rather than have them as indices.</p>

<pre><code>pd.Panel(d).to_frame().reset_index()
major   minor      RAY Index    SPX Index
PX_LAST 2014-11-03  1199.460    2017.81
PX_LAST 2014-11-04  1195.323    2012.10
PX_LAST 2014-11-05  1200.936    2023.57
PX_LAST 2014-11-06  1206.061    2031.21
PX_OPEN 2014-11-03  1200.140    2018.21
PX_OPEN 2014-11-04  1197.690    2015.81
PX_OPEN 2014-11-05  1195.320    2015.29
PX_OPEN 2014-11-06  1200.620    2023.33
</code></pre>

<p>Finally, if you don't like the way the frame looks you can use the transpose function of panel to change the appearance before calling to_frame() see documentation here 
<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Panel.transpose.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Panel.transpose.html</a></p>

<p>Just as an example</p>

<pre><code>pd.Panel(d).transpose(2,0,1).to_frame().reset_index()
major        minor  2014-11-03  2014-11-04  2014-11-05  2014-11-06
RAY Index   PX_LAST 1199.46    1195.323     1200.936    1206.061
RAY Index   PX_OPEN 1200.14    1197.690     1195.320    1200.620
SPX Index   PX_LAST 2017.81    2012.100     2023.570    2031.210
SPX Index   PX_OPEN 2018.21    2015.810     2015.290    2023.330
</code></pre>

<p>Hope this helps.</p>
";;2;;2014-11-07T14:44:33.207;;26803731;2014-11-07T14:44:33.207;;;;;1796649.0;13575090.0;2;7;;;
19876;19876;;;"<p>Since pandas 0.13 you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_values.html""><code>get_values</code></a>:</p>

<pre><code>df.index.get_values()
</code></pre>
";;4;;2014-11-08T11:42:05.453;;26816746;2014-12-12T03:12:17.073;2014-12-12T03:12:17.073;;1240268.0;;2006977.0;17241004.0;2;12;;;
19919;19919;;;"<pre><code>import numpy as np
df1 = df.replace(np.nan, '', regex=True)
</code></pre>

<p>This might help. It will replace all NaNs with an empty string.</p>
";;2;;2014-11-10T06:40:47.507;;26838140;2017-03-08T14:05:51.923;2017-03-08T14:05:51.923;;604687.0;;3728956.0;26837998.0;2;32;;;
19937;19937;;;"<p>I am using pandas 0.14.1 on macos in ipython notebook.  I tried the proposed line above:</p>

<pre><code>df[df['A'].str.contains(""Hello|Britain"")]
</code></pre>

<p>and got an error: </p>

<pre><code>""cannot index with vector containing NA / NaN values""
</code></pre>

<p>but it worked perfectly when an ""==True"" condition was added, like this:</p>

<pre><code>df[df['A'].str.contains(""Hello|Britain"")==True]
</code></pre>
";;1;;2014-11-10T17:05:17.600;;26849064;2014-11-10T17:05:17.600;;;;;2200216.0;11350770.0;2;64;;;
19944;19944;;;"<p>Say you have the following <code>DataFrame</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])
&gt;&gt;&gt; df
       a            b
0  hello  hello world
1   abcd         defg
</code></pre>

<p>You can always use the <code>in</code> operator in a lambda expression to create your filter.</p>

<pre><code>&gt;&gt;&gt; df.apply(lambda x: x['a'] in x['b'], axis=1)
0     True
1    False
dtype: bool
</code></pre>

<p>The trick here is to use the <code>axis=1</code> option in the <code>apply</code> to pass elements to the lambda function row by row, as opposed to column by column.</p>
";;1;;2014-11-10T19:26:27.313;;26851412;2014-11-10T19:26:27.313;;;;;2055368.0;11350770.0;2;15;;;
19978;19978;;;"<p>You can find the dateutil package at <a href=""https://pypi.python.org/pypi/python-dateutil"" rel=""noreferrer"">https://pypi.python.org/pypi/python-dateutil</a>. Extract it to somewhere and run the command:</p>

<p><code>python setup.py install</code></p>

<p>It worked for me!</p>
";;0;;2014-11-11T12:59:19.773;;26865524;2014-11-11T12:59:19.773;;;;;1634092.0;20853474.0;2;10;;;
20008;20008;;;"<p>The problem is with using the default pandas formatting (or whatever formatting you chose). Not sure how things work behind the scenes, but these parameters are trumping the formatting that you pass as in the plot function. You can see a list of them <a href=""https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py"" rel=""noreferrer"">here in the mpl_style dictionary</a></p>

<p>In order to get around it, you can do this:</p>

<pre><code>import pandas as pd
pd.options.display.mpl_style = 'default'
new_style = {'grid': False}
matplotlib.rc('axes', **new_style)
data = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])
data.plot(secondary_y=['B'])
</code></pre>

<p><img src=""https://i.stack.imgur.com/I08uH.png"" alt=""enter image description here""></p>
";;1;;2014-11-11T19:42:20.860;;26873148;2014-11-11T19:42:20.860;;;;;3325052.0;26868304.0;2;8;;;
20059;20059;;;"<p>OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:</p>

<pre><code>def label_race (row):
   if row['eri_hispanic'] == 1 :
      return 'Hispanic'
   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] &gt; 1 :
      return 'Two Or More'
   if row['eri_nat_amer'] == 1 :
      return 'A/I AK Native'
   if row['eri_asian'] == 1:
      return 'Asian'
   if row['eri_afr_amer']  == 1:
      return 'Black/AA'
   if row['eri_hawaiian'] == 1:
      return 'Haw/Pac Isl.'
   if row['eri_white'] == 1:
      return 'White'
   return 'Other'
</code></pre>

<p>You may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled ""row"".</p>

<p>Next, use the apply function in pandas to apply the function - e.g.</p>

<pre><code>df.apply (lambda row: label_race (row),axis=1)
</code></pre>

<p>Note the axis=1 specifier, that means that the application is done at a row, rather than a column level. The results are here:</p>

<pre><code>0           White
1        Hispanic
2           White
3           White
4           Other
5           White
6     Two Or More
7           White
8    Haw/Pac Isl.
9           White
</code></pre>

<p>If you're happy with those results, then run it again, posting the results into a new column in your original dataframe.</p>

<pre><code>df['race_label'] = df.apply (lambda row: label_race (row),axis=1)
</code></pre>

<p>The resultant dataframe looks like this (scroll to the right to see the new column):</p>

<pre><code>      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label
0      MOST    JEFF      E             0          0             0              0             0          1       White         White
1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic
2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White
3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White
4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other
5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White
6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More
7    PACINO      AL      E             0          0             0              0             0          1       White         White
8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.
9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White
</code></pre>
";;6;;2014-11-12T13:11:09.120;;26887820;2014-11-12T13:11:09.120;;;;;4137061.0;26886653.0;2;102;;;
20067;20067;;;"<p>Answers from HYRY and Marius in comments! </p>

<p>One can check either by:</p>

<ul>
<li><p>testing equivalence of the <code>values.base</code> attribute rather than the <code>values</code> attribute, as in:</p>

<p><code>df.values.base is df2.values.base</code> instead of <code>df.values is df2.values</code>.</p></li>
<li>or using the (admittedly internal) <code>_is_view</code> attribute (<code>df2._is_view</code> is <code>True</code>).</li>
</ul>

<p>Thanks everyone!</p>
";;2;;2014-11-12T17:38:16.990;;26893083;2017-01-18T20:37:32.277;2017-01-18T20:37:32.277;;850781.0;;2302819.0;26879073.0;2;9;;;
20072;20072;;;"<p>Call it like this:</p>

<pre><code>yes_records_sample['name'].isnull()
</code></pre>
";;1;;2014-11-12T17:58:36.557;;26893443;2014-11-12T17:58:36.557;;;;;1809530.0;26893419.0;2;21;;;
20146;20146;;;"<pre><code>Series.apply(func, convert_dtype=True, args=(), **kwds)

args : tuple

x = my_series.apply(my_function, args = (arg1,))
</code></pre>
";;6;;2014-11-13T21:12:58.800;;26918510;2017-05-02T14:26:52.463;2017-05-02T14:26:52.463;;1612432.0;;4250045.0;12182744.0;2;22;;;
20271;20271;;;"<p>One way is to select the columns and pass them to <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html""><code>np.unique</code></a>:</p>

<pre><code>&gt;&gt;&gt; np.unique(df[['Col1', 'Col2']])
array(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)
</code></pre>

<p>Note that some versions of Pandas/NumPy may require you to explicitly pass the values from the columns with the <code>.values</code> attribute:</p>

<pre><code>np.unique(df[['Col1', 'Col2']].values)
</code></pre>

<p>A faster way is to use <code>pd.unique</code>. This function uses a hashtable-based algorithm instead of NumPy's sort-based algorithm. You will need to pass a 1D array using <code>ravel()</code>:</p>

<pre><code>&gt;&gt;&gt; pd.unique(df[['Col1', 'Col2']].values.ravel())
array(['Bob', 'Joe', 'Steve', 'Bill', 'Mary'], dtype=object)
</code></pre>

<p>The difference in speed is significant for larger DataFrames:</p>

<pre><code>&gt;&gt;&gt; df1 = pd.concat([df]*100000) # DataFrame with 500000 rows
&gt;&gt;&gt; %timeit np.unique(df1[['Col1', 'Col2']].values)
1 loops, best of 3: 619 ms per loop

&gt;&gt;&gt; %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel())
10 loops, best of 3: 49.9 ms per loop
</code></pre>
";;3;;2014-11-17T16:42:34.227;;26977495;2015-08-10T12:15:21.343;2015-08-10T12:15:21.343;;3923281.0;;3923281.0;26977076.0;2;56;;;
20353;20353;;;"<p>A bit late to the game, but here's a way to create a function that sorts pandas Series, DataFrame, and multiindex DataFrame objects using arbitrary functions.</p>

<p>I make use of the <code>df.iloc[index]</code> method, which references a row in a Series/DataFrame by position (compared to <code>df.loc</code>, which references by value). Using this, we just have to have a function that returns a series of positional arguments:</p>

<pre><code>def sort_pd(key=None,reverse=False,cmp=None):
    def sorter(series):
        series_list = list(series)
        return [series_list.index(i) 
           for i in sorted(series_list,key=key,reverse=reverse,cmp=cmp)]
    return sorter
</code></pre>

<p>You can use this to create custom sorting functions. This works on the dataframe used in Andy Hayden's answer:</p>

<pre><code>df = pd.DataFrame([
    [1, 2, 'March'],
    [5, 6, 'Dec'],
    [3, 4, 'April']], 
  columns=['a','b','m'])

custom_dict = {'March':0, 'April':1, 'Dec':3}
sort_by_custom_dict = sort_pd(key=custom_dict.get)

In [6]: df.iloc[sort_by_custom_dict(df['m'])]
Out[6]:
   a  b  m
0  1  2  March
2  3  4  April
1  5  6  Dec
</code></pre>

<p>This also works on multiindex DataFrames and Series objects:</p>

<pre><code>months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

df = pd.DataFrame([
    ['New York','Mar',12714],
    ['New York','Apr',89238],
    ['Atlanta','Jan',8161],
    ['Atlanta','Sep',5885],
  ],columns=['location','month','sales']).set_index(['location','month'])

sort_by_month = sort_pd(key=months.index)

In [10]: df.iloc[sort_by_month(df.index.get_level_values('month'))]
Out[10]:
                 sales
location  month  
Atlanta   Jan    8161
New York  Mar    12714
          Apr    89238
Atlanta   Sep    5885

sort_by_last_digit = sort_pd(key=lambda x: x%10)

In [12]: pd.Series(list(df['sales'])).iloc[sort_by_last_digit(df['sales'])]
Out[12]:
2    8161
0   12714
3    5885
1   89238
</code></pre>

<p>To me this feels clean, but it uses python operations heavily rather than relying on optimized pandas operations. I haven't done any stress testing but I'd imagine this could get slow on very large DataFrames. Not sure how the performance compares to adding, sorting, then deleting a column. Any tips on speeding up the code would be appreciated!</p>
";;2;;2014-11-19T05:40:45.837;;27009771;2014-11-19T16:09:45.507;2014-11-19T16:09:45.507;;3888719.0;;3888719.0;13838405.0;2;8;;;
20374;20374;;;"<p>The for loop as suggested by @Woody Pride is unnecessary. You can reduce it to:</p>

<pre><code>cols = ['X', 'Y']
df[cols] = df[cols].ffill()
</code></pre>

<p>And I have also added a self containing example: </p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; 
&gt;&gt;&gt; #%% create dataframe
... ts1 = [0, 1, np.nan, np.nan, np.nan, np.nan]
&gt;&gt;&gt; ts2 = [0, 2, np.nan, 3, np.nan, np.nan]
&gt;&gt;&gt; d =  {'X': ts1, 'Y': ts2, 'Z': ts2}
&gt;&gt;&gt; df = pd.DataFrame(data=d)
&gt;&gt;&gt; print(df.head())
    X   Y   Z
0   0   0   0
1   1   2   2
2 NaN NaN NaN
3 NaN   3   3
4 NaN NaN NaN
&gt;&gt;&gt; 
&gt;&gt;&gt; #%% apply forward fill
... col = ['X', 'Y']
&gt;&gt;&gt; df[col] = df[col].ffill()
&gt;&gt;&gt; print(df.head())
   X  Y   Z
0  0  0   0
1  1  2   2
2  1  2 NaN
3  1  3   3
4  1  3 NaN
</code></pre>

<p>(Normally I would have commented on @Woody Pride's answer, but I do not have the rep.)</p>
";;0;;2014-11-19T13:45:13.390;;27018394;2015-12-05T17:56:00.673;2015-12-05T17:56:00.673;;583834.0;;3919462.0;27012151.0;2;20;;;
20390;20390;;;"<p>This question seems to be from a year back but since it is still open here's an update. pandas has introduced a <code>categorical</code> dtype and it operates very similar to <code>factors</code> in R. Please see this link for more information:</p>

<p><a href=""http://pandas-docs.github.io/pandas-docs-travis/categorical.html"" rel=""noreferrer"">http://pandas-docs.github.io/pandas-docs-travis/categorical.html</a></p>

<p>Reproducing a snippet from the link above showing how to create a ""factor"" variable in pandas.</p>

<pre><code>In [1]: s = Series([""a"",""b"",""c"",""a""], dtype=""category"")

In [2]: s
Out[2]: 
0    a
1    b
2    c
3    a
dtype: category
Categories (3, object): [a &lt; b &lt; c]
</code></pre>
";;0;;2014-11-19T17:46:49.703;;27023500;2014-11-19T17:46:49.703;;;;;1312137.0;15124439.0;2;14;;;
20403;20403;;;"<p>I have a hack I use for getting parallelization in Pandas. I break my dataframe into chunks, put each chunk into the element of a list, and then use ipython's parallel bits to do a parallel apply on the list of dataframes. Then I put the list back together using pandas <code>concat</code> function. </p>

<p>This is not generally applicable, however. It works for me because the function I want to apply to each chunk of the dataframe takes about a minute. And the pulling apart and putting together of my data does not take all that long. So this is clearly a kludge. With that said, here's an example. I'm using Ipython notebook so you'll see <code>%%time</code> magic in my code:</p>

<pre><code>## make some example data
import pandas as pd

np.random.seed(1)
n=10000
df = pd.DataFrame({'mygroup' : np.random.randint(1000, size=n), 
                   'data' : np.random.rand(n)})
grouped = df.groupby('mygroup')
</code></pre>

<p>For this example I'm going to make 'chunks' based on the above groupby, but this does not have to be how the data is chunked. Although it's a pretty common pattern. </p>

<pre><code>dflist = []
for name, group in grouped:
    dflist.append(group)
</code></pre>

<p>set up the parallel bits</p>

<pre><code>from IPython.parallel import Client
rc = Client()
lview = rc.load_balanced_view()
lview.block = True
</code></pre>

<p>write a silly function to apply to our data</p>

<pre><code>def myFunc(inDf):
    inDf['newCol'] = inDf.data ** 10
    return inDf
</code></pre>

<p>now let's run the code in serial then in parallel. 
serial first:</p>

<pre><code>%%time
serial_list = map(myFunc, dflist)
CPU times: user 14 s, sys: 19.9 ms, total: 14 s
Wall time: 14 s
</code></pre>

<p>now parallel </p>

<pre><code>%%time
parallel_list = lview.map(myFunc, dflist)

CPU times: user 1.46 s, sys: 86.9 ms, total: 1.54 s
Wall time: 1.56 s
</code></pre>

<p>then it only takes a few ms to merge them back into one dataframe</p>

<pre><code>%%time
combinedDf = pd.concat(parallel_list)
 CPU times: user 296 ms, sys: 5.27 ms, total: 301 ms
Wall time: 300 ms
</code></pre>

<p>I'm running 6 IPython engines on my MacBook, but you can see it drops the execute time down to 2s from 14s. </p>

<p>For really long running stochastic simulations I can use AWS backend by firing up a cluster with <a href=""http://star.mit.edu/cluster/"" rel=""noreferrer"">StarCluster</a>. Much of the time, however, I parallelize just across 8 CPUs on my MBP. </p>
";;4;;2014-11-19T20:35:42.617;;27026479;2014-11-21T15:50:18.037;2014-11-21T15:50:18.037;;37751.0;;37751.0;26187759.0;2;9;;;
20405;20405;;;"<p>This seems to work, although it really should be built in to pandas</p>

<pre><code>import pandas as pd
from joblib import Parallel, delayed
import multiprocessing

def tmpFunc(df):
    df['c'] = df.a + df.b
    return df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)

if __name__ == '__main__':
    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])
    print 'parallel version: '
    print applyParallel(df.groupby(df.index), tmpFunc)

    print 'regular version: '
    print df.groupby(df.index).apply(tmpFunc)

    print 'ideal version (does not work): '
    print df.groupby(df.index).applyParallel(tmpFunc)
</code></pre>
";;4;;2014-11-19T21:46:17.127;;27027632;2014-11-24T21:21:17.983;2014-11-24T21:21:17.983;;4043526.0;;4043526.0;26187759.0;2;43;;;
20446;20446;;;"<p>It looks like you're correct:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; import json
&gt;&gt;&gt; json.dumps(numpy.int32(685))
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.7/json/__init__.py"", line 243, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: 685 is not JSON serializable
</code></pre>

<p>The unfortunate thing here is that numpy numbers' <code>__repr__</code> doesn't give you any hint about what <em>type</em> they are.  They're running around masquerading as <code>int</code>s when they aren't (<em>gasp</em>).  Ultimately, it looks like <code>json</code> is telling you that an <code>int</code> isn't serializable, but really, it's telling you that this particular np.int32 (or whatever type you actually have) isn't serializable.  (No real surprise there -- No np.int32 <em>is</em> serializable).  This is also why the dict that you inevitably printed <em>before</em> passing it to <code>json.dumps</code> looks like it just has integers in it as well.</p>

<p>The easiest workaround here is probably to <a href=""https://docs.python.org/2/library/json.html#json.JSONEncoder"">write your own serializer</a><sup>1</sup>:</p>

<pre><code>class MyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, numpy.integer):
            return int(obj)
        elif isinstance(obj, numpy.floating):
            return float(obj)
        elif isinstance(obj, numpy.ndarray):
            return obj.tolist()
        else:
            return super(MyEncoder, self).default(obj)
</code></pre>

<p>You use it like this:</p>

<pre><code>json.dumps(numpy.float32(1.2), cls=MyEncoder)
json.dumps(numpy.arange(12), cls=MyEncoder)
json.dumps({'a': numpy.int32(42)})
</code></pre>

<p>etc.</p>

<p><sup><sup>1</sup>Or you could just write the default function and pass that as the <code>defaut</code> keyword argument to <code>json.dumps</code>.  In this scenario, you'd replace the last line with <code>raise TypeError</code>, but ... meh.  The class is more extensible :-)</sup></p>
";;3;;2014-11-20T21:47:09.570;;27050186;2014-11-20T21:57:54.537;2014-11-20T21:57:54.537;;748858.0;;748858.0;27050108.0;2;33;;;
20466;20466;;;"<p>The easiest way is to use the <code>replace</code> method on the column. The arguments are a list of the things you want to replace (here <code>['ABC', 'AB']</code>) and what you want to replace them with (the string <code>'A'</code> in this case):</p>

<pre><code>&gt;&gt;&gt; df['BrandName'].replace(['ABC', 'AB'], 'A')
0    A
1    B
2    A
3    D
4    A
</code></pre>

<p>This creates a new Series of values so you need to assign this new column to the correct column name:</p>

<pre><code>df['BrandName'] = df['BrandName'].replace(['ABC', 'AB'], 'A')
</code></pre>
";;0;;2014-11-21T11:29:15.653;;27060328;2016-04-30T21:54:51.917;2016-04-30T21:54:51.917;;3923281.0;;3923281.0;27060098.0;2;25;;;
20487;20487;;;"<p>As mentioned in the comments, you have a dupe column:</p>

<p><img src=""https://i.stack.imgur.com/9kb4c.jpg"" alt=""enter image description here""></p>
";;3;;2014-11-21T16:45:36.437;;27066284;2014-11-21T16:45:36.437;;;;;37751.0;27065133.0;2;22;;;
20596;20596;;;"<p>As long as you're comfortable with the <code>df.loc[condition, column]</code> syntax that pandas allows, this is very easy, just do <code>df['col'] != 'pre'</code> to find all rows that should be changed:</p>

<pre><code>df['col2'] = df['col']
df.loc[df['col'] != 'pre', 'col2'] = 'nonpre'

df
Out[7]: 
    col    col2
0   pre     pre
1  post  nonpre
2     a  nonpre
3     b  nonpre
4  post  nonpre
5   pre     pre
6   pre     pre
</code></pre>
";;2;;2014-11-25T02:46:37.600;;27117982;2014-11-25T02:46:37.600;;;;;1222578.0;27117773.0;2;18;;;
20608;20608;;;"<p>Use concat and pass <code>axis=1</code> and <code>ignore_index=True</code>:</p>

<pre><code>In [38]:

import numpy as np
df = pd.DataFrame({'a':np.arange(5)})
df1 = pd.DataFrame({'b':np.arange(4)})
print(df1)
df
   b
0  0
1  1
2  2
3  3
Out[38]:
   a
0  0
1  1
2  2
3  3
4  4
In [39]:

pd.concat([df,df1], ignore_index=True, axis=1)
Out[39]:
   0   1
0  0   0
1  1   1
2  2   2
3  3   3
4  4 NaN
</code></pre>
";;2;;2014-11-25T12:21:00.117;;27126593;2015-10-28T20:42:05.460;2015-10-28T20:42:05.460;;704848.0;;704848.0;27126511.0;2;13;;;
20637;20637;;;"<p><em>lexsort_depth</em> is the number of levels of a multi-index that are sorted lexically. That is, in an a-b-c-1-2-3 order (normal sort order).</p>

<p>So element indexing <em>will</em> work if a multi-index is not sorted, but the lookups may be quite a bit slower (in 0.15.2, this will show a <code>PerformanceWarning</code> for doing these kinds of lookups, see <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html"">here</a></p>

<p>The reason that sorting in general a good idea is that pandas is able to use hash-based indexing to figure out where the location is in a particular level independently for the level. ; then you can use these indexers to find the final locations.</p>

<p>Pandas takes advantage of <code>np.searchsorted</code> to find these locations when its sorted. If its not sorted, then you have to fallback to some different (slower) methods.</p>

<p><a href=""https://github.com/pydata/pandas/blob/master/pandas/core/index.py#L4269"">here</a> is the code that does this.</p>
";;0;;2014-11-26T00:38:38.953;;27139421;2014-11-26T00:38:38.953;;;;;644898.0;27116739.0;2;9;;;
20649;20649;;;"<p>As indicated in <a href=""https://stackoverflow.com/questions/16929056/pandas-read-csv-dtype-leading-zeros"">this question/answer</a> by <a href=""https://stackoverflow.com/users/2626623/lev-landau"">Lev Landau</a>, there could be a simple solution to use <code>converters</code> option for a certain column in <code>read_csv</code> function. </p>

<pre><code>converters={'column_name': lambda x: str(x)}
</code></pre>

<p>You can refer to more options of <code>read_csv</code> funtion in pandas.io.parsers.read_csv <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>Lets say I have csv file <code>projects.csv</code> like below: </p>

<pre><code>project_name,project_id
Some Project,000245
Another Project,000478
</code></pre>

<p>As for example below code is triming leading zeros: </p>

<pre><code>import csv
from pandas import read_csv

dataframe = read_csv('projects.csv')
print dataframe
</code></pre>

<p>Result:</p>

<pre><code>me@ubuntu:~$ python test_dataframe.py 
      project_name  project_id
0     Some Project         245
1  Another Project         478
me@ubuntu:~$
</code></pre>

<p>Solution code example:</p>

<pre><code>import csv
from pandas import read_csv

dataframe = read_csv('projects.csv', converters={'project_id': lambda x: str(x)})
print dataframe
</code></pre>

<p>Required result:</p>

<pre><code>me@ubuntu:~$ python test_dataframe.py 
      project_name project_id
0     Some Project     000245
1  Another Project     000478
me@ubuntu:~$
</code></pre>
";;0;;2014-11-26T08:38:24.937;;27144549;2014-11-26T09:12:51.447;2017-05-23T12:34:45.947;;-1.0;;2647009.0;13250046.0;2;15;;;
20782;20782;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.15.1/io.html#id2"">Use <code>append=True</code></a> in the call to <code>to_hdf</code>:</p>

<pre><code>import numpy as np
import pandas as pd

filename = '/tmp/test.h5'

df = pd.DataFrame(np.arange(10).reshape((5,2)), columns=['A', 'B'])
print(df)
#    A  B
# 0  0  1
# 1  2  3
# 2  4  5
# 3  6  7
# 4  8  9

# Save to HDF5
df.to_hdf(filename, 'data', mode='w', format='table')
del df    # allow df to be garbage collected

# Append more data
df2 = pd.DataFrame(np.arange(10).reshape((5,2))*10, columns=['A', 'B'])
df2.to_hdf(filename, 'data', append=True)

print(pd.read_hdf(filename, 'data'))
</code></pre>

<p>yields</p>

<pre><code>    A   B
0   0   1
1   2   3
2   4   5
3   6   7
4   8   9
0   0  10
1  20  30
2  40  50
3  60  70
4  80  90
</code></pre>

<p>Note that you need to use <code>format='table'</code> in the first call to <code>df.to_hdf</code> to make the table appendable. Otherwise, the format is <code>'fixed'</code> by default, which is faster for reading and writing, but creates a table which can not be appended to.</p>

<p>Thus, you can process each CSV one at a time, use <code>append=True</code> to build the hdf5 file. Then overwrite the DataFrame or use <code>del df</code> to allow the old DataFrame to be garbage collected.</p>

<hr>

<p>Alternatively, instead of calling <code>df.to_hdf</code>, you could <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.1/io.html#table-format"">append to a HDFStore</a>:</p>

<pre><code>import numpy as np
import pandas as pd

filename = '/tmp/test.h5'
store = pd.HDFStore(filename)

for i in range(2):
    df = pd.DataFrame(np.arange(10).reshape((5,2)) * 10**i, columns=['A', 'B'])
    store.append('data', df)

store.close()

store = pd.HDFStore(filename)
data = store['data']
print(data)
store.close()
</code></pre>

<p>yields</p>

<pre><code>    A   B
0   0   1
1   2   3
2   4   5
3   6   7
4   8   9
0   0  10
1  20  30
2  40  50
3  60  70
4  80  90
</code></pre>
";;0;;2014-11-29T14:18:30.380;;27203245;2014-11-29T15:12:36.010;2014-11-29T15:12:36.010;;190597.0;;190597.0;27203161.0;2;18;;;
20783;20783;;;"<p>This should be possible with PyTables. You'll need to use the <a href=""https://pytables.github.io/usersguide/libref/homogenous_storage.html#earrayclassdescr"" rel=""nofollow"">EArray</a> class though.</p>

<p>As an example, the following is a script I wrote to import chunked training data stored as <code>.npy</code> files into a single <code>.h5</code> file. </p>

<pre><code>import numpy
import tables
import os

training_data = tables.open_file('nn_training.h5', mode='w')
a = tables.Float64Atom()
bl_filter = tables.Filters(5, 'blosc')   # fast compressor at a moderate setting

training_input =  training_data.create_earray(training_data.root, 'X', a,
                                             (0, 1323), 'Training Input',
                                             bl_filter, 4000000)
training_output = training_data.create_earray(training_data.root, 'Y', a,
                                             (0, 27), 'Training Output',
                                             bl_filter, 4000000)

for filename in os.listdir('input'):
    print ""loading {}..."".format(filename)
    a = numpy.load(os.path.join('input', filename))
    print ""writing to h5""
    training_input.append(a)

for filename in os.listdir('output'):
    print ""loading {}..."".format(filename)
    training_output.append(numpy.load(os.path.join('output', filename)))
</code></pre>

<p>Take a look at the docs for detailed instructions, but very briefly, the <code>create_earray</code> function takes 1) a data root or parent node; 2) an array name; 3) a datatype atom; 4) a shape with a <code>0</code> in the dimension you want to expand; 5) a verbose descriptor; 6) a <a href=""https://pytables.github.io/usersguide/libref/helper_classes.html#the-filters-class"" rel=""nofollow"">compression filter</a>; and 7) an expected number of rows along the expandable dimension. Only the first two are required, but you'll probably use all seven in practice. The function accepts a few other optional arguments as well; again, see the docs for details.</p>

<p>Once the array is created, you can use its <code>append</code> method in the expected way.</p>
";;0;;2014-11-29T14:25:30.810;;27203304;2014-12-01T14:10:38.483;2014-12-01T14:10:38.483;;577088.0;;577088.0;27203161.0;2;6;;;
20841;20841;;;"<h1>The deprecated low_memory option</h1>

<p>The <code>low_memory</code> option is not properly deprecated, but it should be, since it does not actually do anything differently[<a href=""https://github.com/pydata/pandas/issues/5888"" rel=""noreferrer"">source</a>]</p>

<p>The reason you get this <code>low_memory</code> warning is because guessing dtypes for each column is very memory demanding. Pandas tries to determine what dtype to set by analyzing the data in each column.</p>

<h1>Dtype Guessing (very bad)</h1>

<p>Pandas can only determine what dtype a column should have once the whole file is read. This means nothing can really be parsed before the whole file is read unless you risk having to change the dtype of that column when you read the last value.</p>

<p>Consider the example of one file which has a column called user_id.
It contains 10 million rows where the user_id is always numbers.
Since pandas cannot know it is only numbers, it will probably keep it as the original strings until it has read the whole file.</p>

<h1>Specifying dtypes (should always be done)</h1>

<p>adding</p>

<pre><code>dtype={'user_id': int}
</code></pre>

<p>to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pd.read_csv()</code></a> call will make pandas know when it starts reading the file, that this is only integers.</p>

<p>Also worth noting is that if the last line in the file would have <code>""foobar""</code> written in the <code>user_id</code> column, the loading would crash if the above dtype was specified.</p>

<h3>Example of broken data that breaks when dtypes are defined</h3>

<pre><code>import pandas as pd
from StringIO import StringIO


csvdata = """"""user_id,username
1,Alice
3,Bob
foobar,Caesar""""""
sio = StringIO(csvdata)
pd.read_csv(sio, dtype={""user_id"": int, ""username"": object})

ValueError: invalid literal for long() with base 10: 'foobar'
</code></pre>

<p>dtypes are typically a numpy thing, read more about them here:
<a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html"" rel=""noreferrer"">http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html</a></p>

<h1>Gotchas, caveats, notes</h1>

<p>Setting <code>dtype=object</code> will silence the above warning, but will not make it more memory efficient, only process efficient if anything.</p>

<p>Setting <code>dtype=unicode</code> will not do anything, since to numpy, a <code>unicode</code> is represented as <code>object</code>.</p>

<h3>Usage of converters</h3>

<p>@sparrow correctly points out the usage of converters to avoid pandas blowing up when encountering <code>'foobar'</code> in a column specified as <code>int</code>. I would like to add that converters are really heavy and inefficient to use in pandas and should be used as a last resort. This is because the read_csv process is a single process.</p>

<p>CSV files can be processed line by line and thus can be processed by multiple converters in parallel more efficiently by simply cutting the file into segments and running multiple processes, something that pandas does not support. But this is a different story.</p>
";;9;;2014-12-01T16:04:11.337;;27232309;2016-11-24T08:06:48.383;2016-11-24T08:06:48.383;;3730397.0;;3730397.0;24251219.0;2;119;;;
20856;20856;;;"<p>Did some quick tests, and perhaps unsurprisingly the built-in version using <code>dataframe.columns.values.tolist()</code> is the fastest:</p>

<pre><code>In [1]: %timeit [column for column in df]
1000 loops, best of 3: 81.6 s per loop

In [2]: %timeit df.columns.values.tolist()
10000 loops, best of 3: 16.1 s per loop

In [3]: %timeit list(df)
10000 loops, best of 3: 44.9 s per loop

In [4]: % timeit list(df.columns.values)
10000 loops, best of 3: 38.4 s per loop
</code></pre>

<p>(I still really like the <code>list(dataframe)</code> though, so thanks EdChum!)</p>
";;0;;2014-12-01T20:31:56.667;;27236748;2015-03-13T16:01:05.353;2015-03-13T16:01:05.353;;4174485.0;;4174485.0;19482970.0;2;44;;;
20889;20889;;;"<p>This error usually rises when you join / assign to a column when the index has duplicate values. Since you are assigning to a row, I suspect that there is a duplicate value in <code>affinity_matrix.columns</code>, perhaps not shown in your question.</p>
";;1;;2014-12-02T05:50:08.170;;27242735;2014-12-02T05:50:08.170;;;;;1809530.0;27236275.0;2;44;;;
20904;20904;;;"<p>I just discovered that with pandas 15.1 it is possible to use categorical series (<a href=""http://pandas.pydata.org/pandas-docs/stable/10min.html#categoricals"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/10min.html#categoricals</a>)</p>

<p>As for your example, lets define the same data-frame and sorter:</p>

<pre><code>import pandas as pd

# Create DataFrame
df = pd.DataFrame(
{'id':[2967, 5335, 13950, 6141, 6169],\
 'Player': ['Cedric Hunter', 'Maurice Baker' ,\
            'Ratko Varda' ,'Ryan Bowen' ,'Adrian Caldwell'],\
 'Year': [1991 ,2004 ,2001 ,2009 ,1997],\
 'Age': [27 ,25 ,22 ,34 ,31],\
 'Tm':['CHH' ,'VAN' ,'TOT' ,'OKC' ,'DAL'],\
 'G':[6 ,7 ,60 ,52 ,81]})

# Define the sorter
sorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL','DEN',\
          'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',\
          'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',\
          'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',\
          'WAS', 'WSB']
</code></pre>

<p>With the data-frame and sorter, which is a category-order, we can do the following in pandas 15.1:</p>

<pre><code># Convert Tm-column to category and in set the sorter as categories hierarchy
# Youc could also do both lines in one just appending the cat.set_categories()
df.Tm = df.Tm.astype(""category"")
df.Tm.cat.set_categories(sorter, inplace=True)

print(df.Tm)
Out[48]: 
0    CHH
1    VAN
2    TOT
3    OKC
4    DAL
Name: Tm, dtype: category
Categories (38, object): [TOT &lt; ATL &lt; BOS &lt; BRK ... UTA &lt; VAN &lt; WAS &lt; WSB]

df.sort_values([""Tm""])  ## 'sort' changed to 'sort_values'
Out[49]: 
   Age   G           Player   Tm  Year     id
2   22  60      Ratko Varda  TOT  2001  13950
0   27   6    Cedric Hunter  CHH  1991   2967
4   31  81  Adrian Caldwell  DAL  1997   6169
3   34  52       Ryan Bowen  OKC  2009   6141
1   25   7    Maurice Baker  VAN  2004   5335
</code></pre>
";;2;;2014-12-02T17:36:40.653;;27255567;2016-04-27T09:17:16.810;2016-04-27T09:17:16.810;;551694.0;;551694.0;23482668.0;2;22;;;
20930;20930;;;"<p>A bit longer than I expected:</p>

<pre><code>&gt;&gt;&gt; df
                samples  subject  trial_num
0  [-0.07, -2.9, -2.44]        1          1
1   [-1.52, -0.35, 0.1]        1          2
2  [-0.17, 0.57, -0.65]        1          3
3  [-0.82, -1.06, 0.47]        2          1
4   [0.79, 1.35, -0.09]        2          2
5   [1.17, 1.14, -1.79]        2          3
&gt;&gt;&gt;
&gt;&gt;&gt; s = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)
&gt;&gt;&gt; s.name = 'sample'
&gt;&gt;&gt;
&gt;&gt;&gt; df.drop('samples', axis=1).join(s)
   subject  trial_num  sample
0        1          1   -0.07
0        1          1   -2.90
0        1          1   -2.44
1        1          2   -1.52
1        1          2   -0.35
1        1          2    0.10
2        1          3   -0.17
2        1          3    0.57
2        1          3   -0.65
3        2          1   -0.82
3        2          1   -1.06
3        2          1    0.47
4        2          2    0.79
4        2          2    1.35
4        2          2   -0.09
5        2          3    1.17
5        2          3    1.14
5        2          3   -1.79
</code></pre>

<p>If you want sequential index, you can apply <code>reset_index(drop=True)</code> to the result.</p>

<p><strong>update</strong>:</p>

<pre><code>&gt;&gt;&gt; res = df.set_index(['subject', 'trial_num'])['samples'].apply(pd.Series).stack()
&gt;&gt;&gt; res = res.reset_index()
&gt;&gt;&gt; res.columns = ['subject','trial_num','sample_num','sample']
&gt;&gt;&gt; res
    subject  trial_num  sample_num  sample
0         1          1           0    1.89
1         1          1           1   -2.92
2         1          1           2    0.34
3         1          2           0    0.85
4         1          2           1    0.24
5         1          2           2    0.72
6         1          3           0   -0.96
7         1          3           1   -2.72
8         1          3           2   -0.11
9         2          1           0   -1.33
10        2          1           1    3.13
11        2          1           2   -0.65
12        2          2           0    0.10
13        2          2           1    0.65
14        2          2           2    0.15
15        2          3           0    0.64
16        2          3           1   -0.10
17        2          3           2   -0.76
</code></pre>
";;2;;2014-12-03T07:44:29.027;;27266225;2016-12-23T14:38:56.897;2016-12-23T14:38:56.897;;1744834.0;;1744834.0;27263805.0;2;30;;;
20946;20946;;;"<p>Just perform a list comprehension to create your columns:</p>

<pre><code>In [28]:

filter_col = [col for col in df if col.startswith('foo')]
filter_col
Out[28]:
['foo.aa', 'foo.bars', 'foo.fighters', 'foo.fox', 'foo.manchu']
In [29]:

df[filter_col]
Out[29]:
   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu
0     1.0         0             0        2         NA
1     2.1         0             1        4          0
2     NaN         0           NaN        1          0
3     4.7         0             0        0          0
4     5.6         0             0        0          0
5     6.8         1             0        5          0
</code></pre>

<p>Another method is to create a series from the columns and use the vectorised str method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.strings.StringMethods.startswith.html#pandas.core.strings.StringMethods.startswith"" rel=""nofollow noreferrer""><code>startswith</code></a>:</p>

<pre><code>In [33]:

df[df.columns[pd.Series(df.columns).str.startswith('foo')]]
Out[33]:
   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu
0     1.0         0             0        2         NA
1     2.1         0             1        4          0
2     NaN         0           NaN        1          0
3     4.7         0             0        0          0
4     5.6         0             0        0          0
5     6.8         1             0        5          0
</code></pre>

<p>In order to achieve what you want you need to add the following to filter the values that don't meet your <code>==1</code> criteria:</p>

<pre><code>In [36]:

df[df[df.columns[pd.Series(df.columns).str.startswith('foo')]]==1]
Out[36]:
   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo
0      NaN       1       NaN           NaN      NaN        NaN     NaN
1      NaN     NaN       NaN             1      NaN        NaN     NaN
2      NaN     NaN       NaN           NaN        1        NaN     NaN
3      NaN     NaN       NaN           NaN      NaN        NaN     NaN
4      NaN     NaN       NaN           NaN      NaN        NaN     NaN
5      NaN     NaN         1           NaN      NaN        NaN     NaN
</code></pre>

<p><strong>EDIT</strong></p>

<p>OK after seeing what you want the convoluted answer is this:</p>

<pre><code>In [72]:

df.loc[df[df[df.columns[pd.Series(df.columns).str.startswith('foo')]] == 1].dropna(how='all', axis=0).index]
Out[72]:
   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo
0      5.0     1.0         0             0        2         NA      NA
1      5.0     2.1         0             1        4          0       0
2      6.0     NaN         0           NaN        1          0       1
5      6.8     6.8         1             0        5          0       0
</code></pre>
";;0;;2014-12-03T15:21:28.167;;27275344;2017-08-22T14:33:55.900;2017-08-22T14:33:55.900;;704848.0;;704848.0;27275236.0;2;25;;;
20947;20947;;;"<p>Now that pandas indexes support  string operations, arguably the best way to select columns beginning with 'foo' is:</p>

<pre><code>df.loc[:, df.columns.str.startswith('foo')]
</code></pre>

<p>Alternatively, you can filter column (or row) labels with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html"" rel=""nofollow noreferrer""><code>df.filter()</code></a>. To specify a regular expression to match the names beginning with <code>foo.</code>:</p>

<pre><code>&gt;&gt;&gt; df.filter(regex=r'^foo\.', axis=1)
   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu
0     1.0         0             0        2         NA
1     2.1         0             1        4          0
2     NaN         0           NaN        1          0
3     4.7         0             0        0          0
4     5.6         0             0        0          0
5     6.8         1             0        5          0
</code></pre>

<p>To select only the required rows (containing a <code>1</code>) and the columns, you can use <code>loc</code>, selecting the columns using <code>filter</code> (or any other method) and the rows using <code>any</code>:</p>

<pre><code>&gt;&gt;&gt; df.loc[(df == 1).any(axis=1), df.filter(regex=r'^foo\.', axis=1).columns]
   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu
0     1.0         0             0        2         NA
1     2.1         0             1        4          0
2     NaN         0           NaN        1          0
5     6.8         1             0        5          0
</code></pre>
";;5;;2014-12-03T15:28:21.123;;27275479;2017-08-25T19:27:20.310;2017-08-25T19:27:20.310;;3923281.0;;3923281.0;27275236.0;2;11;;;
20965;20965;;;"<p>I know this is an old thread but I think the <a href=""https://github.com/ContinuumIO/blaze"">Blaze</a> library is worth checking out.  It's built for these types of situations.</p>

<p><strong>From the docs:</strong></p>

<p>Blaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.</p>

<p><strong>Edit:</strong> By the way, it's supported by ContinuumIO and Travis Oliphant, author of NumPy.</p>
";;1;;2014-12-03T22:09:40.410;;27282644;2014-12-03T22:09:40.410;;;;;2464684.0;14262433.0;2;40;;;
21038;21038;;;"<p>For anyone coming across this question:</p>

<p>Since pandas 0.14, plotting with bars has a 'width' command:
<a href=""https://github.com/pydata/pandas/pull/6644"" rel=""noreferrer"">https://github.com/pydata/pandas/pull/6644</a></p>

<p>The example above can now be solved simply by using</p>

<pre><code>df.plot(kind='bar', stacked=True, width=1)
</code></pre>
";;0;;2014-12-05T17:46:24.480;;27321764;2014-12-05T17:46:24.480;;;;;4329776.0;14824456.0;2;32;;;
21055;21055;;;"<p>You can pass a list of row numbers to <code>skiprows</code> instead of an integer. The reader will then ignore those rows in the list. </p>

<p>By giving the function the integer 10, you're just skipping the first 10 lines. </p>

<p>To keep the first row 0 (as the header) and then skip to row 10, you could write:</p>

<pre><code>pd.read_csv('test.csv', sep='|', skiprows=range(1, 10))
</code></pre>
";;0;;2014-12-05T22:29:50.113;;27325729;2014-12-05T22:36:05.300;2014-12-05T22:36:05.300;;3923281.0;;3923281.0;27325652.0;2;23;;;
21097;21097;;;"<p>To directly answer this question's title (which I understand is not necessarily the OP's problem but could help other users coming across this question) one way to do this is to use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">drop</a> method:</p>

<p><code>df = df.drop(some labels)</code></p>

<p><code>df = df.drop(df[&lt;some boolean condition&gt;].index)</code></p>

<p><strong>Example</strong></p>

<p>To remove all rows where column 'score' is &lt; 50:</p>

<p><code>df = df.drop(df[df.score &lt; 50].index)</code></p>

<p>In place version (as pointed out in comments)</p>

<p><code>df.drop(df[df.score &lt; 50].index, inplace=True)</code></p>

<p><strong>Multiple conditions</strong></p>

<p>(see <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""noreferrer"">Boolean Indexing</a>)</p>

<blockquote>
  <p>The operators are: <code>|</code> for <code>or</code>, <code>&amp;</code> for <code>and</code>, and <code>~</code> for <code>not</code>. These must be
  grouped by using parentheses.</p>
</blockquote>

<p>To remove all rows where column 'score' is &lt; 50 and > 20</p>

<p><code>df = df.drop(df[(df.score &lt; 50) &amp; (df.score &gt; 20)].index)</code></p>
";;5;;2014-12-08T14:26:11.697;;27360130;2017-03-27T15:05:56.163;2017-03-27T15:05:56.163;;155268.0;;155268.0;13851535.0;2;151;;;
21099;21099;;;"<p>Don't forget to add ""inplace=True"" to Wes' answer or set the result to a new DataFrame.</p>

<pre><code>df.sort_index(axis=1, inplace=True)
</code></pre>
";;0;;2014-12-08T15:33:31.697;;27361326;2014-12-08T15:33:31.697;;;;;366640.0;11067027.0;2;13;;;
21104;21104;;;"<p>Just to very explicitly answer the question of how to convert a single column of boolean values to a column of integers 1 or 0:</p>

<p><code>df.somecolumn = df.somecolumn.astype(int)</code></p>
";;2;;2014-12-08T16:36:19.740;;27362540;2014-12-08T16:36:19.740;;;;;155268.0;17383094.0;2;68;;;
21117;21117;;;"<p>Adding to the other answers, in a <code>Series</code> there are also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"">map</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html"">apply</a>. </p>

<p><strong>Apply can make a DataFrame out of a series</strong>; however, map will just put a series in every cell of another series, which is probably not what you want.</p>

<pre><code>In [41]: p=pd.Series([1,2,3])

In [42]: p.apply(lambda x: pd.Series([x, x]))
Out[42]: 
   0  1
0  1  1
1  2  2
2  3  3

In [43]: p.map(lambda x: pd.Series([x, x]))
Out[43]: 
0    0    1
1    1
dtype: int64
1    0    2
1    2
dtype: int64
2    0    3
1    3
dtype: int64
dtype: object
</code></pre>

<p>Also if I had a function with side effects, such as ""connect to a web server"", I'd probably use <code>apply</code> just for the sake of clarity.</p>

<pre><code>series.apply(download_file_for_every_element) 
</code></pre>

<p><strong><code>Map</code> can use not only a function, but also a dictionary or another series.</strong> Let's say you want to manipulate <a href=""http://en.wikipedia.org/wiki/Permutation"">permutations</a>.</p>

<p>Take</p>

<pre><code>1 2 3 4 5
2 1 4 5 3
</code></pre>

<p>The square of this permutation is</p>

<pre><code>1 2 3 4 5
1 2 5 3 4
</code></pre>

<p>You can compute it using <code>map</code>. Not sure if self-application is documented, but it works in <code>0.15.1</code>. </p>

<pre><code>In [39]: p=pd.Series([1,0,3,4,2])

In [40]: p.map(p)
Out[40]: 
0    0
1    1
2    4
3    2
4    3
dtype: int64
</code></pre>
";;1;;2014-12-08T23:30:11.883;;27368948;2014-12-08T23:30:11.883;;;;;1243926.0;19798153.0;2;13;;;
21150;21150;;;"<p>I usually do this using <code>zip</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[i] for i in range(10)], columns=['num'])
&gt;&gt;&gt; df
    num
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9

&gt;&gt;&gt; def powers(x):
&gt;&gt;&gt;     return x, x**2, x**3, x**4, x**5, x**6

&gt;&gt;&gt; df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \
&gt;&gt;&gt;     zip(*df['num'].map(powers))

&gt;&gt;&gt; df
        num     p1      p2      p3      p4      p5      p6
0       0       0       0       0       0       0       0
1       1       1       1       1       1       1       1
2       2       2       4       8       16      32      64
3       3       3       9       27      81      243     729
4       4       4       16      64      256     1024    4096
5       5       5       25      125     625     3125    15625
6       6       6       36      216     1296    7776    46656
7       7       7       49      343     2401    16807   117649
8       8       8       64      512     4096    32768   262144
9       9       9       81      729     6561    59049   531441
</code></pre>
";;4;;2014-12-09T17:26:49.047;;27385043;2017-07-26T14:28:24.953;2017-07-26T14:28:24.953;;2063031.0;;2063031.0;16236684.0;2;65;;;
21181;21181;;;"<p>In case it helps, I have also hit this error when I tried to concatenate two data frames (and as of the time of writing this is the only related hit I can find on google other than the source code). </p>

<p>I don't know whether this answer would have solved the OP's problem (since he/she didn't post enough information), but for me, this was caused when I tried to <code>concat</code> dataframe <code>df1</code> with columns <code>['A', 'B', 'B', 'C']</code> (see the duplicate column headings?) with dataframe <code>df2</code> with columns <code>['A', 'B']</code>. Understandably the duplication caused pandas to throw a wobbly. Change <code>df1</code> to <code>['A', 'B', 'C']</code> (i.e. drop  one of the duplicate columns) and everything works fine.</p>
";;3;;2014-12-10T23:40:06.710;;27412913;2017-08-14T14:31:32.307;2017-08-14T14:31:32.307;;4981721.0;;3805082.0;26226343.0;2;28;;;
21203;21203;;;"<p><code>df.groupby('l_customer_id_i').agg(lambda x: ','.join(x))</code> does already return a dataframe, so you cannot loop over the groups anymore.</p>

<p>In general:</p>

<ul>
<li><p><code>df.groupby(...)</code> returns a <code>GroupBy</code> object (a DataFrameGroupBy or SeriesGroupBy), and with this, you can iterate through the groups (as explained in the docs <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups"">here</a>). You can do something like:</p>

<pre><code>grouped = df.groupby('A')

for name, group in grouped:
    ...
</code></pre></li>
<li><p>When you apply a function on the groupby, in your example <code>df.groupby(...).agg(...)</code> (but this can also be <code>transform</code>, <code>apply</code>, <code>mean</code>, ...), you <strong>combine</strong> the result of <strong>applying</strong> the function to the different groups together in one dataframe (the apply and combine step of the 'split-apply-combine' paradigm of groupby). So the result of this will always be again a DataFrame (or a Series depending on the applied function).</p></li>
</ul>
";;0;;2014-12-11T12:20:01.947;;27422749;2014-12-11T12:20:01.947;;;;;653364.0;27405483.0;2;37;;;
21310;21310;;;"<p>You could use apply() and do something like this</p>

<pre><code>df['que'] = df.apply(lambda x : x['one'] if x['one'] &gt;= x['two'] and x['one'] &lt;= x['three'] else """", axis=1)
</code></pre>

<p>or if you prefer not to use a lambda </p>

<pre><code>def que(x):
    if x['one'] &gt;= x['two'] and x['one'] &lt;= x['three']:
        return x['one']
    else:
        ''
df['que'] = df.apply(que, axis=1)
</code></pre>
";;9;;2014-12-14T22:46:24.087;;27475029;2014-12-14T23:21:20.287;2014-12-14T23:21:20.287;;3437504.0;;3437504.0;27474921.0;2;13;;;
21312;21312;;;"<p>One way is to use a Boolean series to index the column <code>df['one']</code>. This gives you a new column where the <code>True</code> entries have the same value as the same row as <code>df['one']</code> and the <code>False</code> values are <code>NaN</code>.</p>

<p>The Boolean series is just given by your <code>if</code> statement (although it is necessary to use <code>&amp;</code> instead of <code>and</code>):</p>

<pre><code>&gt;&gt;&gt; df['que'] = df['one'][(df['one'] &gt;= df['two']) &amp; (df['one'] &lt;= df['three'])]
&gt;&gt;&gt; df
    one two three   que
0   10  1.2 4.2      10
1   15  70  0.03    NaN
2   8   5   0       NaN
</code></pre>

<p>If you want the <code>NaN</code> values to be replaced by other values, you can use the <code>fillna</code> method on the new column <code>que</code>. I've used <code>0</code> instead of the empty string here:</p>

<pre><code>&gt;&gt;&gt; df['que'] = df['que'].fillna(0)
&gt;&gt;&gt; df
    one two three   que
0   10  1.2   4.2    10
1   15   70  0.03     0
2    8    5     0     0
</code></pre>
";;1;;2014-12-14T22:48:12.133;;27475046;2014-12-14T22:54:11.100;2014-12-14T22:54:11.100;;3923281.0;;3923281.0;27474921.0;2;6;;;
21315;21315;;;"<p>You could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""noreferrer"">np.where</a>. If <code>cond</code> is a boolean array, and <code>A</code> and <code>B</code> are arrays, then</p>

<pre><code>C = np.where(cond, A, B)
</code></pre>

<p>defines C to be equal to <code>A</code> where <code>cond</code> is True, and <code>B</code> where <code>cond</code> is False.</p>

<pre><code>import numpy as np
import pandas as pd

a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]
df = pd.DataFrame(a, columns=['one', 'two', 'three'])

df['que'] = np.where((df['one'] &gt;= df['two']) &amp; (df['one'] &lt;= df['three'])
                     , df['one'], np.nan)
</code></pre>

<p>yields</p>

<pre><code>  one  two three  que
0  10  1.2   4.2   10
1  15   70  0.03  NaN
2   8    5     0  NaN
</code></pre>

<hr>

<p>If you have more than one condition, then you could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""noreferrer"">np.select</a> instead.
For example, if you wish <code>df['que']</code> to equal <code>df['two']</code> when <code>df['one'] &lt; df['two']</code>, then</p>

<pre><code>conditions = [
    (df['one'] &gt;= df['two']) &amp; (df['one'] &lt;= df['three']), 
    df['one'] &lt; df['two']]

choices = [df['one'], df['two']]

df['que'] = np.select(conditions, choices, default=np.nan)
</code></pre>

<p>yields</p>

<pre><code>  one  two three  que
0  10  1.2   4.2   10
1  15   70  0.03   70
2   8    5     0  NaN
</code></pre>

<p>If we can assume that <code>df['one'] &gt;= df['two']</code> when <code>df['one'] &lt; df['two']</code> is
False, then the conditions and choices could be simplified to</p>

<pre><code>conditions = [
    df['one'] &lt; df['two'],
    df['one'] &lt;= df['three']]

choices = [df['two'], df['one']]
</code></pre>

<p>(The assumption may not be true if <code>df['one']</code> or <code>df['two']</code> contain NaNs.)</p>

<hr>

<p>Note that </p>

<pre><code>a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]
df = pd.DataFrame(a, columns=['one', 'two', 'three'])
</code></pre>

<p>defines a DataFrame with string values. Since they look numeric, you might be better off converting those strings to floats:</p>

<pre><code>df2 = df.astype(float)
</code></pre>

<p>This changes the results, however, since strings compare character-by-character, while floats are compared numerically.</p>

<pre><code>In [61]: '10' &lt;= '4.2'
Out[61]: True

In [62]: 10 &lt;= 4.2
Out[62]: False
</code></pre>
";;0;;2014-12-14T23:51:22.563;;27475514;2014-12-15T11:44:27.400;2014-12-15T11:44:27.400;;190597.0;;190597.0;27474921.0;2;22;;;
21336;21336;;;"<p>You just need to use <code>apply</code> on the <code>groupby</code> object. I modified your example data to make this a little more clear:</p>

<pre><code>import pandas
from io import StringIO

csv = StringIO(""""""index,A,B
0,1,0.0
1,1,3.0
2,1,6.0
3,2,0.0
4,2,5.0
5,2,7.0"""""")

df = pandas.read_csv(csv, index_col='index')
groups = df.groupby(by=['A'])
print(groups.apply(lambda g: g[g['B'] == g['B'].max()]))
</code></pre>

<p>Which prints:</p>

<pre><code>         A  B
A index      
1 2      1  6
2 4      2  7
</code></pre>
";;3;;2014-12-15T17:05:39.100;;27489248;2014-12-15T17:05:39.100;;;;;1552748.0;27488080.0;2;18;;;
21408;21408;;;"<p>If you label the columns and index of your DataFrame, pandas will automatically supply appropriate labels:</p>

<pre><code>import pandas as pd
values = [[1,2], [2,5]]
df = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df.columns.name = 'Type'
df.index.name = 'Index'
df.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Video streaming dropout by category')
</code></pre>

<p><img src=""https://i.stack.imgur.com/9Ejsk.png"" alt=""enter image description here""></p>

<p>In this case, you'll still need to supply y-labels manually (e.g., via <code>plt.ylabel</code> as shown in the other answers).</p>
";;3;;2014-12-17T08:06:02.170;;27520877;2014-12-17T08:06:02.170;;;;;809705.0;21487329.0;2;17;;;
21508;21508;;;"<pre><code>from pandas import  DataFrame

df1 = DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})
df2 = DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})

print df2[~df2.isin(df1).all(1)]
print df2[(df2!=df1)].dropna(how='all')
print df2[~(df2==df1)].dropna(how='all')
</code></pre>
";;3;;2014-12-20T10:15:25.163;;27579192;2014-12-21T06:58:12.237;2014-12-21T06:58:12.237;;1924666.0;;1924666.0;18180763.0;2;14;;;
21573;21573;;;"<p>You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.</p>

<pre><code>import pymongo
import pandas as pd
from pymongo import MongoClient
client = MongoClient()
db = client.database_name
collection = db.collection_name
data = pd.DataFrame(list(collection.find()))
</code></pre>
";;0;;2014-12-23T09:15:23.870;;27617290;2016-08-12T23:31:59.250;2016-08-12T23:31:59.250;;492631.0;;2276738.0;16249736.0;2;10;;;
21671;21671;;;"<p>Please refer to the doc <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing"" rel=""noreferrer"">Different Choices for Indexing</a>, it states clearly when and why you should use <strong>.loc, .iloc</strong> over <strong>.ix</strong>, it's about explicit use case:</p>

<blockquote>
  <p>.ix supports mixed integer and label based access. It is primarily
  label based, but will fall back to integer positional access unless
  the corresponding axis is of integer type. .ix is the most general and
  will support any of the inputs in .loc and .iloc. .ix also supports
  floating point label schemes. .ix is exceptionally useful when dealing
  with mixed positional and label based hierachical indexes.</p>
  
  <p>However, when an axis is integer based, ONLY label based access and
  not positional access is supported. Thus, in such cases, its usually
  better to be explicit and use .iloc or .loc.</p>
</blockquote>

<p>Hope this helps.</p>

<h3>Update 22 Mar 2017</h3>

<p>Thanks to comment from @Alexander, <strong>Pandas</strong> is going to deprecate <code>ix</code> in <strong>0.20</strong>, details in <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#deprecate-ix"" rel=""noreferrer"">here</a>.</p>

<p>One of the strong reason behind is because mixing indexes -- positional and label (effectively using <code>ix</code>) has been a significant source of problems for users.</p>

<p>It is expected to migrate to use <code>iloc</code> and <code>loc</code> instead, here is a link on <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-deprecate-ix"" rel=""noreferrer"">how to convert code</a>.</p>
";;5;;2014-12-27T13:40:37.677;;27667801;2017-03-22T09:22:59.917;2017-03-22T09:22:59.917;;3849456.0;;3849456.0;27667759.0;2;50;;;
21691;21691;;;"<p>This expands on Paul's answer. In Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change. Consider the following code:</p>

<pre><code>df = DataFrame({'x': [1,2]})
df_sub = df[0:1]
df_sub.x = -1
print(df)
</code></pre>

<p>You'll get: </p>

<pre><code>x
0 -1
1  2
</code></pre>

<p>In contrast, the following leaves df unchanged:</p>

<pre><code>df_sub_copy = df[0:1].copy()
df_sub_copy.x = -1
</code></pre>
";;0;;2014-12-28T20:01:49.483;;27680109;2014-12-28T20:01:49.483;;;;;4394797.0;27673231.0;2;41;;;
21804;21804;;;"<p>I tried using the dtypes=[datetime, ...] option, but</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime, datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>I encountered the following error:</p>

<pre><code>TypeError: data type not understood
</code></pre>

<p>The only change I had to make is to replace datetime with datetime.datetime</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime.datetime, datetime.datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>
";;2;;2015-01-02T19:48:35.640;;27747726;2015-01-02T19:48:35.640;;;;;1937373.0;21269399.0;2;6;;;
21824;21824;;;"<p>If all your columns are numeric, you can use boolean indexing:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'a': [0, -1, 2], 'b': [-3, 2, 1]})

In [3]: df
Out[3]: 
   a  b
0  0 -3
1 -1  2
2  2  1

In [4]: df[df &lt; 0] = 0

In [5]: df
Out[5]: 
   a  b
0  0  0
1  0  2
2  2  1
</code></pre>

<hr>

<p>For the more general case, <a href=""https://stackoverflow.com/a/12726468/1258041"">this answer</a> shows the private method <code>_get_numeric_data</code>:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'a': [0, -1, 2], 'b': [-3, 2, 1],
                           'c': ['foo', 'goo', 'bar']})

In [3]: df
Out[3]: 
   a  b    c
0  0 -3  foo
1 -1  2  goo
2  2  1  bar

In [4]: num = df._get_numeric_data()

In [5]: num[num &lt; 0] = 0

In [6]: df
Out[6]: 
   a  b    c
0  0  0  foo
1  0  2  goo
2  2  1  bar
</code></pre>

<hr>

<p>With <code>timedelta</code> type, boolean indexing seems to work on separate columns, but not on the whole dataframe. So you can do:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'a': pd.to_timedelta([0, -1, 2], 'd'),
   ...:                    'b': pd.to_timedelta([-3, 2, 1], 'd')})

In [3]: df
Out[3]: 
        a       b
0  0 days -3 days
1 -1 days  2 days
2  2 days  1 days

In [4]: for k, v in df.iteritems():
   ...:     v[v &lt; 0] = 0
   ...:     

In [5]: df
Out[5]: 
       a      b
0 0 days 0 days
1 0 days 2 days
2 2 days 1 days
</code></pre>

<hr>

<p><strong>Update:</strong> comparison with a <code>pd.Timedelta</code> works on the whole DataFrame:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'a': pd.to_timedelta([0, -1, 2], 'd'),
   ...:                    'b': pd.to_timedelta([-3, 2, 1], 'd')})

In [3]: df[df &lt; pd.Timedelta(0)] = 0

In [4]: df
Out[4]: 
       a      b
0 0 days 0 days
1 0 days 2 days
2 2 days 1 days
</code></pre>
";;10;;2015-01-03T20:20:54.233;;27759140;2015-01-03T23:06:08.830;2017-05-23T11:33:13.693;;-1.0;;1258041.0;27759084.0;2;21;;;
21847;21847;;;"<p>Categorical variables seems to be one of the more active areas of development in pandas, so I believe it's changed yet again in pandas 0.15.0 :</p>

<pre><code>df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
</code></pre>

<p>(I lacked sufficient reputation to add this as a comment on David Robinson's answer)</p>
";;1;;2015-01-04T18:51:27.460;;27769102;2015-01-04T18:51:27.460;;;;;1290863.0;21689423.0;2;8;;;
21878;21878;;;"<p>As <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-object-attributes"">documented</a>, you can get the number of groups with <code>len(dfgroup)</code>.</p>
";;1;;2015-01-05T21:12:03.193;;27787977;2015-01-05T21:12:03.193;;;;;1427416.0;27787930.0;2;25;;;
21884;21884;;;"<p>The answer by @chip completely misses the point of two keyword arguments.</p>

<ul>
<li><strong>names</strong> is only necessary when there is no header and you want to specify other arguments using column names rather than integer indices.</li>
<li><strong>usecols</strong> is supposed to provide a filter before reading the whole DataFrame into memory; if used properly, there should never be a need to delete columns after reading.</li>
</ul>

<p>This solution corrects those oddities:</p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""

df = pd.read_csv(StringIO(csv),
        header=0,
        index_col=[""date"", ""loc""], 
        usecols=[""date"", ""loc"", ""x""],
        parse_dates=[""date""])
</code></pre>

<p>Which gives us:</p>

<pre><code>                x
date       loc
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;2;;2015-01-06T02:47:26.400;;27791362;2015-01-06T18:31:35.627;2015-01-06T18:31:35.627;;2486302.0;;2486302.0;15017072.0;2;32;;;
21898;21898;;;"<p>I think <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html"">resample('W')</a> is a better way to do this - by default it groups by weeks ending on Sunday ('W' is the same as 'W-SUN') but you can specify whatever you want.</p>

<p>In your example, try this:</p>

<pre><code>grouped = (df
    .groupby('col1')                
    .apply(lambda g:               # work on groups of col1
        g.set_index('date')        
        [['amount']]
        .resample('W', how='sum')  # sum the amount field across weeks
    )
    .unstack(level=0)              # pivot the col1 index rows to columns
    .fillna(0)
)
grouped.columns=grouped.columns.droplevel()   # drop the 'col1' part of the multi-index column names
print grouped
grouped.plot(kind='bar')
</code></pre>

<p>which should print your data table and make a plot similar to yours, but with ""real"" date labels:</p>

<pre><code>col1         A   B   C   D   E   F   G   H
date                                      
2013-11-03  18  0   9   0   8   0   0   4 
2013-11-10  4   11  0   1   16  2   15  2 
2013-11-17  10  14  19  8   13  6   9   8 
2013-11-24  10  13  13  0   0   13  15  10
2013-12-01  6   3   19  8   8   17  8   12
2013-12-08  5   15  5   7   12  0   11  8 
2013-12-15  8   6   11  11  0   16  6   14
2013-12-22  16  3   13  8   8   11  15  0 
2013-12-29  1   3   6   10  7   7   17  15
2014-01-05  12  7   10  11  6   0   1   12
2014-01-12  13  0   17  0   23  0   10  12
2014-01-19  10  9   2   3   8   1   18  3 
2014-01-26  24  9   8   1   19  10  0   3 
2014-02-02  1   6   16  0   0   10  8   13
</code></pre>
";;0;;2015-01-06T15:39:18.500;;27802006;2015-01-06T15:39:18.500;;;;;2336373.0;24203106.0;2;9;;;
21966;21966;;;"<p>What you want to do is actually again a groupby (on the result of the first groupby): sort and take the first three elements per group.</p>

<p>Starting from the result of the first groupby:</p>

<pre><code>In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})
</code></pre>

<p>We group by the first level of the index:</p>

<pre><code>In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)
</code></pre>

<p>Then we want to sort ('order') each group and take the first three elements:</p>

<pre><code>In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))
</code></pre>

<p>However, for this, there is a shortcut function to do this, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nlargest.html""><code>nlargest</code></a>:</p>

<pre><code>In [65]: g.nlargest(3)
Out[65]:
job     source
market  A         5
        D         4
        B         3
sales   E         7
        C         6
        B         4
dtype: int64
</code></pre>
";;2;;2015-01-08T15:46:41.100;;27844045;2015-01-08T15:58:51.583;2015-01-08T15:58:51.583;;653364.0;;653364.0;27842613.0;2;39;;;
22046;22046;;;"<p>There can be a significant performance difference, of an order of magnitude for multiplications and multiple orders of magnitude for indexing a few random vaues.</p>

<p>I was actually wondering about the same thing and came across this interesting comparison: 
<a href=""http://penandpants.com/2014/09/05/performance-of-pandas-series-vs-numpy-arrays/"">http://penandpants.com/2014/09/05/performance-of-pandas-series-vs-numpy-arrays/</a></p>
";;0;;2015-01-11T16:04:56.743;;27889133;2015-01-11T16:04:56.743;;;;;723090.0;21567842.0;2;10;;;
22047;22047;;;"<p>You are bitten by the case (in)sensitivity issues with PostgreSQL. If you quote the table name in the query, it will work:</p>

<pre><code>df = pd.read_sql_query('select * from ""Stat_Table""',con=engine)
</code></pre>

<p>But personally, I would advise to just always use lower case table names (and column names), also when writing the table to the database to prevent such issues.</p>

<hr>

<p>From the PostgreSQL docs (<a href=""http://www.postgresql.org/docs/8.0/static/sql-syntax.html#SQL-SYNTAX-IDENTIFIERS"" rel=""noreferrer"">http://www.postgresql.org/docs/8.0/static/sql-syntax.html#SQL-SYNTAX-IDENTIFIERS</a>):</p>

<blockquote>
  <p>Quoting an identifier also makes it case-sensitive, whereas unquoted names are always folded to lower case</p>
</blockquote>

<p>To explain a bit more: you have written a table with the name <code>Stat_Table</code> to the database (and sqlalchemy will quote this name, so it will be written as ""Stat_Table"" in the postgres database). When doing the query <code>'select * from Stat_Table'</code> the unquoted table name will be converted to lower case <code>stat_table</code>, and so you get the message that this table is not found. </p>

<p>See eg also <a href=""https://stackoverflow.com/questions/20878932/are-postgresql-column-names-case-sensitive"">Are PostgreSQL column names case-sensitive?</a></p>
";;0;;2015-01-11T17:00:53.127;;27889674;2015-01-11T17:06:03.063;2017-05-23T11:33:24.953;;-1.0;;653364.0;27884268.0;2;26;;;
22070;22070;;;"<p>You could use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>fillna</code></a> method on the DataFrame and specify the method as <code>ffill</code> (forward fill):</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
&gt;&gt;&gt; df.fillna(method='ffill')
   0  1  2
0  1  2  3
1  4  2  3
2  4  2  9
</code></pre>

<p>This method...</p>

<blockquote>
  <p>propagate[s] last valid observation forward to next valid</p>
</blockquote>

<p>To go the opposite way, there's also a <code>bfill</code> method.</p>

<p>This method doesn't modify the DataFrame inplace - you'll need to rebind the returned DataFrame to a variable or else specify <code>inplace=True</code>:</p>

<pre><code>df.fillna(method='ffill', inplace=True)
</code></pre>
";;0;;2015-01-12T15:25:42.520;;27905350;2015-01-12T15:38:35.513;2015-01-12T15:38:35.513;;3923281.0;;3923281.0;27905295.0;2;35;;;
22071;22071;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>pandas.DataFrame.fillna</code></a> with the <code>method='ffill'</code> option. <code>'ffill'</code> stands for 'forward fill' and will propagate last valid observation forward. The alternative is <code>'bfill'</code> which works the same way, but backwards.</p>

<pre><code>import pandas as pd

df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
df = df.fillna(method='ffill')

print(df)
#   0  1  2
#0  1  2  3
#1  4  2  3
#2  4  2  9
</code></pre>

<p>There is also a direct synonym function for this, <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.ffill.html"" rel=""noreferrer""><code>pandas.DataFrame.ffill</code></a>, to make things simpler.</p>
";;0;;2015-01-12T15:25:54.347;;27905354;2015-01-12T15:32:01.573;2015-01-12T15:32:01.573;;3005188.0;;3005188.0;27905295.0;2;7;;;
22172;22172;;;"<p>As I felt similarly confused with <code>.transform</code> operation vs. <code>.apply</code> I found a few answers shedding some light on the issue. <a href=""https://stackoverflow.com/a/13854901/4077912"">This answer</a> for example was very helpful.</p>

<p>My takeout so far is that <code>.transform</code> will work (or deal) with <code>Series</code> (columns) <strong>in isolation from each other</strong>. What this means is that in your last two calls:</p>

<pre><code>df.groupby('A').transform(lambda x: (x['C'] - x['D']))
df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())
</code></pre>

<p>You asked <code>.transform</code> to take values from two columns and 'it' actually does not 'see' both of them at the same time (so to speak). <code>transform</code> will look at the dataframe columns one by one and return back a series (or group of series) 'made' of scalars which are repeated <code>len(input_column)</code> times.</p>

<p>So this scalar, that should be used by <code>.transform</code> to make the <code>Series</code> is a result of some reduction function applied on an input <code>Series</code> (and only on ONE series/column at a time).</p>

<p>Consider this example (on your dataframe):</p>

<pre><code>zscore = lambda x: (x - x.mean()) / x.std() # Note that it does not reference anything outside of 'x' and for transform 'x' is one column.
df.groupby('A').transform(zscore)
</code></pre>

<p>will yield:</p>

<pre><code>       C      D
0  0.989  0.128
1 -0.478  0.489
2  0.889 -0.589
3 -0.671 -1.150
4  0.034 -0.285
5  1.149  0.662
6 -1.404 -0.907
7 -0.509  1.653
</code></pre>

<p>Which is exactly the same as if you would use it on only on one column at a time:</p>

<pre><code>df.groupby('A')['C'].transform(zscore)
</code></pre>

<p>yielding:</p>

<pre><code>0    0.989
1   -0.478
2    0.889
3   -0.671
4    0.034
5    1.149
6   -1.404
7   -0.509
</code></pre>

<p>Note that <code>.apply</code> in the last example (<code>df.groupby('A')['C'].apply(zscore)</code>) would work in exactly the same way, but it would fail if you tried using it on a dataframe:</p>

<pre><code>df.groupby('A').apply(zscore)
</code></pre>

<p>gives error:</p>

<pre><code>ValueError: operands could not be broadcast together with shapes (6,) (2,)
</code></pre>

<p>So where else  is <code>.transform</code> useful? The simplest case is trying to assign results of reduction function back to original dataframe.</p>

<pre><code>df['sum_C'] = df.groupby('A')['C'].transform(sum)
df.sort('A') # to clearly see the scalar ('sum') applies to the whole column of the group
</code></pre>

<p>yielding:</p>

<pre><code>     A      B      C      D  sum_C
1  bar    one  1.998  0.593  3.973
3  bar  three  1.287 -0.639  3.973
5  bar    two  0.687 -1.027  3.973
4  foo    two  0.205  1.274  4.373
2  foo    two  0.128  0.924  4.373
6  foo    one  2.113 -0.516  4.373
7  foo  three  0.657 -1.179  4.373
0  foo    one  1.270  0.201  4.373
</code></pre>

<p>Trying the same with <code>.apply</code> would give <code>NaNs</code> in <code>sum_C</code>.
Because <code>.apply</code> would return a reduced <code>Series</code>, which it does not know how to broadcast back:</p>

<pre><code>df.groupby('A')['C'].apply(sum)
</code></pre>

<p>giving:</p>

<pre><code>A
bar    3.973
foo    4.373
</code></pre>

<p>There are also cases when <code>.transform</code> is used to filter the data:</p>

<pre><code>df[df.groupby(['B'])['D'].transform(sum) &lt; -1]

     A      B      C      D
3  bar  three  1.287 -0.639
7  foo  three  0.657 -1.179
</code></pre>

<p>I hope this adds a bit more clarity.</p>
";;0;;2015-01-14T20:34:49.010;;27951930;2016-11-30T17:36:37.717;2017-05-23T12:34:42.373;;-1.0;;4077912.0;27517425.0;2;59;;;
22179;22179;;;"<p>As of pandas v0.14.1, you can utilize <code>select_dtypes()</code> to select columns by dtype</p>

<pre><code>In [2]: df = pd.DataFrame({'NAME': list('abcdef'),
    'On_Time': [True, False] * 3,
    'On_Budget': [False, True] * 3})

In [3]: df.select_dtypes(include=['bool'])
Out[3]:
  On_Budget On_Time
0     False    True
1      True   False
2     False    True
3      True   False
4     False    True
5      True   False

In [4]: mylist = list(df.select_dtypes(include=['bool']).columns)

In [5]: mylist
Out[5]: ['On_Budget', 'On_Time']
</code></pre>
";;0;;2015-01-14T23:29:22.847;;27954411;2015-01-14T23:29:22.847;;;;;2671338.0;22470690.0;2;52;;;
22227;22227;;;"<pre><code>&gt;&gt;&gt; mask = df['ids'].str.contains('ball')    
&gt;&gt;&gt; mask
0     True
1     True
2    False
3     True
Name: ids, dtype: bool

&gt;&gt;&gt; df[mask]
     ids  vals
0  aball     1
1  bball     2
3  fball     4
</code></pre>
";;0;;2015-01-15T23:56:36.213;;27975191;2015-01-15T23:56:36.213;;;;;3820991.0;27975069.0;2;6;;;
22228;22228;;;"<pre><code>In [3]: df[df['ids'].str.contains(""ball"")]
Out[3]:
     ids  vals
0  aball     1
1  bball     2
3  fball     4
</code></pre>
";;7;;2015-01-15T23:59:55.273;;27975230;2015-01-15T23:59:55.273;;;;;975114.0;27975069.0;2;47;;;
22230;22230;;;"<pre><code>df[df['ids'].str.contains('ball', na = False)] # valid for (at least) pandas version 0.17.1
</code></pre>

<p>Step-by-step explanation (from inner to outer):</p>

<ul>
<li><code>df['ids']</code> selects the <code>ids</code> column of the data frame (techincally, the object <code>df[ids]</code> is of type <code>pandas.Series</code>)</li>
<li><code>df['ids'].str</code> allows us to apply vectorized string methods (e.g., <code>lower</code>, <code>contains</code>) to the Series</li>
<li><code>df['ids'].str.contains('ball')</code> checks <em>each</em> element of the Series as to whether the element value has the string 'ball' as a substring. The result is a Series of Booleans indicating <code>True</code> or <code>False</code> about the existence of a 'ball' substring.</li>
<li><code>df[df['ids'].str.contains('ball')]</code> applies the Boolean 'mask' to the dataframe and returns a view containing appropriate records.</li>
<li><code>na = False</code> removes NA / NaN values from consideration; otherwise a ValueError may be returned.</li>
</ul>
";;3;;2015-01-16T01:04:57.423;;27975789;2017-01-01T01:27:17.157;2017-01-01T01:27:17.157;;415282.0;;415282.0;27975069.0;2;25;;;
22251;22251;;;"<p>EDIT: I just learned a much neater way to do this using the <code>.transform</code> group by method: </p>

<pre><code>def get_max_rows(df):
    B_maxes = df.groupby('A').B.transform(max)
    return df[df.B == B_maxes] 
</code></pre>

<p><code>B_maxes</code> is a series which identically indexed as the original <code>df</code> containing the maximum value of <code>B</code> for each <code>A</code> group.  You can pass lots of functions to the transform method. I think once they have output either as a scalar or vector of the same length. You can even pass some strings as common function names like <code>'median'</code>. 
This is slightly different to Paul H's method in that 'A' won't be an index in the result, but you can easily set that after.</p>

<pre><code>import numpy as np
import pandas as pd
df_lots_groups = pd.DataFrame(np.random.rand(30000, 3), columns = list('BCD')
df_lots_groups['A'] = np.random.choice(range(10000), 30000)

%timeit get_max_rows(df_lots_groups)
100 loops, best of 3: 2.86 ms per loop

%timeit df_lots_groups.groupby('A').apply(lambda df: df[ df.B == df.B.max()])
1 loops, best of 3: 5.83 s per loop
</code></pre>

<p>EDIT:</p>

<p>Here's a abstraction which allows you to select rows from groups using any valid comparison operator and any valid groupby method:</p>

<pre><code>def get_group_rows(df, group_col, condition_col, func=max, comparison='=='):
    g = df.groupby(group_col)[condition_col]
    condition_limit = g.transform(func)
    df.query('condition_col {} @condition_limit'.format(comparison))
</code></pre>

<p>So, for example, if you want all rows in above the median B-value in each A-group you call </p>

<pre><code>get_group_rows(df, 'A', 'B', 'median', '&gt;')
</code></pre>

<p>A few examples:</p>

<pre><code>%timeit get_group_rows(df_lots_small_groups, 'A', 'B', 'max', '==')
100 loops, best of 3: 2.84 ms per loop
%timeit get_group_rows(df_lots_small_groups, 'A', 'B', 'mean', '!=')
100 loops, best of 3: 2.97 ms per loop
</code></pre>
";;3;;2015-01-16T16:00:29.317;;27987908;2015-09-22T10:40:27.763;2015-09-22T10:40:27.763;;3868428.0;;3868428.0;27488080.0;2;8;;;
22286;22286;;;"<p>For me (Mac OS X Maverics, Python 2.7)</p>

<pre><code>easy_install --upgrade numpy
</code></pre>

<p>helped. After this you can install up-to-date packages <em>pandas</em>, <em>scikit-learn</em>, e.t.c. using <strong>pip</strong>:</p>

<pre><code>pip install pandas
</code></pre>
";;3;;2015-01-17T12:57:38.820;;27999688;2015-01-17T12:57:38.820;;;;;4165040.0;17709641.0;2;35;;;
22308;22308;;;"<p>You could access the underlying array and call its <code>tolist</code> method:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1,2,3],[3,4,5]])
&gt;&gt;&gt; lol = df.values.tolist()
&gt;&gt;&gt; lol
[[1L, 2L, 3L], [3L, 4L, 5L]]
</code></pre>
";;3;;2015-01-18T03:18:39.310;;28006809;2015-01-18T03:18:39.310;;;;;487339.0;28006793.0;2;50;;;
22314;22314;;;"<p>Use the new <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tseries.common.DatetimeProperties.dayofweek.html#pandas.tseries.common.DatetimeProperties.dayofweek""><code>dt.dayofweek</code></a> property:</p>

<pre><code>In [2]:

df['weekday'] = df['Timestamp'].dt.dayofweek
df
Out[2]:
            Timestamp  Value  weekday
0 2012-06-01 00:00:00    100        4
1 2012-06-01 00:15:00    150        4
2 2012-06-01 00:30:00    120        4
3 2012-06-01 01:00:00    220        4
4 2012-06-01 01:15:00     80        4
</code></pre>

<p>In the situation where the <code>Timestamp</code> is your index you need to reset the index and then call the <code>dt.dayofweek</code> property:</p>

<pre><code>In [14]:

df = df.reset_index()
df['weekday'] = df['Timestamp'].dt.dayofweek
df
Out[14]:
            Timestamp  Value  weekday
0 2012-06-01 00:00:00    100        4
1 2012-06-01 00:15:00    150        4
2 2012-06-01 00:30:00    120        4
3 2012-06-01 01:00:00    220        4
4 2012-06-01 01:15:00     80        4
</code></pre>

<p>Strangely if you try to create a series from the index in order to not reset the index you get <code>NaN</code> values as does using the result of <code>reset_index</code> to call the <code>dt.dayofweek</code> property without assigning the result of <code>reset_index</code> back to the original df:</p>

<pre><code>In [16]:

df['weekday'] = pd.Series(df.index).dt.dayofweek
df
Out[16]:
                     Value  weekday
Timestamp                          
2012-06-01 00:00:00    100      NaN
2012-06-01 00:15:00    150      NaN
2012-06-01 00:30:00    120      NaN
2012-06-01 01:00:00    220      NaN
2012-06-01 01:15:00     80      NaN
In [17]:

df['weekday'] = df.reset_index()['Timestamp'].dt.dayofweek
df
Out[17]:
                     Value  weekday
Timestamp                          
2012-06-01 00:00:00    100      NaN
2012-06-01 00:15:00    150      NaN
2012-06-01 00:30:00    120      NaN
2012-06-01 01:00:00    220      NaN
2012-06-01 01:15:00     80      NaN
</code></pre>

<p><strong>EDIT</strong></p>

<p>As pointed out to me by user @joris you can just access the <code>weekday</code> attribute of the index so the following will work and is more compact:</p>

<p><code>df['Weekday'] = df.index.weekday</code></p>
";;14;;2015-01-18T12:14:03.533;;28009526;2015-01-18T18:11:24.987;2015-01-18T18:11:24.987;;704848.0;;704848.0;28009370.0;2;29;;;
22334;22334;;;"<p>To know if your dataframe <code>dataset</code> has suitable content you can explicitly convert to a numpy array:</p>

<pre><code>dataset_array = dataset.values
print(dataset_array.dtype)
print(dataset_array)
</code></pre>

<p>If the array has an homogeneous numerical <code>dtype</code> (typically <code>numpy.float64</code>) then it should be fine for scikit-learn 0.15.2 and later. You might still need to normalize the data with <code>sklearn.preprocessing.StandardScaler</code> for instance.</p>

<p>If your data frame is heterogeneously typed, the <code>dtype</code> of the corresponding numpy array will be <code>object</code> which is not suitable for scikit-learn. You need to extract a numerical representation for all the relevant features (for instance by extracting dummy variables for categorical features) and drop the columns that are not suitable features (e.g. sample identifiers).</p>
";;0;;2015-01-19T08:47:42.283;;28020783;2015-01-19T08:47:42.283;;;;;163740.0;28017091.0;2;9;;;
22584;22584;;;"<p>call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html#pandas.concat"" rel=""noreferrer""><code>concat</code></a> and pass param <code>axis=1</code> to concatenate column-wise:</p>

<pre><code>In [5]:

pd.concat([df_a,df_b], axis=1)
Out[5]:
        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \
0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   
1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   
2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   

  Treatment1  inte1  
0          C   1100  
1          C   1050  
2          C   1010  
</code></pre>

<p>There is a useful guide to the various methods of <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"" rel=""noreferrer"">merging, joining and concatenating</a> online.</p>

<p>For example, as you have no clashing columns you can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html#pandas.DataFrame.merge"" rel=""noreferrer""><code>merge</code></a> and use the indices as they have the same number of rows:</p>

<pre><code>In [6]:

df_a.merge(df_b, left_index=True, right_index=True)
Out[6]:
        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \
0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   
1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   
2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   

  Treatment1  inte1  
0          C   1100  
1          C   1050  
2          C   1010  
</code></pre>

<p>And for the same reasons as above a simple <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html#pandas.DataFrame.join"" rel=""noreferrer""><code>join</code></a> works too:</p>

<pre><code>In [7]:

df_a.join(df_b)
Out[7]:
        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \
0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   
1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   
2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   

  Treatment1  inte1  
0          C   1100  
1          C   1050  
2          C   1010  
</code></pre>
";;10;;2015-01-25T10:37:10.867;;28135445;2015-01-25T10:42:36.283;2015-01-25T10:42:36.283;;704848.0;;704848.0;28135436.0;2;35;;;
22603;22603;;;"<p>I just tried editing this, but I wasn't logged in, so I'm not sure where my edit went. I was trying to incorporate multiple selection. So I think a better answer is:</p>

<p>For a single value, the most straightforward (human readable) is probably:</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>For lists of values you can also use:</p>

<pre><code>df.loc[df['column_name'].isin(some_values)]
</code></pre>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
               'B': 'one one two three two two one three'.split(),
               'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>If you have multiple criteria you want to select against, you can put them in a list and use 'isin':</p>

<pre><code>print(df.loc[df['B'].isin(['one','three'])])
</code></pre>

<p>yields</p>

<pre><code>      A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>Note, however, that if you wish to do this many times, it is more efficient to make A the index first, and then use df.loc:</p>

<pre><code>df = df.set_index(['A'])
print(df.loc['foo'])
</code></pre>

<p>yields</p>

<pre><code>  A      B  C   D
foo    one  0   0
foo    two  2   4
foo    two  4   8
foo    one  6  12
foo  three  7  14
</code></pre>
";;0;;2015-01-25T23:27:11.520;;28142820;2015-01-25T23:27:11.520;;;;;3954026.0;17071871.0;2;7;;;
22626;22626;;;"<p>If the variable holding the dataframe is called df, then:</p>

<pre><code>len(df.columns)
</code></pre>

<p>gives the number of columns.</p>

<p>And for those who want the number of rows:</p>

<pre><code>len(df.index)
</code></pre>

<p>For a tuple containing the number of both rows and columns:</p>

<pre><code>df.shape
</code></pre>
";;2;;2015-01-26T12:50:07.543;;28150450;2015-01-26T12:50:07.543;;;;;1585017.0;20297332.0;2;9;;;
22637;22637;;;"<p>You could use <code>select_dtypes</code> method of DataFrame. It includes two parameters include and exclude. So isNumeric would look like:</p>

<pre><code>numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

newdf = df.select_dtypes(include=numerics)
</code></pre>
";;2;;2015-01-26T17:39:29.420;;28155580;2015-01-26T17:39:29.420;;;;;1560279.0;25039626.0;2;38;;;
22642;22642;;;"<p>Filters can be chained using a Pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"">query</a>:</p>

<pre><code>df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])
df_filtered = df.query('a&gt;0').query('0&lt;b&lt;2')
</code></pre>

<p>Filters can also be combined in a single query:</p>

<pre><code>df_filtered = df.query('a&gt;0 and 0&lt;b&lt;2')
</code></pre>
";;2;;2015-01-26T21:44:19.280;;28159296;2015-01-26T21:44:19.280;;;;;3884938.0;11869910.0;2;47;;;
22646;22646;;;"<p>You can use <code>pd.to_datetime()</code> to convert to a datetime object. It takes a format parameter, but in your case I don't think you need it.</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame( {'Symbol':['A','A','A'] ,
    'Date':['02/20/2015','01/15/2016','08/21/2015']})
&gt;&gt;&gt; df
         Date Symbol
0  02/20/2015      A
1  01/15/2016      A
2  08/21/2015      A
&gt;&gt;&gt; df['Date'] =pd.to_datetime(df.Date)
&gt;&gt;&gt; df.sort('Date') # This now sorts in date order
        Date Symbol
0 2015-02-20      A
2 2015-08-21      A
1 2016-01-15      A
</code></pre>

<p>For future search, you can change the sort statement:</p>

<pre><code>&gt;&gt;&gt; df.sort_values(by='Date') # This now sorts in date order
        Date Symbol
0 2015-02-20      A
2 2015-08-21      A
1 2016-01-15      A
</code></pre>
";;6;;2015-01-27T00:45:58.577;;28161433;2016-07-05T09:44:57.160;2016-07-05T09:44:57.160;;911441.0;;2362381.0;28161356.0;2;27;;;
22693;22693;;;"<p>After painful experimentation to find something faster than the accepted answer, I got this to work. It ran around 100x faster on the dataset I tried it on.</p>

<p>If someone knows a way to make this more elegant, by all means please modify my code. I couldn't find a way that works without setting the other columns you want to keep as the index and then resetting the index and re-naming the columns, but I'd imagine there's something else that works.</p>

<pre><code>b = DataFrame(a.var1.str.split(',').tolist(), index=a.var2).stack()
b = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0
b.columns = ['var1', 'var2'] # renaming var1
</code></pre>
";;1;;2015-01-28T00:28:46.950;;28182629;2015-01-28T00:28:46.950;;;;;801698.0;12680754.0;2;36;;;
22714;22714;;;"<p>You can use list comprehension on a dataframe to count frequencies of the columns as such</p>

<pre><code>[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)]
</code></pre>

<p>Breakdown:</p>

<pre><code>my_series.select_dtypes(include=['O']) 
</code></pre>

<blockquote>
  <blockquote>
    <p>Selects just the categorical data</p>
  </blockquote>
</blockquote>

<pre><code>list(my_series.select_dtypes(include=['O']).columns) 
</code></pre>

<blockquote>
  <blockquote>
    <p>Turns the columns from above into a list</p>
  </blockquote>
</blockquote>

<pre><code>[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)] 
</code></pre>

<blockquote>
  <blockquote>
    <p>Iterates through the list above and applies value_counts() to each of the columns</p>
  </blockquote>
</blockquote>
";;0;;2015-01-28T12:28:15.337;;28192263;2015-01-28T12:28:15.337;;;;;786326.0;12207326.0;2;6;;;
22736;22736;;;"<p>For the second count I think just subtract the number of rows from the number of rows returned from <code>dropna</code>:</p>

<pre><code>In [14]:

from numpy.random import randn
df = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],
               columns=['one', 'two', 'three'])
df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
df
Out[14]:
        one       two     three
a -0.209453 -0.881878  3.146375
b       NaN       NaN       NaN
c  0.049383 -0.698410 -0.482013
d       NaN       NaN       NaN
e -0.140198 -1.285411  0.547451
f -0.219877  0.022055 -2.116037
g       NaN       NaN       NaN
h -0.224695 -0.025628 -0.703680
In [18]:

df.shape[0] - df.dropna().shape[0]
Out[18]:
3
</code></pre>

<p>The first could be achieved using the built in methods:</p>

<pre><code>In [30]:

df.isnull().values.ravel().sum()
Out[30]:
9
</code></pre>

<p><strong>Timings</strong></p>

<pre><code>In [34]:

%timeit sum([True for idx,row in df.iterrows() if any(row.isnull())])
%timeit df.shape[0] - df.dropna().shape[0]
%timeit sum(map(any, df.apply(pd.isnull)))
1000 loops, best of 3: 1.55 ms per loop
1000 loops, best of 3: 1.11 ms per loop
1000 loops, best of 3: 1.82 ms per loop
In [33]:

%timeit sum(df.isnull().values.ravel())
%timeit df.isnull().values.ravel().sum()
%timeit df.isnull().sum().sum()
1000 loops, best of 3: 215 s per loop
1000 loops, best of 3: 210 s per loop
1000 loops, best of 3: 605 s per loop
</code></pre>

<p>So my alternatives are a little faster for a df of this size</p>

<p><strong>Update</strong></p>

<p>So for a df with 80,000 rows I get the following:</p>

<pre><code>In [39]:

%timeit sum([True for idx,row in df.iterrows() if any(row.isnull())])
%timeit df.shape[0] - df.dropna().shape[0]
%timeit sum(map(any, df.apply(pd.isnull)))
%timeit np.count_nonzero(df.isnull())
1 loops, best of 3: 9.33 s per loop
100 loops, best of 3: 6.61 ms per loop
100 loops, best of 3: 3.84 ms per loop
1000 loops, best of 3: 395 s per loop
In [40]:

%timeit sum(df.isnull().values.ravel())
%timeit df.isnull().values.ravel().sum()
%timeit df.isnull().sum().sum()
%timeit np.count_nonzero(df.isnull().values.ravel())
1000 loops, best of 3: 675 s per loop
1000 loops, best of 3: 679 s per loop
100 loops, best of 3: 6.56 ms per loop
1000 loops, best of 3: 368 s per loop
</code></pre>

<p>Actually <code>np.count_nonzero</code> wins this hands down.</p>
";;10;;2015-01-28T18:19:34.570;;28199556;2015-01-28T18:56:56.307;2015-01-28T18:56:56.307;;704848.0;;704848.0;28199524.0;2;14;;;
22738;22738;;;"<p>What about <code>numpy.count_nonzero</code>:</p>

<pre><code> np.count_nonzero(df.isnull().values)   
 np.count_nonzero(df.isnull())           # also works  
</code></pre>

<p><code>count_nonzero</code> is pretty quick.  However, I constructed a dataframe from a (1000,1000) array and randomly inserted 100 nan values at different positions and measured the times of the various answers in iPython:</p>

<pre><code>%timeit np.count_nonzero(df.isnull().values)
1000 loops, best of 3: 1.89 ms per loop

%timeit df.isnull().values.ravel().sum()
100 loops, best of 3: 3.15 ms per loop

%timeit df.isnull().sum().sum()
100 loops, best of 3: 15.7 ms per loop
</code></pre>

<p>Not a huge time improvement over the OPs original but possibly less confusing in the code, your decision.  There isn't really any difference in execution time
between the two <code>count_nonzero</code> methods (with and without <code>.values</code>).</p>
";;1;;2015-01-28T18:52:37.547;;28200127;2015-01-28T18:52:37.547;;;;;991807.0;28199524.0;2;7;;;
22748;22748;;;"<p>Multiple years have passed since this answer was written so I though I might contribute a little bit. With the refactoring of pandas, attempting to set a value at a location with</p>

<pre><code>M.iloc[index][col]
</code></pre>

<p>May give you a warning about trying to set a value in a slice.</p>

<pre><code>SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
</code></pre>

<p>I believe the more ""pythonic"" way to do this at the current time is with the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.set_value.html"">pandas.DataFrame.set_value</a> instruction. Note that this instruction returns the resulting DataFrame.</p>

<pre><code>M.set_value(index,column,new_value)
</code></pre>

<p>I just thought I'd post this here after figuring out the source of the warnings that can be generated by the .iloc or .ix approaches.</p>

<p>The set_value approach also works for multiindex DataFrames by putting the multiple levels of the index in as a tuple (e.g. replacing column with (col,subcol) )</p>
";;1;;2015-01-28T21:30:29.080;;28202781;2015-11-17T16:57:30.187;2015-11-17T16:57:30.187;;1334771.0;;1373804.0;12100497.0;2;11;;;
22797;22797;;;"<p>You can index dataframe columns by the position using <code>ix</code>.</p>

<pre><code>df1.ix[:,1]
</code></pre>

<p>This returns the first column for example. (0 would be the index)</p>

<pre><code>df1.ix[0,]
</code></pre>

<p>This returns the first row.</p>

<pre><code>df1.ix[:,1]
</code></pre>

<p>This would be the value at the intersection of row 0 and column 1:</p>

<pre><code>df1.ix[0,1]
</code></pre>

<p>and so on. So you can <code>enumerate()</code> <code>returns.keys():</code> and use the number to index the dataframe.</p>
";;0;;2015-01-29T15:51:24.227;;28218909;2015-01-29T15:51:24.227;;;;;2362381.0;28218698.0;2;11;;;
22827;22827;;;"<p>Since you mind the objects in the columns being lists, I would use a generator to remove the lists wrapping your items:</p>

<pre><code>import pandas as pd
import numpy as np
rows = [(u'KY', [u'McConnell'], [u'Grimes'], [u'Rep']),
        (u'AR', [u'Cotton'], [u'Pryor'], [u'Dem']),
        (u'MI', [u'Land'], [u'Peters'], [])]

def get(r, nth):
    '''helper function to retrieve item from nth list in row r'''
    return r[nth][0] if r[nth] else np.nan

def remove_list_items(list_of_records):
    for r in list_of_records:
        yield r[0], get(r, 1), get(r, 2), get(r, 3)
</code></pre>

<p>The generator works similarly to this function, but instead of materializing a list unnecessarily in memory as an intermediate step, it just passes each row that would be in the list to the consumer of the list of rows:</p>

<pre><code>def remove_list_items(list_of_records):
    result = []
    for r in list_of_records:
        result.append((r[0], get(r, 1), get(r, 2), get(r, 3)))
    return result
</code></pre>

<p>And then compose your DataFrame passing your data through the generator, (or the list version, if you wish.)</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame.from_records(
        remove_list_items(rows), 
        columns=[""State"", ""R"", ""D"", ""incumbent""])
&gt;&gt;&gt; df
  State          R       D incumbent
0    KY  McConnell  Grimes       Rep
1    AR     Cotton   Pryor       Dem
2    MI       Land  Peters       NaN
</code></pre>

<p>Or you could use a list comprehension or a generator expression (shown) to do essentially the same:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame.from_records(
      ((r[0], get(r, 1), get(r, 2), get(r, 3)) for r in rows), 
      columns=[""State"", ""R"", ""D"", ""incumbent""])
</code></pre>
";;1;;2015-01-30T01:13:30.500;;28227679;2015-01-30T04:47:43.133;2015-01-30T04:47:43.133;;541136.0;;541136.0;28227612.0;2;9;;;
22838;22838;;;"<p>use <code>merge</code> if you are not joining on the index:</p>

<pre><code>merged = pd.merge(DataFrameA,DataFrameB, on=['Code','Date'])
</code></pre>

<p><strong>Follow up to question below:</strong></p>

<p>Here is a replicable example:</p>

<pre><code>import pandas as pd
# create some timestamps for date column
i = pd.to_datetime(pd.date_range('20140601',periods=2))

#create two dataframes to merge
df = pd.DataFrame({'code': ['ABC','EFG'], 'date':i,'col1': [10,100]})
df2 = pd.DataFrame({'code': ['ABC','EFG'], 'date':i,'col2': [10,200]})

#merge on columns (default join is inner)
pd.merge(df, df2, on =['code','date'])
</code></pre>

<p>This results is:</p>

<pre><code>    code    col1    date    col2
0   ABC     10      2014-06-01  10
1   EFG     100     2014-06-02  200
</code></pre>

<p>What happens when you run this code?</p>
";;1;;2015-01-30T04:21:10.383;;28229188;2016-01-06T17:19:42.300;2016-01-06T17:19:42.300;;2362381.0;;2362381.0;28228781.0;2;19;;;
22845;22845;;;"<p>It's quite common to use boolean indexing for this kind of task. With this method, you find out where column <code>a</code> is equal to <code>1</code> and then sum the corresponding rows of column <code>b</code>. You can use <code>loc</code> to handle the indexing:</p>

<pre><code>&gt;&gt;&gt; df.loc[df['a'] == 1, 'b'].sum()
15
</code></pre>

<p>The alternative approach is to use <code>groupby</code> to split the dataframe into parts according to the value in column <code>a</code>. You can then sum each part and pull out the value that the 1's added up to:</p>

<pre><code>&gt;&gt;&gt; df.groupby('a')['b'].sum()[1]
15
</code></pre>

<p>The <code>groupby</code> approach is much slower than using boolean indexing, but is useful if you want check the sums for other values in column <code>a</code>.</p>
";;6;;2015-01-30T12:53:51.827;;28236391;2015-10-22T21:50:44.490;2015-10-22T21:50:44.490;;3923281.0;;3923281.0;28236305.0;2;23;;;
22885;22885;;;"<p>i'd use the pandas replace function, very simple and powerful as you can use regex. Below i'm using the regex \D to remove any non-digit characters but obviously you could get quite creative with regex.</p>

<pre><code>data['result'].replace(regex=True,inplace=True,to_replace=r'\D',value=r'')
</code></pre>
";;2;;2015-01-31T14:57:04.193;;28252957;2015-01-31T14:57:04.193;;;;;4121920.0;13682044.0;2;19;;;
22925;22925;;;"<p>As written, your XML is not valid. I'm guessing it's supposed to be </p>

<pre><code>&lt;author type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com""&gt;
    &lt;documents count=""N""&gt;
        &lt;document KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...] [...]
]]&gt;
        &lt;/document&gt;
    &lt;/documents&gt;
&lt;/author&gt;
</code></pre>

<p>If my guess is correct, then you can easily use <code>xml</code> (from the Python standard library) to convert to a <code>pandas.DataFrame</code>.  If not, you'll have to either fix it or write something that will parse it as is.  Assuming you've fixed it or the original XML <em>is</em> valid, and you've just made a mistake in creating your example, here's what I would do</p>

<pre><code>import pandas as pd
import xml.etree.ElementTree as ET

def iter_docs(author):
    author_attr = author.attrib
    for doc in author.iterfind('.//document'):
        doc_dict = author_attr.copy()
        doc_dict.update(doc.attrib)
        doc_dict['data'] = doc.text
        yield doc_dict

xml_data='''\
&lt;author type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com""&gt;
    &lt;documents count=""N""&gt;
        &lt;document KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]
]]&gt;
        &lt;/document&gt;
        &lt;document KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...] [...]
]]&gt;
        &lt;/document&gt;
    &lt;/documents&gt;
&lt;/author&gt;
'''

etree = ET.fromstring(xml_data) #create an ElementTree object 
doc_df = pd.DataFrame(list(iter_docs(etree)))
</code></pre>

<p>If there are multiple authors in your original document, then I would add the following generator:</p>

<pre><code>def iter_author(etree):
    for author in etree.iterfind('.//author'):
        for row in iter_docs(author):
            yield row
</code></pre>

<p>and change <code>doc_df = pd.DataFrame(list(iter_docs(etree)))</code> to <code>doc_df = pd.DataFrame(list(iter_author(etree)))</code></p>

<p>Have a look at the <code>ElementTree</code> <a href=""https://docs.python.org/2/library/xml.etree.elementtree.html"" rel=""noreferrer"">tutorial</a> provided in the <code>xml</code> library <a href=""https://docs.python.org/2/library/xml.html"" rel=""noreferrer"">documentation</a></p>
";;0;;2015-02-01T20:08:36.743;;28267291;2015-02-01T20:08:36.743;;;;;1940479.0;28259301.0;2;16;;;
22932;22932;;;"<pre><code>In [5]:

import pandas as pd

test = {
383:    3.000000,
663:    1.000000,
726:    1.000000,
737:    9.000000,
833:    8.166667
}

s = pd.Series(test)
s = s[s != 1]
s
Out[0]:
383    3.000000
737    9.000000
833    8.166667
dtype: float64
</code></pre>
";;2;;2015-02-02T06:31:31.357;;28272238;2016-01-04T21:57:59.043;2016-01-04T21:57:59.043;;690430.0;;4340076.0;28272137.0;2;29;;;
22987;22987;;;"<h2>Version 1:</h2>

<p>You can create your axis, and then use the <code>ax</code> keyword of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.plot.html"" rel=""noreferrer""><code>DataFrameGroupBy.plot</code></a> to add everything to this axis:</p>

<pre><code>import matplotlib.pyplot as plt

p_df = pd.DataFrame({""class"": [1,1,2,2,1], ""a"": [2,3,2,3,2]})
fig, ax = plt.subplots(figsize=(8,6))
bp = p_df.groupby('class').plot(kind='kde', ax=ax)
</code></pre>

<p>This is the result:</p>

<p><img src=""https://i.stack.imgur.com/7N1E4.png"" alt=""plot""></p>

<p>Unfortunately, the labeling of the legend does not make too much sense here.</p>

<h2>Version 2:</h2>

<p>Another way would be to loop through the groups and plot the curves manually:</p>

<pre><code>classes = [""class 1""] * 5 + [""class 2""] * 5
vals = [1,3,5,1,3] + [2,6,7,5,2]
p_df = pd.DataFrame({""class"": classes, ""vals"": vals})

fig, ax = plt.subplots(figsize=(8,6))
for label, df in p_df.groupby('class'):
    df.vals.plot(kind=""kde"", ax=ax, label=label)
plt.legend()
</code></pre>

<p>This way you can easily control the legend. This is the result:</p>

<p><img src=""https://i.stack.imgur.com/EIajP.png"" alt=""plot2""></p>
";;0;;2015-02-03T12:40:33.177;;28299215;2017-01-21T14:29:30.683;2017-01-21T14:29:30.683;;454773.0;;2272172.0;28293028.0;2;29;;;
23032;23032;;;"<p>There's a flag for that:</p>

<pre><code>In [11]: df = pd.DataFrame([[""foo1""], [""foo2""], [""bar""], [np.nan]], columns=['a'])

In [12]: df.a.str.contains(""foo"")
Out[12]:
0     True
1     True
2    False
3      NaN
Name: a, dtype: object

In [13]: df.a.str.contains(""foo"", na=False)
Out[13]:
0     True
1     True
2    False
3    False
Name: a, dtype: bool
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.strings.StringMethods.replace.html""><code>str.replace</code></a> docs:</p>

<blockquote>
  <p>na : default NaN, fill value for missing values.</p>
</blockquote>

<hr>

<p>So you can do the following:</p>

<pre><code>In [21]: df.loc[df.a.str.contains(""foo"", na=False)]
Out[21]:
      a
0  foo1
1  foo2
</code></pre>
";;2;;2015-02-04T01:35:31.080;;28312011;2015-02-04T01:35:31.080;;;;;1240268.0;28311655.0;2;40;;;
23159;23159;;;"<p>you can use <code>sort_index</code>:</p>

<pre><code> df.sort_index(by='col1', ascending=False)
</code></pre>

<p>This outputs:</p>

<pre><code>             col1
idx1    idx2    
1       C    105
        B    102
        A    99
        D    97
2       A    19
        D    17
        B    14
        C    10
</code></pre>
";;6;;2015-02-06T17:40:57.377;;28371611;2015-02-06T20:20:32.993;2015-02-06T20:20:32.993;;2362381.0;;2362381.0;28371308.0;2;11;;;
23160;23160;;;"<p>This is an older thread, but I just wanted to dump my workaround solution here. I initially tried the <code>chunksize</code> parameter (even with quite small values like 10000), but it didn't help much; had still technical issues with the memory size (my CSV was ~ 7.5 Gb). </p>

<p>Right now, I just read chunks of the CSV files in a for-loop approach and add them e.g., to an SQLite database step by step:</p>

<pre><code>import pandas as pd
import sqlite3
from pandas.io import sql
import subprocess

# In and output file paths
in_csv = '../data/my_large.csv'
out_sqlite = '../data/my.sqlite'

table_name = 'my_table' # name for the SQLite database table
chunksize = 100000 # number of lines to process at each iteration

# columns that should be read from the CSV file
columns = ['molecule_id','charge','db','drugsnow','hba','hbd','loc','nrb','smiles']

# Get number of lines in the CSV file
nlines = subprocess.check_output('wc -l %s' % in_csv, shell=True)
nlines = int(nlines.split()[0]) 

# connect to database
cnx = sqlite3.connect(out_sqlite)

# Iteratively read CSV and dump lines into the SQLite table
for i in range(0, nlines, chunksize):

    df = pd.read_csv(in_csv,  
            header=None,  # no header, define column header manually later
            nrows=chunksize, # number of rows to read at each iteration
            skiprows=i)   # skip rows that were already read

    # columns to read        
    df.columns = columns

    sql.to_sql(df, 
                name=table_name, 
                con=cnx, 
                index=False, # don't use CSV file index
                index_label='molecule_id', # use a unique column from DataFrame as index
                if_exists='append') 
cnx.close()    
</code></pre>
";;3;;2015-02-06T17:46:34.837;;28371706;2015-02-06T20:46:56.283;2015-02-06T20:46:56.283;;2489252.0;;2489252.0;11622652.0;2;31;;;
23201;23201;;;"<p>Unfortunately those two are incompatible. A <code>CountVectorizer</code> produces a sparse matrix and the RandomForestClassifier requires a dense matrix. It is possible to convert using <code>X.todense()</code>. Doing this will substantially increase your memory footprint.</p>

<p>Below is sample code to do this that I found at <a href=""http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"">http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html</a> which allows you to call .todense() in a pipeline stage.</p>

<pre><code>class DenseTransformer(TransformerMixin):

    def transform(self, X, y=None, **fit_params):
        return X.todense()

    def fit_transform(self, X, y=None, **fit_params):
        self.fit(X, y, **fit_params)
        return self.transform(X)

    def fit(self, X, y=None, **fit_params):
        return self
</code></pre>

<p>Once you have your <code>DenseTransformer</code>, you are able to add it as a pipeline step.</p>

<pre><code>pipeline = Pipeline([
     ('vectorizer', CountVectorizer()), 
     ('to_dense', DenseTransformer()), 
     ('classifier', RandomForestClassifier())
])
</code></pre>

<p>Another option would be to use a classifier meant for sparse data like <code>LinearSVC</code>. </p>

<pre><code>from sklearn.svm import LinearSVC
pipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', LinearSVC())])
</code></pre>
";;3;;2015-02-07T17:03:01.180;;28384887;2016-01-12T06:31:28.407;2016-01-12T06:31:28.407;;2673526.0;;2673526.0;28384680.0;2;28;;;
23224;23224;;;"<p>Slightly shorter is:</p>

<pre><code>df = df.fillna('')
</code></pre>

<p>This will fill na's (e.g. NaN's) with ''.</p>

<p>Edit:
If you want to fill a single column, you can use:</p>

<pre><code>df.column1 = df.column1.fillna('')
</code></pre>
";;3;;2015-02-08T05:44:58.460;;28390992;2016-04-01T02:19:45.753;2016-04-01T02:19:45.753;;3427777.0;;3427777.0;26837998.0;2;95;;;
23275;23275;;;"<p>The <a href=""https://rpy2.readthedocs.io/"" rel=""nofollow noreferrer""><code>rpy2</code></a> module is made for this:</p>

<pre><code>from rpy2.robjects import r, pandas2ri
pandas2ri.activate()

r['iris'].head()
</code></pre>

<p>yields</p>

<pre><code>   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species
1           5.1          3.5           1.4          0.2  setosa
2           4.9          3.0           1.4          0.2  setosa
3           4.7          3.2           1.3          0.2  setosa
4           4.6          3.1           1.5          0.2  setosa
5           5.0          3.6           1.4          0.2  setosa
</code></pre>

<hr>

<p>Up to pandas 0.19 you could use pandas' own <a href=""https://pandas.pydata.org/pandas-docs/stable/r_interface.html"" rel=""nofollow noreferrer""><code>rpy</code></a> interface:</p>

<pre><code>import pandas.rpy.common as rcom
iris = rcom.load_data('iris')
print(iris.head())
</code></pre>

<p>yields</p>

<pre><code>   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species
1           5.1          3.5           1.4          0.2  setosa
2           4.9          3.0           1.4          0.2  setosa
3           4.7          3.2           1.3          0.2  setosa
4           4.6          3.1           1.5          0.2  setosa
5           5.0          3.6           1.4          0.2  setosa
</code></pre>

<hr>

<p><code>rpy2</code> also provides a way <a href=""http://rpy.sourceforge.net/rpy2/doc-2.3/html/robjects_convert.html"" rel=""nofollow noreferrer"">to convert <code>R</code> objects into Python objects</a>:</p>

<pre><code>import pandas as pd
import rpy2.robjects as ro
import rpy2.robjects.conversion as conversion
from rpy2.robjects import pandas2ri
pandas2ri.activate()

R = ro.r

df = conversion.ri2py(R['mtcars'])
print(df.head())
</code></pre>

<p>yields</p>

<pre><code>    mpg  cyl  disp   hp  drat     wt   qsec  vs  am  gear  carb
0  21.0    6   160  110  3.90  2.620  16.46   0   1     4     4
1  21.0    6   160  110  3.90  2.875  17.02   0   1     4     4
2  22.8    4   108   93  3.85  2.320  18.61   1   1     4     1
3  21.4    6   258  110  3.08  3.215  19.44   1   0     3     1
4  18.7    8   360  175  3.15  3.440  17.02   0   0     3     2
</code></pre>
";;4;;2015-02-09T19:04:55.760;;28417338;2017-06-06T10:31:47.083;2017-06-06T10:31:47.083;;190597.0;;190597.0;28417293.0;2;13;;;
23280;23280;;;"<p>Any publically available .csv file can be loaded into pandas extremely quickly using its URL. Here is an example using one of the UCLA .csv datasets.</p>

<pre><code>import pandas as pd

file_name = ""http://www.ats.ucla.edu/stat/data/binary.csv""

df = pd.read_csv(file_name)

df.head()
</code></pre>

<p>The output here being the .csv file header you just loaded from the given URL.</p>

<pre><code>&gt;&gt;&gt;df.head()
   admit  gre   gpa  prestige
0      0  380  3.61         3
1      1  660  3.67         3
2      1  800  4.00         1
3      1  640  3.19         4
4      0  520  2.93         4
</code></pre>
";;0;;2015-02-09T21:37:13.727;;28419871;2015-02-09T21:37:13.727;;;;;2917381.0;28417293.0;2;7;;;
23393;23393;;;"<p>you can create the dummies from the single new observation, and then reindex this frames columns using the columns from the original indicator matrix:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'cat':['a','b','c','d'],'val':[1,2,5,10]})
df1 = pd.get_dummies(pd.DataFrame({'cat':['a'],'val':[1]}))
dummies_frame = pd.get_dummies(df)
df1.reindex(columns = dummies_frame.columns, fill_value=0)
</code></pre>

<p>returns:</p>

<pre><code>        val     cat_a   cat_b   cat_c   cat_d
  0     1       1       0       0       0
</code></pre>
";;6;;2015-02-11T23:33:19.463;;28466662;2016-11-02T22:50:35.160;2016-11-02T22:50:35.160;;1636598.0;;2362381.0;28465633.0;2;13;;;
23412;23412;;;"<p>Like this?</p>

<pre><code>dfTest = pd.DataFrame({
           'A':[14.00,90.20,90.95,96.27,91.21],
           'B':[103.02,107.26,110.35,114.23,114.68], 
           'C':['big','small','big','small','small']
         })
dfTest[['A','B']] = dfTest[['A','B']].apply(
                           lambda x: MinMaxScaler().fit_transform(x))
dfTest

    A           B           C
0   0.000000    0.000000    big
1   0.926219    0.363636    small
2   0.935335    0.628645    big
3   1.000000    0.961407    small
4   0.938495    1.000000    small
</code></pre>
";;3;;2015-02-12T13:51:03.917;;28479181;2016-05-15T13:43:17.910;2016-05-15T13:43:17.910;;85150.0;;4559546.0;24645153.0;2;15;;;
23450;23450;;;"<p>Posting @joris 's comment as an answer:</p>

<blockquote>
  <p>display(df) (with from IPython.display import display), or print df.to_html()</p>
</blockquote>

<hr>

<p>Or in code :</p>

<pre><code>from IPython.display import display
display(df)  # OR
print df.to_html()
</code></pre>
";;2;;2015-02-13T19:28:17.763;;28507257;2016-12-26T01:38:00.440;2016-12-26T01:38:00.440;;5546267.0;;4172145.0;26873127.0;2;20;;;
23496;23496;;;"<p>I don't know what you mean by inefficient but if you mean in terms of typing it could be easier to just select the cols of interest and assign back to the df:</p>

<pre><code>df = df[cols_of_interest]
</code></pre>

<p>Where <code>cols_of_interest</code> is a list of the columns you care about.</p>

<p>Or you can slice the columns and pass this to <code>drop</code>:</p>

<pre><code>df.drop(df.ix[:,'Unnamed: 24':'Unnamed: 60'].head(0).columns, axis=1)
</code></pre>

<p>The call to <code>head</code> just selects 0 rows as we're only interested in the column names rather than data</p>

<p><strong>update</strong></p>

<p>Another method would be simpler would be to use the boolean mask from <code>str.contains</code> and invert it to mask the columns:</p>

<pre><code>In [2]:
df = pd.DataFrame(columns=['a','Unnamed: 1', 'Unnamed: 1','foo'])
df

Out[2]:
Empty DataFrame
Columns: [a, Unnamed: 1, Unnamed: 1, foo]
Index: []

In [4]:
~df.columns.str.contains('Unnamed:')

Out[4]:
array([ True, False, False,  True], dtype=bool)

In [5]:
df[df.columns[~df.columns.str.contains('Unnamed:')]]

Out[5]:
Empty DataFrame
Columns: [a, foo]
Index: []
</code></pre>
";;2;;2015-02-16T09:58:11.210;;28538738;2016-05-15T08:13:20.323;2016-05-15T08:13:20.323;;704848.0;;704848.0;28538536.0;2;13;;;
23499;23499;;;"<p>This is probably a good way to do what you want. It will delete all columns that contain 'Unnamed' in their header.</p>

<pre><code>for col in df.columns:
    if 'Unnamed' in col:
        del df[col]
</code></pre>
";;2;;2015-02-16T11:26:33.240;;28540395;2015-02-16T11:26:33.240;;;;;2930651.0;28538536.0;2;10;;;
23502;23502;;;"<p>You can edit a subset of a dataframe by using loc:</p>

<pre><code>df.loc[&lt;row selection&gt;, &lt;column selection&gt;]
</code></pre>

<p>In this case:</p>

<pre><code>w.loc[w.female != 'female', 'female'] = 0
w.loc[w.female == 'female', 'female'] = 1
</code></pre>
";;2;;2015-02-16T12:27:17.763;;28541443;2015-02-16T12:27:17.763;;;;;2874515.0;23307301.0;2;45;;;
23545;23545;;;"<p>This will be possible with seaborn version 0.6 (currently in the master branch on github) using the <code>stripplot</code> function. Here's an example:</p>

<pre><code>import seaborn as sns
tips = sns.load_dataset(""tips"")
sns.boxplot(x=""day"", y=""total_bill"", data=tips)
sns.stripplot(x=""day"", y=""total_bill"", data=tips,
              size=4, jitter=True, edgecolor=""gray"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/6Gszv.png"" alt=""enter image description here""></p>
";;0;;2015-02-17T16:16:43.903;;28565940;2015-02-17T16:16:43.903;;;;;1533576.0;23519135.0;2;12;;;
23601;23601;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_json.html""><code>orient='records'</code></a></p>

<pre><code>print df.reset_index().to_json(orient='records')

[
     {""id"":0,""location"":""[50, 50]""},
     {""id"":1,""location"":""[60, 60]""},
     {""id"":2,""location"":""[70, 70]""},
     {""id"":3,""location"":""[80, 80]""}
]
</code></pre>
";;2;;2015-02-18T18:13:49.687;;28590865;2015-02-18T18:13:49.687;;;;;1744834.0;28590663.0;2;29;;;
23612;23612;;;"<pre><code>test3 = pd.concat([test1, test2], axis=1)
test3.columns = ['a','b']
</code></pre>
";;3;;2015-02-18T23:11:25.453;;28595765;2015-02-18T23:11:25.453;;;;;1168043.0;28595701.0;2;28;;;
23654;23654;;;"<p>As suggested by lgautier, it can be done with <code>pandas2ri</code>.</p>

<p>Here is sample code for convert rpy dataframe (<code>rdf</code>) to pandas dataframe (<code>pd_df</code>):</p>

<pre><code>from rpy2.robjects import pandas2ri

pd_df = pandas2ri.ri2py_dataframe(rdf)
</code></pre>
";;0;;2015-02-19T16:43:12.510;;28611938;2016-06-22T18:08:24.690;2016-06-22T18:08:24.690;;1306923.0;;1759944.0;20630121.0;2;7;;;
23753;23753;;;"<p>Random forests in 0.16-dev now accept sparse data. </p>
";;0;;2015-02-21T17:05:07.443;;28648525;2015-02-21T17:05:07.443;;;;;2246788.0;28384680.0;2;14;;;
23754;23754;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer""><code>pd.to_numeric</code></a> (introduced in version 0.17) to convert a column or a Series to a numeric type. The function can also be applied over multiple columns of a DataFrame using <code>apply</code>.</p>

<p>Importantly, the function also takes an <code>errors</code> key word argument that lets you force not-numeric values to be <code>NaN</code>, or simply ignore columns containing these values.</p>

<p>Example uses are shown below.</p>

<h3>Individual column / Series</h3>

<p>Here's an example using a Series of strings <code>s</code> which has the object dtype:</p>

<pre><code>&gt;&gt;&gt; s = pd.Series(['1', '2', '4.7', 'pandas', '10'])
&gt;&gt;&gt; s
0         1
1         2
2       4.7
3    pandas
4        10
dtype: object
</code></pre>

<p>The function's default behaviour is to raise if it can't convert a value. In this case, it can't cope with the string 'pandas':</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')
ValueError: Unable to parse string
</code></pre>

<p>Rather than fail, we might want 'pandas' to be considered a missing/bad value. We can coerce invalid values to <code>NaN</code> as follows:</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s, errors='coerce')
0     1.0
1     2.0
2     4.7
3     NaN
4    10.0
dtype: float64
</code></pre>

<p>The third option is just to ignore the operation if an invalid value is encountered:</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s, errors='ignore')
# the original Series is returned untouched
</code></pre>

<h3>Multiple columns / entire DataFrames</h3>

<p>We might want to apply this operation to multiple columns. Processing each column in turn is tedious, so we can use <code>DataFrame.apply</code> to have the function act on each column.</p>

<p>Borrowing the DataFrame from the question:</p>

<pre><code>&gt;&gt;&gt; a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
&gt;&gt;&gt; df = pd.DataFrame(a, columns=['col1','col2','col3'])
&gt;&gt;&gt; df
  col1 col2  col3
0    a  1.2   4.2
1    b   70  0.03
2    x    5     0
</code></pre>

<p>Then we can write:</p>

<pre><code>df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)
</code></pre>

<p>and now 'col2' and 'col3' have dtype <code>float64</code> as desired.</p>

<p>However, we might not know which of our columns can be converted reliably to a numeric type. In that case we can just write:</p>

<pre><code>df.apply(pd.to_numeric, errors='ignore')
</code></pre>

<p>Then the function will be applied to the <em>whole</em> DataFrame. Columns that can be converted to a numeric type will be converted, while columns that cannot (e.g. they contain non-digit strings or dates) will be left alone.</p>

<p>There is also <code>pd.to_datetime</code> and <code>pd.to_timedelta</code> for conversion to dates and timestamps.</p>

<h3>Soft conversions</h3>

<p>Version 0.21.0 introduces the method <code>infer_objects()</code> for converting columns of a DataFrame that have an object datatype to a more specific type.</p>

<p>For example, let's create a DataFrame with two columns of object type, with one holding integers and the other holding strings of integers:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')
&gt;&gt;&gt; df.dtypes
a    object
b    object
dtype: object
</code></pre>

<p>Then using <code>infer_objects()</code>, we can change the type of column 'a' to int64:</p>

<pre><code>&gt;&gt;&gt; df = df.infer_objects()
&gt;&gt;&gt; df.dtypes
a     int64
b    object
dtype: object
</code></pre>

<p>Column 'b' has been left alone since its values were strings, not integers. If we wanted to try and force the conversion of both columns to an integer type, we could use <code>df.astype(int)</code> instead.</p>
";;7;;2015-02-21T17:37:02.897;;28648923;2017-07-29T10:40:53.817;2017-07-29T10:40:53.817;;3923281.0;;3923281.0;15891038.0;2;270;;;
23763;23763;;;"<p>Here's an example DataFrame which show this, it has duplicate values with the same index. The question is, do you want to aggregate these or keep them as multiple rows?</p>

<pre><code>In [11]: df
Out[11]:
   0  1  2      3
0  1  2  a  16.86
1  1  2  a  17.18
2  1  4  a  17.03
3  2  5  b  17.28

In [12]: df.pivot_table(values=3, index=[0, 1], columns=2, aggfunc='mean')  # desired?
Out[12]:
2        a      b
0 1
1 2  17.02    NaN
  4  17.03    NaN
2 5    NaN  17.28

In [13]: df1 = df.set_index([0, 1, 2])

In [14]: df1
Out[14]:
           3
0 1 2
1 2 a  16.86
    a  17.18
  4 a  17.03
2 5 b  17.28

In [15]: df1.unstack(2)
ValueError: Index contains duplicate entries, cannot reshape
</code></pre>

<hr>

<p>One solution is to <code>reset_index</code> (and get back to <code>df</code>) and use <code>pivot_table</code>.</p>

<pre><code>In [16]: df1.reset_index().pivot_table(values=3, index=[0, 1], columns=2, aggfunc='mean')
Out[16]:
2        a      b
0 1
1 2  17.02    NaN
  4  17.03    NaN
2 5    NaN  17.28
</code></pre>

<p>Another option (if you don't want to aggregate) is to append a dummy level, unstack it, then drop the dummy level...</p>
";;1;;2015-02-21T22:29:20.803;;28652153;2015-02-21T22:29:20.803;;;;;1240268.0;28651079.0;2;19;;;
23837;23837;;;"<p>Your idea with <code>cumsum</code> and <code>diff</code> works. It doesn't look too complicated; not sure if there's an even shorter solution. First, we compute the cumulative sum, operate on that, and then go back (<code>diff</code> is kinda sorta the inverse function of <code>cumsum</code>).</p>

<pre><code>import math

c = values.cumsum() - ALLOWANCE
# now we've got [-15, -5, 20, 50]
c[c &lt; 0] = 0 # negative values don't make sense here

# (c - c.shift(1)) # &lt;-- what I had first: diff by accident

# it is important that we don't fill with 0, in case that the first
# value is greater than ALLOWANCE
c.diff().fillna(math.max(0, values[0] - ALLOWANCE))
</code></pre>
";;0;;2015-02-23T15:33:54.177;;28677342;2015-02-23T15:33:54.177;;;;;1071311.0;28676916.0;2;8;;;
23838;23838;;;"<p>Following your initial idea of <code>cumsum</code> and <code>diff</code>, you could write:</p>

<pre><code>&gt;&gt;&gt; (values.cumsum() - ALLOWANCE).clip_lower(0).diff().fillna(0)
0     0
1     0
2    20
3    30
dtype: float64
</code></pre>

<p>This is the cumulative sum of <code>values</code> minus the allowance. Negative values are clipped to zeros (since we don't care about numbers until we have overdrawn our allowance). From there, you can calculate the difference.</p>

<p>However, if the first value might be greater than the allowance, the following two-line variation is preferred:</p>

<pre><code>s = (values.cumsum() - ALLOWANCE).clip_lower(0)
desired = s.diff().fillna(s)
</code></pre>

<p>This fills the first <code>NaN</code> value with the ""first value - allowance"" value. So in the case where <code>ALLOWANCE</code> is lowered to 75, it returns <code>desired</code> as <code>Series([10, 10, 25, 30])</code>.</p>
";;4;;2015-02-23T15:34:31.950;;28677358;2015-03-10T16:05:02.537;2015-03-10T16:05:02.537;;3923281.0;;3923281.0;28676916.0;2;10;;;
23848;23848;;;"<p>pandas has vectorized string operations, so you can just filter out the rows that contain the string you don't want:</p>

<pre><code>In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))

In [92]: df
Out[92]:
   A          C
0  5        foo
1  3        bar
2  5  fooXYZbar
3  6        bat

In [93]: df[df.C.str.contains(""XYZ"") == False]
Out[93]:
   A    C
0  5  foo
1  3  bar
3  6  bat
</code></pre>
";;1;;2015-02-23T17:49:32.777;;28680078;2015-02-23T17:49:32.777;;;;;417414.0;28679930.0;2;39;;;
23987;23987;;;"<p>You can also use the index for the sheet:</p>

<pre><code>xls = pd.ExcelFile('path_to_file.xls')
sheet1 = xls.parse(0)
</code></pre>

<p>will give the first worksheet. for the second worksheet:</p>

<pre><code>sheet2 = xls.parse(1)
</code></pre>
";;1;;2015-02-25T22:58:34.673;;28731311;2015-02-25T22:58:34.673;;;;;401737.0;26521266.0;2;12;;;
24054;24054;;;"<p>I set up a direct comparison to test them, found that their assumptions can <a href=""https://stackoverflow.com/questions/28755617/why-do-r-and-statsmodels-give-slightly-different-anova-results"" title=""differ slightly"">differ slightly</a> , got a hint from a statistician, and here is an example of ANOVA on a pandas dataframe matching R's results:</p>

<pre><code>import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols


# R code on R sample dataset

#&gt; anova(with(ChickWeight, lm(weight ~ Time + Diet)))
#Analysis of Variance Table
#
#Response: weight
#           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)
#Time        1 2042344 2042344 1576.460 &lt; 2.2e-16 ***
#Diet        3  129876   43292   33.417 &lt; 2.2e-16 ***
#Residuals 573  742336    1296
#write.csv(file='ChickWeight.csv', x=ChickWeight, row.names=F)

cw = pd.read_csv('ChickWeight.csv')

cw_lm=ols('weight ~ Time + C(Diet)', data=cw).fit() #Specify C for Categorical
print(sm.stats.anova_lm(cw_lm, typ=2))
#                  sum_sq   df            F         PR(&gt;F)
#C(Diet)    129876.056995    3    33.416570   6.473189e-20
#Time      2016357.148493    1  1556.400956  1.803038e-165
#Residual   742336.119560  573          NaN            NaN
</code></pre>
";;0;;2015-02-27T01:40:31.847;;28756099;2015-02-27T01:40:31.847;2017-05-23T12:17:55.503;;-1.0;;1168043.0;25537399.0;2;12;;;
24125;24125;;;"<p>Here's one alternative method - it doesn't use any of Pandas' functionality (so might not be completely in the spirit of your question), but it is fast and concise. </p>

<p>You can temporarily adjust the precision unit of the underlying NumPy <code>datetime64</code> datatype, changing it from <code>[ns]</code> to <code>[h]</code>:</p>

<pre><code>df['dt'].values.astype('&lt;M8[h]')
</code></pre>

<p>This truncates everything to hour precision. For example:</p>

<pre><code>&gt;&gt;&gt; df
                       dt
0     2014-10-01 10:02:45
1     2014-10-01 13:08:17
2     2014-10-01 17:39:24

&gt;&gt;&gt; df['dt2'] = df['dt'].values.astype('&lt;M8[h]')
&gt;&gt;&gt; df
                      dt                     dt2
0    2014-10-01 10:02:45     2014-10-01 10:00:00
1    2014-10-01 13:08:17     2014-10-01 13:00:00
2    2014-10-01 17:39:24     2014-10-01 17:00:00

&gt;&gt;&gt; df.dtypes
dt     datetime64[ns]
dt2    datetime64[ns]
</code></pre>

<p>The same method should work for any other unit: months <code>'M'</code>, minutes <code>'m'</code>, and so on:</p>

<ul>
<li>Keep up to year: <code>'&lt;M8[Y]'</code></li>
<li>Keep up to month: <code>'&lt;M8[M]'</code></li>
<li>Keep up to day: <code>'&lt;M8[D]'</code></li>
<li>Keep up to minute: <code>'&lt;M8[m]'</code></li>
<li>Keep up to second: <code>'&lt;M8[s]'</code></li>
</ul>
";;1;;2015-02-28T16:24:54.190;;28783971;2017-07-06T20:38:19.110;2017-07-06T20:38:19.110;;3923281.0;;3923281.0;28773342.0;2;15;;;
24299;24299;;;"<p>Rather than</p>

<pre><code>gb.get_group('foo')
</code></pre>

<p>I prefer using <code>gb.groups</code></p>

<pre><code>df.loc[gb.groups['foo']]
</code></pre>

<p>Because in this way you can choose multiple columns as well. for example:</p>

<pre><code>df.loc[gb.groups['foo'],('A','B')]
</code></pre>
";;1;;2015-03-04T05:25:09.863;;28847219;2015-03-04T05:25:09.863;;;;;3513420.0;14734533.0;2;12;;;
24371;24371;;;"<p>The method you are looking for is Series.combine. 
However, it seems some care has to be taken around datatypes. 
In your example, you would (as I did when testing the answer) naively call </p>

<pre><code>df['col_3'] = df.col_1.combine(df.col_2, func=get_sublist)
</code></pre>

<p>However, this throws the error: </p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p>My best guess is that it seems to expect the result to be of the same type as the series calling the method (df.col_1 here). However, the following works:</p>

<pre><code>df['col_3'] = df.col_1.astype(object).combine(df.col_2, func=get_sublist)

df

   ID   col_1   col_2   col_3
0   1   0   1   [a, b]
1   2   2   4   [c, d, e]
2   3   3   5   [d, e, f]
</code></pre>
";;1;;2015-03-05T15:20:26.063;;28881373;2015-03-05T15:20:26.063;;;;;3868428.0;13331698.0;2;9;;;
24374;24374;;;"<p>I wanted to do the same, and I had first problems with the split finction, then problems with installing pandas 0.15.2, so I went back to my old version, and wrote a little function that works very well. I hope this can help!</p>

<pre><code># input - df: a Dataframe, chunkSize: the chunk size
# output - a list of DataFrame
# purpose - splits the DataFrame into smaller of max size chunkSize (last is smaller)
def splitDataFrameIntoSmaller(df, chunkSize = 10000): 
    listOfDf = list()
    numberChunks = len(df) // chunkSize + 1
    for i in range(numberChunks):
        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])
    return listOfDf
</code></pre>
";;1;;2015-03-05T15:49:46.663;;28882020;2015-03-05T15:49:46.663;;;;;4587967.0;17315737.0;2;6;;;
24415;24415;;;"<p>Using the Pandas type <a href=""http://pandas.pydata.org/pandas-docs/dev/timedeltas.html""><code>Timedelta</code></a> available since v0.15.0 you also can do:</p>

<pre><code>In[1]: import pandas as pd
In[2]: df = pd.DataFrame([ pd.Timestamp('20150111'), 
                           pd.Timestamp('20150301') ], columns=['date'])
In[3]: df['today'] = pd.Timestamp('20150315')
In[4]: df
Out[4]: 
        date      today
0 2015-01-11 2015-03-15
1 2015-03-01 2015-03-15

In[5]: (df['today'] - df['date']).dt.days
Out[5]: 
0    63
1    14
dtype: int64
</code></pre>
";;0;;2015-03-06T13:50:33.917;;28900208;2015-03-06T13:50:33.917;;;;;1071319.0;16103238.0;2;19;;;
24422;24422;;;"<p>One method would be to store the result of an inner merge form both dfs, then we can simply select the rows when one column's values are not in this common:</p>

<pre><code>In [119]:

common = df1.merge(df2,on=['col1','col2'])
print(common)
df1[(~df1.col1.isin(common.col1))&amp;(~df1.col2.isin(common.col2))]
   col1  col2
0     1    10
1     2    11
2     3    12
Out[119]:
   col1  col2
3     4    13
4     5    14
</code></pre>

<p><strong>EDIT</strong></p>

<p>Another method as you've found is to use <code>isin</code> which will produce <code>NaN</code> rows which you can drop:</p>

<pre><code>In [138]:

df1[~df1.isin(df2)].dropna()
Out[138]:
   col1  col2
3     4    13
4     5    14
</code></pre>

<p>However if df2 does not start rows in the same manner then this won't work:</p>

<pre><code>df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})
</code></pre>

<p>will produce the entire df:</p>

<pre><code>In [140]:

df1[~df1.isin(df2)].dropna()
Out[140]:
   col1  col2
0     1    10
1     2    11
2     3    12
3     4    13
4     5    14
</code></pre>
";;6;;2015-03-06T15:35:38.897;;28902170;2015-03-06T15:52:30.980;2015-03-06T15:52:30.980;;704848.0;;704848.0;28901683.0;2;46;;;
24482;24482;;;"<p>The most succinct way to go is:</p>

<pre><code>x.plot(kind=""bar"").legend(bbox_to_anchor=(1.2, 0.5))
</code></pre>

<p>or in general</p>

<pre><code>x.plot(kind=""bar"").legend(*args, **kwargs)
</code></pre>
";;0;;2015-03-07T16:25:55.740;;28917065;2015-03-07T16:25:55.740;;;;;907575.0;11348183.0;2;16;;;
24509;24509;;;"<p>Firstly <code>freq_series.plot</code> returns an axis <em>not</em> a figure so to make my answer a little more clear I've changed your given code to refer to it as <code>ax</code> rather than <code>fig</code> to be more consistent with other code examples.</p>

<p>You can get the list of the bars produced in the plot from the <code>ax.patches</code> member. Then you can use the technique demonstrated in <a href=""http://matplotlib.org/examples/api/barchart_demo.html"">this matplotlib gallery example</a> to add the labels using the <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.text""><code>ax.text</code></a> method.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

frequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data

freq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.

x_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]

# now to plot the figure...
plt.figure(figsize=(12, 8))
ax = freq_series.plot(kind='bar')
ax.set_title(""Amount Frequency"")
ax.set_xlabel(""Amount ($)"")
ax.set_ylabel(""Frequency"")
ax.set_xticklabels(x_labels)

rects = ax.patches

# Now make some labels
labels = [""label%d"" % i for i in xrange(len(rects))]

for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.savefig(""image.png"")
</code></pre>

<p>This produces a labelled plot that looks like:</p>

<p><img src=""https://i.stack.imgur.com/0RMaz.png"" alt=""enter image description here""></p>
";;6;;2015-03-08T20:52:34.593;;28931750;2015-03-08T20:59:09.570;2015-03-08T20:59:09.570;;4293020.0;;4293020.0;28931224.0;2;49;;;
24650;24650;;;"<p>Use the vectorised <a href=""http://pandas.pydata.org/pandas-docs/stable/api.html#string-handling""><code>str</code></a> method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.strings.StringMethods.replace.html#pandas.core.strings.StringMethods.replace""><code>replace</code></a>:</p>

<pre><code>In [30]:

df['range'] = df['range'].str.replace(',','-')
df
Out[30]:
      range
0    (2-30)
1  (50-290)
</code></pre>

<p><strong>EDIT</strong></p>

<p>So if we look at what you tried and why it didn't work:</p>

<pre><code>df['range'].replace(',','-',inplace=True)
</code></pre>

<p>from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html#pandas.Series.replace"">docs</a> we see this desc:</p>

<blockquote>
  <p>str or regex: str: string exactly matching to_replace will be replaced
  with value</p>
</blockquote>

<p>So because the str values do not match, no replacement occurs, compare with the following:</p>

<pre><code>In [43]:

df = pd.DataFrame({'range':['(2,30)',',']})
df['range'].replace(',','-', inplace=True)
df['range']
Out[43]:
0    (2,30)
1         -
Name: range, dtype: object
</code></pre>

<p>here we get an exact match on the second row and the replacement occurs.</p>
";;2;;2015-03-11T12:22:44.493;;28986536;2015-03-11T13:16:46.777;2015-03-11T13:16:46.777;;704848.0;;704848.0;28986489.0;2;19;;;
24662;24662;;;"<p>Perhaps the pandas interface has changed since @Rutger answered, but in the version I'm using (0.15.2), the <code>date_parser</code> function receives a list of dates instead of a single value. In this case, his code should be updated like so:</p>

<pre><code>dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]

df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)
</code></pre>
";;0;;2015-03-11T16:03:34.387;;28991603;2015-03-11T16:03:34.387;;;;;302898.0;17465045.0;2;9;;;
24766;24766;;;"<p>Given this df:</p>

<pre><code>        date
0 2001-08-10
1 2002-08-31
2 2003-08-29
3 2006-06-21
4 2002-03-27
5 2003-07-14
6 2004-06-15
7 2003-08-14
8 2003-07-29
</code></pre>

<p>and, if it's not already the case:</p>

<pre><code>df[""date""] = df[""date""].astype(""datetime64"")
</code></pre>

<p>To show the count of dates by month: </p>

<pre><code>df.groupby(df[""date""].dt.month).count().plot(kind=""bar"")
</code></pre>

<p><code>.dt</code> allows you to access the datetime properties.</p>

<p>Which will give you:</p>

<p><img src=""https://i.stack.imgur.com/eom7L.png"" alt=""groupby date month""></p>

<p>You can replace month by year, day, etc..</p>

<p>If you want to distinguish year and month for instance, just do:</p>

<pre><code>df.groupby([df[""date""].dt.year, df[""date""].dt.month]).count().plot(kind=""bar"")
</code></pre>

<p>Which gives:</p>

<p><img src=""https://i.stack.imgur.com/wHObD.png"" alt=""groupby date month year""></p>

<p>Was it what you wanted ? Is this clear ?</p>

<p>Hope this helps !</p>
";;8;;2015-03-13T15:57:45.677;;29036738;2017-03-21T18:06:02.983;2017-03-21T18:06:02.983;;3297428.0;;3297428.0;27365467.0;2;61;;;
24771;24771;;;"<p>I know this is late to the game here but a few points.</p>

<p>Whether or not a <code>DataFrame</code> is considered a <code>TimeSeries</code> is the type of index. In your case, your index is already a <code>TimeSeries</code>, so you are good to go. For more information on all the cool slicing you can do with a the pd.timeseries index, take a look at <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#datetime-indexing"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/timeseries.html#datetime-indexing</a></p>

<p>Now, others might arrive here because they have a column <code>'DateTime'</code> that they want to make an index, in which case the answer is simple</p>

<pre><code>ts = df.set_index('DateTime')
</code></pre>
";;0;;2015-03-13T18:31:05.440;;29039484;2015-10-26T05:29:53.780;2015-10-26T05:29:53.780;;12892.0;;4499429.0;16822996.0;2;8;;;
24846;24846;;;"<p>You can set the output display to match your current terminal width:</p>

<pre><code>pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])
</code></pre>
";;7;;2015-03-16T10:04:56.283;;29074073;2015-03-16T10:04:56.283;;;;;509706.0;11707586.0;2;15;;;
24942;24942;;;"<p>You can also directly <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">query</a> your DataFrame for this information.</p>

<pre><code>rpt.query('STK_ID in (600809,600141,600329)')
</code></pre>

<p>Or similarly search for ranges:</p>

<pre><code>rpt.query('60000 &lt; STK_ID &lt; 70000')
</code></pre>
";;1;;2015-03-17T20:12:10.800;;29108799;2015-10-27T15:39:51.610;2015-10-27T15:39:51.610;;193906.0;;3884938.0;12065885.0;2;29;;;
25107;25107;;;"<p>Assuming your DataFrame is in <code>df</code>:</p>

<pre><code>df.Temp_Rating.fillna(df.Farheit, inplace=True)
del df['Farheit']
df.columns = 'File heat Observations'.split()
</code></pre>

<p>First replace any <code>NaN</code> values with the corresponding value of <code>df.Farheit</code>. Delete the <code>'Farheit'</code> column. Then rename the columns. Here's the resulting <code>DataFrame</code>:</p>

<p><img src=""https://i.stack.imgur.com/jhdfe.png"" alt=""resulting DataFrame""></p>
";;0;;2015-03-21T00:03:59.620;;29177664;2015-03-21T00:03:59.620;;;;;240490.0;29177498.0;2;42;;;
25168;25168;;;"<p><code>datetime64[ns]</code> is a general dtype, while <code>&lt;M8[ns]</code> is a specific dtype. General dtypes map to specific dtypes, but may be different from one installation of NumPy to the next.</p>

<p>On a machine whose byte order is little endian, there is no difference between 
<code>np.dtype('datetime64[ns]')</code> and <code>np.dtype('&lt;M8[ns]')</code>:</p>

<pre><code>In [6]: np.dtype('datetime64[ns]') == np.dtype('&lt;M8[ns]')
Out[6]: True
</code></pre>

<p>However, on a big endian machine, <code>np.dtype('datetime64[ns]')</code> would equal <code>np.dtype('&gt;M8[ns]')</code>.</p>

<p>So <code>datetime64[ns]</code> maps to either <code>&lt;M8[ns]</code> or <code>&gt;M8[ns]</code> depending on the endian-ness of the machine.</p>

<p>There are many other similar examples of general dtypes mapping to specific dtypes:
<code>int64</code> maps to <code>&lt;i8</code> or <code>&gt;i8</code>, and <code>int</code> maps to either <code>int32</code> or <code>int64</code>
depending on the bit architecture of the OS and how NumPy was compiled.</p>

<hr>

<p>Apparently, the repr of the datetime64 dtype has change since the time the book was written to show the endian-ness of the dtype.</p>
";;0;;2015-03-23T19:25:43.450;;29218694;2017-06-18T18:44:58.837;2017-06-18T18:44:58.837;;190597.0;;190597.0;29206612.0;2;22;;;
25218;25218;;;"<p>You could use <code>groupby</code> to split the DataFrame into subgroups according to the color:</p>

<pre><code>for key, grp in df.groupby(['color']):
</code></pre>

<hr>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_table('data', sep='\s+')
fig, ax = plt.subplots()
labels = []
for key, grp in df.groupby(['color']):
    ax = grp.plot(ax=ax, kind='line', x='x', y='y', c=key)
    labels.append(key)
lines, _ = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
plt.show()
</code></pre>

<p>yields
<img src=""https://i.stack.imgur.com/Ismc8.png"" alt=""enter image description here""></p>
";;0;;2015-03-24T13:26:55.410;;29233885;2015-03-24T13:26:55.410;;;;;190597.0;29233283.0;2;24;;;
25221;25221;;;"<p>Using spark to read in a CSV file to <code>pandas</code> is quite a roundabout method for achieving the end goal of reading a CSV file into memory.</p>

<p>It seems like you might be misunderstanding the use cases of the technologies in play here.</p>

<p>Spark is for distributed computing (though it can be used locally). It's generally far too heavyweight to be used for simply reading in a CSV file.</p>

<p>In your example, the <code>sc.textFile</code> method will simply give you a spark RDD that is effectively a list of text lines. This likely isn't what you want. No type inference will be performed, so if you want to sum a column of numbers in your CSV file, you won't be able to because they are still strings as far as Spark is concerned.</p>

<p>Just use <code>pandas.read_csv</code> and read the whole CSV into memory. Pandas will automatically infer the type of each column. Spark doesn't do this.</p>

<p>Now to answer your questions:</p>

<p><strong>Does it store the Pandas object to local memory</strong>:</p>

<p>Yes. <code>toPandas()</code> will convert the Spark DataFrame into a Pandas DataFrame, which is of course in memory.</p>

<p><strong>Does Pandas low-level computation handled all by Spark</strong></p>

<p>No. Pandas runs its own computations, there's no interplay between spark and pandas, there's simply <em>some</em> API compatibility.</p>

<p><strong>Does it exposed all pandas dataframe functionality?</strong></p>

<p>No. For example, <code>Series</code> objects have an <code>interpolate</code> method which isn't available in PySpark <code>Column</code> objects. There are many many methods and functions that are in the pandas API that are not in the PySpark API.</p>

<p><strong>Can I convert it toPandas and just be done with it, without so much touching DataFrame API?</strong></p>

<p>Absolutely. In fact, you probably shouldn't even use Spark at all in this case. <code>pandas.read_csv</code> will likely handle your use case unless you're working with a <em>huge</em> amount of data.</p>

<p>Try to solve your problem with simple, low-tech, easy-to-understand libraries, and <em>only</em> go to something more complicated as you need it. Many times, you won't need the more complex technology.</p>
";;7;;2015-03-24T13:31:58.083;;29233999;2015-03-24T13:31:58.083;;;;;564538.0;29226210.0;2;26;;;
25246;25246;;;"<p>You can do what you are trying to do, but the syntax is not quite as nice as what you'd like:</p>

<pre><code>In [93]: cols = list(df.loc[:,'A':'C']) + ['E'] + list(df.loc[:,'G':'I'])

In [94]: df[cols]
Out[94]: 
          A         B         C         E         G         H         I
0 -0.814688 -1.060864 -0.008088  2.697203 -0.763874  1.793213 -0.019520
1  0.549824  0.269340  0.405570 -0.406695 -0.536304 -1.231051  0.058018
2  0.879230 -0.666814  1.305835  0.167621 -1.100355  0.391133  0.317467
</code></pre>

<p>And here's another way, using <code>filter</code> and regular expressions.  It's more concise for this particular case but not as general.</p>

<pre><code>df.filter(regex='[A-CEG-I]')
</code></pre>

<p>Note also that it's based entirely on column names whereas the first method is based on column position.  And also, while it may not be as simple as it is here given more realistic column names, regexes are very powerful so there's a chance you could use a regex in some way in many other cases.</p>
";;1;;2015-03-24T21:07:05.650;;29242900;2015-04-23T15:22:16.700;2015-04-23T15:22:16.700;;3877338.0;;3877338.0;29241836.0;2;17;;;
25264;25264;;;"<p>I had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:</p>

<pre><code>In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(df.A == 1) &amp; (df.D == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6
</code></pre>

<p>But I found that, if you wrap each condition in <code>(... == True)</code> and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: </p>

<pre><code>df[((df.A==1) == True) | ((df.D==6) == True)]
</code></pre>
";;1;;2015-03-25T04:00:24.160;;29247205;2015-03-25T04:00:24.160;;;;;2200216.0;11869910.0;2;6;;;
25307;25307;;;"<p>You can assign values in the loop using df.set_value:</p>

<pre><code>for i, row in df.iterrows():
  ifor_val = something
  if &lt;condition&gt;:
    ifor_val = something_else
  df.set_value(i,'ifor',ifor_val)
</code></pre>

<p>if you don't need the row values you could simply iterate over the indices of df, but I kept the original for-loop in case you need the row value for something not shown here.  </p>
";;3;;2015-03-25T17:07:22.027;;29262040;2015-03-25T17:07:22.027;;;;;311124.0;23330654.0;2;42;;;
25361;25361;;;"<p>Ivan's answer is great, but it looks like it can be slightly simplified, also removing the need to depend on joblib:</p>

<pre><code>from multiprocessing import Pool, cpu_count

def applyParallel(dfGrouped, func):
    with Pool(cpu_count()) as p:
        ret_list = p.map(func, [group for name, group in dfGrouped])
    return pandas.concat(ret_list)
</code></pre>

<p>By the way: this can not replace <em>any</em> groupby.apply(), but it will cover the typical cases: e.g. it should cover cases 2 and 3 <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.apply.html"" rel=""noreferrer"" title=""typical cases"">in the documentation</a>, while you should obtain the behaviour of case 1 by giving the argument <code>axis=1</code> to the final <code>pandas.concat()</code> call.</p>
";;0;;2015-03-26T14:47:24.680;;29281494;2015-11-12T08:13:44.327;2015-11-12T08:13:44.327;;2858145.0;;2858145.0;26187759.0;2;20;;;
25379;25379;;;"<p>In order to read a csv in that doesn't have a header and for only certain columns you need to pass params <code>header=None</code> and <code>usecols=[3,6]</code> for the 4th and 7th columns:</p>

<pre><code>df = pd.read_csv(file_path, header=None, usecols=[3,6])
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv"" rel=""noreferrer"">docs</a></p>
";;0;;2015-03-26T19:48:50.207;;29287549;2015-03-26T19:48:50.207;;;;;704848.0;29287224.0;2;41;;;
25473;25473;;;"<p>Pandas will recognise a value as null if it is a <code>np.nan</code> object, which will print as <code>NaN</code> in the DataFrame. Your missing values are probably empty strings, which Pandas does not recognise as null. To rectify this, you can convert the empty stings (or whatever is in your empty cells) to <code>np.nan</code> objects using <code>replace()</code>, and then call <code>dropna()</code>on your DataFrame to delete rows with null tenants.</p>

<p>To demonstrate, I create a DataFrame with some random values and some empty strings in a <code>Tenants</code> column:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; 
&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(10, 2), columns=list('AB'))
&gt;&gt;&gt; df['Tenant'] = np.random.choice(['Babar', 'Rataxes', ''], 10)
&gt;&gt;&gt; print df

          A         B   Tenant
0 -0.588412 -1.179306    Babar
1 -0.008562  0.725239         
2  0.282146  0.421721  Rataxes
3  0.627611 -0.661126    Babar
4  0.805304 -0.834214         
5 -0.514568  1.890647    Babar
6 -1.188436  0.294792  Rataxes
7  1.471766 -0.267807    Babar
8 -1.730745  1.358165  Rataxes
9  0.066946  0.375640         
</code></pre>

<p>Now I replace any empty strings in the <code>Tenants</code> column with <code>np.nan</code> objects, like so:</p>

<pre><code>&gt;&gt;&gt; df['Tenant'].replace('', np.nan, inplace=True)
&gt;&gt;&gt; print df

          A         B   Tenant
0 -0.588412 -1.179306    Babar
1 -0.008562  0.725239      NaN
2  0.282146  0.421721  Rataxes
3  0.627611 -0.661126    Babar
4  0.805304 -0.834214      NaN
5 -0.514568  1.890647    Babar
6 -1.188436  0.294792  Rataxes
7  1.471766 -0.267807    Babar
8 -1.730745  1.358165  Rataxes
9  0.066946  0.375640      NaN
</code></pre>

<p>Now I can drop the null values:</p>

<pre><code>&gt;&gt;&gt; df.dropna(subset=['Tenant'], inplace=True)
&gt;&gt;&gt; print df

          A         B   Tenant
0 -0.588412 -1.179306    Babar
2  0.282146  0.421721  Rataxes
3  0.627611 -0.661126    Babar
5 -0.514568  1.890647    Babar
6 -1.188436  0.294792  Rataxes
7  1.471766 -0.267807    Babar
8 -1.730745  1.358165  Rataxes
</code></pre>
";;3;;2015-03-28T07:46:29.167;;29314880;2015-03-28T07:46:29.167;;;;;4311245.0;29314033.0;2;28;;;
25477;25477;;;"<p>You can either Drop the columns you do not need OR Select the ones you need</p>

<pre><code>    ##Using DataFrame.drop
    df.drop(df.columns[[1, 2]], axis=1, inplace=True)

    # drop by Name
    df1 = df1.drop(['B', 'C'], axis=1)


    ## Select the ones you want
    df1 = df[['a','d']]
</code></pre>
";;0;;2015-03-28T15:54:50.590;;29319200;2015-03-28T15:54:50.590;;;;;3923448.0;14940743.0;2;113;;;
25479;25479;;;"<p>value_counts omits NaN by default so you're most likely dealing with """".</p>

<p>So you can just filter them out like</p>

<pre><code>filter = df[""Tenant""] != """"
dfNew = df[filter]
</code></pre>
";;3;;2015-03-28T16:19:17.380;;29319460;2015-03-28T16:19:17.380;;;;;3437504.0;29314033.0;2;11;;;
25489;25489;;;"<p>I've seen this error before and it typically does have to do with pandas referencing an old version of numpy. But reinstalling may not help if your python path is still pointing to an old version of numpy. </p>

<p>When you install numpy via pip, pip will tell you where it was installed. Something like</p>

<pre><code>pip install numpy==1.9.2
Requirement already satisfied (use --upgrade to upgrade): numpy==1.9.2 in /Library/Python/2.7/site-packages
Cleaning up...
</code></pre>

<p>So you have the correct version of numpy installed. But when you go into python </p>

<pre><code>$ python
Python 2.7.6 (default, Sep  9 2014, 15:04:36) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.__file__
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/__init__.pyc'
&gt;&gt;&gt; numpy.version.version
'1.8.0rc1'
</code></pre>

<p>Your path might be pointing at a different numpy. </p>

<p>Easiest solution I've found for this is simply to remove the unwanted version of numpy (moving it to a _bak folder for safety)</p>

<pre><code>mv /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy_bak
</code></pre>

<p>And now when I start python </p>

<pre><code>$ python
Python 2.7.6 (default, Sep  9 2014, 15:04:36) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.__file__
'/Library/Python/2.7/site-packages/numpy/__init__.pyc'
&gt;&gt;&gt; numpy.version.version
'1.9.2'
</code></pre>

<p>I've got the version I want. </p>

<p>For more complex workflows where different applications might need different versions of various packages, virtualenvs are a great way to go <a href=""http://docs.python-guide.org/en/latest/dev/virtualenvs/"">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a>. But I think for your case where you just want pandas and numpy to play nice, this approach should work fine. </p>
";;2;;2015-03-28T19:01:17.157;;29321298;2015-03-28T19:01:17.157;;;;;3965256.0;26067692.0;2;18;;;
25509;25509;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv""><code>chunksize</code></a>:</p>

<pre><code>for df in pd.read_csv('matrix.txt',sep=',', header = None, chunksize=1):
    #do something
</code></pre>

<p>To answer your second part do this:</p>

<pre><code>df = pd.read_csv('matrix.txt',sep=',', header = None, skiprows=1000, chunksize=1000)
</code></pre>

<p>This will skip the first 1000 rows and then only read the next 1000 rows giving you rows 1000-2000, unclear if you require the end points to be included or not but you can fiddle the numbers to get what you want.</p>
";;1;;2015-03-29T20:49:27.610;;29334672;2015-03-29T20:56:55.300;2015-03-29T20:56:55.300;;704848.0;;704848.0;29334463.0;2;25;;;
25612;25612;;;"<p>There are two possible solutions:</p>

<ul>
<li>Use a boolean mask, then use <code>df.loc[mask]</code></li>
<li>Set the date column as a DatetimeIndex, then use <code>df[start_date : end_date]</code></li>
</ul>

<hr>

<p><strong>Using a boolean mask</strong>:</p>

<p>Ensure <code>df['date']</code> is a Series with dtype <code>datetime64[ns]</code>:</p>

<pre><code>df['date'] = pd.to_datetime(df['date'])  
</code></pre>

<p>Make a boolean mask. <code>start_date</code> and <code>end_date</code> can be <code>datetime.datetime</code>s,
<code>np.datetime64</code>s, <code>pd.Timestamp</code>s, or even datetime strings:</p>

<pre><code>mask = (df['date'] &gt; start_date) &amp; (df['date'] &lt;= end_date)
</code></pre>

<p>Select the sub-DataFrame:</p>

<pre><code>df.loc[mask]
</code></pre>

<p>or re-assign to <code>df</code></p>

<pre><code>df = df.loc[mask]
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
mask = (df['date'] &gt; '2000-6-1') &amp; (df['date'] &lt;= '2000-6-10')
print(df.loc[mask])
</code></pre>

<p>yields</p>

<pre><code>            0         1         2       date
153  0.208875  0.727656  0.037787 2000-06-02
154  0.750800  0.776498  0.237716 2000-06-03
155  0.812008  0.127338  0.397240 2000-06-04
156  0.639937  0.207359  0.533527 2000-06-05
157  0.416998  0.845658  0.872826 2000-06-06
158  0.440069  0.338690  0.847545 2000-06-07
159  0.202354  0.624833  0.740254 2000-06-08
160  0.465746  0.080888  0.155452 2000-06-09
161  0.858232  0.190321  0.432574 2000-06-10
</code></pre>

<hr>

<p><strong>Using a DatetimeIndex</strong>:</p>

<p>If you are going to do a lot of selections by date, it may be quicker to set the
<code>date</code> column as the index first. Then you can select rows by date using
<code>df.loc[start_date:end_date]</code>.</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
df = df.set_index(['date'])
print(df.loc['2000-6-1':'2000-6-10'])
</code></pre>

<p>yields</p>

<pre><code>                   0         1         2
date                                    
2000-06-01  0.040457  0.326594  0.492136    # &lt;- includes start_date
2000-06-02  0.279323  0.877446  0.464523
2000-06-03  0.328068  0.837669  0.608559
2000-06-04  0.107959  0.678297  0.517435
2000-06-05  0.131555  0.418380  0.025725
2000-06-06  0.999961  0.619517  0.206108
2000-06-07  0.129270  0.024533  0.154769
2000-06-08  0.441010  0.741781  0.470402
2000-06-09  0.682101  0.375660  0.009916
2000-06-10  0.754488  0.352293  0.339337
</code></pre>

<p>Some caveats:</p>

<ul>
<li>When using <code>df.loc[start_date : end_date]</code> both end-points are included in result. </li>
<li>Unlike the boolean mask solution, the <code>start_date</code> and <code>end_date</code> must be dates in the DatetimeIndex. </li>
</ul>

<hr>

<p>Also note that <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer""><code>pd.read_csv</code> has a <code>parse_dates</code> parameter</a> which you could use to parse the <code>date</code> column as <code>datetime64</code>s. Thus, if you use <code>parse_dates</code>, you would not need to use <code>df['date'] = pd.to_datetime(df['date'])</code>. </p>
";;0;;2015-03-31T13:49:49.977;;29370182;2017-06-19T14:36:51.780;2017-06-19T14:36:51.780;;190597.0;;190597.0;29370057.0;2;104;;;
25615;25615;;;"<p>And for the other case, assuming it are strings that look like tuples:</p>

<pre><code>In [74]: df['stats'].str[1:-1].str.split(',', expand=True).astype(float)
Out[74]:
          0         1         2         3         4
0 -0.009242  0.410000 -0.742016  0.003683  0.002517
1  0.041154  0.318231  0.758717  0.002640  0.010654
2 -0.014435  0.168438 -0.808703  0.000817  0.003166
3  0.034346  0.288731  0.950845  0.000001  0.003373
4  0.009052  0.151031  0.670257  0.012179  0.003022
5 -0.004797  0.171615 -0.552879  0.050032  0.002180
</code></pre>

<p>(note: for older versions of pandas (&lt; 0.16.1), you need to use <code>return_type='frame'</code> instead of the expand keyword)</p>

<p>By the way, if it are tuples and not strings, you can simply do the following:</p>

<pre><code>df['stats'].apply(pd.Series)
</code></pre>

<p>This last one can also be combined with the above (if the <code>return_type='frame'</code> is not yet available):</p>

<pre><code>df['stats'].str[1:-1].str.split(',').apply(pd.Series).astype(float)
</code></pre>
";;3;;2015-03-31T14:13:34.333;;29370709;2017-01-26T13:17:16.540;2017-01-26T13:17:16.540;;653364.0;;653364.0;29370211.0;2;34;;;
25661;25661;;;"<p>This is definitely an issue of delimiter, as most of the csv CSV are got create using <code>sep='/t'</code> so try to <code>read_csv</code> using the tab character <code>(\t)</code> using separator <code>/t</code>. so, try to open using following code line.</p>

<pre><code>data=pd.read_csv(""File_path"", sep='\t')
</code></pre>
";;2;;2015-04-01T05:42:48.260;;29383624;2017-06-01T13:31:55.817;2017-06-01T13:31:55.817;;3954379.0;;4447920.0;18039057.0;2;7;;;
25809;25809;;;"<p>You can use <a href=""http://matplotlib.org/examples/pylab_examples/matshow.html"" rel=""noreferrer""><code>matshow()</code></a>  from matplotlib:</p>

<p><code>plt.matshow(dataframe.corr())</code></p>
";;0;;2015-04-03T13:04:18.873;;29432741;2015-10-15T22:57:00.590;2015-10-15T22:57:00.590;;3510736.0;;3297428.0;29432629.0;2;64;;;
25858;25858;;;"<p>Since 0.14.1 there's a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html""><code>select_dtypes</code></a> method so you can do this more elegantly/generally.</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2.2, 'three']], columns=['A', 'B', 'C'])

In [12]: df.select_dtypes(include=['int'])
Out[12]:
   A
0  1
</code></pre>

<blockquote>
  <p>To select all numeric types use the numpy dtype numpy.number</p>
</blockquote>

<pre><code>In [13]: df.select_dtypes(include=[np.number])
Out[13]:
   A    B
0  1  2.2

In [14]: df.select_dtypes(exclude=[object])
Out[14]:
   A    B
0  1  2.2
</code></pre>
";;0;;2015-04-04T05:07:53.923;;29442936;2015-04-04T05:07:53.923;;;;;1240268.0;21271581.0;2;21;;;
25886;25886;;;"<p>From your above needs, you will need to use both Python (to export pandas data frame) and VBA (to delete existing worksheet content and copy/paste external data). </p>

<p>With Python: use the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_csv.html"" rel=""noreferrer"">to_csv</a> or <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_excel.html"" rel=""noreferrer"">to_excel</a> methods. I recommend the to_csv method which performs better with larger datasets.</p>

<pre><code># DF TO EXCEL
from pandas import ExcelWriter

writer = ExcelWriter('PythonExport.xlsx')
yourdf.to_excel(writer,'Sheet5')
writer.save()

# DF TO CSV
yourdf.to_csv('PythonExport.csv', sep=',')
</code></pre>

<p>With VBA: <a href=""https://msdn.microsoft.com/en-us/library/office/ff837760.aspx"" rel=""noreferrer"">copy</a> and <a href=""https://msdn.microsoft.com/en-us/library/office/ff839476.aspx"" rel=""noreferrer"">paste</a> source to destination ranges.</p>

<p>Fortunately, in VBA you can call Python scripts using Shell (assuming your OS is Windows).</p>

<pre><code>Sub DataFrameImport()
  'RUN PYTHON TO EXPORT DATA FRAME
  Shell ""C:\pathTo\python.exe fullpathOfPythonScript.py"", vbNormalFocus

  'CLEAR EXISTING CONTENT
  ThisWorkbook.Worksheets(5).Cells.Clear

  'COPY AND PASTE TO WORKBOOK
  Workbooks(""PythonExport"").Worksheets(1).Cells.Copy
  ThisWorkbook.Worksheets(5).Range(""A1"").Select
  ThisWorkbook.Worksheets(5).Paste
End Sub
</code></pre>

<p>Alternatively, you can do vice versa: run a macro (ClearExistingContent) with Python. Be sure your Excel file is a macro-enabled (.xlsm) one with a saved macro to delete Sheet 5 content only. Note: macros cannot be saved with csv files.</p>

<pre><code>import os
import win32com.client
from pandas import ExcelWriter

if os.path.exists(""C:\Full Location\To\excelsheet.xlsm""):
  xlApp=win32com.client.Dispatch(""Excel.Application"")
  wb = xlApp.Workbooks.Open(Filename=""C:\Full Location\To\excelsheet.xlsm"")

  # MACRO TO CLEAR SHEET 5 CONTENT
  xlApp.Run(""ClearExistingContent"")
  wb.Save() 
  xlApp.Quit()
  del xl

  # WRITE IN DATA FRAME TO SHEET 5
  writer = ExcelWriter('C:\Full Location\To\excelsheet.xlsm')
  yourdf.to_excel(writer,'Sheet5')
  writer.save() 
</code></pre>
";;4;;2015-04-05T19:10:12.690;;29461151;2015-04-05T19:10:12.690;;;;;1422451.0;29459461.0;2;20;;;
25901;25901;;;"<p>One possible solution to your problem would be to use <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.2/merging.html"">merge</a>.  Checking if any row (all columns) from another dataframe (df2) are present in df1 is equivalent to determining the intersection of the the two dataframes.  This can be accomplished using the following function:</p>

<pre><code>pd.merge(df1, df2, on=['A', 'B', 'C', 'D'], how='inner')
</code></pre>

<p>For example, if df1 was</p>

<pre><code>    A           B            C          D
0   0.403846    0.312230    0.209882    0.397923
1   0.934957    0.731730    0.484712    0.734747
2   0.588245    0.961589    0.910292    0.382072
3   0.534226    0.276908    0.323282    0.629398
4   0.259533    0.277465    0.043652    0.925743
5   0.667415    0.051182    0.928655    0.737673
6   0.217923    0.665446    0.224268    0.772592
7   0.023578    0.561884    0.615515    0.362084
8   0.346373    0.375366    0.083003    0.663622
9   0.352584    0.103263    0.661686    0.246862
</code></pre>

<p>and df2 was defined as:</p>

<pre><code>     A          B            C           D
0   0.259533    0.277465    0.043652    0.925743
1   0.667415    0.051182    0.928655    0.737673
2   0.217923    0.665446    0.224268    0.772592
3   0.023578    0.561884    0.615515    0.362084
4   0.346373    0.375366    0.083003    0.663622
5   2.000000    3.000000    4.000000    5.000000
6   14.000000   15.000000   16.000000   17.000000
</code></pre>

<p>The function <code>pd.merge(df1, df2, on=['A', 'B', 'C', 'D'], how='inner')</code> produces:</p>

<pre><code>     A           B           C           D
0   0.259533    0.277465    0.043652    0.925743
1   0.667415    0.051182    0.928655    0.737673
2   0.217923    0.665446    0.224268    0.772592
3   0.023578    0.561884    0.615515    0.362084
4   0.346373    0.375366    0.083003    0.663622
</code></pre>

<p>The results are all of the rows (all columns) that are both in df1 and df2.</p>

<p>We can also modify this example if the columns are not the same in df1 and df2 and just compare the row values that are the same for a subset of the columns.  If we modify the original example:</p>

<pre><code>df1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))
df2 = df1.ix[4:8]
df2.reset_index(drop=True,inplace=True)
df2.loc[-1] = [2, 3, 4, 5]
df2.loc[-2] = [14, 15, 16, 17]
df2.reset_index(drop=True,inplace=True)
df2 = df2[['A', 'B', 'C']] # df2 has only columns A B C
</code></pre>

<p>Then we can look at the common columns using <code>common_cols = list(set(df1.columns) &amp; set(df2.columns))</code> between the two dataframes then merge:</p>

<pre><code>pd.merge(df1, df2, on=common_cols, how='inner')
</code></pre>

<p><strong>EDIT:</strong> New question (comments), having identified the rows from df2 that were also present in the first dataframe (df1), is it possible to take the result of the pd.merge() and to then drop the rows from df2 that are also present in df1</p>

<p>I do not know of a straightforward way to accomplish the task of dropping the rows from df2 that are also present in df1.  That said, you could use the following:</p>

<pre><code>ds1 = set(tuple(line) for line in df1.values)
ds2 = set(tuple(line) for line in df2.values)
df = pd.DataFrame(list(ds2.difference(ds1)), columns=df2.columns)
</code></pre>

<p>There probably exists a better way to accomplish that task but i am unaware of such a method / function.  </p>

<p><strong>EDIT 2:</strong> How to drop the rows from df2 that are also present in df1 as shown in @WR answer.</p>

<p>The method provided <code>df2[~df2['A'].isin(df12['A'])]</code> does not account for all types of situations.  Consider the following DataFrames:</p>

<p>df1:</p>

<pre><code>   A  B  C  D
0  6  4  1  6
1  7  6  6  8
2  1  6  2  7
3  8  0  4  1
4  1  0  2  3
5  8  4  7  5
6  4  7  1  1
7  3  7  3  4
8  5  2  8  8
9  3  2  8  4
</code></pre>

<p>df2:</p>

<pre><code>   A  B  C  D
0  1  0  2  3
1  8  4  7  5
2  4  7  1  1
3  3  7  3  4
4  5  2  8  8
5  1  1  1  1
6  2  2  2  2
</code></pre>

<p>df12:</p>

<pre><code>   A  B  C  D
0  1  0  2  3
1  8  4  7  5
2  4  7  1  1
3  3  7  3  4
4  5  2  8  8
</code></pre>

<p>Using the above DataFrames with the goal of dropping rows from df2 that are also present in df1 would result in the following:</p>

<pre><code>   A  B  C  D
0  1  1  1  1
1  2  2  2  2
</code></pre>

<p>Rows (1, 1, 1, 1) and (2, 2, 2, 2) are in df2 and not in df1.  Unfortunately, using the provided method (<code>df2[~df2['A'].isin(df12['A'])]</code>) results in:</p>

<pre><code>   A  B  C  D
6  2  2  2  2
</code></pre>

<p>This occurs because the value of 1 in column A is found in both the intersection DataFrame (i.e. (1, 0, 2, 3)) and df2 and thus removes both (1, 0, 2, 3) and (1, 1, 1, 1).  This is unintended since the row (1, 1, 1, 1) is not in df1 and should not be removed.  </p>

<p>I think the following will provide a solution.  It creates a dummy column that is later used to subset the DataFrame to the desired results:</p>

<pre><code>df12['key'] = 'x'
temp_df = pd.merge(df2, df12, on=df2.columns.tolist(), how='left')
temp_df[temp_df['key'].isnull()].drop('key', axis=1)
</code></pre>
";;5;;2015-04-06T01:54:35.630;;29464365;2015-04-11T06:56:05.583;2015-04-11T06:56:05.583;;4340076.0;;4340076.0;29464234.0;2;20;;;
25904;25904;;;"<p>It seems Pandas has change its API to avoid ambiguity when handling regex. Now you should use:</p>

<pre><code>df.replace({'\n': '&lt;br&gt;'}, regex=True)
</code></pre>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': ['1\n', '2\n', '3'], 'b': ['4\n', '5', '6\n']})
&gt;&gt;&gt; df
   a    b
0  1\n  4\n
1  2\n  5
2  3    6\n

&gt;&gt;&gt; df.replace({'\n': '&lt;br&gt;'}, regex=True)
   a      b
0  1&lt;br&gt;  4&lt;br&gt;
1  2&lt;br&gt;  5
2  3      6&lt;br&gt;
</code></pre>
";;1;;2015-04-06T04:10:35.787;;29465238;2015-04-06T04:10:35.787;;;;;426209.0;25698710.0;2;6;;;
25957;25957;;;"<p>Its gets even simpler (by pandas 0.16.0) : </p>

<pre><code>df.columns.tolist()
</code></pre>

<p>will give you the column names in a nice list.</p>
";;0;;2015-04-07T14:50:33.247;;29494537;2015-04-26T20:23:02.197;2015-04-26T20:23:02.197;;170005.0;;170005.0;19482970.0;2;19;;;
25972;25972;;;"<p>To plot just a selection of your columns you can select the columns of interest by passing a list to the subscript operator:</p>

<pre><code>ax = df[['V1','V2']].plot(kind='bar', title =""V comp"", figsize=(15, 10), legend=True, fontsize=12)
</code></pre>

<p>What you tried was <code>df['V1','V2']</code> this will raise a <code>KeyError</code> as correctly no column exists with that label, although it looks funny at first you have to consider that your are passing a list hence the double square brackets <code>[[]]</code>.</p>

<pre><code>import matplotlib.pyplot as plt
ax = df[['V1','V2']].plot(kind='bar', title =""V comp"", figsize=(15, 10), legend=True, fontsize=12)
ax.set_xlabel(""Hour"", fontsize=12)
ax.set_ylabel(""V"", fontsize=12)
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/xFvyV.png"" alt=""enter image description here""></p>
";;4;;2015-04-07T18:56:00.907;;29499109;2016-10-28T15:38:48.003;2016-10-28T15:38:48.003;;3447047.0;;704848.0;29498652.0;2;19;;;
25978;25978;;;"<p>I used the following </p>

<pre><code>grouped = df.groupby('A')

df = grouped.aggregate(lambda x: tuple(x))

df['grouped'] = df['B'] + df['C']
</code></pre>
";;4;;2015-04-07T20:06:14.627;;29500330;2015-04-07T20:06:14.627;;;;;2623954.0;19530568.0;2;40;;;
26021;26021;;;"<p><code>df['Name']='abc'</code> will add the new column and set all rows to that value:</p>

<pre><code>In [79]:

df
Out[79]:
         Date, Open, High,  Low,  Close
0  01-01-2015,  565,  600,  400,    450
In [80]:

df['Name'] = 'abc'
df
Out[80]:
         Date, Open, High,  Low,  Close Name
0  01-01-2015,  565,  600,  400,    450  abc
</code></pre>
";;0;;2015-04-08T14:09:52.107;;29517089;2015-04-08T14:09:52.107;;;;;704848.0;29517072.0;2;40;;;
26022;26022;;;"<p>Single liner works</p>

<pre><code>df['Name'] = 'abc'
</code></pre>

<p>Creates a <code>Name</code> column and sets all rows to <code>abc</code> value</p>
";;0;;2015-04-08T14:10:27.033;;29517102;2015-04-08T14:10:27.033;;;;;2137255.0;29517072.0;2;11;;;
26037;26037;;;"<p>As mentioned in a comment, starting from pandas 0.15, you have a chunksize option in <code>read_sql</code> to read and process the query chunk by chunk: </p>

<pre><code>sql = ""SELECT * FROM My_Table""
for chunk in pd.read_sql_query(sql , engine, chunksize=5):
    print(chunk)
</code></pre>

<p>Reference: <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.2/io.html#querying"">http://pandas.pydata.org/pandas-docs/version/0.15.2/io.html#querying</a></p>
";;6;;2015-04-08T18:22:48.623;;29522443;2016-10-24T15:11:19.133;2016-10-24T15:11:19.133;;690430.0;;690430.0;18107953.0;2;16;;;
26064;26064;;;"<p>For people looking at this today, I would recommend the Seaborn <code>heatmap()</code> as documented with a tutorial <a href=""http://seaborn.pydata.org/generated/seaborn.heatmap.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The example above would be done as follows:
</p>

<pre><code>import numpy as np 
from pandas import DataFrame
import seaborn as sns
%matplotlib

Index= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']
Cols = ['A', 'B', 'C', 'D']
df = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)

sns.heatmap(df)
</code></pre>

<p>Where <code>%matplotlib</code> is an IPython magic function for those unfamiliar.</p>
";;7;;2015-04-09T02:00:13.673;;29528483;2017-08-23T12:29:16.150;2017-08-23T12:29:16.150;;937365.0;;937365.0;12286607.0;2;62;;;
26066;26066;;;"<p>Below should work in most cases:</p>

<pre><code>df = pd.read_sql(query.statement, query.session.bind)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html"" rel=""noreferrer""><code>pandas.read_sql</code></a> documentation for more information on the parameters.</p>
";;5;;2015-04-09T02:40:13.443;;29528804;2017-03-17T09:00:53.033;2017-03-17T09:00:53.033;;99594.0;;99594.0;29525808.0;2;67;;;
26070;26070;;;"<p><code>df.isnull().any().any()</code> should do it.</p>
";;0;;2015-04-09T05:16:56.700;;29530303;2015-04-09T05:16:56.700;;;;;1567452.0;29530232.0;2;11;;;
26072;26072;;;"<p>You have a couple options. </p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10,6))
# Make a few areas have NaN values
df.iloc[1:3,1] = np.nan
df.iloc[5,3] = np.nan
df.iloc[7:9,5] = np.nan
</code></pre>

<p>Now the data frame looks something like this:</p>

<pre><code>          0         1         2         3         4         5
0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281
1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952
2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425
3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797
4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722
5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814
6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368
7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN
8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN
9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810
</code></pre>

<ul>
<li><strong>Option 1</strong>: <code>df.isnull().any().any()</code> - This returns a boolean value</li>
</ul>

<p>You know of the <code>isnull()</code> which would return a dataframe like this:</p>

<pre><code>       0      1      2      3      4      5
0  False  False  False  False  False  False
1  False   True  False  False  False  False
2  False   True  False  False  False  False
3  False  False  False  False  False  False
4  False  False  False  False  False  False
5  False  False  False   True  False  False
6  False  False  False  False  False  False
7  False  False  False  False  False   True
8  False  False  False  False  False   True
9  False  False  False  False  False  False
</code></pre>

<p>If you make it <code>df.isnull().any()</code>, you can find just the columns that have <code>NaN</code> values:</p>

<pre><code>0    False
1     True
2    False
3     True
4    False
5     True
dtype: bool
</code></pre>

<p>One more <code>.any()</code> will tell you if any of the above are <code>True</code></p>

<pre><code>&gt; df.isnull().any().any()
True
</code></pre>

<ul>
<li><strong>Option 2</strong>: <code>df.isnull().sum().sum()</code> - This returns an integer of the total number of <code>NaN</code> values:</li>
</ul>

<p>This operates the same way as the <code>.any().any()</code> does, by first giving a summation of the number of <code>NaN</code> values in a column, then the summation of those values:</p>

<pre><code>df.isnull().sum()
0    0
1    2
2    0
3    1
4    0
5    2
dtype: int64
</code></pre>

<p>Then to get the total:</p>

<pre><code>df.isnull().sum().sum()
5
</code></pre>
";;0;;2015-04-09T05:37:26.530;;29530559;2015-04-09T05:37:26.530;;;;;189134.0;29530232.0;2;59;;;
26074;26074;;;"<p><a href=""https://stackoverflow.com/users/1567452/jwilner"">jwilner</a>'s response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:</p>

<pre><code>df.isnull().values.any()
</code></pre>

<p>For example:</p>

<pre><code>In [2]: df = pd.DataFrame(np.random.randn(1000,1000))

In [3]: df[df &gt; 0.9] = pd.np.nan

In [4]: %timeit df.isnull().any().any()
100 loops, best of 3: 14.7 ms per loop

In [5]: %timeit df.isnull().values.sum()
100 loops, best of 3: 2.15 ms per loop

In [6]: %timeit df.isnull().sum().sum()
100 loops, best of 3: 18 ms per loop

In [7]: %timeit df.isnull().values.any()
1000 loops, best of 3: 948 s per loop
</code></pre>

<p><code>df.isnull().sum().sum()</code> is a bit slower, but of course, has additional information -- the number of <code>NaNs</code>.</p>
";;8;;2015-04-09T05:39:54.403;;29530601;2015-08-31T17:35:09.690;2017-05-23T12:34:26.463;;-1.0;;100904.0;29530232.0;2;134;;;
26082;26082;;;"<p>Just add the call to <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.show"" rel=""nofollow noreferrer""><code>plt.show()</code></a> after you plot the graph (you might want to <code>import matplotlib.pyplot</code> to do that), like this:</p>

<pre><code>from pandas import *
import matplotlib.pyplot as plt
%matplotlib inline

ys = [[0,1,2,3,4],[4,3,2,1,0]]
x_ax = [0,1,2,3,4]

for y_ax in ys:
    ts = Series(y_ax,index=x_ax)
    ts.plot(kind='bar', figsize=(15,5))
    plt.show()
</code></pre>
";;1;;2015-04-09T08:27:56.210;;29533502;2017-03-22T14:20:51.943;2017-03-22T14:20:51.943;;12892.0;;1027367.0;29532894.0;2;13;;;
26083;26083;;;"<p>In the IPython notebook the best way to do this is often with subplots. You create multiple axes on the same figure and then render the figure in the notebook. For example:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

%matplotlib inline

ys = [[0,1,2,3,4],[4,3,2,1,0]]
x_ax = [0,1,2,3,4]

fig, axs = plt.subplots(ncols=2, figsize=(10, 4))
for i, y_ax in enumerate(ys):
    pd.Series(y_ax, index=x_ax).plot(kind='bar', ax=axs[i])
    axs[i].set_title('Plot number {}'.format(i+1))
</code></pre>

<p>generates the following charts</p>

<p><img src=""https://i.stack.imgur.com/lm6E3.png"" alt=""enter image description here""></p>
";;0;;2015-04-09T08:38:12.910;;29533687;2015-04-09T08:38:12.910;;;;;751572.0;29532894.0;2;11;;;
26099;26099;;;"<p>To add a <code>Total</code> column which is the sum across the row:</p>

<pre><code>df['Total'] = df.sum(axis=1)
</code></pre>
";;0;;2015-04-09T14:23:05.150;;29541211;2016-12-08T19:53:46.337;2016-12-08T19:53:46.337;;818492.0;;4123181.0;20804673.0;2;26;;;
26120;26120;;;"<p>Here is a vectorized numpy version of the same function:</p>

<pre><code>import numpy as np

def haversine_np(lon1, lat1, lon2, lat2):
    """"""
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)

    All args must be of equal length.    

    """"""
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2

    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c
    return km
</code></pre>

<p>The inputs are all arrays of values, and it should be able to do millions of points instantly.  The requirement is that the inputs are ndarrays but the columns of your pandas table will work.</p>

<p>For example, with randomly generated values:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; lon1, lon2, lat1, lat2 = np.random.randn(4, 1000000)
&gt;&gt;&gt; df = pandas.DataFrame(data={'lon1':lon1,'lon2':lon2,'lat1':lat1,'lat2':lat2})
&gt;&gt;&gt; km = haversine_np(df['lon1'],df['lat1'],df['lon2'],df['lat2'])
</code></pre>

<p>Looping through arrays of data is very slow in python.  Numpy provides functions that operate on entire arrays of data, which lets you avoid looping and drastically improve performance.</p>

<p>This is an example of <a href=""http://en.wikipedia.org/wiki/Array_programming"">vectorization</a>.</p>
";;4;;2015-04-09T19:03:50.033;;29546836;2015-04-09T19:15:50.363;2015-04-09T19:15:50.363;;1390022.0;;1390022.0;29545704.0;2;19;;;
26126;26126;;;"<p>Purely for the sake of an illustrative example, I took the <code>numpy</code> version in the answer from @ballsdotballs and also made a companion C implementation to be called via <code>ctypes</code>. Since <code>numpy</code> is such a highly optimized tool, there is little chance that my C code will be as efficient, but it should be somewhat close. The big advantage here is that by running through an example with C types, it can help you see how you can connect up your own personal C functions to Python without too much overhead. This is especially nice when you just want to optimize a small piece of a bigger computation by writing that small piece in some C source rather than Python. Simply using <code>numpy</code> will solve the problem most of the time, but for those cases when you don't really need all of <code>numpy</code> and you don't want to add the coupling to require use of <code>numpy</code> data types throughout some code, it's very handy to know how to drop down to the built-in <code>ctypes</code> library and do it yourself.</p>

<p>First let's create our C source file, called <code>haversine.c</code>:</p>

<pre class=""lang-c prettyprint-override""><code>#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

int haversine(size_t n, 
              double *lon1, 
              double *lat1, 
              double *lon2, 
              double *lat2,
              double *kms){

    if (   lon1 == NULL 
        || lon2 == NULL 
        || lat1 == NULL 
        || lat2 == NULL
        || kms == NULL){
        return -1;
    }

    double km, dlon, dlat;
    double iter_lon1, iter_lon2, iter_lat1, iter_lat2;

    double km_conversion = 2.0 * 6367.0; 
    double degrees2radians = 3.14159/180.0;

    int i;
    for(i=0; i &lt; n; i++){
        iter_lon1 = lon1[i] * degrees2radians;
        iter_lat1 = lat1[i] * degrees2radians;
        iter_lon2 = lon2[i] * degrees2radians;
        iter_lat2 = lat2[i] * degrees2radians;

        dlon = iter_lon2 - iter_lon1;
        dlat = iter_lat2 - iter_lat1;

        km = pow(sin(dlat/2.0), 2.0) 
           + cos(iter_lat1) * cos(iter_lat2) * pow(sin(dlon/2.0), 2.0);

        kms[i] = km_conversion * asin(sqrt(km));
    }

    return 0;
}

// main function for testing
int main(void) {
    double lat1[2] = {16.8, 27.4};
    double lon1[2] = {8.44, 1.23};
    double lat2[2] = {33.5, 20.07};
    double lon2[2] = {14.88, 3.05};
    double kms[2]  = {0.0, 0.0};
    size_t arr_size = 2;

    int res;
    res = haversine(arr_size, lon1, lat1, lon2, lat2, kms);
    printf(""%d\n"", res);

    int i;
    for (i=0; i &lt; arr_size; i++){
        printf(""%3.3f, "", kms[i]);
    }
    printf(""\n"");
}
</code></pre>

<p>Note that we're trying to keep with C conventions. Explicitly passing data arguments by reference, using <code>size_t</code> for a size variable, and expecting our <code>haversine</code> function to work by mutating one of the passed inputs such that it will contain the expected data on exit. The function actually returns an integer, which is a success/failure flag that could be used by other C-level consumers of the function. </p>

<p>We're going to need to find a way to handle all of these little C-specific issues inside of Python.</p>

<p>Next let's put our <code>numpy</code> version of the function along with some imports and some test data into a file called <code>haversine.py</code>:</p>

<pre><code>import time
import ctypes
import numpy as np
from math import radians, cos, sin, asin, sqrt

def haversine(lon1, lat1, lon2, lat2):
    """"""
    Calculate the great circle distance between two points 
    on the earth (specified in decimal degrees)
    """"""
    # convert decimal degrees to radians 
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    # haversine formula 
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = (np.sin(dlat/2)**2 
         + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2)
    c = 2 * np.arcsin(np.sqrt(a)) 
    km = 6367 * c
    return km

if __name__ == ""__main__"":
    lat1 = 50.0 * np.random.rand(1000000)
    lon1 = 50.0 * np.random.rand(1000000)
    lat2 = 50.0 * np.random.rand(1000000)
    lon2 = 50.0 * np.random.rand(1000000)

    t0 = time.time()
    r1 = haversine(lon1, lat1, lon2, lat2)
    t1 = time.time()
    print t1-t0, r1
</code></pre>

<p>I chose to make lats and lons (in degrees) that are randomly chosen between 0 and 50, but it doesn't matter too much for this explanation.</p>

<p>The next thing we need to do is to compile our C module in such a way that it can be dynamically loaded by Python. I'm using a Linux system (you can find examples for other systems very easily on Google), so my goal is to compile <code>haversine.c</code> into a shared object, like so:</p>

<pre><code>gcc -shared -o haversine.so -fPIC haversine.c -lm
</code></pre>

<p>We can also compile to an executable and run it to see what the C program's <code>main</code> function displays:</p>

<pre><code>&gt; gcc haversine.c -o haversine -lm
&gt; ./haversine
0
1964.322, 835.278, 
</code></pre>

<p>Now that we have compiled the shared object <code>haversine.so</code>, we can use <code>ctypes</code> to load it in Python and we need to supply the path to the file to do so:</p>

<pre><code>lib_path = ""/path/to/haversine.so"" # Obviously use your real path here.
haversine_lib = ctypes.CDLL(lib_path)
</code></pre>

<p>Now <code>haversine_lib.haversine</code> acts pretty much just like a Python function, except that we might need to do some manual type marshaling to make sure the inputs and outputs are interpreted correctly.</p>

<p><code>numpy</code> actually provides some nice tools for this and the one I'll use here is <code>numpy.ctypeslib</code>. We're going to build a <em>pointer type</em> that will allow us to pass around <code>numpy.ndarrays</code> to these <code>ctypes</code>-loaded functions as through they were pointers. Here's the code:</p>

<pre><code>arr_1d_double = np.ctypeslib.ndpointer(dtype=np.double, 
                                       ndim=1, 
                                       flags='CONTIGUOUS')

haversine_lib.haversine.restype = ctypes.c_int
haversine_lib.haversine.argtypes = [ctypes.c_size_t,
                                    arr_1d_double, 
                                    arr_1d_double,
                                    arr_1d_double,
                                    arr_1d_double,
                                    arr_1d_double] 
</code></pre>

<p>Notice that we tell the <code>haversine_lib.haversine</code> function proxy to interpret its arguments according to the types we want.</p>

<p>Now, to test it out <em>from Python</em> what remains is to just make a size variable, and an array that will be mutated (just like in the C code) to contain the result data, then we can call it:</p>

<pre><code>size = len(lat1)
output = np.empty(size, dtype=np.double)
print ""=====""
print output
t2 = time.time()
res = haversine_lib.haversine(size, lon1, lat1, lon2, lat2, output)
t3 = time.time()
print t3 - t2, res
print type(output), output
</code></pre>

<p>Putting it all together in the <code>__main__</code> block of <code>haversine.py</code>, the whole file now looks like this:</p>

<pre><code>import time
import ctypes
import numpy as np
from math import radians, cos, sin, asin, sqrt

def haversine(lon1, lat1, lon2, lat2):
    """"""
    Calculate the great circle distance between two points 
    on the earth (specified in decimal degrees)
    """"""
    # convert decimal degrees to radians 
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    # haversine formula 
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = (np.sin(dlat/2)**2 
         + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2)
    c = 2 * np.arcsin(np.sqrt(a)) 
    km = 6367 * c
    return km

if __name__ == ""__main__"":
    lat1 = 50.0 * np.random.rand(1000000)
    lon1 = 50.0 * np.random.rand(1000000)
    lat2 = 50.0 * np.random.rand(1000000)
    lon2 = 50.0 * np.random.rand(1000000)

    t0 = time.time()
    r1 = haversine(lon1, lat1, lon2, lat2)
    t1 = time.time()
    print t1-t0, r1

    lib_path = ""/home/ely/programming/python/numpy_ctypes/haversine.so""
    haversine_lib = ctypes.CDLL(lib_path)
    arr_1d_double = np.ctypeslib.ndpointer(dtype=np.double, 
                                           ndim=1, 
                                           flags='CONTIGUOUS')

    haversine_lib.haversine.restype = ctypes.c_int
    haversine_lib.haversine.argtypes = [ctypes.c_size_t,
                                        arr_1d_double, 
                                        arr_1d_double,
                                        arr_1d_double,
                                        arr_1d_double,
                                        arr_1d_double]

    size = len(lat1)
    output = np.empty(size, dtype=np.double)
    print ""=====""
    print output
    t2 = time.time()
    res = haversine_lib.haversine(size, lon1, lat1, lon2, lat2, output)
    t3 = time.time()
    print t3 - t2, res
    print type(output), output
</code></pre>

<p>To run it, which will run and time the Python and <code>ctypes</code> versions separately and print some results, we can just do</p>

<pre><code>python haversine.py
</code></pre>

<p>which displays:</p>

<pre><code>0.111340045929 [  231.53695005  3042.84915093   169.5158946  ...,  1359.2656769
  2686.87895954  3728.54788207]
=====
[  6.92017600e-310   2.97780954e-316   2.97780954e-316 ...,
   3.20676686e-001   1.31978329e-001   5.15819721e-001]
0.148446083069 0
&lt;type 'numpy.ndarray'&gt; [  231.53675618  3042.84723579   169.51575588 ...,  1359.26453029
  2686.87709456  3728.54493339]
</code></pre>

<p>As expected, the <code>numpy</code> version is slightly faster (0.11 seconds for vectors with length of 1 million) but our quick and dirty <code>ctypes</code> version is no slouch: a respectable 0.148 seconds on the same data.</p>

<p>Let's compare this to a naive for-loop solution in Python:</p>

<pre><code>from math import radians, cos, sin, asin, sqrt

def slow_haversine(lon1, lat1, lon2, lat2):
    n = len(lon1)
    kms = np.empty(n, dtype=np.double)
    for i in range(n):
       lon1_v, lat1_v, lon2_v, lat2_v = map(
           radians, 
           [lon1[i], lat1[i], lon2[i], lat2[i]]
       )

       dlon = lon2_v - lon1_v 
       dlat = lat2_v - lat1_v 
       a = (sin(dlat/2)**2 
            + cos(lat1_v) * cos(lat2_v) * sin(dlon/2)**2)
       c = 2 * asin(sqrt(a)) 
       kms[i] = 6367 * c
    return kms
</code></pre>

<p>When I put this into the same Python file as the others and time it on the same million-element data, I consistently see a time of around 2.65 seconds on my machine.</p>

<p>So by quickly switching to <code>ctypes</code> we improve the speed by a factor of about 18. For many calculations that can benefit from access to bare, contiguous data, you often see gains much higher even than this.</p>

<p>Just to be super clear, I am not at all endorsing this as a better option than just using <code>numpy</code>. This is precisely the problem that <code>numpy</code> was built to solve, and so homebrewing your own <code>ctypes</code> code whenever it both (a) makes sense to incorporate <code>numpy</code> data types in your application and (b) there exists an easy way to map your code into a <code>numpy</code> equivalent, is not very efficient.</p>

<p>But it's still very helpful to know how to do this for those occasions when you prefer to write something in C yet call it in Python, or situations where a dependence on <code>numpy</code> is not practical (in an embedded system where <code>numpy</code> cannot be installed, for example).</p>
";;0;;2015-04-09T20:28:24.997;;29548349;2015-04-09T20:39:04.760;2015-04-09T20:39:04.760;;567620.0;;567620.0;29545704.0;2;9;;;
26135;26135;;;"<p>You can do this by <code>apply(pd.Series)</code> on that column:</p>

<pre><code>In [13]: df = pd.DataFrame({'a':[1,2], 'b':[(1,2), (3,4)]})

In [14]: df
Out[14]:
   a       b
0  1  (1, 2)
1  2  (3, 4)

In [16]: df['b'].apply(pd.Series)
Out[16]:
   0  1
0  1  2
1  3  4

In [17]: df[['b1', 'b2']] = df['b'].apply(pd.Series)

In [18]: df
Out[18]:
   a       b  b1  b2
0  1  (1, 2)   1   2
1  2  (3, 4)   3   4
</code></pre>

<p>This works because it makes of each tuple a Series, which is then seen as a row of a dataframe.</p>
";;3;;2015-04-09T22:55:11.927;;29550458;2015-04-09T22:57:15.220;2015-04-09T22:57:15.220;;653364.0;;653364.0;29550414.0;2;35;;;
26189;26189;;;"<p>You can shuffle the rows of a dataframe by indexing with a shuffled index. For this, you can eg use <code>np.random.permutation</code> (but <code>np.random.choice</code> is also a possibility):</p>

<pre><code>In [12]: df = pd.read_csv(StringIO(s), sep=""\s+"")

In [13]: df
Out[13]: 
    Col1  Col2  Col3  Type
0      1     2     3     1
1      4     5     6     1
20     7     8     9     2
21    10    11    12     2
45    13    14    15     3
46    16    17    18     3

In [14]: df.iloc[np.random.permutation(len(df))]
Out[14]: 
    Col1  Col2  Col3  Type
46    16    17    18     3
45    13    14    15     3
20     7     8     9     2
0      1     2     3     1
1      4     5     6     1
21    10    11    12     2
</code></pre>

<p>If you want to keep the index numbered from 1, 2, .., n as in your example, you can simply reset the index: <code>df_shuffled.reset_index(drop=True)</code></p>
";;0;;2015-04-11T10:26:59.100;;29576803;2015-04-11T10:26:59.100;;;;;653364.0;29576430.0;2;33;;;
26205;26205;;;"<p>I'm also a big fan of dplyr for R and am working to improve my knowledge of Pandas. Since you don't have a specific problem, I'd suggest checking out the post below that breaks down the entire introductory dplyr vignette and shows how all of it can be done with Pandas. </p>

<p>For example, the author demonstrates chaining with the pipe operator in R:</p>

<pre><code> flights %&gt;%
   group_by(year, month, day) %&gt;%
   select(arr_delay, dep_delay) %&gt;%
   summarise(
      arr = mean(arr_delay, na.rm = TRUE),
      dep = mean(dep_delay, na.rm = TRUE)
       ) %&gt;%
   filter(arr &gt; 30 | dep &gt; 30)
</code></pre>

<p>And here is the Pandas implementation: </p>

<pre><code>flights.groupby(['year', 'month', 'day'])
   [['arr_delay', 'dep_delay']]
   .mean()
   .query('arr_delay &gt; 30 | dep_delay &gt; 30')
</code></pre>

<p>There are many more comparisons of how to implement dplyr like operations with Pandas at the original post.
<a href=""http://nbviewer.ipython.org/gist/TomAugspurger/6e052140eaa5fdb6e8c0"" rel=""noreferrer"">http://nbviewer.ipython.org/gist/TomAugspurger/6e052140eaa5fdb6e8c0</a></p>
";;2;;2015-04-12T02:26:51.127;;29585283;2015-04-12T03:43:42.293;2015-04-12T03:43:42.293;;3221829.0;;3221829.0;26878476.0;2;23;;;
26366;26366;;;"<p>Based on this post: <a href=""https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range"">https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range</a></p>

<p>You can do the following:</p>

<pre><code>def normalize(df):
    result = df.copy()
    for feature_name in df.columns:
        max_value = df[feature_name].max()
        min_value = df[feature_name].min()
        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result
</code></pre>

<p>You don't need to stay worrying about whether your values are negative or positive. And the values should be nicely spread out between 0 and 1.</p>
";;0;;2015-04-15T13:25:06.763;;29651514;2015-04-15T13:25:06.763;2017-04-13T12:44:17.437;;-1.0;;507706.0;26414913.0;2;13;;;
26381;26381;;;"<p>Strictly speaking, an <a href=""http://en.wikipedia.org/wiki/Adjacency_matrix"">adjacency matrix</a> is boolean, with 1 indicating the presence of a connection and 0 indicating the absence. Since many of the values in your <code>a_numpy</code> matrix are > 1, I will assume that they correspond to edge weights in your graph.</p>

<pre><code>import igraph

# get the row, col indices of the non-zero elements in your adjacency matrix
conn_indices = np.where(a_numpy)

# get the weights corresponding to these indices
weights = a_numpy[conn_indices]

# a sequence of (i, j) tuples, each corresponding to an edge from i -&gt; j
edges = zip(*conn_indices)

# initialize the graph from the edge sequence
G = igraph.Graph(edges=edges, directed=True)

# assign node names and weights to be attributes of the vertices and edges
# respectively
G.vs['label'] = node_names
G.es['weight'] = weights

# I will also assign the weights to the 'width' attribute of the edges. this
# means that igraph.plot will set the line thicknesses according to the edge
# weights
G.es['width'] = weights

# plot the graph, just for fun
igraph.plot(G, layout=""rt"", labels=True, margin=80)
</code></pre>

<p><img src=""https://i.stack.imgur.com/k7Ibo.png"" alt=""enter image description here""></p>
";;0;;2015-04-15T17:39:39.547;;29657079;2015-04-15T19:13:46.150;2015-04-15T19:13:46.150;;1461210.0;;1461210.0;29655111.0;2;11;;;
26398;26398;;;"<p>You'll need to use the <code>HTML()</code> or <code>display()</code> functions from IPython's display module:</p>

<pre class=""lang-python prettyprint-override""><code>from IPython.display import display, HTML

# Assuming that dataframes df1 and df2 are already defined:
print ""Dataframe 1:""
display(df1)
print ""Dataframe 2:""
HTML(df2.to_html())
</code></pre>

<p>Note that if you just <code>print df1.to_html()</code> you'll get the raw, unrendered HTML. </p>

<p>You can also import from <code>IPython.core.display</code> with the same effect</p>
";;1;;2015-04-16T04:39:57.280;;29665452;2015-09-19T19:25:38.740;2015-09-19T19:25:38.740;;1549844.0;;4619397.0;26873127.0;2;119;;;
26411;26411;;;"<p>In igraph you can use <a href=""http://igraph.org/python/doc/igraph.GraphBase-class.html#Adjacency""><code>igraph.Graph.Adjacency</code></a> to create a graph from an adjacency matrix without having to use <code>zip</code>. There are some things to be aware of when a weighted adjacency matrix is used and stored in a <code>np.array</code> or <code>pd.DataFrame</code>.</p>

<ul>
<li><p><code>igraph.Graph.Adjacency</code> can't take an <code>np.array</code> as argument, but that is easily solved using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html""><code>tolist</code></a>.</p></li>
<li><p>Integers in adjacency-matrix are interpreted as number of edges between nodes rather than weights, solved by using adjacency as boolean.</p></li>
</ul>

<p>An example of how to do it:</p>

<pre><code>import igraph
import pandas as pd

node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]], index=node_names, columns=node_names)

# Get the values as np.array, it's more convenenient.
A = a.values

# Create graph, A.astype(bool).tolist() or (A / A).tolist() can also be used.
g = igraph.Graph.Adjacency((A &gt; 0).tolist())

# Add edge weights and node labels.
g.es['weight'] = A[A.nonzero()]
g.vs['label'] = node_names  # or a.index/a.columns
</code></pre>

<p>You can reconstruct your adjacency dataframe using <a href=""http://igraph.org/python/doc/igraph.Graph-class.html#get_adjacency""><code>get_adjacency</code></a> by:</p>

<pre><code>df_from_g = pd.DataFrame(g.get_adjacency(attribute='weight').data,
                         columns=g.vs['label'], index=g.vs['label'])
(df_from_g == a).all().all()  # --&gt; True
</code></pre>
";;2;;2015-04-16T11:21:46.177;;29673192;2015-04-16T11:21:46.177;;;;;3635816.0;29655111.0;2;18;;;
26417;26417;;;"<p>A couple of improvement's on <a href=""https://stackoverflow.com/posts/21361994/revisions"">HYRY's answer</a>:</p>

<ul>
<li>call <code>display</code> after <code>clear_output</code> so that you end up with one plot, rather than two, when the cell is interrupted.</li>
<li>catch the <code>KeyboardInterrupt</code>, so that the cell output isn't littered with the traceback.</li>
</ul>



<pre><code>import matplotlib.pylab as plt
import pandas as pd
import numpy as np
import time
from IPython import display
%matplotlib inline

i = pd.date_range('2013-1-1',periods=100,freq='s')

while True:
    try:
        plt.plot(pd.Series(data=np.random.randn(100), index=i))
        display.display(plt.gcf())
        display.clear_output(wait=True)
        time.sleep(1)
    except KeyboardInterrupt:
        break
</code></pre>
";;3;;2015-04-16T13:09:09.800;;29675706;2015-11-15T13:59:52.660;2017-05-23T12:34:30.577;;-1.0;;3149788.0;21360361.0;2;9;;;
26593;26593;;;"<p>Not a real answer but a workaround, as suggested by Tom Augspurger, is that you can just use the working line plot type and specify dots instead of lines:</p>

<pre><code>df.plot(x='x', y='y', style=""."")
</code></pre>
";;1;;2015-04-20T00:48:42.280;;29737663;2017-03-23T15:18:00.623;2017-03-23T15:18:00.623;;199148.0;;1655496.0;27472548.0;2;14;;;
26697;26697;;;"<p>When you don't have a MultiIndex, <code>df.columns</code> is just an array of column names so you can do:</p>

<pre><code>df.ix[:, df.columns != 'b']

          a         c         d
0  0.561196  0.013768  0.772827
1  0.882641  0.615396  0.075381
2  0.368824  0.651378  0.397203
3  0.788730  0.568099  0.869127
</code></pre>
";;2;;2015-04-21T05:27:40.323;;29763653;2015-04-21T05:27:40.323;;;;;1222578.0;29763620.0;2;52;;;
26705;26705;;;"<p>Use <code>del df.index.name</code></p>

<pre><code>In [16]: df
Out[16]:
         Column 1
foo
Apples          1
Oranges         2
Puppies         3
Ducks           4

In [17]: del df.index.name

In [18]: df
Out[18]:
         Column 1
Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>
";;0;;2015-04-21T07:38:20.177;;29765839;2015-04-21T07:38:20.177;;;;;100904.0;29765548.0;2;15;;;
26707;26707;;;"<p>Alternatively you can just assign <code>None</code> to the <code>index.name</code> attribute:</p>

<pre><code>In [125]:

df.index.name = None
df
Out[125]:
         Column 1

Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>
";;2;;2015-04-21T07:56:57.097;;29766187;2015-04-21T10:57:54.147;2015-04-21T10:57:54.147;;704848.0;;704848.0;29765548.0;2;15;;;
26731;26731;;;"<p>If you are dealing with a multi-index dataframe, you may be interested in extracting only the column of one name of the multi-index. You can do this as</p>

<pre><code>df.index.get_level_values('name_sub_index')
</code></pre>

<p>and of course <code>name_sub_index</code> must be an element of the <code>FrozenList</code> <code>df.index.names</code></p>
";;0;;2015-04-21T14:11:44.450;;29774704;2015-04-21T14:11:44.450;;;;;1409938.0;17241004.0;2;18;;;
26798;26798;;;"<p>Call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html#pandas.Series.map""><code>map</code></a> and pass the dict, this will perform a lookup and return the associated value for that key:</p>

<pre><code>In [248]:

d = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}
df['D'] = df['U'].map(d)
df
Out[248]:
     U   L   D
0  111  en  en
1  112  en  en
2  112  es  en
3  113  es  es
4  113  ja  es
5  113  zh  es
6  114  es  es
</code></pre>
";;7;;2015-04-22T10:40:42.527;;29794993;2015-04-22T10:40:42.527;;;;;704848.0;29794959.0;2;30;;;
26869;26869;;;"<p>Use <code>df.T.to_dict().values()</code>, like below:</p>

<pre><code>In [1]: df
Out[1]:
   customer  item1   item2   item3
0         1  apple    milk  tomato
1         2  water  orange  potato
2         3  juice   mango   chips

In [2]: df.T.to_dict().values()
Out[2]:
[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
 {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
 {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>

<hr>

<p>As John Galt mentions in <a href=""https://stackoverflow.com/a/29816143/2358206"">his answer </a>, you should probably instead use <code>df.to_dict('records')</code>. It's faster than transposing manually.</p>

<pre><code>In [20]: timeit df.T.to_dict().values()
1000 loops, best of 3: 395 s per loop

In [21]: timeit df.to_dict('records')
10000 loops, best of 3: 53 s per loop
</code></pre>
";;2;;2015-04-23T06:36:59.207;;29815523;2015-04-23T07:58:38.667;2017-05-23T10:31:29.113;;-1.0;;2358206.0;29815129.0;2;45;;;
26870;26870;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.to_dict.html#pandas.DataFrame.to_dict""><code>df.to_dict('records')</code></a> -- gives the output without having to transpose externally.</p>

<pre><code>In [2]: df.to_dict('records')
Out[2]:
[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
 {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
 {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>
";;2;;2015-04-23T07:08:44.357;;29816143;2015-12-03T21:38:38.370;2015-12-03T21:38:38.370;;169275.0;;2137255.0;29815129.0;2;67;;;
26937;26937;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration"" rel=""noreferrer"">groupby filter</a>:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])

In [12]: df
Out[12]:
   A  B
0  1  2
1  1  4
2  5  6

In [13]: df.groupby(""A"").filter(lambda x: len(x) &gt; 1)
Out[13]:
   A  B
0  1  2
1  1  4
</code></pre>

<p>I recommend reading the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration"" rel=""noreferrer"">split-combine-section of the docs</a>.</p>
";;2;;2015-04-24T00:50:54.607;;29836852;2015-04-24T00:50:54.607;;;;;1240268.0;29836836.0;2;19;;;
26939;26939;;;"<p>A interesting question! my answer as below:</p>

<pre><code>import pandas as pd

def sublst(row):
    return lst[row['J1']:row['J2']]

df = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})
print df
lst = ['a','b','c','d','e','f']

df['J3'] = df.apply(sublst,axis=1)
print df
</code></pre>

<p>Output:</p>

<pre><code>  ID  J1  J2
0  1   0   1
1  2   2   4
2  3   3   5
  ID  J1  J2      J3
0  1   0   1     [a]
1  2   2   4  [c, d]
2  3   3   5  [d, e]
</code></pre>

<p>I changed the column name to ID,J1,J2,J3 to ensure ID &lt; J1 &lt; J2 &lt; J3, so the column display in right sequence.</p>

<p>One more brief version:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})
print df
lst = ['a','b','c','d','e','f']

df['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)
print df
</code></pre>
";;0;;2015-04-24T02:33:45.843;;29837754;2015-04-24T02:39:14.127;2015-04-24T02:39:14.127;user4284784;;user4284784;;13331698.0;2;24;;;
27123;27123;;;"<p>You can use <code>options.display.max_colwidth</code> to specify you want to see more in the default representation:</p>

<pre><code>In [2]: df
Out[2]:
                                                 one
0                                                one
1                                                two
2  This is very long string very long string very...

In [3]: pd.options.display.max_colwidth
Out[3]: 50

In [4]: pd.options.display.max_colwidth = 100

In [5]: df
Out[5]:
                                                                               one
0                                                                              one
1                                                                              two
2  This is very long string very long string very long string veryvery long string
</code></pre>

<p>And indeed, if you just want to inspect the one value, by accessing it (as a scalar, not as a row as <code>df.iloc[2]</code> does) you also see the full string:</p>

<pre><code>In [7]: df.iloc[2,0]    # or df.loc[2,'one']
Out[7]: 'This is very long string very long string very long string veryvery long string'
</code></pre>
";;0;;2015-04-27T17:59:42.453;;29902819;2015-04-27T17:59:42.453;;;;;653364.0;29902714.0;2;23;;;
27154;27154;;;"<p>One more variation</p>

<p>Many of the operations done in pandas can also be done as a db query (sql, mongo)</p>

<p>Using a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)</p>

<p>Later, you can perform post processing using pandas.</p>

<p>The advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.</p>

<p>And although the query language and pandas are different, it's usually not complicated to translate part of the logic from one to another.</p>
";;0;;2015-04-28T05:22:21.403;;29910919;2015-04-28T05:22:21.403;;;;;590335.0;14262433.0;2;11;;;
27162;27162;;;"<p>Simply do,</p>

<pre><code>df = df[['mean'] + df.columns[:-1].tolist()]
</code></pre>
";;7;;2015-04-28T09:50:19.167;;29916004;2017-07-12T08:23:10.807;2017-07-12T08:23:10.807;;1296136.0;;1296136.0;13148429.0;2;8;;;
27171;27171;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html""><code>idxmax()</code></a> to find the column with the greatest value on each row:</p>

<pre><code>&gt;&gt;&gt; df.idxmax(axis=1)
0    Communications
1          Business
2    Communications
3    Communications
4          Business
dtype: object
</code></pre>

<p>To create the new column use <code>df['Max'] = df.idxmax(axis=1)</code>.</p>
";;2;;2015-04-28T12:25:57.970;;29919489;2015-04-28T12:27:40.973;2015-04-28T12:27:40.973;;3923281.0;;3923281.0;29919306.0;2;45;;;
27181;27181;;;"<p>Just assign the column names in the order you want them, to <code>&lt;dataframe&gt;.columns</code> like below:</p>

<pre><code>In [39]: df
Out[39]: 
          0         1         2         3         4  mean
0  0.172742  0.915661  0.043387  0.712833  0.190717     1
1  0.128186  0.424771  0.590779  0.771080  0.617472     1
2  0.125709  0.085894  0.989798  0.829491  0.155563     1
3  0.742578  0.104061  0.299708  0.616751  0.951802     1
4  0.721118  0.528156  0.421360  0.105886  0.322311     1
5  0.900878  0.082047  0.224656  0.195162  0.736652     1
6  0.897832  0.558108  0.318016  0.586563  0.507564     1
7  0.027178  0.375183  0.930248  0.921786  0.337060     1
8  0.763028  0.182905  0.931756  0.110675  0.423398     1
9  0.848996  0.310562  0.140873  0.304561  0.417808     1

In [40]: df = df[['mean', 4,3,2,1]]
</code></pre>

<p>Now, 'mean' column comes out in the front:</p>

<pre><code>In [41]: df
Out[41]: 
   mean         4         3         2         1
0     1  0.190717  0.712833  0.043387  0.915661
1     1  0.617472  0.771080  0.590779  0.424771
2     1  0.155563  0.829491  0.989798  0.085894
3     1  0.951802  0.616751  0.299708  0.104061
4     1  0.322311  0.105886  0.421360  0.528156
5     1  0.736652  0.195162  0.224656  0.082047
6     1  0.507564  0.586563  0.318016  0.558108
7     1  0.337060  0.921786  0.930248  0.375183
8     1  0.423398  0.110675  0.931756  0.182905
9     1  0.417808  0.304561  0.140873  0.310562
</code></pre>
";;3;;2015-04-28T14:19:49.323;;29922207;2015-04-28T14:19:49.323;;;;;170005.0;13148429.0;2;72;;;
27202;27202;;;"<p>You ask about binning with non-unique bin edges, for which I have a fairly simple answer.  In the case of your example, your intent and the behavior of qcut diverge where in the <code>pandas.tools.tile.qcut</code> function where bins are defined:</p>

<p><code>bins = algos.quantile(x, quantiles)</code></p>

<p>Which, because your data is 50% 0s, causes bins to be returned with multiple bin edges at the value 0 for any value of quantiles greater than 2.  I see two possible resolutions.  In the first, the fractile space is divided evenly, binning all 0s, but not only 0s, in the first bin.  In the second, the fractile space is divided evenly for values greater than 0, binning all 0s and only 0s in the first bin.</p>

<pre><code>import numpy as np
import pandas as pd
import pandas.core.algorithms as algos
from pandas import Series
</code></pre>

<p>In both cases, I'll create some random sample data fitting your description of 50% zeroes and the remaining values between 1 and 100</p>

<pre><code>zs = np.zeros(300)
rs = np.random.randint(1, 100, size=300)
arr=np.concatenate((zs, rs))
ser = Series(arr)
</code></pre>

<p><strong>Solution 1: bin 1 contains both 0s and low values</strong></p>

<pre><code>bins = algos.quantile(np.unique(ser), np.linspace(0, 1, 11))
result = pd.tools.tile._bins_to_cuts(ser, bins, include_lowest=True)
</code></pre>

<p>The result is</p>

<pre><code>In[61]: result.value_counts()
Out[61]: 
[0, 9.3]        323
(27.9, 38.2]     37
(9.3, 18.6]      37
(88.7, 99]       35
(57.8, 68.1]     32
(68.1, 78.4]     31
(78.4, 88.7]     30
(38.2, 48.5]     27
(48.5, 57.8]     26
(18.6, 27.9]     22
dtype: int64
</code></pre>

<p><strong>Solution 2: bin1 contains only 0s</strong></p>

<pre><code>mx = np.ma.masked_equal(arr, 0, copy=True)
bins = algos.quantile(arr[~mx.mask], np.linspace(0, 1, 11))
bins = np.insert(bins, 0, 0)
bins[1] = bins[1]-(bins[1]/2)
result = pd.tools.tile._bins_to_cuts(arr, bins, include_lowest=True)
</code></pre>

<p>The result is:    </p>

<pre><code>In[133]: result.value_counts()
Out[133]: 
[0, 0.5]        300
(0.5, 11]        32
(11, 18.8]       28
(18.8, 29.7]     30
(29.7, 39]       35
(39, 50]         26
(50, 59]         31
(59, 71]         31
(71, 79.2]       27
(79.2, 90.2]     30
(90.2, 99]       30
dtype: int64
</code></pre>

<p>There is work that could be done to Solution 2 to make it a little prettier I think, but you can see that the masked array is a useful tool to approach your goals.</p>
";;1;;2015-04-28T21:28:07.990;;29930255;2015-04-28T21:28:07.990;;;;;1558016.0;20158597.0;2;12;;;
27285;27285;;;"<p>So the term <em>broadcasting</em> comes from <a href=""http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""noreferrer"">numpy</a>, simply put it explains the rules of the output that will result when you perform operations between n-dimensional arrays (could be panels, dataframes, series) or scalar values.</p>

<h2>Broadcasting using a scalar value</h2>

<p>So the simplest case is just multiplying by a scalar value:</p>

<pre><code>In [4]:
s = pd.Series(np.arange(5))
s

Out[4]:
0    0
1    1
2    2
3    3
4    4
dtype: int32

In [5]:    
s * 10

Out[5]:
0     0
1    10
2    20
3    30
4    40
dtype: int32
</code></pre>

<p>and we get the same expected results with a dataframe:</p>

<pre><code>In [6]:    
df = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4)})
df

Out[6]:
          a         b
0  0.216920  0.652193
1  0.968969  0.033369
2  0.637784  0.856836
3 -2.303556  0.426238

In [7]:    
df * 10

Out[7]:
           a         b
0   2.169204  6.521925
1   9.689690  0.333695
2   6.377839  8.568362
3 -23.035557  4.262381
</code></pre>

<p>So what is technically happening here is that the scalar value has been <em>broadcasted</em> along the same dimensions of the Series and DataFrame above.</p>

<h2>Broadcasting using a 1-D array</h2>

<p>Say we have a 2-D dataframe of shape 4 x 3 (4 rows x 3 columns) we can perform an operation along the x-axis by using a 1-D Series that is the same length as the row-length:</p>

<pre><code>In [8]:
df = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4), 'c':np.random.randn(4)})
df

Out[8]:
          a         b         c
0  0.122073 -1.178127 -1.531254
1  0.011346 -0.747583 -1.967079
2 -0.019716 -0.235676  1.419547
3  0.215847  1.112350  0.659432

In [26]:    
df.iloc[0]

Out[26]:
a    0.122073
b   -1.178127
c   -1.531254
Name: 0, dtype: float64

In [27]:    
df + df.iloc[0]

Out[27]:
          a         b         c
0  0.244146 -2.356254 -3.062507
1  0.133419 -1.925710 -3.498333
2  0.102357 -1.413803 -0.111707
3  0.337920 -0.065777 -0.871822
</code></pre>

<p>the above looks funny at first until you understand what is happening, I took the first row of values and added this row-wise to the df, it can be visualised using this pic (sourced from <a href=""http://wiki.scipy.org/EricsBroadcastingDoc"" rel=""noreferrer""><code>scipy</code></a>):</p>

<p><img src=""https://i.stack.imgur.com/kzNxo.gif"" alt=""enter image description here""></p>

<p>The general rule is this:</p>

<blockquote>
  <p>In order to broadcast, the size of the trailing axes for both arrays
  in an operation must either be the same size or one of them must be
  one.</p>
</blockquote>

<p>So if I tried to add a 1-D array that didn't match in length, say one with 4 elements, unlike numpy which will raise a <code>ValueError</code>, in Pandas you'll get a df full of <code>NaN</code> values:</p>

<pre><code>In [30]:
df + pd.Series(np.arange(4))

Out[30]:
    a   b   c   0   1   2   3
0 NaN NaN NaN NaN NaN NaN NaN
1 NaN NaN NaN NaN NaN NaN NaN
2 NaN NaN NaN NaN NaN NaN NaN
3 NaN NaN NaN NaN NaN NaN NaN
</code></pre>

<p>Now some of the great things about pandas is that it will try to align using existing column names and row labels, this can get in the way of trying to perform a fancier broadcasting like this:</p>

<p><img src=""https://i.stack.imgur.com/HcM6z.gif"" alt=""enter image description here""></p>

<pre><code>In [55]:
df[['a']] + df.iloc[0]

Out[55]:
          a   b   c
0  0.244146 NaN NaN
1  0.133419 NaN NaN
2  0.102357 NaN NaN
3  0.337920 NaN NaN
</code></pre>

<p>In the above I use double subscripting to force the shape to be (4,1) but we see a problem when trying to broadcast using the first row as the column alignment only aligns on the first column. To get the same form of broadcasting to occur like the diagram above shows we have to decompose to numpy arrays which then become anonymous data:</p>

<pre><code>In [56]:
df[['a']].values + df.iloc[0].values

Out[56]:
array([[ 0.24414608, -1.05605392, -1.4091805 ],
       [ 0.13341899, -1.166781  , -1.51990758],
       [ 0.10235701, -1.19784299, -1.55096957],
       [ 0.33792013, -0.96227987, -1.31540645]])
</code></pre>

<p>It's also possible to broadcast in 3-dimensions but I don't go near that stuff often but the numpy, scipy and pandas book have examples that show how that works.</p>

<p>Generally speaking the thing to remember is that aside from scalar values which are simple, for n-D arrays the minor/trailing axes length must match or one of them must be 1.</p>
";;2;;2015-04-29T22:00:53.830;;29955358;2015-09-15T14:46:52.180;2015-09-15T14:46:52.180;;704848.0;;704848.0;29954263.0;2;28;;;
27291;27291;;;"<p>An alternative solution is to use the brilliant plotting package <code>seaborn</code>. Importing only the api avoids changing <code>matplotlib.rcParams</code>, which is the default behavior.</p>

<pre><code>import seaborn.apionly as sns
iris = sns.load_dataset('iris')
print(iris.head())
</code></pre>

<p>yields the same as for the R solution.</p>

<pre><code>   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
</code></pre>

<p><strong>Edit</strong>: If you do not want to import <code>seaborn</code> at all, but still want to access <a href=""https://github.com/mwaskom/seaborn-data"" rel=""noreferrer"">its sample data sets</a>, you can use  @andrewwowens's approach for the seaborn sample data:</p>

<pre><code>iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')
</code></pre>

<p>Note that the sample data sets containing categorical columns have their <a href=""https://github.com/mwaskom/seaborn/blob/10bdb18f47bb5fc0a30d34954ff6f174b4cf5881/seaborn/utils.py#L396"" rel=""noreferrer"">column type modified by <code>sns.load_dataset()</code></a> and the result might not be the same by getting it from the url directly.</p>

<p><strong>Edit:</strong> The iris and tips sample data sets are also available in pandas' github repo <a href=""https://github.com/pydata/pandas/tree/master/pandas/tests/data"" rel=""noreferrer"">here</a>.</p>
";;0;;2015-04-29T23:12:01.963;;29956221;2016-06-13T14:01:36.273;2016-06-13T14:01:36.273;;2166823.0;;2166823.0;28417293.0;2;13;;;
27324;27324;;;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.count.html""><code>count()</code></a> method returns the number of non-<code>NaN</code> values in each column:</p>

<pre><code>&gt;&gt;&gt; df1.count()
a    3
b    2
d    1
dtype: int64
</code></pre>

<p>Similarly, <code>count(axis=1)</code> returns the number of non-<code>NaN</code> values in each row.</p>
";;0;;2015-04-30T15:01:44.450;;29971188;2015-04-30T15:01:44.450;;;;;3923281.0;29971075.0;2;31;;;
27388;27388;;;"<p>Django Pandas solves this rather neatly: <a href=""https://github.com/chrisdev/django-pandas/"" rel=""noreferrer"">https://github.com/chrisdev/django-pandas/</a></p>

<p>From the README:</p>

<pre><code>class MyModel(models.Model):
    full_name = models.CharField(max_length=25)
    age = models.IntegerField()
    department = models.CharField(max_length=3)
    wage = models.FloatField()

from django_pandas.io import read_frame
qs = MyModel.objects.all()
df = read_frame(qs)
</code></pre>
";;1;;2015-05-01T16:25:15.653;;29990874;2015-05-01T16:25:15.653;;;;;173308.0;11697887.0;2;6;;;
27424;27424;;;"<p>In current versions of Pandas, <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.plot.html""><code>DataFrame.plot</code></a> features the <code>layout</code> keyword for this purpose.</p>

<pre><code>df.plot(subplots=True, layout=(2,2), ...)
</code></pre>
";;0;;2015-05-02T07:21:27.757;;29999590;2015-05-02T07:21:27.757;;;;;56541.0;14888473.0;2;7;;;
27443;27443;;;"<p>You could set index on predefined order using <code>reindex</code> like</p>

<pre><code>In [14]: df.reindex([""Z"", ""C"", ""A""])
Out[14]:
company  Amazon  Apple  Yahoo
Z             0      0    150
C           173      0      0
A             0    130      0
</code></pre>

<p>However, if it's alphabetical order, you could use <code>sort_index(ascending=False)</code></p>

<pre><code>In [12]: df.sort_index(ascending=False)
Out[12]:
company  Amazon  Apple  Yahoo
name
Z             0      0    150
C           173      0      0
A             0    130      0
</code></pre>

<p>Like pointed below, you need to assign it to some variable</p>

<pre><code>In [13]: df = df.sort_index(ascending=False)
</code></pre>
";;1;;2015-05-03T03:43:59.100;;30010004;2017-08-09T06:55:01.253;2017-08-09T06:55:01.253;;2137255.0;;2137255.0;30009948.0;2;23;;;
27479;27479;;;"<p><strong>loc:</strong> only work on index<Br>
<strong>iloc:</strong> work on position<Br>
<strong>ix:</strong> You can get data from dataframe without it being in the index<Br>
<strong>at:</strong> get scalar values. It's a very fast loc<Br>
<strong>iat:</strong> Get scalar values. It's a very fast iloc<Br></p>

<p><a href=""http://pyciencia.blogspot.com/2015/05/obtener-y-filtrar-datos-de-un-dataframe.html"">http://pyciencia.blogspot.com/2015/05/obtener-y-filtrar-datos-de-un-dataframe.html</a></p>
";;2;;2015-05-04T04:31:58.457;;30022658;2016-02-10T23:53:58.563;2016-02-10T23:53:58.563;;1797250.0;;4860926.0;28757389.0;2;70;;;
27484;27484;;;"<p>You're running into the different conventions for Pandas <code>DataFrame</code> indexing versus NumPy <code>ndarray</code> indexing.  The arrays <code>train_index</code> and <code>test_index</code> are collections of row indices.  But <code>data</code> is a Pandas <code>DataFrame</code> object, and when you use a single index into that object, as in <code>data[train_index]</code>, Pandas is expecting <code>train_index</code> to contain <em>column</em> labels rather than row indices.  You can either convert the dataframe to a NumPy array, using <code>.values</code>:</p>

<pre><code>data_array = data.values
for train_index, test_index in sss:
    xtrain, xtest = data_array[train_index], data_array[test_index]
    ytrain, ytest = target[train_index], target[test_index]
</code></pre>

<p>or use the Pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-position""><code>.iloc</code></a> accessor:</p>

<pre><code>for train_index, test_index in sss:
    xtrain, xtest = data.iloc[train_index], data.iloc[test_index]
    ytrain, ytest = target[train_index], target[test_index]
</code></pre>

<p>I tend to favour the second approach, since it gives <code>xtrain</code> and <code>xtest</code> of type <code>DataFrame</code> rather than <code>ndarray</code>, and so keeps the column labels.</p>
";;5;;2015-05-04T07:43:29.597;;30025025;2015-05-04T09:37:53.040;2015-05-04T09:37:53.040;;270986.0;;270986.0;30023927.0;2;36;;;
27488;27488;;;"<p>Here's on approach to do it using one <code>apply</code></p>

<p>Say, <code>df</code> is like</p>

<pre><code>In [64]: df
Out[64]:
       mydate     mytime
0  2011-01-01 2011-11-14
1  2011-01-02 2011-11-15
2  2011-01-03 2011-11-16
3  2011-01-04 2011-11-17
4  2011-01-05 2011-11-18
5  2011-01-06 2011-11-19
6  2011-01-07 2011-11-20
7  2011-01-08 2011-11-21
8  2011-01-09 2011-11-22
9  2011-01-10 2011-11-23
10 2011-01-11 2011-11-24
11 2011-01-12 2011-11-25
</code></pre>

<p>We'll take the lambda function out to separate line for readability and define it like</p>

<pre><code>In [65]: lambdafunc = lambda x: pd.Series([x['mytime'].hour,
                                           x['mydate'].isocalendar()[1],
                                           x['mydate'].weekday()])
</code></pre>

<p>And, <code>apply</code> and store the result to <code>df[['hour', 'weekday', 'weeknum']]</code></p>

<pre><code>In [66]: df[['hour', 'weekday', 'weeknum']] = df.apply(lambdafunc, axis=1)
</code></pre>

<p>And, the output is like</p>

<pre><code>In [67]: df
Out[67]:
       mydate     mytime  hour  weekday  weeknum
0  2011-01-01 2011-11-14     0       52        5
1  2011-01-02 2011-11-15     0       52        6
2  2011-01-03 2011-11-16     0        1        0
3  2011-01-04 2011-11-17     0        1        1
4  2011-01-05 2011-11-18     0        1        2
5  2011-01-06 2011-11-19     0        1        3
6  2011-01-07 2011-11-20     0        1        4
7  2011-01-08 2011-11-21     0        1        5
8  2011-01-09 2011-11-22     0        1        6
9  2011-01-10 2011-11-23     0        2        0
10 2011-01-11 2011-11-24     0        2        1
11 2011-01-12 2011-11-25     0        2        2
</code></pre>
";;1;;2015-05-04T09:57:09.980;;30027273;2015-05-04T09:57:09.980;;;;;2137255.0;30026815.0;2;15;;;
27557;27557;;;"<p>You could specify the dtype directly when constructing the DataFrame:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(index=range(0,4),columns=['A'], dtype='float')
&gt;&gt;&gt; df.dtypes
A    float64
dtype: object
</code></pre>

<p>Specifying the dtype forces Pandas to try creating the DataFrame with that type, rather than trying to infer it.</p>
";;0;;2015-05-05T12:48:36.290;;30053435;2015-05-05T12:48:36.290;;;;;3923281.0;30053329.0;2;10;;;
27558;27558;;;"<p>Simply pass the desired representative as a scalar first argument, like <code>0</code>, <code>math.inf</code> or, in this case, <code>np.nan</code>. The constructor then initializes the value array to the size specified by <code>index</code> and <code>columns</code>:</p>

<pre><code> &gt;&gt;&gt; df = pd.DataFrame(np.nan, index=[0,1,2,3], columns=['A'])
 &gt;&gt;&gt; df.dtypes
 A    float64
 dtype: object
</code></pre>
";;1;;2015-05-05T12:51:01.000;;30053507;2017-01-27T12:34:25.580;2017-01-27T12:34:25.580;;2375855.0;;2375855.0;30053329.0;2;29;;;
27571;27571;;;"<p>You could first find if element is <code>NaN</code> or not by <code>isnull()</code> and then take row-wise <code>sum(axis=1)</code></p>

<pre><code>In [195]: df.isnull().sum(axis=1)
Out[195]:
0    0
1    0
2    0
3    3
4    0
5    0
dtype: int64
</code></pre>

<p>And, if you want the output as list, you can</p>

<pre><code>In [196]: df.isnull().sum(axis=1).tolist()
Out[196]: [0, 0, 0, 3, 0, 0]
</code></pre>
";;0;;2015-05-05T17:15:39.247;;30059290;2015-05-05T17:15:39.247;;;;;2137255.0;30059260.0;2;20;;;
27602;27602;;;"<p>You may also consider using scipy's <code>mode</code> function which ignores NaN. A solution using it could look like:</p>

<pre><code>from scipy.stats import mode
from numpy import nan
df = DataFrame({""a"": [1,2,2,4,2], ""b"": [nan, nan, nan, 3, 3]})
print mode(df)
</code></pre>

<p>The output would look like </p>

<pre><code>(array([[ 2.,  3.]]), array([[ 3.,  2.]]))
</code></pre>

<p>meaning that the most common values are <code>2</code> for the first columns and <code>3</code> for the second, with frequencies <code>3</code> and <code>2</code> respectively.</p>
";;0;;2015-05-05T22:00:49.557;;30063996;2015-05-05T22:00:49.557;;;;;2040015.0;15138973.0;2;6;;;
27606;27606;;;"<p>To solve this specifically for linear SVM, we first have to understand the formulation of the SVM in sklearn and the differences that it has to MultinomialNB.</p>

<p>The reason why the <code>most_informative_feature_for_class</code> works for MultinomialNB is because the output of the <code>coef_</code> is essentially the log probability of features given a class (and hence would be of size <code>[nclass, n_features]</code>, due to the formulation of the naive bayes problem. But if we check the <a href=""http://scikit-learn.org/stable/modules/svm.html#multi-class-classification"" rel=""nofollow noreferrer"">documentation</a> for SVM, the <code>coef_</code> is not that simple. Instead <code>coef_</code> for (linear) SVM is <code>[n_classes * (n_classes -1)/2, n_features]</code> because each of the binary models are fitted to every possible class. </p>

<p>If we do possess some knowledge on which particular coefficient we're interested in, we could alter the function to look like the following:</p>

<pre><code>def most_informative_feature_for_class_svm(vectorizer, classifier,  classlabel, n=10):
    labelid = ?? # this is the coef we're interested in. 
    feature_names = vectorizer.get_feature_names()
    svm_coef = classifier.coef_.toarray() 
    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]

    for coef, feat in topn:
        print feat, coef
</code></pre>

<p>This would work as intended and print out the labels and the top n features according to the coefficient vector that you're after. </p>

<p>As for getting the correct output for a particular class, that would depend on the assumptions and what you aim to output. I suggest reading through the multi-class documentation within the SVM documentation to get a feel for what you're after. </p>

<p>So using the <code>train.txt</code> <a href=""https://stackoverflow.com/a/26977579/1992167"">file</a> which was described in this <a href=""https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c"">question</a>, we can get some kind of output, though in this situation it isn't particularly descriptive or helpful to interpret. Hopefully this helps you.</p>

<pre><code>import codecs, re, time
from itertools import chain

import numpy as np

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

trainfile = 'train.txt'

# Vectorizing data.
train = []
word_vectorizer = CountVectorizer(analyzer='word')
trainset = word_vectorizer.fit_transform(codecs.open(trainfile,'r','utf8'))
tags = ['bs','pt','es','sr']

# Training NB
mnb = MultinomialNB()
mnb.fit(trainset, tags)

from sklearn.svm import SVC
svcc = SVC(kernel='linear', C=1)
svcc.fit(trainset, tags)

def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):
    labelid = list(classifier.classes_).index(classlabel)
    feature_names = vectorizer.get_feature_names()
    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]

    for coef, feat in topn:
        print classlabel, feat, coef

def most_informative_feature_for_class_svm(vectorizer, classifier,  n=10):
    labelid = 3 # this is the coef we're interested in. 
    feature_names = vectorizer.get_feature_names()
    svm_coef = classifier.coef_.toarray() 
    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]

    for coef, feat in topn:
        print feat, coef

most_informative_feature_for_class(word_vectorizer, mnb, 'pt')
print 
most_informative_feature_for_class_svm(word_vectorizer, svcc)
</code></pre>

<p>with output:</p>

<pre><code>pt teve -4.63472898823
pt tive -4.63472898823
pt todas -4.63472898823
pt vida -4.63472898823
pt de -4.22926388012
pt foi -4.22926388012
pt mais -4.22926388012
pt me -4.22926388012
pt as -3.94158180767
pt que -3.94158180767

no 0.0204081632653
parecer 0.0204081632653
pone 0.0204081632653
por 0.0204081632653
relacin 0.0204081632653
una 0.0204081632653
visto 0.0204081632653
ya 0.0204081632653
es 0.0408163265306
lo 0.0408163265306
</code></pre>
";;5;;2015-05-05T23:41:41.733;;30065040;2015-05-05T23:41:41.733;2017-05-23T12:33:14.957;;-1.0;;1992167.0;30017491.0;2;10;;;
27649;27649;;;"<p>You could try this:   </p>

<pre><code>df.to_html(classes = 'my_class"" id = ""my_id')
</code></pre>

<p>It's like a SQL injection basically.<br>
Pandas' to_html function uses double quotes around the class. You can use single quotes to define the classes argument, and put double quotes inside them to end pandas' class. Then put opening double quotes around your id name but let pandas' close those double quotes for you. The output will be the following:  </p>

<pre><code>'&lt;table border=""1"" class=""dataframe my_class"" id = ""my_id""&gt;...'
</code></pre>

<p>Hope that helps.</p>
";;3;;2015-05-06T21:04:21.433;;30087487;2015-05-06T21:04:21.433;;;;;4606749.0;15079118.0;2;12;;;
27736;27736;;;"<p>Assuming all the values in the dataframe are numeric,</p>

<pre><code># Convert DataFrame to matrix
mat = dataset.as_matrix()
# Using sklearn
km = sklearn.cluster.KMeans(n_clusters=5)
km.fit(mat)
# Get cluster assignment labels
labels = km.labels_
# Format results as a DataFrame
results = pandas.DataFrame([dataset.index,labels]).T
</code></pre>

<p>Alternatively, you could try <a href=""https://github.com/jackmaney/k-means-plus-plus-pandas"">KMeans++ for Pandas</a>.</p>
";;1;;2015-05-07T20:57:10.513;;30111487;2015-05-07T20:57:10.513;;;;;3915498.0;28017091.0;2;16;;;
27776;27776;;;"<p>There is no <code>str</code> accessor for datetimes and you can't do <code>dates.astype(str)</code> either, you can call <code>apply</code> and use <code>datetime.strftime</code>:</p>

<pre><code>In [73]:

dates = pd.to_datetime(pd.Series(['20010101', '20010331']), format = '%Y%m%d')
dates.apply(lambda x: x.strftime('%Y-%m-%d'))
Out[73]:
0    2001-01-01
1    2001-03-31
dtype: object
</code></pre>

<p>You can change the format of your date strings using whatever you like: <a href=""https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior"" rel=""noreferrer"">https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior</a></p>

<p><strong>update</strong></p>

<p>As of version <code>0.17.0</code> you can do this using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.strftime.html#pandas.Series.dt.strftime"" rel=""noreferrer""><code>dt.strftime</code></a></p>

<pre><code>dates.dt.strftime('%Y-%m-%d')
</code></pre>

<p>will now work</p>
";;4;;2015-05-08T20:25:12.607;;30132313;2015-12-08T09:42:03.370;2015-12-08T09:42:03.370;;704848.0;;704848.0;30132282.0;2;16;;;
27792;27792;;;"<p>The plotting code assumes that each bar in a bar plot deserves its own label.
You could override this assumption by specifying your own formatter:</p>

<pre><code>ax.xaxis.set_major_formatter(formatter)
</code></pre>

<p>The <code>pandas.tseries.converter.TimeSeries_DateFormatter</code> that Pandas uses to
format the dates in the ""good"" plot works well with <em>line plots</em> when the
x-values are dates. However, with a <em>bar plot</em> the x-values (at least those
received by <code>TimeSeries_DateFormatter.__call__</code>) are merely integers <em>starting
at zero</em>. If you try to use <code>TimeSeries_DateFormatter</code> with a bar plot, all the labels thus start at the Epoch, 1970-1-1 UTC, since this is the date which corresponds to zero. So the formatter used for line plots is unfortunately useless for bar
plots (at least as far as I can see).</p>

<p>The easiest way I see to produce the desired formatting is to generate and set the labels explicitly:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.ticker as ticker

start = pd.to_datetime(""5-1-2012"")
idx = pd.date_range(start, periods= 365)
df = pd.DataFrame({'A':np.random.random(365), 'B':np.random.random(365)})
df.index = idx
df_ts = df.resample('W', how= 'max')

ax = df_ts.plot(kind='bar', x=df_ts.index, stacked=True)

# Make most of the ticklabels empty so the labels don't get too crowded
ticklabels = ['']*len(df_ts.index)
# Every 4th ticklable shows the month and day
ticklabels[::4] = [item.strftime('%b %d') for item in df_ts.index[::4]]
# Every 12th ticklabel includes the year
ticklabels[::12] = [item.strftime('%b %d\n%Y') for item in df_ts.index[::12]]
ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))
plt.gcf().autofmt_xdate()

plt.show()
</code></pre>

<p>yields
<a href=""https://i.stack.imgur.com/nmzF0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nmzF0.png"" alt=""enter image description here""></a></p>
";;4;;2015-05-09T02:05:01.580;;30135182;2015-11-11T21:38:09.727;2015-11-11T21:38:09.727;;190597.0;;190597.0;30133280.0;2;28;;;
27951;27951;;;"<p>You should be able to just do this, which @DSM just taught me in another thread:</p>

<pre><code>data = read_table('sample.txt', skiprows=3, header=None, delim_whitespace=True)
</code></pre>

<p><a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#csv-text-files"" rel=""noreferrer"">Documentation</a></p>
";;1;;2015-05-12T20:53:51.517;;30201213;2015-10-26T14:56:58.930;2015-10-26T14:56:58.930;;396967.0;;778533.0;12021730.0;2;24;;;
27974;27974;;;"<p>Not sure about older version of pandas, but in 0.16 the value of a particular cell can be set based on multiple column values.</p>

<p>Extending the answer provided by @waitingkuo, the same operation can also be done based on values of multiple columns.</p>

<pre><code>d.loc[(d.day== 'sun') &amp; (d.flavour== 'banana') &amp; (d.year== 2009),'sales'] = 100
</code></pre>
";;0;;2015-05-13T07:48:46.753;;30208749;2015-05-13T07:48:46.753;;;;;3299878.0;17729853.0;2;6;;;
27998;27998;;;"<p>To begin, note that quantiles is just the most general term for things like percentiles, quartiles, and medians.  You specified five bins in your example, so you are asking <code>qcut</code> for quintiles.</p>

<p>So, when you ask for quintiles with <code>qcut</code>, the bins will be chosen so that you have the same number of records in each bin.  You have 30 records, so should have 6 in each bin (your output should look like this, although the breakpoints will differ due to the random draw):</p>

<pre><code>pd.qcut(factors, 5).value_counts()

[-2.578, -0.829]    6
(-0.829, -0.36]     6
(-0.36, 0.366]      6
(0.366, 0.868]      6
(0.868, 2.617]      6
</code></pre>

<p>Conversely, for <code>cut</code> you will see something more uneven:</p>

<pre><code>pd.cut(factors, 5).value_counts()

(-2.583, -1.539]    5
(-1.539, -0.5]      5
(-0.5, 0.539]       9
(0.539, 1.578]      9
(1.578, 2.617]      2
</code></pre>

<p>That's because <code>cut</code> will choose the bins to be evenly spaced according to the values themselves and not the <em>frequency</em> of those values.  Hence, because you drew from a random normal, you'll see higher frequencies in the inner bins and fewer in the outer.  This is essentially going to be a tabular form of a histogram (which you would expect to be fairly bell shaped with 30 records).</p>
";;1;;2015-05-13T12:33:00.470;;30214901;2015-05-13T16:58:14.910;2015-05-13T16:58:14.910;;3877338.0;;3877338.0;30211923.0;2;38;;;
28026;28026;;;"<p>EDIT:</p>

<p>As user jezrael points out below, <code>dt.weekday_name</code> was added in version 0.18.1
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.weekday_name.html"" rel=""noreferrer"">Pandas Docs</a></p>

<pre><code>import pandas as pd

df = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})
df['my_dates'] = pd.to_datetime(df['my_dates'])
df['day_of_week'] = df['my_dates'].dt.weekday_name
</code></pre>

<p>Output:</p>

<pre><code>    my_dates  myvals day_of_week
0 2015-01-01       1    Thursday
1 2015-01-02       2      Friday
2 2015-01-03       3    Saturday
</code></pre>

<hr>

<p>Original Answer:</p>

<p>Use this:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.dayofweek.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.dayofweek.html</a></p>

<p>See this:</p>

<p><a href=""https://stackoverflow.com/questions/28009370/get-weekday-for-datetime-in-pandas-dataframe"">Get weekday/day-of-week for Datetime column of DataFrame</a></p>

<p>If you want a string instead of an integer do something like this:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})
df['my_dates'] = pd.to_datetime(df['my_dates'])
df['day_of_week'] = df['my_dates'].dt.dayofweek

days = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}

df['day_of_week'] = df['day_of_week'].apply(lambda x: days[x])
</code></pre>

<p>Output:</p>

<pre><code>    my_dates  myvals day_of_week
0 2015-01-01       1       Thurs
1 2015-01-02       2         Fri
2 2015-01-01       3       Thurs
</code></pre>
";;10;;2015-05-13T18:36:36.210;;30222759;2017-02-10T02:10:30.903;2017-05-23T12:02:20.300;;-1.0;;2317751.0;30222533.0;2;25;;;
28216;28216;;;"<p>As mentioned by larsmans, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""noreferrer"">LabelEncoder() only takes a 1-d array as an argument</a>. That said, it is quite easy to roll your own label encoder that operates on multiple columns of your choosing, and returns a transformed dataframe. My code here is based in part on Zac Stewart's excellent blog post found <a href=""http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"" rel=""noreferrer"">here</a>.</p>

<p>Creating a custom encoder involves simply creating a class that responds to the <code>fit()</code>, <code>transform()</code>, and <code>fit_transform()</code> methods. In your case, a good start might be something like this:
</p>

<pre><code>import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

# Create some toy data in a Pandas dataframe
fruit_data = pd.DataFrame({
    'fruit':  ['apple','orange','pear','orange'],
    'color':  ['red','orange','green','green'],
    'weight': [5,6,3,4]
})

class MultiColumnLabelEncoder:
    def __init__(self,columns = None):
        self.columns = columns # array of column names to encode

    def fit(self,X,y=None):
        return self # not relevant here

    def transform(self,X):
        '''
        Transforms columns of X specified in self.columns using
        LabelEncoder(). If no columns specified, transforms all
        columns in X.
        '''
        output = X.copy()
        if self.columns is not None:
            for col in self.columns:
                output[col] = LabelEncoder().fit_transform(output[col])
        else:
            for colname,col in output.iteritems():
                output[colname] = LabelEncoder().fit_transform(col)
        return output

    def fit_transform(self,X,y=None):
        return self.fit(X,y).transform(X)
</code></pre>

<p>Suppose we want to encode our two categorical attributes (<code>fruit</code> and <code>color</code>), while leaving the numeric attribute <code>weight</code> alone. We could do this as follows:
</p>

<pre><code>MultiColumnLabelEncoder(columns = ['fruit','color']).fit_transform(fruit_data)
</code></pre>

<p>Which transforms our <code>fruit_data</code> dataset from</p>

<p><img src=""https://i.stack.imgur.com/aqGcU.png"" alt=""enter image description here""> to </p>

<p><img src=""https://i.stack.imgur.com/xISwE.png"" alt=""enter image description here""></p>

<p>Passing it a dataframe consisting entirely of categorical variables and omitting the <code>columns</code> parameter will result in every column being encoded (which I believe is what you were originally looking for):
</p>

<pre><code>MultiColumnLabelEncoder().fit_transform(fruit_data.drop('weight',axis=1))
</code></pre>

<p>This transforms</p>

<p><img src=""https://i.stack.imgur.com/zKgcI.png"" alt=""enter image description here""> to</p>

<p><img src=""https://i.stack.imgur.com/5KwKW.png"" alt=""enter image description here"">.</p>

<p>Note that it'll probably choke when it tries to encode attributes that are already numeric (add some code to handle this if you like).</p>

<p>Another nice feature about this is that we can use this custom transformer in a pipeline:
</p>

<pre><code>encoding_pipeline = Pipeline([
    ('encoding',MultiColumnLabelEncoder(columns=['fruit','color']))
    # add more pipeline steps as needed
])
encoding_pipeline.fit_transform(fruit_data)
</code></pre>
";;9;;2015-05-15T19:27:05.350;;30267328;2015-05-15T19:27:05.350;;;;;1610342.0;24458645.0;2;43;;;
28297;28297;;;"<p>A little helper function I use with some header checking safeguards to handle it all:</p>

<pre><code>def appendDFToCSV_void(df, csvFilePath, sep="",""):
    import os
    if not os.path.isfile(csvFilePath):
        df.to_csv(csvFilePath, mode='a', index=False, sep=sep)
    elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):
        raise Exception(""Columns do not match!! Dataframe has "" + str(len(df.columns)) + "" columns. CSV file has "" + str(len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + "" columns."")
    elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():
        raise Exception(""Columns and column order of dataframe and csv file do not match!!"")
    else:
        df.to_csv(csvFilePath, mode='a', index=False, sep=sep, header=False)
</code></pre>
";;0;;2015-05-17T22:49:32.053;;30292938;2015-05-17T22:49:32.053;;;;;4088927.0;17530542.0;2;10;;;
28326;28326;;;"<p>For fast solution replace this</p>

<pre><code>xcell.style = xcell.style.copy(**style_kwargs)
</code></pre>

<p>with</p>

<pre><code>pass
</code></pre>

<p>At pandas/io/excel.py line 778.</p>

<p>openpyxl upgraded their api and pandas also need to be updated for support openpyxl.</p>
";;4;;2015-05-18T13:38:23.327;;30304735;2015-05-18T13:38:23.327;;;;;1189066.0;30102232.0;2;8;;;
28377;28377;;;"<p>I made an example that contains both missing values in training and the test sets</p>

<p>I just picked a strategy to replace missing data with the mean, using the <code>Imputer</code> class. There are other strategies.</p>

<pre><code>from __future__ import print_function

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import Imputer

X_train = [[0, 0, np.nan], [np.nan, 1, 1]]
Y_train = [0, 1]
X_test_1 = [0, 0, np.nan]
X_test_2 = [0, np.nan, np.nan]
X_test_3 = [np.nan, 1, 1]

# Create our imputer to replace missing values with the mean e.g.
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp = imp.fit(X_train)

# Impute our data, then train
X_train_imp = imp.transform(X_train)
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(X_train_imp, Y_train)

for X_test in [X_test_1, X_test_2, X_test_3]:
    # Impute each test item, then predict
    X_test_imp = imp.transform(X_test)
    print(X_test, '-&gt;', clf.predict(X_test_imp))

# Results
[0, 0, nan] -&gt; [0]
[0, nan, nan] -&gt; [0]
[nan, 1, 1] -&gt; [1]
</code></pre>
";;5;;2015-05-19T07:26:53.977;;30319249;2015-05-19T07:26:53.977;;;;;238639.0;30317119.0;2;9;;;
28405;28405;;;"<p>One solution is to use <code>np.random.choice</code>:</p>

<pre><code>import numpy as np
df1['randomNumCol'] = np.random.choice(range(1, 6), df1.shape[0])
</code></pre>

<p>In order to make the results reproducible you can set the seed with <code>np.random.seed(42)</code>.</p>
";;2;;2015-05-19T13:43:59.673;;30327470;2015-05-19T13:43:59.673;;;;;18601.0;30327417.0;2;23;;;
28415;28415;;;"<p>If you want to first take mean on <code>['cluster', 'org']</code> combination and then again take mean on <code>cluster</code> groups</p>

<pre><code>In [59]: (df.groupby(['cluster', 'org'], as_index=False).mean()
            .groupby('cluster')['time'].mean())
Out[59]:
cluster
1          15
2          54
3           6
Name: time, dtype: int64
</code></pre>

<p>If you wan't mean values by <code>cluster</code> only, then you could</p>

<pre><code>In [58]: df.groupby(['cluster']).mean()
Out[58]:
              time
cluster
1        12.333333
2        54.000000
3         6.000000
</code></pre>

<p>You could <code>groupby</code> on <code>['cluster', 'org']</code> and then take <code>mean()</code></p>

<pre><code>In [57]: df.groupby(['cluster', 'org']).mean()
Out[57]:
               time
cluster org
1       a    438886
        c        23
2       d      9874
        h        34
3       w         6
</code></pre>
";;7;;2015-05-19T14:37:03.580;;30328738;2015-05-19T17:16:51.320;2015-05-19T17:16:51.320;;2137255.0;;2137255.0;30328646.0;2;26;;;
28496;28496;;;"<p>In <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"">version 0.18 of pandas, the DataFrame constructor</a> has no options for creating a dataframe like another dataframe with NaN instead of the values.</p>

<p>The code you use <code>df2 = pd.DataFrame(columns=df1.columns,index=df1.index)</code> is the most logical way, the only way to improve on it is to spell out even more what you are doing is to add <code>data=None</code>, so that other coders directly see that you intentionally leave out the data from this new DataFrame you are creating.</p>

<p>TLDR; So my suggestion is:</p>

<h3>Explicit is better than implicit</h3>

<pre><code>df2 = pd.DataFrame(data=None, columns=df1.columns,index=df1.index)
</code></pre>

<p>Very much like yours, but more spelled out.</p>
";;5;;2015-05-20T16:22:44.410;;30355286;2016-04-08T17:48:26.530;2016-04-08T17:48:26.530;;3730397.0;;3730397.0;27467730.0;2;13;;;
28505;28505;;;"<p>You can provide this column to <code>fillna</code> (see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html"" rel=""noreferrer"">docs</a>), it will use those values on matching indexes to fill:</p>

<pre><code>In [17]: df['Cat1'].fillna(df['Cat2'])
Out[17]:
0    cat
1    dog
2    cat
3    ant
Name: Cat1, dtype: object
</code></pre>
";;2;;2015-05-20T18:14:27.920;;30357382;2015-05-20T18:16:55.510;2015-05-20T18:16:55.510;;653364.0;;653364.0;30357276.0;2;30;;;
28527;28527;;;"<p>From the comments above, it seems that this is planned for <code>pandas</code> some time (there's also an interesting-looking <a href=""https://pypi.python.org/pypi/rosetta/0.2.4""><code>rosetta</code> project</a> which I just noticed).</p>

<p>However, until every parallel functionality is incorporated into <code>pandas</code>, I noticed that it's very easy to write efficient &amp; non-memory-copying parallel augmentations to <code>pandas</code> directly using <a href=""http://cython.org/""><code>cython</code></a> + <a href=""http://www.google.co.il/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CB0QFjAA&amp;url=http%3A%2F%2Fwww.openmp.org%2F&amp;ei=HKpdVfyVJcj8ULXHgcAF&amp;usg=AFQjCNGlD5aZM8ZP3Qx7WXT74Y7C54jLNQ&amp;bvm=bv.93756505,d.d24"">OpenMP</a> and C++.</p>

<p>Here's a short example of writing a parallel groupby-sum, whose use is something like this:</p>

<pre><code>import pandas as pd
import para_group_demo

df = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})
print para_group_demo.sum(df.a, df.b)
</code></pre>

<p>and output is:</p>

<pre><code>     sum
key     
0      6
1      11
2      4
</code></pre>

<hr>

<p><strong>Note</strong> Doubtlessly, this simple example's functionality will eventually be part of <code>pandas</code>. Some things, however, will be more natural to parallelize in C++ for some time, and it's important to be aware of how easy it is to combine this into <code>pandas</code>.</p>

<hr>

<p>To do this, I wrote a simple single-source-file extension whose code follows.</p>

<p>It starts with some imports and type definitions</p>

<pre><code>from libc.stdint cimport int64_t, uint64_t
from libcpp.vector cimport vector
from libcpp.unordered_map cimport unordered_map

cimport cython
from cython.operator cimport dereference as deref, preincrement as inc
from cython.parallel import prange

import pandas as pd

ctypedef unordered_map[int64_t, uint64_t] counts_t
ctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t
ctypedef vector[counts_t] counts_vec_t
</code></pre>

<p>The C++ <code>unordered_map</code> type is for summing by a single thread, and the <code>vector</code> is for summing by all threads.</p>

<p>Now to the function <code>sum</code>. It starts off with <a href=""http://docs.cython.org/src/userguide/memoryviews.html"">typed memory views</a> for fast access:</p>

<pre><code>def sum(crit, vals):
    cdef int64_t[:] crit_view = crit.values
    cdef int64_t[:] vals_view = vals.values
</code></pre>

<p>The function continues by dividing the semi-equally to the threads (here hardcoded to 4), and having each thread sum the entries in its range:</p>

<pre><code>    cdef uint64_t num_threads = 4
    cdef uint64_t l = len(crit)
    cdef uint64_t s = l / num_threads + 1
    cdef uint64_t i, j, e
    cdef counts_vec_t counts
    counts = counts_vec_t(num_threads)
    counts.resize(num_threads)
    with cython.boundscheck(False):
        for i in prange(num_threads, nogil=True): 
            j = i * s
            e = j + s
            if e &gt; l:
                e = l
            while j &lt; e:
                counts[i][crit_view[j]] += vals_view[j]
                inc(j)
</code></pre>

<p>When the threads have completed, the function merges all the results (from the different ranges) into a single <code>unordered_map</code>:</p>

<pre><code>    cdef counts_t total
    cdef counts_it_t it, e_it
    for i in range(num_threads):
        it = counts[i].begin()
        e_it = counts[i].end()
        while it != e_it:
            total[deref(it).first] += deref(it).second
            inc(it)        
</code></pre>

<p>All that's left is to create a <code>DataFrame</code> and return the results:</p>

<pre><code>    key, sum_ = [], []
    it = total.begin()
    e_it = total.end()
    while it != e_it:
        key.append(deref(it).first)
        sum_.append(deref(it).second)
        inc(it)

    df = pd.DataFrame({'key': key, 'sum': sum_})
    df.set_index('key', inplace=True)
    return df
</code></pre>
";;0;;2015-05-21T10:18:25.933;;30370897;2015-05-21T10:18:25.933;;;;;3510736.0;11728836.0;2;10;;;
28540;28540;;;"<p>As per their <a href=""http://pandas.pydata.org/pandas-docs/stable/install.html"">documentation</a>, pandas depends </p>

<blockquote>
  <p>on openpyxl version 1.6.1 or higher, but lower than 2.0.0</p>
</blockquote>

<p>The last <a href=""https://openpyxl.readthedocs.org/en/latest/"">openpyxl</a> version lower than 2.0.0 being version 1.8.6, you should simply remove your current openpyxl version and run:</p>

<pre><code>pip install openpyxl==1.8.6
</code></pre>

<p>if your using pip, or find an equivalent way to install this specific version.</p>
";;1;;2015-05-21T15:37:04.020;;30378303;2015-05-21T15:37:04.020;;;;;4563613.0;30102232.0;2;11;;;
28551;28551;;;"<pre><code>old_names = ['$a', '$b', '$c', '$d', '$e'] 
new_names = ['a', 'b', 'c', 'd', 'e']
df.rename(columns=dict(zip(old_names, new_names)), inplace=True)
</code></pre>

<p>This way you can manually edit the <code>new_names</code> as you wish.
Works great when you need to rename only a few columns to correct mispellings, accents, remove special characters etc.</p>
";;4;;2015-05-21T17:48:33.580;;30380922;2015-05-21T17:54:25.673;2015-05-21T17:54:25.673;;4116995.0;;4116995.0;11346283.0;2;38;;;
28673;28673;;;"<h2>How to create sample datasets</h2>

<p>This is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code.</p>

<p>After importing numpy and pandas, be sure to provide a random seed if you want folks to be able to exactly reproduce your data and results.</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(123)
</code></pre>

<h3>A kitchen sink example</h3>

<p>Here's an example showing a variety of things you can do.  All kinds of useful sample dataframes could be created from a subset of this:</p>

<pre><code>df = pd.DataFrame({ 

    # some ways to create random data
    'a':np.random.randn(6),
    'b':np.random.choice( [5,7,np.nan], 6),
    'c':np.random.choice( ['panda','python','shark'], 6),

    # some ways to create systematic groups for indexing or groupby
    # this is similar to r's expand.grid(), see note 2 below
    'd':np.repeat( range(3), 2 ),
    'e':np.tile(   range(2), 3 ),

    # a date range and set of random dates
    'f':pd.date_range('1/1/2011', periods=6, freq='D'),
    'g':np.random.choice( pd.date_range('1/1/2011', periods=365, 
                          freq='D'), 6, replace=False) 
    })
</code></pre>

<p>This produces:</p>

<pre><code>          a   b       c  d  e          f          g
0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12
1  0.997345   7   shark  0  1 2011-01-02 2011-11-10
2  0.282978   5   panda  1  0 2011-01-03 2011-10-30
3 -1.506295   7  python  1  1 2011-01-04 2011-09-07
4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27
5  1.651437   7  python  2  1 2011-01-06 2011-02-03
</code></pre>

<p>Some notes:</p>

<ol>
<li><code>np.repeat</code> and <code>np.tile</code> (columns <code>d</code> and <code>e</code>) are very useful for creating groups and indices in a very regular way.  For 2 columns, this can be used to easily duplicate r's <code>expand.grid()</code> but is also more flexible in ability to provide a subset of all permutations.  However, for 3 or more columns the syntax quickly becomes unwieldy.</li>
<li>For a more direct replacement for r's <code>expand.grid()</code> see the <code>itertools</code> solution in the <a href=""http://pandas.pydata.org/pandas-docs/version/0.16.1/cookbook.html#creating-example-data"" rel=""nofollow noreferrer"">pandas cookbook</a> or the <code>np.meshgrid</code> solution shown <a href=""https://stackoverflow.com/questions/12130883/r-expand-grid-function-in-python"">here</a>.  Those will allow any number of dimensions.</li>
<li>You can do quite a bit with <code>np.random.choice</code>.  For example, in column <code>g</code>, we have a random selection of 6 dates from 2011.  Additionally, by setting <code>replace=False</code> we can assure these dates are unique -- very handy if we want to use this as an index with unique values.</li>
</ol>

<h3>Fake stock market data</h3>

<p>In addition to taking subsets of the above code, you can further combine the techniques to do just about anything.  For example, here's a short example that combines <code>np.tile</code> and <code>date_range</code> to create sample ticker data for 4 stocks covering the same dates:</p>

<pre><code>stocks = pd.DataFrame({ 
    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),
    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),
    'price':(np.random.randn(100).cumsum() + 10) })
</code></pre>

<p>Now we have a sample dataset with 100 lines (25 dates per ticker), but we have only used 4 lines to do it, making it easy for everyone else to reproduce without copying and pasting 100 lines of code.  You can then display subsets of the data if it helps to explain your question:</p>

<pre><code>&gt;&gt;&gt; stocks.head(5)

        date      price ticker
0 2011-01-01   9.497412   aapl
1 2011-01-02  10.261908   aapl
2 2011-01-03   9.438538   aapl
3 2011-01-04   9.515958   aapl
4 2011-01-05   7.554070   aapl

&gt;&gt;&gt; stocks.groupby('ticker').head(2)

         date      price ticker
0  2011-01-01   9.497412   aapl
1  2011-01-02  10.261908   aapl
25 2011-01-01   8.277772   goog
26 2011-01-02   7.714916   goog
50 2011-01-01   5.613023   yhoo
51 2011-01-02   6.397686   yhoo
75 2011-01-01  11.736584   msft
76 2011-01-02  11.944519   msft
</code></pre>
";;2;;2015-05-24T14:22:30.473;;30424537;2017-01-14T14:32:57.520;2017-05-23T11:47:22.050;;-1.0;;3877338.0;20109391.0;2;28;;;
28725;28725;;;"<p>I would use scikit-learn's own training_test_split, and generate it from the index</p>

<pre><code>from sklearn.cross_validation import train_test_split


y = df.pop('output')
X = df

X_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.2)
X.iloc[X_train] # return dataframe train
</code></pre>
";;2;;2015-05-26T09:33:30.157;;30454743;2015-10-13T11:11:23.943;2015-10-13T11:11:23.943;;1296136.0;;1296136.0;24147278.0;2;20;;;
28755;28755;;;"<p>Try calling <code>read_csv</code> with <code>encoding='latin1'</code>, <code>encoding='iso-8859-1'</code> or <code>encoding='cp1252'</code>; these the various encodings found on Windows.</p>
";;0;;2015-05-26T23:23:08.373;;30470630;2017-04-27T23:40:20.570;2017-04-27T23:40:20.570;;839957.0;;839957.0;30462807.0;2;27;;;
28870;28870;;;"<pre><code>&gt;&gt;&gt; list(my_dataframe)
['y', 'gdp', 'cap']
</code></pre>

<p>To list the columns of a dataframe while in debugger mode, use a list comprehension:</p>

<pre><code>&gt;&gt;&gt; [c for c in my_dataframe]
['y', 'gdp', 'cap']
</code></pre>
";;3;;2015-05-28T15:58:05.373;;30511605;2016-07-05T19:35:09.237;2016-07-05T19:35:09.237;;2411802.0;;2411802.0;19482970.0;2;21;;;
28874;28874;;;"<p>Assumed imports:</p>

<pre><code>import pandas as pd
</code></pre>

<p><a href=""https://stackoverflow.com/a/23671390/366309"">John Galt's answer</a> is basically a <code>reduce</code> operation.  If I have more than a handful of dataframes, I'd put them in a list like this (generated via list comprehensions or loops or whatnot):</p>

<pre><code>dfs = [df0, df1, df2, dfN]
</code></pre>

<p>Assuming they have some common column, like <code>name</code> in your example, I'd do the following:</p>

<pre><code>df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)
</code></pre>

<p>That way, your code should work with whatever number of dataframes you want to merge.</p>

<p><em>Edit August 1, 2016</em>: For those using Python 3: <code>reduce</code> has been moved into <code>functools</code>. So to use this function, you'll first need to import that module.</p>
";;4;;2015-05-28T17:08:50.627;;30512931;2016-12-08T15:34:38.423;2017-05-23T12:18:30.023;;-1.0;;366309.0;23668427.0;2;133;;;
28879;28879;;;"<p>Old post, but may be interesting: an idea (which is destructive, but does the job if you want it quick and dirty) is to rename columns using underscores:</p>

<pre><code>df1.columns = [c.replace(' ', '_') for c in df1.columns]
</code></pre>
";;1;;2015-05-28T18:42:30.483;;30514678;2015-05-28T18:42:30.483;;;;;168717.0;13757090.0;2;18;;;
28924;28924;;;"<p>I think you're almost there, try removing the extra square brackets around the lst's (Also you don't need to specify the column names when you're creating a dataframe from a dict like this):</p>

<pre><code>lst1 = range(100)
lst2 = range(100)
lst3 = range(100)
percentile_list = pd.DataFrame(
    {'lst1Tite': lst1,
     'lst2Tite': lst2,
     'lst3Tite': lst3
    })

percentile_list
    lst1Tite  lst2Tite  lst3Tite
0          0         0         0
1          1         1         1
2          2         2         2
3          3         3         3
4          4         4         4
5          5         5         5
6          6         6         6
...
</code></pre>

<p>If you need a somewhat more performant solution you can use <code>np.column_stack</code> rather than <code>zip</code> as in your first attempt, this has around a 2x speedup on the example here, however comes at bit of a cost of readability in my opinion:</p>

<pre><code>percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), 
                               columns=['lst1tite', 'lst2itie', 'lst3tite'])
</code></pre>
";;2;;2015-05-29T06:40:18.807;;30522778;2017-04-18T05:02:15.260;2017-04-18T05:02:15.260;;839957.0;;839957.0;30522724.0;2;59;;;
28928;28928;;;"<p><strong>Users</strong> </p>

<ol>
<li><p>When you need to append some new user just make a new list of all user details and append it</p></li>
<li><p>Easily sortable as @StevenRumbalski suggested</p></li>
<li><p>Searching will be easy</p></li>
<li><p>This is more compact and easily manageable as record grows (for some very high number of records I think we will need something better than users too)</p></li>
</ol>

<p><strong>Users2</strong></p>

<ol>
<li>Personally I am seeing this for the first time and I wouldn't approach this if I have a high number of records.</li>
</ol>

<p>PS: But I would like to learn advantages of users2 over users
    Again a nice question</p>
";;0;;2015-05-29T07:05:49.553;;30523225;2015-05-29T07:11:21.110;2015-05-29T07:11:21.110;;4067759.0;;4067759.0;30522982.0;2;6;;;
28939;28939;;;"<p>This relates to <a href=""https://en.wikipedia.org/wiki/Column-oriented_DBMS"">column oriented databases</a> versus row oriented. Your first example is a row oriented data structure, and the second is column oriented. In the particular case of Python, the first could be made notably more efficient using <a href=""https://docs.python.org/3/reference/datamodel.html#slots"">slots</a>, such that the dictionary of columns doesn't need to be duplicated for every row. </p>

<p>Which form works better depends a lot on what you do with the data; for instance, row oriented is natural if you only ever access all of any row. Column oriented meanwhile makes much better use of caches and such when you're searching by a particular field (in Python, this may be reduced by the heavy use of references; types like <a href=""https://docs.python.org/3/library/array.html"">array</a> can optimize that). Traditional row oriented databases frequently use column oriented sorted indices to speed up lookups, and knowing these techniques you can implement any combination using a key-value store. </p>

<p>Pandas does convert both your examples to the same format, but the conversion itself is more expensive for the row oriented structure, simply because every individual dictionary must be read. All of these costs may be marginal. </p>

<p>There's a third option not evident in your example: In this case, you only have two columns, one of which is an integer ID in a contiguous range from 0. This can be stored in the order of the entries itself, meaning the entire structure would be found in the list you've called <code>users2['name']</code>; but notably, the entries are incomplete without their position. The list translates into rows using enumerate(). It is common for databases to have this special case also (for instance, sqlite <a href=""https://www.sqlite.org/lang_createtable.html#rowid"">rowid</a>). </p>

<p>In general, start with a data structure that keeps your code sensible, and optimize only when you know your use cases and have a measurable performance issue. Tools like Pandas probably means most projects will function just fine without finetuning. </p>
";;0;;2015-05-29T08:46:39.683;;30525128;2015-05-29T08:46:39.683;;;;;379311.0;30522982.0;2;23;;;
28963;28963;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html""><code>drop_duplicates</code></a> method to get the unique rows in a DataFrame:</p>

<pre><code>In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})

In [30]: df
Out[30]:
   a  b
0  1  3
1  2  4
2  1  3
3  2  5

In [32]: df.drop_duplicates()
Out[32]:
   a  b
0  1  3
1  2  4
3  2  5
</code></pre>

<p>You can also provide the <code>subset</code> keyword argument if you only want to use certain columns to determine uniqueness. See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"">docstring</a>.</p>
";;1;;2015-05-29T14:18:00.987;;30531939;2015-05-29T14:18:00.987;;;;;653364.0;30530663.0;2;52;;;
28973;28973;;;"<p>My understanding is that this question is better answered over in <a href=""https://stackoverflow.com/questions/26921943/pandas-intersection-of-two-data-frames-based-on-column-entries"">this post</a>.</p>

<p>But briefly, the answer to the OP with this method is simply:</p>

<pre><code>s1 = pd.merge(df1, df2, how='inner', on=['user_id'])
</code></pre>

<p>Which gives s1 with 5 columns: user_id and the other two columns from each of df1 and df2.</p>
";;0;;2015-05-29T17:47:24.753;;30535957;2015-05-30T01:12:30.153;2017-05-23T11:54:53.223;;-1.0;;2055448.0;19618912.0;2;32;;;
28999;28999;;;"<p>As documented in <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html"">http://pandas.pydata.org/pandas-docs/stable/text.html</a>:</p>

<pre><code>df.columns = df.columns.str.replace('$','')
</code></pre>
";;0;;2015-05-30T13:24:05.190;;30546734;2015-05-30T13:24:05.190;;;;;1551810.0;11346283.0;2;76;;;
29019;29019;;;"<p>You can also use <code>pd.concat</code>, which is particularly helpful when you are joining more than two dataframes:</p>

<pre><code>bigdata = pd.concat([data1, data2], ignore_index=True)
</code></pre>
";;0;;2015-05-31T11:47:29.527;;30557040;2015-05-31T11:47:29.527;;;;;2063031.0;12850345.0;2;22;;;
29056;29056;;;"<p>You can also use <code>df.apply()</code> to iterate over rows and access multiple columns for a function.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"">docs: DataFrame.apply()</a></p>

<pre><code>def valuation_formula(x, y):
    return x * y * 0.5

df['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)
</code></pre>
";;3;;2015-06-01T06:24:44.837;;30566899;2015-06-01T06:24:44.837;;;;;1803298.0;16476924.0;2;37;;;
29124;29124;;;"<p>I just wanted to provide a bit of an update/special case since it looks like people still come here. If you're using a multi-index or otherwise using an index-slicer the inplace=True option may not be enough to update the slice you've chosen. For example in a 2x2 level multi-index this will not change any values (as of pandas 0.15):</p>

<pre class=""lang-py prettyprint-override""><code>idx = pd.IndexSlice
df.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)
</code></pre>

<p>The ""problem"" is that the chaining breaks the fillna ability to update the original dataframe. I put ""problem"" in quotes because there are good reasons for the design decisions that led to not interpreting through these chains in certain situations. Also, this is a complex example (though I really ran into it), but the same may apply to fewer levels of indexes depending on how you slice.</p>

<p>The solution is DataFrame.update:</p>

<pre class=""lang-py prettyprint-override""><code>df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))
</code></pre>

<p>It's one line, reads reasonably well (sort of) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like!</p>

<p>If anybody can find places this doesn't work please post in the comments, I've been messing with it and looking at the source and it seems to solve at least my multi-index slice problems.</p>
";;0;;2015-06-02T05:13:34.193;;30587837;2015-12-16T18:29:03.930;2015-12-16T18:29:03.930;;889722.0;;1373804.0;13295735.0;2;15;;;
29128;29128;;;"<p>As already suggested by HYRY in the comments, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""noreferrer"">Series.map</a> is the way to go here. Just set the index to the resulting series.</p>

<p>Simple example:</p>

<pre><code>df = pd.DataFrame({'d': [1, 2, 3]}, index=['FOO', 'BAR', 'BAZ'])
df
        d
FOO     1
BAR     2
BAZ     3

df.index = df.index.map(str.lower)
df
        d
foo     1
bar     2
baz     3
</code></pre>

<h1>Index != Series</h1>

<p>As pointed out by @OP. the <code>df.index.map(str.lower)</code> call returns a numpy array.
This is because dataframe indices <em>are</em> based on numpy arrays, not Series.</p>

<p>The only way of making the index into a Series is to create a Series from it.</p>

<pre><code>pd.Series(df.index.map(str.lower))
</code></pre>

<h3>Caveat</h3>

<p>The <code>Index</code> class now subclasses the <code>StringAccessorMixin</code>, which means that you can do the above operation as follows</p>

<pre><code>df.index.str.lower()
</code></pre>

<p>This still produces an Index object, not a Series.</p>
";;3;;2015-06-02T07:47:14.723;;30590280;2017-05-15T08:56:14.170;2017-05-15T08:56:14.170;;3730397.0;;3730397.0;20025325.0;2;30;;;
29296;29296;;;"<p>If you do not want to create a new row but simply put it in the empty cell then use:</p>

<pre><code>df.columns.name = 'foo'
</code></pre>

<p>Otherwise use: </p>

<pre><code>df.index.name = 'foo'
</code></pre>
";;1;;2015-06-04T00:42:03.133;;30633167;2015-06-04T00:42:03.133;;;;;3647167.0;18022845.0;2;7;;;
29351;29351;;;"<p>You can see e.gs. in the <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html#targeting-different-subplots"" rel=""nofollow noreferrer"">documentation</a> demonstrating joris answer. Also from the documentation, you could also set <code>subplots=True</code> and <code>layout=(,)</code> within the pandas <code>plot</code> function:  </p>

<pre><code>df.plot(subplots=True, layout=(1,2))
</code></pre>

<p>You could also use <code>fig.add_subplot()</code> which takes subplot grid parameters such as 221, 222, 223, 224, etc. as described in the post <a href=""https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-111-mean-in-fig-add-subplot111"">here</a>. Nice examples of plot on pandas data frame, including subplots, can be seen in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/fonnesbeck/5850463/raw/a29d9ffb863bfab09ff6c1fc853e1d5bf69fe3e4/3.+Plotting+and+Visualization.ipynb"" rel=""nofollow noreferrer"">this ipython notebook</a>.</p>
";;1;;2015-06-04T15:26:01.850;;30647987;2015-11-03T23:28:43.383;2017-05-23T11:47:19.010;;-1.0;;3594865.0;22483588.0;2;13;;;
29373;29373;;;"<h3>Pickle is better for the following:</h3>

<ol>
<li>Numerical data or anything that uses the buffer protocol (numpy arrays) (though only if you use a somewhat recent <code>protocol=</code>)</li>
<li>Python specific objects like classes, functions, etc.. (although here you should look at <code>dill</code>)</li>
</ol>

<h3>MsgPack is better for the following:</h3>

<ol>
<li>Cross language interoperation.  It's an alternative to JSON with some improvements</li>
<li>Performance on text data and Python objects.  It's a decent factor faster than Pickle at this under any setting. </li>
</ol>

<p>As @Jeff noted above this blogpost may be of interest <a href=""http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/"">http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/</a></p>
";;3;;2015-06-04T19:26:32.597;;30652445;2015-06-04T19:33:51.360;2015-06-04T19:33:51.360;;616616.0;;616616.0;30651724.0;2;10;;;
29381;29381;;;"<p>Using the engine in place of the raw_connection() worked:</p>

<pre><code>import pandas as pd
import mysql.connector
from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)
data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)
</code></pre>

<p>not clear on why when I tried this yesterday it gave me the earlier error</p>
";;2;;2015-06-04T20:56:29.050;;30653988;2015-06-04T20:56:29.050;;;;;2364773.0;30631325.0;2;37;;;
29478;29478;;;"<p>You can also use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html"" rel=""noreferrer"">option_context</a>, with one or more options:</p>

<pre><code>with pd.option_context('display.max_rows', None, 'display.max_columns', 3):
    print(df)
</code></pre>

<p>This will automatically return the options to their previous values.</p>
";;6;;2015-06-07T09:22:15.250;;30691921;2017-04-15T20:18:49.637;2017-04-15T20:18:49.637;;1291563.0;;550269.0;19124601.0;2;221;;;
29521;29521;;;"<p>For posterity: as of <a href=""http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#v0-15-0-october-18-2014"" rel=""noreferrer"">0.15.0</a>, there is a handy <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#basics-dt-accessors"" rel=""noreferrer"">.dt accessor</a> you can use to pull such values from a datetime/period series (in the above case, just <code>sales.timestamp.dt.hour</code>!</p>
";;2;;2015-06-08T19:20:23.023;;30717361;2016-12-15T20:24:58.693;2016-12-15T20:24:58.693;;2389827.0;;2389827.0;25129144.0;2;24;;;
29557;29557;;;"<p>Old thread but thought I would share my solution with 2d extrapolation/interpolation, respecting index values, which also works on demand. Code ended up a bit weird so let me know if there is a better solution:</p>

<pre><code>import pandas
from   numpy import nan
import numpy

dataGrid = pandas.DataFrame({1: {1: 1, 3: 2},
                             2: {1: 3, 3: 4}})


def getExtrapolatedInterpolatedValue(x, y):
    global dataGrid
    if x not in dataGrid.index:
        dataGrid.ix[x] = nan
        dataGrid = dataGrid.sort()
        dataGrid = dataGrid.interpolate(method='index', axis=0).ffill(axis=0).bfill(axis=0)

    if y not in dataGrid.columns.values:
        dataGrid = dataGrid.reindex(columns=numpy.append(dataGrid.columns.values, y))
        dataGrid = dataGrid.sort_index(axis=1)
        dataGrid = dataGrid.interpolate(method='index', axis=1).ffill(axis=1).bfill(axis=1)

    return dataGrid[y][x]


print getExtrapolatedInterpolatedValue(2, 1.4)
&gt;&gt;2.3
</code></pre>
";;1;;2015-06-09T13:48:34.420;;30733959;2015-09-25T14:54:39.510;2015-09-25T14:54:39.510;;117268.0;;441462.0;10464738.0;2;6;;;
29573;29573;;;"<p>This is conciser:</p>

<pre><code># select the float columns
df_num = df.select_dtypes(include=[np.float])
# select non-numeric columns
df_num = df.select_dtypes(exclude=[np.number])
</code></pre>
";;2;;2015-06-09T18:34:13.417;;30740087;2015-06-09T18:34:13.417;;;;;489564.0;21720022.0;2;21;;;
29686;29686;;;"<p>I got the dreaded <code>SettingWithCopyWarning</code>, and it wasn't fixed by using the iloc syntax. My DataFrame was created by read_sql from an ODBC source. Using a suggestion by lowtech above, the following worked for me:</p>

<pre><code>df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))
</code></pre>

<p>This worked fine to insert the column at the end. I don't know if it is the most efficient, but I don't like warning messages. I think there is a better solution, but I can't find it, and I think it depends on some aspect of the index.<br>
<em>Note</em>. That this only works once and will give an error message if trying to overwrite and existing column.<br>
<strong>Note</strong> As above and from 0.16.0 assign is the best solution. See documentation <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign</a> 
Works well for data flow type where you don't overwrite your intermediate values.</p>
";;1;;2015-06-11T09:45:04.733;;30777185;2016-10-21T11:32:43.910;2016-10-21T11:32:43.910;;3649456.0;;3649456.0;12555323.0;2;12;;;
29688;29688;;;"<p>Chaining conditions creates long lines, which are discouraged by pep8. 
Using the .query method forces to use strings, which is powerful but unpythonic and not very dynamic. </p>

<p>Once each of the filters is in place, one approach is</p>

<pre><code>import numpy as np
import functools
def conjunction(*conditions):
    return functools.reduce(np.logical_and, conditions)

c_1 = data.col1 == True
c_2 = data.col2 &lt; 64
c_3 = data.col3 != 4

data_filtered = data[conjunction(c1,c2,c3)]
</code></pre>

<p>np.logical operates on and is fast, but does not take more than two arguments, which is handled by functools.reduce. </p>

<p>Note that this still has some redundancies: a) shortcutting does not happen on a global level b) Each of the individual conditions runs on the whole initial data. Still, I expect this to be efficient enough for many applications and it is very readable. </p>
";;3;;2015-06-11T10:28:54.297;;30778300;2015-06-11T10:28:54.297;;;;;2240326.0;13611065.0;2;10;;;
29698;29698;;;"<p>To be safe (in the sense: this will work in all cases), you need to index both row index and columns, for which you can use <code>pd.IndexSlice</code> to do this easily:</p>

<pre><code>In [26]: idx = pd.IndexSlice

In [27]: df.loc[idx[:, :, 'C1', :],:]
Out[27]:
                           value
first second third fourth
A0    B0     C1    D0          2
                   D1          3
      B1     C1    D0         10
                   D1         11
A1    B0     C1    D0         18
                   D1         19
      B1     C1    D0         26
                   D1         27
A2    B0     C1    D0         34
                   D1         35
      B1     C1    D0         42
                   D1         43
A3    B0     C1    D0         50
                   D1         51
      B1     C1    D0         58
                   D1         59
</code></pre>

<p>Here <code>idx[:, :, 'C1', :]</code> is an easier way to write <code>[slice(None), slice(None),'C1', slice(None)]</code>. Instead of <code>pd.IndexSlice</code>, you can also use <code>np.s_</code> which is a bit shorter.</p>

<p>The reason that the other ones work, I am not fully sure of. But see the note in the documentation here: <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers"">http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers</a> (the first red warning box) where it is stated that:</p>

<blockquote>
  <p>You should specify all axes in the <code>.loc</code> specifier, meaning the indexer for the index and for the columns. Their are some ambiguous cases where the passed indexer could be mis-interpreted as indexing <em>both</em> axes, rather than into say the MuliIndex for the rows.</p>
</blockquote>
";;1;;2015-06-11T13:06:10.643;;30781664;2015-06-11T13:12:32.950;2015-06-11T13:12:32.950;;653364.0;;653364.0;30781037.0;2;8;;;
29730;29730;;;"<pre><code>df[df.Letters=='C'].Letters.item()
</code></pre>

<p>This returns the first element in the Index/Series returned from that selection. In this case, the value is always the first element. </p>

<p><strong>EDIT:</strong></p>

<p>Or you can run a loc() and access the first element that way. This was shorter and is the way I have implemented it in the past. </p>

<ul>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.item.html?highlight=item#pandas.Index.item"" rel=""noreferrer"">Pandas Index doc</a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.item.html?highlight=item#pandas.Series.item"" rel=""noreferrer"">Pandas Series doc</a></li>
</ul>
";;1;;2015-06-11T18:11:25.287;;30788360;2017-03-06T18:39:09.117;2017-03-06T18:39:09.117;;2127296.0;;1389801.0;30787901.0;2;29;;;
29732;29732;;;"<p>Use the <code>values</code> attribute to return the values as a np array and then use <code>[0]</code> to get the first value:</p>

<pre><code>In [4]:
df.loc[df.Letters=='C','Letters'].values[0]

Out[4]:
'C'
</code></pre>

<p><strong>EDIT</strong></p>

<p>I personally prefer to access the columns using subscript operators:</p>

<pre><code>df.loc[df['Letters'] == 'C', 'Letters'].values[0]
</code></pre>

<p>This avoids issues where the column names can have spaces or dashes <code>-</code> which mean that accessing using <code>.</code>.</p>
";;5;;2015-06-11T18:21:35.933;;30788555;2015-06-12T15:28:46.047;2015-06-12T15:28:46.047;;704848.0;;704848.0;30787901.0;2;8;;;
29795;29795;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html""><code>DataFrame.filter</code></a> this way:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame(np.array([[2,4,4],[4,3,3],[5,9,1]]),columns=['d','t','didi'])
&gt;&gt;
   d  t  didi
0  2  4     4
1  4  3     3
2  5  9     1

df.filter(regex=(""d.*""))

&gt;&gt;
   d  didi
0  2     4
1  4     3
2  5     1
</code></pre>

<p>The idea is to select columns by <code>regex</code></p>
";;0;;2015-06-12T17:04:16.457;;30808571;2015-06-12T17:04:16.457;;;;;4759209.0;30808430.0;2;32;;;
29798;29798;;;"<p>Use <code>select</code>:</p>

<pre><code>import pandas as pd

df = pd.DataFrame([[10, 14, 12, 44, 45, 78]], columns=['a', 'b', 'c', 'd1', 'd2', 'd3'])

df.select(lambda col: col.startswith('d'), axis=1)
</code></pre>

<p>Result:</p>

<pre><code>   d1  d2  d3
0  44  45  78
</code></pre>

<p>This is a nice solution if you're not comfortable with regular expressions.</p>
";;0;;2015-06-12T17:12:38.387;;30808690;2015-06-12T17:20:29.327;2015-06-12T17:20:29.327;;2467140.0;;2467140.0;30808430.0;2;9;;;
29949;29949;;;"<p>If you're using pandas.read_csv you can directly sample when loading the data, by using the skiprows parameter. Here is a short article I've written on this - <a href=""https://nikolaygrozev.wordpress.com/2015/06/16/fast-and-simple-sampling-in-pandas-when-loading-data-from-files/"">https://nikolaygrozev.wordpress.com/2015/06/16/fast-and-simple-sampling-in-pandas-when-loading-data-from-files/</a></p>
";;2;;2015-06-16T04:24:11.907;;30858753;2015-06-16T07:03:17.947;2015-06-16T07:03:17.947;;3682523.0;;3682523.0;12190874.0;2;6;;;
30177;30177;;;"<p>I'd <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html#pandas.concat""><code>concat</code></a> using a DataFrame ctor:</p>

<pre><code>In [23]:
df = pd.DataFrame(columns=['A'])
df

Out[23]:
Empty DataFrame
Columns: [A]
Index: []

In [24]:    
pd.concat([df,pd.DataFrame(columns=list('BCD'))])

Out[24]:
Empty DataFrame
Columns: [A, B, C, D]
Index: []
</code></pre>

<p>So by passing a list containing your original df, and a new one with the columns you wish to add, this will return a new df with the additional columns.</p>
";;6;;2015-06-18T22:13:41.110;;30926717;2015-06-18T22:13:41.110;;;;;704848.0;30926670.0;2;24;;;
30221;30221;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reindex.html""><code>df.reindex</code></a> to add new columns:</p>

<pre><code>In [18]: df = pd.DataFrame(np.random.randint(10, size=(5,1)), columns=['A'])

In [19]: df
Out[19]: 
   A
0  4
1  7
2  0
3  7
4  6

In [20]: df.reindex(columns=list('ABCD'))
Out[20]: 
   A   B   C   D
0  4 NaN NaN NaN
1  7 NaN NaN NaN
2  0 NaN NaN NaN
3  7 NaN NaN NaN
4  6 NaN NaN NaN
</code></pre>

<p><code>reindex</code> will return a new DataFrame, with columns appearing in the order they are listed:</p>

<pre><code>In [31]: df.reindex(columns=list('DCBA'))
Out[31]: 
    D   C   B  A
0 NaN NaN NaN  4
1 NaN NaN NaN  7
2 NaN NaN NaN  0
3 NaN NaN NaN  7
4 NaN NaN NaN  6
</code></pre>

<p>The <code>reindex</code> method as a <code>fill_value</code> parameter as well:</p>

<pre><code>In [22]: df.reindex(columns=list('ABCD'), fill_value=0)
Out[22]: 
   A  B  C  D
0  4  0  0  0
1  7  0  0  0
2  0  0  0  0
3  7  0  0  0
4  6  0  0  0
</code></pre>
";;0;;2015-06-19T17:00:52.147;;30943503;2015-06-19T17:00:52.147;;;;;190597.0;30926670.0;2;26;;;
30337;30337;;;"<p>Pandas 0.16.1 have a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"">sample</a> method for that.</p>
";;2;;2015-06-22T03:13:46.380;;30971633;2015-06-22T03:13:46.380;;;;;3773209.0;12190874.0;2;13;;;
30413;30413;;;"<p>You could also achieve that by renaming the columns:</p>

<p><code>df.columns = ['a', 'b']</code></p>

<p>This involves a manual step but could be an option especially if you would eventually rename your data frame.</p>
";;0;;2015-06-23T00:29:18.180;;30991980;2015-06-23T00:29:18.180;;;;;3594865.0;22233488.0;2;10;;;
30478;30478;;;"<p>Similar to unutbu above, you could also use <code>applymap</code> as follows:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])

df = df.applymap(""${0:.2f}"".format)
</code></pre>
";;0;;2015-06-24T04:43:40.630;;31017785;2017-07-11T14:17:31.787;2017-07-11T14:17:31.787;;2565457.0;;3594865.0;20937538.0;2;7;;;
30505;30505;;;"<p>For pandas being compiled on Elastic Beanstalk, make sure to have both packages: <code>gcc-c++</code> <em>and</em> <code>python-devel</code></p>

<pre><code>packages:
   yum:
      gcc-c++: []
      python-devel: []
</code></pre>
";;1;;2015-06-24T12:35:03.967;;31026736;2015-06-24T12:35:03.967;;;;;1245622.0;29516084.0;2;8;;;
30517;30517;;;"<p>You can simply use <code>value_counts</code> on the series:</p>

<pre><code>df['colour'].value_counts().plot(kind='bar')
</code></pre>

<p><a href=""https://i.stack.imgur.com/ouoSE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ouoSE.png"" alt=""enter image description here""></a></p>
";;1;;2015-06-24T14:50:53.620;;31029857;2017-08-22T14:52:18.867;2017-08-22T14:52:18.867;;2411802.0;;2411802.0;31029560.0;2;43;;;
30518;30518;;;"<p>like this : </p>

<pre><code>df.groupby('colour').size().plot(kind='bar')
</code></pre>
";;0;;2015-06-24T14:51:08.000;;31029861;2015-06-24T14:51:08.000;;;;;3749146.0;31029560.0;2;9;;;
30532;30532;;;"<p>You might find useful <a href=""http://statsmodels.sourceforge.net/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html"" rel=""nofollow noreferrer""><code>mosaic</code></a> plot from statsmodels. Which can also give statistical highlighting for the variances.</p>

<pre><code>from statsmodels.graphics.mosaicplot import mosaic
plt.rcParams['font.size'] = 16.0
mosaic(df, ['direction', 'colour']);
</code></pre>

<p><img src=""https://i.stack.imgur.com/xo97n.png"" alt=""enter image description here""></p>

<p>But beware of the 0 sized cell - they will cause problems with labels.</p>

<p>See <a href=""https://stackoverflow.com/a/31031988/4077912"">this answer</a> for details</p>
";;2;;2015-06-24T17:53:05.237;;31033603;2015-06-24T19:27:49.963;2017-05-23T10:31:12.887;;-1.0;;4077912.0;31029560.0;2;6;;;
30540;30540;;;"<p>Similar question as: <a href=""https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-columns"">pandas: How do I split text in a column into multiple rows?</a></p>

<p>You could do:</p>

<pre><code>&gt;&gt; a=pd.DataFrame({""var1"":""a,b,c d,e,f"".split(),""var2"":[1,2]})
&gt;&gt; s = a.var1.str.split("","").apply(pd.Series, 1).stack()
&gt;&gt; s.index = s.index.droplevel(-1)
&gt;&gt; del a['var1']
&gt;&gt; a.join(s)
   var2 var1
0     1    a
0     1    b
0     1    c
1     2    d
1     2    e
1     2    f
</code></pre>
";;1;;2015-06-24T21:01:57.303;;31036962;2015-06-24T21:01:57.303;2017-05-23T12:18:25.157;;-1.0;;1894184.0;12680754.0;2;7;;;
30542;30542;;;"<pre><code>mycolumns = ['A', 'B']
df = pd.DataFrame(columns=mycolumns)
rows = [[1,2],[3,4],[5,6]]
for row in rows:
    df.loc[len(df)] = row
</code></pre>
";;1;;2015-06-24T21:06:31.733;;31037040;2015-06-24T21:06:31.733;;;;;4441538.0;10715965.0;2;34;;;
30546;30546;;;"<p>If you only want the mean of the <code>weight</code> column, select the column (which is a Series) and call <code>.mean()</code>:</p>

<pre><code>In [479]: df
Out[479]: 
         ID  birthyear    weight
0    619040       1962  0.123123
1    600161       1963  0.981742
2  25602033       1963  1.312312
3    624870       1987  0.942120

In [480]: df[""weight""].mean()
Out[480]: 0.83982437500000007
</code></pre>
";;1;;2015-06-24T21:26:16.677;;31037360;2015-06-24T21:26:16.677;;;;;487339.0;31037298.0;2;41;;;
30624;30624;;;"<p>For me skyjur's answer almost worked. I had to set the engine for the writer explicitly with:</p>

<pre><code>writer = pd.ExcelWriter(excel_file, engine='openpyxl')
</code></pre>

<p>otherwise it would throw</p>

<pre><code>AttributeError: 'Workbook' object has no attribute 'add_worksheet'
</code></pre>
";;4;;2015-06-25T22:22:15.443;;31061820;2015-06-25T22:22:15.443;;;;;1894184.0;20219254.0;2;21;;;
30655;30655;;;"<p>For MultiIndex you can extract its subindex using </p>

<pre><code>df['si_name'] = R.index.get_level_values('si_name') 
</code></pre>

<p>where <code>si_name</code> is the name of the subindex.</p>
";;0;;2015-06-26T14:13:58.033;;31075478;2015-06-26T14:13:58.033;;;;;1796223.0;20461165.0;2;18;;;
30667;30667;;;"<p>Multiple column search with dataframe:</p>

<pre><code>frame[frame.filename.str.match('*.'+MetaData+'.*') &amp; frame.file_path.str.match('C:\test\test.txt')]
</code></pre>
";;2;;2015-06-26T15:10:26.923;;31076657;2016-02-17T23:37:46.133;2016-02-17T23:37:46.133;;1030110.0;;5053619.0;15325182.0;2;6;;;
30722;30722;;;"<p>Here is how to create a DataFrame where <strong>each series is a row</strong>.</p>

<p>For a single Series (resulting in a single-row DataFrame):</p>

<pre><code>series = pd.Series([1,2], index=['a','b'])
df = pd.DataFrame([series])
</code></pre>

<p>For multiple series with identical indices:</p>

<pre><code>cols = ['a','b']
list_of_series = [pd.Series([1,2],index=cols), pd.Series([3,4],index=cols)]
df = pd.DataFrame(list_of_series, columns=cols)
</code></pre>

<p>For multiple series with possibly different indices:</p>

<pre><code>list_of_series = [pd.Series([1,2],index=['a','b']), pd.Series([3,4],index=['a','c'])]
df = pd.concat(list_of_series, axis=1).transpose()
</code></pre>

<p>To create a DataFrame where <strong>each series is a column</strong>, see the answers by others. Alternatively, one can create a  DataFrame where each series is a row, as above, and then use <code>df.transpose()</code>. However, the latter approach is inefficient if the columns have different data types.</p>
";;0;;2015-06-28T08:57:34.607;;31097813;2015-06-28T09:38:02.737;2015-06-28T09:38:02.737;;188986.0;;188986.0;23521511.0;2;11;;;
30743;30743;;;"<p>The R and Python are not strictly identical because you build a data frame in Python/rpy2 whereas you use vectors (without a data frame) in R.</p>

<p>Otherwise, the conversion shipping with <code>rpy2</code> appears to be working here:</p>

<pre><code>from rpy2.robjects import pandas2ri
pandas2ri.activate()
robjects.globalenv['dataframe'] = dataframe
M = stats.lm('y~x', data=base.as_symbol('dataframe'))
</code></pre>

<p>The result:</p>

<pre><code>&gt;&gt;&gt; print(base.summary(M).rx2('coefficients'))
            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)      0.6  1.1489125 0.522233 0.6376181
x                0.8  0.3464102 2.309401 0.1040880
</code></pre>
";;4;;2015-06-28T23:43:17.700;;31105951;2015-06-28T23:43:17.700;;;;;294017.0;30922213.0;2;10;;;
30951;30951;;;"<p>List comprehension is another way to create another column conditionally. If you are working with object dtypes in columns, like in your example, list comprehensions typically outperform most other methods.</p>

<p>Example list comprehension:</p>

<pre><code>df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]
</code></pre>

<p><strong>%timeit tests:</strong></p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
%timeit df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]
%timeit df['color'] = np.where(df['Set']=='Z', 'green', 'red')
%timeit df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')

1000 loops, best of 3: 239 s per loop
1000 loops, best of 3: 523 s per loop
1000 loops, best of 3: 263 s per loop
</code></pre>
";;1;;2015-07-02T00:27:06.443;;31173785;2017-08-16T16:49:28.483;2017-08-16T16:49:28.483;;4561314.0;;1803298.0;19913659.0;2;42;;;
31023;31023;;;"<p>After calling <code>pandas2ri.activate()</code> some conversions from Pandas objects to R objects happen automatically. For example, you can use</p>

<pre><code>M = R.lm('y~x', data=df)
</code></pre>

<p>instead of </p>

<pre><code>robjects.globalenv['dataframe'] = dataframe
M = stats.lm('y~x', data=base.as_symbol('dataframe'))
</code></pre>

<hr>

<pre><code>import pandas as pd
from rpy2 import robjects as ro
from rpy2.robjects import pandas2ri
pandas2ri.activate()
R = ro.r

df = pd.DataFrame({'x': [1,2,3,4,5], 
                   'y': [2,1,3,5,4]})

M = R.lm('y~x', data=df)
print(R.summary(M).rx2('coefficients'))
</code></pre>

<p>yields</p>

<pre><code>            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)      0.6  1.1489125 0.522233 0.6376181
x                0.8  0.3464102 2.309401 0.1040880
</code></pre>
";;0;;2015-07-03T12:12:58.207;;31206596;2015-07-03T12:12:58.207;;;;;190597.0;30922213.0;2;16;;;
31135;31135;;;"<p>You can just use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html""><code>np.savetxt</code></a> and access the np attribute <code>.values</code>:</p>

<pre><code>np.savetxt(r'c:\data\np.txt', df.values, fmt='%d')
</code></pre>

<p>yields:</p>

<pre><code>18 55 1 70
18 55 2 67
18 57 2 75
18 58 1 35
19 54 2 70
</code></pre>

<p>or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv""><code>to_csv</code></a>:</p>

<pre><code>df.to_csv(r'c:\data\pandas.txt', header=None, index=None, sep=' ', mode='a')
</code></pre>

<p>Note for <code>np.savetxt</code> you'd have to pass a filehandle that has been created with append mode.</p>
";;4;;2015-07-06T13:33:07.287;;31247247;2015-07-06T13:56:34.573;2015-07-06T13:56:34.573;;704848.0;;704848.0;31247198.0;2;22;;;
31136;31136;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv"" rel=""noreferrer"">pandas.DataFrame.to_csv()</a>, and setting both <code>index</code> and <code>header</code> to <code>False</code>:</p>

<pre><code>In [97]: print df.to_csv(sep=' ', index=False, header=False)
18 55 1 70
18 55 2 67
18 57 2 75
18 58 1 35
19 54 2 70
</code></pre>

<p><code>pandas.DataFrame.to_csv</code> can write to a file directly, for more info you can refer to the docs linked above.</p>
";;3;;2015-07-06T13:34:52.743;;31247279;2015-07-06T13:36:16.403;2015-07-06T13:36:16.403;;3849456.0;;3849456.0;31247198.0;2;6;;;
31182;31182;;;"<p>I have the same error and have decided that it is a bug. It seems to be caused by the presence of NaN values in a DataFrame in Spyder. I have uninstalled and reinstalled all packages and nothing has effected it. NaN  values are supported and are completely valid in DataFrames especially if they have a DateTime index. </p>

<p>In the end I have settled for suppressing this warnings as follows. </p>

<pre><code>import warnings
warnings.simplefilter(action = ""ignore"", category = RuntimeWarning)
</code></pre>
";;0;;2015-07-07T00:34:15.127;;31257931;2015-07-07T00:34:15.127;;;;;1664953.0;30519487.0;2;26;;;
31291;31291;;;"<h3>tl;dr</h3>

<p>The pandas equivalent to </p>

<pre><code>select * from table where column_name = some_value
</code></pre>

<p>is</p>

<pre><code>table[table.column_name == some_value]
</code></pre>

<p>Multiple conditions:</p>

<pre><code>table((table.column_name == some_value) | (table.column_name2 == some_value2))
</code></pre>

<p>or</p>

<pre><code>table.query('column_name == some_value | column_name2 == some_value2')
</code></pre>

<h3>Code example</h3>

<pre><code>import pandas as pd

# Create data set
d = {'foo':[100, 111, 222], 
     'bar':[333, 444, 555]}
df = pd.DataFrame(d)

# Full dataframe:
df

# Shows:
#    bar   foo 
# 0  333   100
# 1  444   111
# 2  555   222

# Output only the row(s) in df where foo is 222:
df[df.foo == 222]

# Shows:
#    bar  foo
# 2  555  222
</code></pre>

<p>In the above code it is the line <code>df[df.foo == 222]</code> that gives the rows based on the column value, <code>222</code> in this case.</p>

<p>Multiple conditions are also possible:</p>

<pre><code>df[(df.foo == 222) | (df.bar == 444)]
#    bar  foo
# 1  444  111
# 2  555  222
</code></pre>

<p>But at that point I would recommend using the query function, since it's less verbose and yields the same result:</p>

<pre><code>df.query('foo == 222 | bar == 444')
</code></pre>
";;2;;2015-07-08T15:17:38.740;;31296878;2017-03-13T09:57:05.670;2017-03-13T09:57:05.670;;955014.0;;955014.0;17071871.0;2;58;;;
31294;31294;;;"<p>I'd recommend using the <code>log=True</code> parameter in the pyplot hist function:</p>

<pre><code>import matplotlib.pyplot as plt    
plt.hist(df['column_name'], log=True) 
</code></pre>
";;1;;2015-07-08T15:45:06.853;;31297540;2015-07-08T15:45:06.853;;;;;5094722.0;21033720.0;2;20;;;
31454;31454;;;"<p>""It's an optional dependency <a href=""http://pandas.pydata.org/pandas-docs/stable/install.html#optional-dependencies"" rel=""noreferrer"">pandas.pydata.org/pandas-docs/</a> You can install it separately."" -- (<a href=""https://stackoverflow.com/users/42346/bernie"">bernie</a>, in a now-deleted commented.)</p>
";;1;2015-07-10T01:39:54.913;2015-07-10T01:39:54.913;;31331100;2017-01-06T16:51:10.850;2017-01-06T16:51:10.850;;8747.0;;8747.0;31329627.0;2;15;;;
31458;31458;;;"<p>You have <code>unicode</code> values in your DataFrame. Files store bytes, which means all <code>unicode</code> have to be encoded into bytes before they can be stored in a file. You have to specify an encoding, such as <code>utf-8</code>. For example, </p>

<pre><code>df.to_csv('path', header=True, index=False, encoding='utf-8')
</code></pre>

<p>If you don't specify an encoding, then the encoding used by <code>df.to_csv</code> defaults to <code>ascii</code> in Python2, or <code>utf-8</code> in Python3.</p>
";;0;;2015-07-10T02:23:38.980;;31331449;2015-07-10T02:23:38.980;;;;;190597.0;31331358.0;2;24;;;
31540;31540;;;"<p>You could do everything with your map and lambda:</p>

<pre><code>tokenized=map(lambda msg, ft1, ft2: features([msg,ft1,ft2]), posts.message,posts.feature_1, posts.feature_2)
</code></pre>

<p>This saves doing your interim temp step and iterates through the 3 columns.</p>

<p>Another solution would be convert the messages into their CountVectorizer sparse matrix and join this matrix with the feature values from the posts dataframe (this skips having to construct a dict and produces a sparse matrix similar to what you would get with DictVectorizer):</p>

<pre><code>import scipy as sp
posts = pd.read_csv('post.csv')

# Create vectorizer for function to use
vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))
y = posts[""score""].values.astype(np.float32) 

X = sp.sparse.hstack((vectorizer.fit_transform(posts.message),posts[['feature_1','feature_2']].values),format='csr')
X_columns=vectorizer.get_feature_names()+posts[['feature_1','feature_2']].columns.tolist()


posts
Out[38]: 
   ID              message  feature_1  feature_2  score
0   1   'This is the text'          4          7     10
1   2  'This is more text'          3          2      9
2   3   'More random text'          3          2      9

X_columns
Out[39]: 
[u'is',
 u'is more',
 u'is the',
 u'more',
 u'more random',
 u'more text',
 u'random',
 u'random text',
 u'text',
 u'the',
 u'the text',
 u'this',
 u'this is',
 'feature_1',
 'feature_2']

X.toarray()
Out[40]: 
array([[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 4, 7],
       [1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 3, 2],
       [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 3, 2]])
</code></pre>

<p>Additionally sklearn-pandas has DataFrameMapper which does what you're looking for too:</p>

<pre><code>from sklearn_pandas import DataFrameMapper
mapper = DataFrameMapper([
    (['feature_1', 'feature_2'], None),
    ('message',CountVectorizer(binary=True, ngram_range=(1, 2)))
])
X=mapper.fit_transform(posts)

X
Out[71]: 
array([[4, 7, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
       [3, 2, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1],
       [3, 2, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0]])
</code></pre>

<p>Note:X is not sparse when using this last method.</p>

<pre><code>X_columns=mapper.features[0][0]+mapper.features[1][1].get_feature_names()

X_columns
Out[76]: 
['feature_1',
 'feature_2',
 u'is',
 u'is more',
 u'is the',
 u'more',
 u'more random',
 u'more text',
 u'random',
 u'random text',
 u'text',
 u'the',
 u'the text',
 u'this',
 u'this is']
</code></pre>
";;0;;2015-07-10T22:48:31.220;;31351465;2015-07-11T20:49:05.530;2015-07-11T20:49:05.530;;5029279.0;;5029279.0;30653642.0;2;10;;;
31549;31549;;;"<p>pandas dataframe plot will return the <code>ax</code> for you, And then you can start to manipulate the axes whatever you want.</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(100,5))

# you get ax from here
ax = df.plot()
type(ax)  # matplotlib.axes._subplots.AxesSubplot

# manipulate
vals = ax.get_yticks()
ax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])
</code></pre>

<p><img src=""https://i.stack.imgur.com/lZTy0.png"" alt=""enter image description here""></p>
";;2;;2015-07-11T13:36:31.507;;31357733;2015-07-11T13:41:52.883;2015-07-11T13:41:52.883;;5014134.0;;5014134.0;31357611.0;2;40;;;
31579;31579;;;"<p>You can use the familiar Matplotlib style calling a <code>figure</code> and <code>subplot</code>, but you simply need to specify the current axis using <code>plt.gca()</code>. An example:</p>

<pre><code>plt.figure(1)
plt.subplot(2,2,1)
df.A.plot() #no need to specify for first axis
plt.subplot(2,2,2)
df.B.plot(ax=plt.gca())
plt.subplot(2,2,3)
df.C.plot(ax=plt.gca())
</code></pre>

<p>etc...</p>
";;0;;2015-07-12T03:29:19.617;;31364094;2015-07-12T03:38:04.843;2015-07-12T03:38:04.843;;2592560.0;;2592560.0;22483588.0;2;7;;;
31580;31580;;;"<h3><code>map_partitions</code></h3>

<p>You can apply your function to all of the partitions of your dataframe with the <code>map_partitions</code> function.</p>

<pre><code>df.map_partitions(func, columns=...)
</code></pre>

<p>Note that func will be given only part of the dataset at a time, not the entire dataset like with <code>pandas apply</code> (which presumably you wouldn't want if you want to do parallelism.)</p>

<h3><code>map</code> / <code>apply</code></h3>

<p>You can map a function row-wise across a series with <code>map</code></p>

<pre><code>df.mycolumn.map(func)
</code></pre>

<p>You can map a function row-wise across a dataframe with <code>apply</code></p>

<pre><code>df.apply(func, axis=1)
</code></pre>

<h3>Threads vs Processes</h3>

<p>As of version 0.6.0 <code>dask.dataframes</code> parallelizes with threads. Custom Python functions will not receive much benefit from thread-based parallelism.  You could try processes instead</p>

<pre><code>df = dd.read_csv(...)

from dask.multiprocessing import get
df.map_partitions(func, columns=...).compute(get=get)
</code></pre>

<h3>But avoid <code>apply</code></h3>

<p>However, you should really avoid <code>apply</code> with custom Python functions, both in Pandas and in Dask.  This is often a source of poor performance.  It could be that if you find a way to do your operation in a vectorized manner then it could be that your Pandas code will be 100x faster and you won't need dask.dataframe at all.</p>

<h3>Consider <code>numba</code></h3>

<p>For your particular problem you might consider <a href=""http://numba.pydata.org/""><code>numba</code></a>.  This significantly improves your performance.</p>

<pre><code>In [1]: import numpy as np
In [2]: import pandas as pd
In [3]: s = pd.Series([10000]*120)

In [4]: %paste
def slow_func(k):
    A = np.random.normal(size = k) # k = 10000
    s = 0
    for a in A:
        if a &gt; 0:
            s += 1
        else:
            s -= 1
    return s
## -- End pasted text --

In [5]: %time _ = s.apply(slow_func)
CPU times: user 345 ms, sys: 3.28 ms, total: 348 ms
Wall time: 347 ms

In [6]: import numba
In [7]: fast_func = numba.jit(slow_func)

In [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead
CPU times: user 179 ms, sys: 0 ns, total: 179 ms
Wall time: 175 ms

In [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain
CPU times: user 68.8 ms, sys: 27 s, total: 68.8 ms
Wall time: 68.7 ms
</code></pre>

<p>Disclaimer, I work for the company that makes both <code>numba</code> and <code>dask</code> and employs many of the <code>pandas</code> developers.</p>
";;5;;2015-07-12T03:35:33.483;;31364127;2016-03-31T22:34:24.427;2016-03-31T22:34:24.427;;616616.0;;616616.0;31361721.0;2;30;;;
31634;31634;;;"<p>Try this function, which also displays variable names for the correlation matrix:</p>

<pre><code>def plot_corr(df,size=10):
    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.

    Input:
        df: pandas DataFrame
        size: vertical and horizontal size of the plot'''

    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)
    plt.xticks(range(len(corr.columns)), corr.columns);
    plt.yticks(range(len(corr.columns)), corr.columns);
</code></pre>
";;0;;2015-07-13T13:10:12.080;;31384328;2015-07-13T13:16:16.007;2015-07-13T13:16:16.007;;1796223.0;;1796223.0;29432629.0;2;32;;;
31672;31672;;;"<p>You can use the df.reindex() function in pandas.
df is</p>

<pre><code>                      Net  Upper   Lower  Mid  Zsore
Answer option                                      
More than once a day  0%  0.22%  -0.12%    2     65
Once a day            0%  0.32%  -0.19%    3     45
Several times a week  2%  2.45%   1.10%    4     78
Once a week           1%  1.63%  -0.40%    6     65
</code></pre>

<p>define an list of column names</p>

<pre><code>cols = df.columns.tolist()
cols
Out[13]: ['Net', 'Upper', 'Lower', 'Mid', 'Zsore']
</code></pre>

<p>move the column name to wherever you want</p>

<pre><code>cols.insert(0, cols.pop(cols.index('Mid')))
cols
Out[16]: ['Mid', 'Net', 'Upper', 'Lower', 'Zsore']
</code></pre>

<p>then use <code>df.reindex()</code> function to reorder</p>

<pre><code>df = df.reindex(columns= cols)
</code></pre>

<p>out put is: df</p>

<pre><code>                      Mid  Upper   Lower Net  Zsore
Answer option                                      
More than once a day    2  0.22%  -0.12%  0%     65
Once a day              3  0.32%  -0.19%  0%     45
Several times a week    4  2.45%   1.10%  2%     78
Once a week             6  1.63%  -0.40%  1%     65
</code></pre>
";;0;;2015-07-14T01:11:52.263;;31396042;2015-09-22T05:28:41.857;2015-09-22T05:28:41.857;;4931206.0;;4931206.0;25122099.0;2;13;;;
31804;31804;;;"<h2>Drop by index</h2>

<p>delete first, second and fourth columns:</p>

<pre><code>df.drop(df.columns[[0,1,3]], axis=1, inplace=True)
</code></pre>

<p>delete first column:</p>

<pre><code>df.drop(df.columns[[0]], axis=1, inplace=True)
</code></pre>

<p>There is an optional parameter <code>inplace</code> so that the original
data can be modified without creating a copy.</p>

<h2>Popped</h2>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"">Column selection, addition, deletion</a></p>

<p>delete column <code>column-name</code>:</p>

<pre><code>df.pop('column-name')
</code></pre>

<h2>Examples:</h2>

<pre><code>df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])
</code></pre>

<p><code>print df</code>:</p>

<pre><code>   one  two  three
A    1    2      3
B    4    5      6
C    7    8      9
</code></pre>

<p><code>df.drop(df.columns[[0]], axis=1, inplace=True)</code><br>
<code>print df</code>:</p>

<pre><code>   two  three
A    2      3
B    5      6
C    8      9
</code></pre>

<p><code>three = df.pop('three')</code><br>
<code>print df</code>:</p>

<pre><code>   two
A    2
B    5
C    8
</code></pre>
";;3;;2015-07-15T13:37:23.217;;31431997;2015-09-06T23:04:14.083;2015-09-06T23:04:14.083;;1549844.0;;2901002.0;13411544.0;2;56;;;
31879;31879;;;"<p>You have to read it line by line. For example, you can use the following code provided by <a href=""http://www.reddit.com/user/ryptophan"" rel=""noreferrer"">ryptophan</a> on <a href=""http://www.reddit.com/r/MachineLearning/comments/33eglq/python_help_jsoncsv_pandas/"" rel=""noreferrer"">reddit</a>:</p>

<pre><code>import pandas as pd

# read the entire file into a python array
with open('your.json', 'rb') as f:
    data = f.readlines()

# remove the trailing ""\n"" from each line
data = map(lambda x: x.rstrip(), data)

# each element of 'data' is an individual JSON object.
# i want to convert it into an *array* of JSON objects
# which, in and of itself, is one large JSON object
# basically... add square brackets to the beginning
# and end, and have all the individual business JSON objects
# separated by a comma
data_json_str = ""["" + ','.join(data) + ""]""

# now, load it into pandas
data_df = pd.read_json(data_json_str)
</code></pre>
";;7;;2015-07-16T15:23:43.487;;31458385;2015-07-16T15:23:43.487;;;;;4959258.0;30088006.0;2;26;;;
31963;31963;;;"<p>More simply:</p>

<p><code>result.div(result.sum(axis=1), axis=0)</code></p>

<p>(Edited to use code highlighting)</p>
";;0;;2015-07-17T16:58:41.620;;31480994;2015-08-15T23:06:42.627;2015-08-15T23:06:42.627;;2788482.0;;2788482.0;26537878.0;2;9;;;
32061;32061;;;"<p>This answer is similar to that provided by @tanemaki, but uses a <code>lambda</code> expression instead of <code>scipy stats</code>.</p>

<pre><code>df = pd.DataFrame(np.random.randn(100, 3), columns=list('ABC'))

df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() &lt; 3).all(axis=1)]
</code></pre>

<p>To filter the DataFrame where only ONE column (e.g. 'B') is within three standard deviations:</p>

<pre><code>df[((df.B - df.B.mean()) / df.B.std()).abs() &lt; 3]
</code></pre>
";;0;;2015-07-19T15:44:23.873;;31502974;2015-07-19T17:35:45.137;2015-07-19T17:35:45.137;;2411802.0;;2411802.0;23199796.0;2;13;;;
32077;32077;;;"<p>You need to select that column:</p>

<pre><code>In [41]:
df.loc[df['First Season'] &gt; 1990, 'First Season'] = 1
df

Out[41]:
                 Team  First Season  Total Games
0      Dallas Cowboys          1960          894
1       Chicago Bears          1920         1357
2   Green Bay Packers          1921         1339
3      Miami Dolphins          1966          792
4    Baltimore Ravens             1          326
5  San Franciso 49ers          1950         1003
</code></pre>

<p>So the syntax here is:</p>

<pre><code>df.loc[&lt;mask&gt;(here mask is generating the labels to index) , &lt;optional column(s)&gt; ]
</code></pre>

<p>You can check the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label"">docs</a> and also the <a href=""http://pandas.pydata.org/pandas-docs/stable/10min.html#selection-by-label"">10 minutes to pandas</a> which shows the semantics</p>

<p><strong>EDIT</strong></p>

<p>If you want to generate a boolean indicator then you can just use the boolean condition to generate a boolean Series and cast the dtype to <code>int</code> this will convert <code>True</code> and <code>False</code> to <code>1</code> and <code>0</code> respectively:</p>

<pre><code>In [43]:
df['First Season'] = (df['First Season'] &gt; 1990).astype(int)
df

Out[43]:
                 Team  First Season  Total Games
0      Dallas Cowboys             0          894
1       Chicago Bears             0         1357
2   Green Bay Packers             0         1339
3      Miami Dolphins             0          792
4    Baltimore Ravens             1          326
5  San Franciso 49ers             0         1003
</code></pre>
";;3;;2015-07-20T08:37:09.107;;31512025;2015-07-20T08:51:27.270;2015-07-20T08:51:27.270;;704848.0;;704848.0;31511997.0;2;30;;;
32196;32196;;;"<h1>Windows memory limitation</h1>

<p>Memory errors happens a lot with python when using the 32bit version in Windows. This is because 32bit processes <a href=""https://msdn.microsoft.com/en-us/library/aa366778.aspx"" rel=""noreferrer"">only gets 2GB of memory to play with</a> by default.</p>

<h1>Tricks for lowering memory usage</h1>

<p>If you are not using 32bit python in windows but are looking to improve on your memory efficiency while reading csv files, there is a trick.</p>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">pandas.read_csv function</a> takes an option called <code>dtype</code>. This lets pandas know what types exist inside your csv data.</p>

<h3>How this works</h3>

<p>By default, pandas will try to guess what dtypes your csv file has. This is a very heavy operation because while it is determining the dtype, it has to keep all raw data as objects (strings) in memory.</p>

<h3>Example</h3>

<p>Let's say your csv looks like this:</p>

<pre><code>name, age, birthday
Alice, 30, 1985-01-01
Bob, 35, 1980-01-01
Charlie, 25, 1990-01-01
</code></pre>

<p>This example is of course no problem to read into memory, but it's just an example.</p>

<p>If pandas were to read the above csv file <em>without</em> any dtype option, the age would be stored as strings in memory until pandas has read enough lines of the csv file to make a qualified guess.</p>

<p>I think the default in pandas is to read 1,000,000 rows before guessing the dtype.</p>

<h3>Solution</h3>

<p>By specifying <code>dtype={'age':int}</code> as an option to the <code>.read_csv()</code> will let pandas know that age should be interpreted as a number. This saves you lots of memory.</p>

<h3>Problem with corrupt data</h3>

<p>However, if your csv file would be corrupted, like this:</p>

<pre><code>name, age, birthday
Alice, 30, 1985-01-01
Bob, 35, 1980-01-01
Charlie, 25, 1990-01-01
Dennis, 40+, None-Ur-Bz
</code></pre>

<p>Then specifying <code>dtype={'age':int}</code> will break the <code>.read_csv()</code> command, because it cannot cast <code>""40+""</code> to int. So sanitize your data carefully!</p>

<p>Here you can see how the memory usage of a pandas dataframe is a lot higher when floats are kept as strings:</p>

<h2>Try it yourself</h2>

<pre><code>df = pd.DataFrame(pd.np.random.choice(['1.0', '0.6666667', '150000.1'],(100000, 10)))
resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
# 224544 (~224 MB)

df = pd.DataFrame(pd.np.random.choice([1.0, 0.6666667, 150000.1],(100000, 10)))
resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
# 79560 (~79 MB)
</code></pre>
";;3;;2015-07-21T14:14:02.710;;31541600;2017-02-16T08:58:04.550;2017-02-16T08:58:04.550;;3730397.0;;3730397.0;17557074.0;2;12;;;
32204;32204;;;"<p>I thought I would bring some more data to the discussion.</p>

<p>I ran a series of tests on this issue.</p>

<p>By using the python <code>resource</code> package I got the memory usage of my process.</p>

<p>And by writing the csv into a <code>StringIO</code> buffer, I could easily measure the size of it in bytes.</p>

<p>I ran two experiments, each one creating 20 dataframes of increasing sizes between 10,000 lines and 1,000,000 lines. Both having 10 columns.</p>

<p>In the first experiment I used only floats in my dataset.</p>

<p>This is how the memory increased in comparison to the csv file as a function of the number of lines. (Size in Megabytes)</p>

<p><img src=""https://i.stack.imgur.com/x9cQs.png"" alt=""Memory and CSV size in Megabytes as a function of the number of rows with float entries""></p>

<p>The second experiment I had the same approach, but the data in the dataset consisted of only short strings.</p>

<p><img src=""https://i.stack.imgur.com/5shue.png"" alt=""Memory and CSV size in Megabytes as a function of the number of rows with string entries""></p>

<p>It seems that the relation of the size of the csv and the size of the dataframe can vary quite a lot, but the size in memory will always be bigger by a factor of 2-3 (for the frame sizes in this experiment)</p>

<p>I would love to complete this answer with more experiments, please comment if you want me to try something special.</p>
";;0;;2015-07-21T15:29:36.647;;31543407;2015-07-21T15:29:36.647;;;;;3730397.0;18089667.0;2;10;;;
32240;32240;;;"<p>Below is what I came up with.</p>

<p>Its not pretty but it works.</p>

<p>Im still new to Bokeh (&amp; Python for that matter) so if anyone wants to suggest a better way to do this, please feel free.</p>

<p><img src=""https://i.stack.imgur.com/flkwJ.png"" alt=""enter image description here""></p>

<pre><code>import pandas as pd
import numpy as np
from bokeh.charts import TimeSeries
from bokeh.models import HoverTool
from bokeh.plotting import show

toy_df = pd.DataFrame(data=np.random.rand(5,3), columns = ('a', 'b' ,'c'), index = pd.DatetimeIndex(start='01-01-2015',periods=5, freq='d'))       

 _tools_to_show = 'box_zoom,pan,save,hover,resize,reset,tap,wheel_zoom'        

p = figure(width=1200, height=900, x_axis_type=""datetime"", tools=_tools_to_show)


# FIRST plot ALL lines (This is a hack to get it working, why can't i pass in a dataframe to multi_line?)   
# It's not pretty but it works. 
# what I want to do!: p.multi_line(df)
ts_list_of_list = []
for i in range(0,len(toy_df.columns)):
    ts_list_of_list.append(toy_df.index.T)

vals_list_of_list = toy_df.values.T.tolist()

# Define colors because otherwise multi_line will use blue for all lines...
cols_to_use =  ['Black', 'Red', 'Lime']
p.multi_line(ts_list_of_list, vals_list_of_list, line_color=cols_to_use)


# THEN put  scatter one at a time on top of each one to get tool tips (HACK! lines with tooltips not yet supported by Bokeh?) 
for (name, series) in toy_df.iteritems():
    # need to repmat the name to be same dimension as index
    name_for_display = np.tile(name, [len(toy_df.index),1])

    source = ColumnDataSource({'x': toy_df.index, 'y': series.values, 'series_name': name_for_display, 'Date': toy_df.index.format()})
    # trouble formating x as datestring, so pre-formating and using an extra column. It's not pretty but it works.

    p.scatter('x', 'y', source = source, fill_alpha=0, line_alpha=0.3, line_color=""grey"")     

    hover = p.select(dict(type=HoverTool))
    hover.tooltips = [(""Series"", ""@series_name""), (""Date"", ""@Date""),  (""Value"", ""@y{0.00%}""),]
    hover.mode = 'mouse'

show(p)
</code></pre>
";;0;;2015-07-21T21:27:22.960;;31549924;2015-07-21T21:27:22.960;;;;;5028529.0;31496628.0;2;8;;;
32263;32263;;;"<p><code>str</code> attribute contains most of the methods in string.</p>

<pre><code>df['ID'] = df['ID'].str.zfill(15)
</code></pre>

<p>See more: <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html"">http://pandas.pydata.org/pandas-docs/stable/text.html</a></p>
";;0;;2015-07-22T04:18:59.840;;31553791;2015-07-22T04:18:59.840;;;;;3582729.0;23836277.0;2;13;;;
32313;32313;;;"<p>You want the following:</p>

<pre><code>In [20]:
df.groupby(['Name','Type','ID']).count().reset_index()

Out[20]:
    Name   Type  ID  Count
0  Book1  ebook   1      2
1  Book2  paper   2      2
2  Book3  paper   3      1
</code></pre>

<p>In your case the 'Name', 'Type' and 'ID' cols match in values so we can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""noreferrer""><code>groupby</code></a> on these, call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html#pandas.core.groupby.GroupBy.count"" rel=""noreferrer""><code>count</code></a> and then <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index"" rel=""noreferrer""><code>reset_index</code></a>.</p>

<p>An alternative approach would be to add the 'Count' column using <code>transform</code> and then call <code>drop_duplicates</code>:</p>

<pre><code>In [25]:
df['Count'] = df.groupby(['Name'])['ID'].transform('count')
df.drop_duplicates()

Out[25]:
    Name   Type  ID  Count
0  Book1  ebook   1      2
1  Book2  paper   2      2
2  Book3  paper   3      1
</code></pre>
";;4;;2015-07-22T17:17:59.077;;31569866;2015-07-22T18:14:50.343;2015-07-22T18:14:50.343;;704848.0;;704848.0;31569549.0;2;10;;;
32314;32314;;;"<p>A workaround is to transpose the <code>DataFrame</code> and iterate over the rows.</p>

<pre><code>for column_name, column in df.transpose().iterrows():
    print column_name
</code></pre>
";;0;;2015-07-22T17:40:19.047;;31570270;2015-07-22T17:40:19.047;;;;;1442821.0;28218698.0;2;6;;;
32324;32324;;;"<p>For me @Charles Duffy comment solved it.
Put this in your env:</p>

<p><code>LC_ALL=C</code></p>

<p>You can add it to your .bashrc with a line like this:</p>

<p><code>export LC_ALL=C</code></p>

<p>But take in care that you'll affect all other programs. So you may want to use it just for the pip run:</p>

<p><code>$ LC_ALL=C pip install ...</code></p>
";;5;;2015-07-22T20:16:57.910;;31573180;2017-06-05T13:02:57.650;2017-06-05T13:02:57.650;;330624.0;;330624.0;26473681.0;2;29;;;
32369;32369;;;"<p>The use of <code>.loc</code> is recommended here because the methods <code>df.Age.isnull()</code>, <code>df.Gender == i</code> and <code>df.Pclass == j+1</code> may return a view of slices of the data frame or may return a copy. This can confuse pandas.</p>

<p>If you don't use <code>.loc</code> you end up calling all 3 conditions in series which leads you to a problem called <em>chained indexing</em>. When you use <code>.loc</code> however you access all your conditions in one step and pandas is no longer confused.</p>

<p>You can read more about this along with some examples of when not using <code>.loc</code> will cause the operation to fail in the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"">pandas documentation</a>.</p>

<p>The simple answer is that while you can often get away with not using <code>.loc</code> and simply typing (for example)</p>

<pre><code>df['Age_fill'][(df.Age.isnull()) &amp; (df.Gender == i) &amp; (df.Pclass == j+1)] \
                                                          = median_ages[i,j]
</code></pre>

<p>you'll always get the <code>SettingWithCopy</code> warning and your code will be a little messier for it.</p>

<p>In my experience <code>.loc</code> has taken me a while to get my head around and it's been a bit annoying updating my code. But it's really super simple and very intuitive: <code>df.loc[row_index,col_indexer]</code>.</p>

<p>For more information see the pandas documentation on <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"">Indexing and Selecting Data</a>.</p>
";;0;;2015-07-23T11:22:05.550;;31585881;2015-07-23T11:22:05.550;;;;;2316203.0;31571217.0;2;15;;;
32400;32400;;;"<p><em>Note: in pandas version 0.20.0 and above, <code>ix</code> is <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecated</a> and the use of <code>loc</code> and <code>iloc</code> is encouraged instead. I have left the parts of this answer that describe <code>ix</code> intact as a reference for users of earlier versions of pandas. Examples have been added below showing alternatives to  <code>ix</code></em>.</p>

<hr>

<p>First, a recap:</p>

<ul>
<li><code>loc</code> works on <em>labels</em> in the index. </li>
<li><code>iloc</code> works on the <em>positions</em> in the index (so it only takes integers).</li>
<li><code>ix</code> usually tries to behave like <code>loc</code> but falls back to behaving like <code>iloc</code> if the label is not in the index.</li>
</ul>

<p>It's important to note some subtleties that can make <code>ix</code> slightly tricky to use:</p>

<ul>
<li><p>if the index is of integer type, <code>ix</code> will only use label-based indexing and not fall back to position-based indexing. If the label is not in the index, an error is raised.</p></li>
<li><p>if the index does not contain <em>only</em> integers, then given an integer, <code>ix</code> will immediately use position-based indexing rather than label-based indexing. If however <code>ix</code> is given another type (e.g. a string), it can use label-based indexing. </p></li>
</ul>

<hr>

<p>To illustrate the differences between the three methods, consider the following Series:</p>

<pre><code>&gt;&gt;&gt; s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])
&gt;&gt;&gt; s
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN
4    NaN
5    NaN
</code></pre>

<p>Then <code>s.iloc[:3]</code> returns the first 3 rows (since it looks at the position) and <code>s.loc[:3]</code> returns the first 8 rows (since it looks at the labels):</p>

<pre><code>&gt;&gt;&gt; s.iloc[:3] # slice the first three rows
49   NaN
48   NaN
47   NaN

&gt;&gt;&gt; s.loc[:3] # slice up to and including label 3
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN

&gt;&gt;&gt; s.ix[:3] # the integer is in the index so s.ix[:3] works like loc
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN
</code></pre>

<p>Notice <code>s.ix[:3]</code> returns the same Series as <code>s.loc[:3]</code> since it looks for the label first rather than going by position (and the index is of integer type).</p>

<p>What if we try with an integer label that isn't in the index (say <code>6</code>)?</p>

<p>Here <code>s.iloc[:6]</code> returns the first 6 rows of the Series as expected. However, <code>s.loc[:6]</code> raises a KeyError since <code>6</code> is not in the index. </p>

<pre><code>&gt;&gt;&gt; s.iloc[:6]
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN

&gt;&gt;&gt; s.loc[:6]
KeyError: 6

&gt;&gt;&gt; s.ix[:6]
KeyError: 6
</code></pre>

<p>As per the subtleties noted above, <code>s.ix[:6]</code> now raises a KeyError because it tries to work like <code>loc</code> but can't find a <code>6</code> in the index. Because our index is of integer type it doesn't fall back to behaving like <code>iloc</code>.</p>

<p>If, however, our index was of mixed type, given an integer <code>ix</code> would behave like <code>iloc</code> immediately instead of raising a KeyError:</p>

<pre><code>&gt;&gt;&gt; s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])
&gt;&gt;&gt; s2.index.is_mixed() # index is mix of types
True
&gt;&gt;&gt; s2.ix[:6] # behaves like iloc given integer
a   NaN
b   NaN
c   NaN
d   NaN
e   NaN
1   NaN
</code></pre>

<p>Keep in mind that <code>ix</code> can still accept non-integers and behave like <code>loc</code>:</p>

<pre><code>&gt;&gt;&gt; s2.ix[:'c'] # behaves like loc given non-integer
a   NaN
b   NaN
c   NaN
</code></pre>

<p>As general advice, if you're only indexing using labels, or only indexing using integer positions, stick with <code>loc</code> or <code>iloc</code> to avoid unexpected results - try not use <code>ix</code>.</p>

<hr>

<h3>Combining position-based and label-based indexing</h3>

<p>Sometimes given a DataFrame, you will want to mix label and positional indexing methods for the rows and columns.</p>

<p>For example, consider the following DataFrame. How best to slice the rows up to and including 'c' <em>and</em> take the first four columns?</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.nan, 
                      index=list('abcde'),
                      columns=['x','y','z', 8, 9])
&gt;&gt;&gt; df
    x   y   z   8   9
a NaN NaN NaN NaN NaN
b NaN NaN NaN NaN NaN
c NaN NaN NaN NaN NaN
d NaN NaN NaN NaN NaN
e NaN NaN NaN NaN NaN
</code></pre>

<p>In earlier versions of pandas (before 0.20.0) <code>ix</code> lets you do this quite neatly - we can slice the rows by label and the columns by position (note that for the columns, <code>ix</code> default to position-based slicing since the label <code>4</code> is not a column name):</p>

<pre><code>&gt;&gt;&gt; df.ix[:'c', :4]
    x   y   z   8
a NaN NaN NaN NaN
b NaN NaN NaN NaN
c NaN NaN NaN NaN
</code></pre>

<p>In later versions of pandas, we can achieve this result using <code>iloc</code> and the help of another method:</p>

<pre><code>&gt;&gt;&gt; df.iloc[:df.index.get_loc('c') + 1, :4]
    x   y   z   8
a NaN NaN NaN NaN
b NaN NaN NaN NaN
c NaN NaN NaN NaN
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.Index.get_loc.html"" rel=""noreferrer""><code>get_loc()</code></a> is an index method meaning ""get the position of the label in this index"". Note that since slicing with <code>iloc</code> is exclusive of its endpoint, we must add 1 to this value if we want row 'c' as well.</p>

<p>There are further examples in pandas' documentation <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">here</a>.</p>
";;11;;2015-07-23T16:59:47.117;;31593712;2017-05-06T09:25:49.550;2017-05-06T09:25:49.550;;3923281.0;;3923281.0;31593201.0;2;370;;;
32402;32402;;;"<p><code>iloc</code> works based on integer positioning. So no matter what your row labels are, you can always, e.g., get the first row by doing</p>

<pre><code>df.iloc[0]
</code></pre>

<p>or the last five rows by doing</p>

<pre><code>df.iloc[-5:]
</code></pre>

<p>You can also use it on the columns. This retrieves the 3rd column:</p>

<pre><code>df.iloc[:, 2]    # the : in the first position indicates all rows
</code></pre>

<p>You can combine them to get intersections of rows and columns:</p>

<pre><code>df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)
</code></pre>

<p>On the other hand, <code>.loc</code> use named indices. Let's set up a data frame with strings as row and column labels:</p>

<pre><code>df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])
</code></pre>

<p>Then we can get the first row by</p>

<pre><code>df.loc['a']     # equivalent to df.iloc[0]
</code></pre>

<p>and the second two rows of the <code>'date'</code> column by </p>

<pre><code>df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]
</code></pre>

<p>and so on. Now, it's probably worth pointing out that the default row and column indices for a <code>DataFrame</code> are integers from 0 and in this case <code>iloc</code> and <code>loc</code> would work in the same way. This is why your three examples are equivalent. <strong>If you had a non-numeric index such as strings or datetimes,</strong> <code>df.loc[:5]</code> <strong>would raise an error.</strong> </p>

<p>Also, you can do column retrieval just by using the data frame's <code>__getitem__</code>:</p>

<pre><code>df['time']    # equivalent to df.loc[:, 'time']
</code></pre>

<p>Now suppose you want to mix position and named indexing, that is, indexing using names on rows and positions on columns (to clarify, I mean select from our data frame, rather than creating a data frame with strings in the row index and integers in the column index). This is where <code>.ix</code> comes in:</p>

<pre><code>df.ix[:2, 'time']    # the first two rows of the 'time' column
</code></pre>

<p>EDIT:
I think it's also worth mentioning that you can pass boolean vectors to the <code>loc</code> method as well. For example:</p>

<pre><code> b = [True, False, True]
 df.loc[b] 
</code></pre>

<p>Will return the 1st and 3rd rows of <code>df</code>. This is equivalent to <code>df[b]</code> for selection, but it can also be used for assigning via boolean vectors: </p>

<pre><code>df.loc[b, 'name'] = 'Mary', 'John'
</code></pre>
";;2;;2015-07-23T17:17:27.827;;31594055;2016-07-30T17:22:17.997;2016-07-30T17:22:17.997;;428862.0;;3868428.0;31593201.0;2;59;;;
32437;32437;;;"<p>Pandas is written by people that really know what people want to do.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#id45"" rel=""noreferrer"">Since version <code>0.13</code></a> there's a function <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_clipboard.html"" rel=""noreferrer""><code>pd.read_clipboard</code></a> which is absurdly effective at making this ""just work"".</p>

<p>Copy and paste the part of the code in the question that starts <code>bar foo</code>, (i.e. the DataFrame) and do this in a Python interpreter:</p>

<pre><code>In [53]: import pandas as pd
In [54]: df = pd.read_clipboard()

In [55]: df
Out[55]: 
   bar  foo
0    4    1
1    5    2
2    6    3
</code></pre>

<h1>Caveats</h1>

<ul>
<li>Don't include the iPython <code>In</code> or <code>Out</code> stuff or it won't work</li>
<li>If you have a named index, you currently need to add <code>engine='python'</code> (see <a href=""https://github.com/pydata/pandas/issues/6893"" rel=""noreferrer"">this issue</a> on GitHub). The 'c' engine is currently broken when the index is named.</li>
<li>It's not brilliant at MultiIndexes:</li>
</ul>

<p>Try this:</p>

<pre><code>                      0         1         2
level1 level2                              
foo    a       0.518444  0.239354  0.364764
       b       0.377863  0.912586  0.760612
bar    a       0.086825  0.118280  0.592211
</code></pre>

<p>which doesn't work at all, or this:</p>

<pre><code>              0         1         2
foo a  0.859630  0.399901  0.052504
    b  0.231838  0.863228  0.017451
bar a  0.422231  0.307960  0.801993
</code></pre>

<p>Which works, but returns something totally incorrect!</p>
";;3;;2015-07-24T12:45:23.867;;31610890;2015-09-03T13:31:32.170;2015-09-03T13:31:32.170;;2071807.0;;2071807.0;31610889.0;2;17;;;
32443;32443;;;"<p>I believe the pylab magic was removed when they transitioned from IPython to a more general Jupyter notebook.</p>

<p>Try:</p>

<pre><code>%matplotlib inline
</code></pre>

<p>Also when you get a message like:</p>

<pre><code>""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".
</code></pre>

<p>That's just IPython displaying the object.  You need to specify IPython display it.  Hence the matplotlib inline magic.</p>
";;3;;2015-07-24T13:22:33.207;;31611678;2016-01-22T15:07:57.557;2016-01-22T15:07:57.557;;4967110.0;;4967110.0;31609600.0;2;71;;;
32484;32484;;;"<p>You should use <code>()</code> to group your boolean vector to remove ambiguity. </p>

<pre><code>df = df[(df['closing_price'] &gt;= 99) &amp; (df['closing_price'] &lt;= 101)]
</code></pre>
";;0;;2015-07-24T19:04:19.420;;31617974;2015-07-24T19:04:19.420;;;;;5014134.0;31617845.0;2;17;;;
32648;32648;;;"<p>Here is a method that creates a sparse scipy matrix based on data and indices of person and thing.  <code>person_u</code> and <code>thing_u</code> are lists representing the unique entries for your rows and columns of pivot you want to create.  Note: this assumes that your count column already has the value you want in it.</p>

<pre><code>from scipy.sparse import csr_matrix

person_u = list(sort(frame.person.unique()))
thing_u = list(sort(frame.thing.unique()))

data = frame['count'].tolist()
row = frame.person.astype('category', categories=person_u).cat.codes
col = frame.thing.astype('category', categories=thing_u).cat.codes
sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))

&gt;&gt;&gt; sparse_matrix 
&lt;3x4 sparse matrix of type '&lt;type 'numpy.int64'&gt;'
    with 6 stored elements in Compressed Sparse Row format&gt;

&gt;&gt;&gt; sparse_matrix.todense()

matrix([[0, 1, 0, 1],
        [1, 0, 0, 1],
        [1, 0, 1, 0]])
</code></pre>

<p>Based on your original question, the scipy sparse matrix should be sufficient for your needs, but should you wish to have a sparse dataframe you can do the following:</p>

<pre><code>dfs=pd.SparseDataFrame([ pd.SparseSeries(sparse_matrix[i].toarray().ravel(), fill_value=0) 
                              for i in np.arange(sparse_matrix.shape[0]) ], index=person_u, columns=thing_u, default_fill_value=0)

&gt;&gt;&gt; dfs
     a  b  c  d
him  0  1  0  1
me   1  0  0  1
you  1  0  1  0

&gt;&gt;&gt; type(dfs)
pandas.sparse.frame.SparseDataFrame
</code></pre>
";;2;;2015-07-28T14:33:36.997;;31679396;2015-07-31T20:24:04.060;2015-07-31T20:24:04.060;;5029279.0;;5029279.0;31661604.0;2;19;;;
33272;33272;;;"<p>And if you want to add a row, you can use a dictionary:</p>

<pre><code>df = pd.DataFrame()
df = df.append({'name': 'Zed', 'age': 9, 'height': 2}, ignore_index=True)
</code></pre>

<p>which gives you:</p>

<pre><code>   age  height name
0    9       2  Zed
</code></pre>
";;1;;2015-08-05T17:38:03.043;;31839240;2015-08-05T17:38:03.043;;;;;1504411.0;16597265.0;2;39;;;
33325;33325;;;"<pre><code>df = pd.DataFrame({'A':['a', 'b', 'c'], 'B':[54, 67, 89]}, index=[100, 200, 300])

df

                        A   B
                100     a   54
                200     b   67
                300     c   89
In [19]:    
df.loc[100]

Out[19]:
A     a
B    54
Name: 100, dtype: object

In [20]:    
df.iloc[0]

Out[20]:
A     a
B    54
Name: 100, dtype: object

In [24]:    
df2 = df.set_index([df.index,'A'])
df2

Out[24]:
        B
    A   
100 a   54
200 b   67
300 c   89

In [25]:    
df2.ix[100, 'a']

Out[25]:    
B    54
Name: (100, a), dtype: int64
</code></pre>
";;0;;2015-08-06T15:00:52.720;;31859215;2015-11-09T22:48:10.347;2015-11-09T22:48:10.347;;4441538.0;;4441538.0;28757389.0;2;28;;;
33428;33428;;;"<p>I've just found a great tool for that need, it is called <a href=""https://pypi.python.org/pypi/tabulate"">tabulate</a>.</p>

<p>It prints tabular data and works with <code>DataFrame</code>.</p>

<pre><code>from tabulate import tabulate
import pandas as pd

df = pd.DataFrame({'col_two' : [0.0001, 1e-005 , 1e-006, 1e-007],
                   'column_3' : ['ABCD', 'ABCD', 'long string', 'ABCD']})
print tabulate(df, headers='keys', tablefmt='psql')

+----+-----------+-------------+
|    |   col_two | column_3    |
|----+-----------+-------------|
|  0 |    0.0001 | ABCD        |
|  1 |    1e-05  | ABCD        |
|  2 |    1e-06  | long string |
|  3 |    1e-07  | ABCD        |
+----+-----------+-------------+
</code></pre>

<p>Note: There is an open <a href=""https://bitbucket.org/astanin/python-tabulate/pull-requests/21/option-to-not-include-exclude-an-index/diff"">Pull request</a> that will permit to include/exclude the index.</p>
";;2;;2015-08-07T19:30:27.240;;31885295;2016-09-06T14:41:36.857;2016-09-06T14:41:36.857;;435093.0;;4413446.0;18528533.0;2;38;;;
33502;33502;;;"<p>You can also use <code>date_range</code> for this purpose.</p>

<pre><code>In [3]: pd.date_range('2011-01-05', '2011-01-09', freq=BDay())

Out[3]: DatetimeIndex(['2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B', tz=None)
</code></pre>

<p><strong>EDIT</strong></p>

<p>Or even more simple</p>

<pre><code>In [7]: pd.bdate_range('2011-01-05', '2011-01-09')

Out[7]: DatetimeIndex(['2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B', tz=None)
</code></pre>

<p>Note that both start and end dates are inclusive.
Source: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.bdate_range.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.bdate_range.html</a></p>
";;0;;2015-08-09T18:55:16.710;;31907749;2017-02-10T12:08:03.540;2017-02-10T12:08:03.540;;1545579.0;;4413446.0;13019719.0;2;12;;;
33583;33583;;;"<p>So I believe in the latest releases of pandas (version 0.16.0), you could throw in the <code>comment='#'</code> parameter into <code>pd.read_csv</code> and this should skip commented out lines. </p>

<p>These github issues shows that you can do this:</p>

<ul>
<li><a href=""https://github.com/pydata/pandas/issues/10548"" rel=""noreferrer"">https://github.com/pydata/pandas/issues/10548</a></li>
<li><a href=""https://github.com/pydata/pandas/issues/4623"" rel=""noreferrer"">https://github.com/pydata/pandas/issues/4623</a></li>
</ul>

<p>See the documentation on <code>read_csv</code>: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>
";;0;;2015-08-11T00:19:13.540;;31931208;2015-08-11T00:34:23.583;2015-08-11T00:34:23.583;;2014591.0;;2014591.0;18366797.0;2;9;;;
33604;33604;;;"<p>You can easily do this though,</p>

<pre><code>df.apply(LabelEncoder().fit_transform)
</code></pre>

<p>EDIT:</p>

<p>Since this answer is over a year ago, and generated many upvotes (including a bounty), I should probably extend this further.</p>

<p>For inverse_transform and transform, you have to do a little bit of hack.</p>

<pre><code>from collections import defaultdict
d = defaultdict(LabelEncoder)
</code></pre>

<p>With this, you now retain all columns <code>LabelEncoder</code> as dictionary.</p>

<pre><code># Encoding the variable
fit = df.apply(lambda x: d[x.name].fit_transform(x))

# Inverse the encoded
fit.apply(lambda x: d[x.name].inverse_transform(x))

# Using the dictionary to label future data
df.apply(lambda x: d[x.name].transform(x))
</code></pre>
";;10;;2015-08-11T10:21:03.760;;31939145;2016-08-22T10:41:08.247;2016-08-22T10:41:08.247;;1296136.0;;1296136.0;24458645.0;2;121;;;
33670;33670;;;"<p>A CDF or cumulative distribution function plot is basically a graph with on the X-axis the sorted values and on the Y-axis the cumulative distribution. So, I would create a new series with the sorted values as index and the cumulative distribution as values.</p>

<p>First create an example series:</p>

<pre><code>import pandas as pd
import numpy as np
ser = pd.Series(np.random.normal(size=100))
</code></pre>

<p>Sort the series:</p>

<pre><code>ser = ser.sort_values()
</code></pre>

<p>Now, before proceeding, append again the last (and largest) value. This step is important especially for small sample sizes in order to get an unbiased CDF:</p>

<pre><code>ser[len(ser)] = ser.iloc[-1]
</code></pre>

<p>Create a new series with the sorted values as index and the cumulative distribution as values:</p>

<pre><code>cum_dist = np.linspace(0.,1.,len(ser))
ser_cdf = pd.Series(cum_dist, index=ser)
</code></pre>

<p>Finally, plot the function as steps:</p>

<pre><code>ser_cdf.plot(drawstyle='steps')
</code></pre>
";;3;;2015-08-12T16:57:35.000;;31971245;2016-06-06T17:20:15.007;2016-06-06T17:20:15.007;;2543372.0;;1551810.0;25577352.0;2;10;;;
33806;33806;;;"<p>One could simply use dplyr from Python.</p>

<p>There is an interface to <code>dplyr</code> in rpy2 (introduced with rpy2-2.7.0) that lets you write things like:</p>

<pre><code>dataf = (DataFrame(mtcars).
         filter('gear&gt;3').
         mutate(powertoweight='hp*36/wt').
         group_by('gear').
         summarize(mean_ptw='mean(powertoweight)'))
</code></pre>

<p>There is an <a href=""http://rpy2.readthedocs.org/en/version_2.7.x/lib_dplyr.html"" rel=""noreferrer"">example in the documentation</a>. This part of the doc is (also) a jupyter notebook. Look for the links near the top of page.</p>

<p>An other answer to the question is comparing R's dplyr and pandas (see @lgallen). That same R one-liner chaining dplyr statements write's essentially the same in rpy2's interface to dplyr.</p>

<p>R:</p>

<pre><code>flights %&gt;%
   group_by(year, month, day) %&gt;%
   select(arr_delay, dep_delay) %&gt;%
   summarise(
      arr = mean(arr_delay, na.rm = TRUE),
      dep = mean(dep_delay, na.rm = TRUE)
      ) %&gt;%
   filter(arr &gt; 30 | dep &gt; 30)
</code></pre>

<p>Python+rpy2:</p>

<pre><code>(DataFrame(flights).
 group_by('year', 'month', 'day').
 select('arr_delay', 'dep_delay').
 summarize(arr = 'mean(arr_delay, na.rm=TRUE)',
           dep = 'mean(dep_delay, na.rm=TRUE)').
 filter('arr &gt; 30 | dep &gt; 30'))
</code></pre>
";;2;;2015-08-13T23:43:18.207;;32000194;2016-05-07T19:43:38.587;2016-05-07T19:43:38.587;;294017.0;;294017.0;26878476.0;2;13;;;
33840;33840;;;"<p>First, to convert a Categorical column to its numerical codes, you can do this easier with: <code>dataframe['c'].cat.codes</code>.<br>
Further, it is possible to select automatically all columns with a certain dtype in a dataframe using <code>select_dtypes</code>. This way, you can apply above operation on multiple and automatically selected columns.</p>

<p>First making an example dataframe:</p>

<pre><code>In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})

In [76]: df['col2'] = df['col2'].astype('category')

In [77]: df['col3'] = df['col3'].astype('category')

In [78]: df.dtypes
Out[78]:
col1       int64
col2    category
col3    category
dtype: object
</code></pre>

<p>Then by using <code>select_dtypes</code> to select the columns, and then applying <code>.cat.codes</code> on each of these columns, you can get the following result:</p>

<pre><code>In [80]: cat_columns = df.select_dtypes(['category']).columns

In [81]: cat_columns
Out[81]: Index([u'col2', u'col3'], dtype='object')

In [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)

In [84]: df
Out[84]:
   col1  col2  col3
0     1     0     0
1     2     1     1
2     3     2     0
3     4     0     1
4     5     1     1
</code></pre>
";;3;;2015-08-14T14:01:35.697;;32011969;2015-08-14T14:01:35.697;;;;;653364.0;32011359.0;2;55;;;
33842;33842;;;"<p>You could use a <a href=""http://pandas.pydata.org/pandas-docs/stable/cookbook.html#resampling"" rel=""noreferrer""><code>pd.TimeGrouper</code></a> to group the DatetimeIndex'ed DataFrame by hour:</p>

<pre><code>grouper = df.groupby([pd.TimeGrouper('1H'), 'Location'])
</code></pre>

<p>use <code>count</code> to count the number of events in each group:</p>

<pre><code>grouper['Event'].count()
#                      Location
# 2014-08-25 21:00:00  HK          1
#                      LDN         1
# 2014-08-25 22:00:00  LDN         2
# Name: Event, dtype: int64
</code></pre>

<p>use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html"" rel=""noreferrer""><code>unstack</code></a> to move the <code>Location</code> index level to a column level:</p>

<pre><code>grouper['Event'].count().unstack()
# Out[49]: 
# Location             HK  LDN
# 2014-08-25 21:00:00   1    1
# 2014-08-25 22:00:00 NaN    2
</code></pre>

<p>and then use <code>fillna</code> to change the NaNs into zeros.</p>

<hr>

<p>Putting it all together,</p>

<pre><code>grouper = df.groupby([pd.TimeGrouper('1H'), 'Location'])
result = grouper['Event'].count().unstack('Location').fillna(0)
</code></pre>

<p>yields</p>

<pre><code>Location             HK  LDN
2014-08-25 21:00:00   1    1
2014-08-25 22:00:00   0    2
</code></pre>
";;0;;2015-08-14T14:10:21.223;;32012129;2015-08-14T14:15:27.077;2015-08-14T14:15:27.077;;190597.0;;190597.0;32012012.0;2;18;;;
34012;34012;;;"<p>Came across this when I was searching for this type of groupby. Wes' code above didn't work for me, not sure if it's because changes in <code>pandas</code> over time. </p>

<p>In <code>pandas 0.16.2</code>, what I did in the end was:</p>

<pre><code>grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))])
grp.count()
</code></pre>

<p>You'd have (hour, minute) tuples as the grouped index. If you want multi-index:</p>

<pre><code>grp = data.groupby(by=[data.datetime_col.map(lambda x : x.hour),
                       data.datetime_col.map(lambda x : x.minute)])
</code></pre>
";;0;;2015-08-18T08:10:47.937;;32066997;2015-08-18T08:35:12.573;2015-08-18T08:35:12.573;;3123992.0;;3123992.0;16266019.0;2;6;;;
34072;34072;;;"<p>You can do it using group by:</p>

<pre><code>c_maxes = df.groupby(['A', 'B']).C.transform(max)
df = df[df.C == c_maxes]
</code></pre>

<p><code>c_maxes</code> is a <code>Series</code> of the maximum values of <code>C</code> in each group but which is of the same length and with the same index as <code>df</code>. If you haven't used <code>.transform</code> then printing <code>c_maxes</code> might be a good idea to see how it works. </p>

<p>Another approach using <code>drop_duplicates</code> would be </p>

<pre><code>df.sort('C').drop_duplicates(subset=['A', 'B'], take_last=True)
</code></pre>

<p>Not sure which is more efficient but I guess the first approach as it doesn't involve sorting. </p>

<p>EDIT:
From <code>pandas 0.18</code> up the second solution would be <code>df.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')</code> or, alternatively, <code>df.sort_values('C', ascending=False).drop_duplicates(subset=['A', 'B'])</code>. In any case, the <code>groupby</code> solution seems to be significantly more performing: </p>

<pre><code>%timeit -n 10 df[df.groupby(['A', 'B']).C.max == df.C]
10 loops, best of 3: 25.7 ms per loop

%timeit -n 10 df.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')
10 loops, best of 3: 101 ms per loop
</code></pre>
";;0;;2015-08-19T11:34:02.543;;32094352;2016-11-07T09:38:42.263;2016-11-07T09:38:42.263;;3868428.0;;3868428.0;32093829.0;2;18;;;
34092;34092;;;"<p>Note that the answer from @unutbu will be correct until you want to set the value to something new, then it will not work if your dataframe is a view.</p>

<pre><code>In [4]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])
In [5]: df['bar'] = 100
In [6]: df['bar'].iloc[0] = 99
/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.16.0_19_g8d2818e-py2.7-macosx-10.9-x86_64.egg/pandas/core/indexing.py:118: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self._setitem_with_indexer(indexer, value)
</code></pre>

<p>Another approach that will consistently work with both setting and getting is:</p>

<pre><code>In [7]: df.loc[df.index[0], 'foo']
Out[7]: 'A'
In [8]: df.loc[df.index[0], 'bar'] = 99
In [9]: df
Out[9]:
  foo  bar
0   A   99
2   B  100
1   C  100
</code></pre>
";;1;;2015-08-19T18:42:35.197;;32103253;2015-08-19T18:42:35.197;;;;;1068490.0;25254016.0;2;9;;;
34095;34095;;;"<p>Apart from above answers use can use <code>df.axes</code> to get the tuple with row and column indexes and then use <code>len()</code> function:</p>

<pre><code>total_rows=len(df.axes[0])
total_cols=len(df.axes[1])
</code></pre>
";;0;;2015-08-19T19:07:17.247;;32103678;2015-08-19T19:28:21.340;2015-08-19T19:28:21.340;;1820286.0;;4651101.0;15943769.0;2;11;;;
34119;34119;;;"<p>If you don't care whether the headers have borders around them and bold font, and you don't want to restrict the version of openpyxl, the quickest way is to overwrite the <code>header_style</code> dictionary to be <code>None</code>.</p>

<p>If you also have dates or datetimes, you must also explicitly set the workbook's <code>date</code> and <code>datetime</code> formats to <code>None</code>:</p>

<pre><code>from datetime import datetime
import pandas as pd
pd.core.format.header_style = None  # &lt;--- Workaround for header formatting

dt = datetime.now()
d = datetime.date(datetime.now())
df1 = pd.DataFrame([{'c1': 'alpha', 'c2': 1}, {'c1': 'beta', 'c2': 2}])
df2 = pd.DataFrame([{'c1': dt, 'c2': d}, {'c1': dt, 'c2': d}])

with pd.ExcelWriter('output.xlsx') as writer:
    writer.date_format = None # &lt;--- Workaround for date formatting
    writer.datetime_format = None  # &lt;--- this one for datetime
    df1.to_excel(writer,'Sheet1')
    df2.to_excel(writer,'Sheet2')
</code></pre>

<p>Not clear why the keyword arguments never make it through the openpyxl deprecation wrapper...but they don't.  In addition if you're formatting any other cells use the new <a href=""http://openpyxl.readthedocs.org/"" rel=""nofollow"">openpyxl api</a>.</p>

<p>All of these problems go away if you have Anaconda, by the way.</p>
";;3;;2015-08-19T23:56:53.433;;32107505;2015-09-12T19:33:25.137;2015-09-12T19:33:25.137;;3273031.0;;3273031.0;30102232.0;2;8;;;
34193;34193;;;"<p>You need to create a new list of your columns in the desired order, then use <code>df = df[cols]</code> to rearrange the columns in this new order.</p>

<pre><code>cols = ['mean']  + [col for col in df if col != 'mean']
df = df[cols]
</code></pre>

<p>You can also use a more general approach.  In this example, the last column (indicated by -1) is inserted as the first column.</p>

<pre><code>cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]
df = df[cols]
</code></pre>

<p>You can also use this approach for reordering columns in a desired order if they are present in the DataFrame.</p>

<pre><code>inserted_cols = ['a', 'b', 'c']
cols = ([col for col in inserted_cols if col in df] 
        + [col for col in df if col not in inserted cols])
df = df[cols]
</code></pre>
";;0;;2015-08-21T02:18:52.887;;32131398;2016-04-12T20:14:12.717;2016-04-12T20:14:12.717;;2411802.0;;2411802.0;13148429.0;2;16;;;
34260;34260;;;"<p>Note, a modification to @Chang She's response, as of pandas 0.16, the ""-"" operator is scheduled for deprecation.  The <code>difference()</code> method is encouraged in its place.</p>

<pre><code>exclude = ['bad col1', 'bad col2']
df.ix[:, df.columns.difference(exclude)].hist() 
</code></pre>
";;1;;2015-08-22T05:41:13.047;;32152755;2015-08-22T05:41:13.047;;;;;4531270.0;13003051.0;2;8;;;
34538;34538;;;"<p>Pass param <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot""><code>rot=0</code></a> to rotate the xticks:</p>

<pre><code>import matplotlib
matplotlib.style.use('ggplot')
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame({ 'celltype':[""foo"",""bar"",""qux"",""woz""], 's1':[5,9,1,7], 's2':[12,90,13,87]})
df = df[[""celltype"",""s1"",""s2""]]
df.set_index([""celltype""],inplace=True)
df.plot(kind='bar',alpha=0.75, rot=0)
plt.xlabel("""")
plt.show()
</code></pre>

<p>yields plot:</p>

<p><a href=""https://i.stack.imgur.com/JPdxv.png""><img src=""https://i.stack.imgur.com/JPdxv.png"" alt=""enter image description here""></a></p>
";;0;;2015-08-27T08:18:49.903;;32244161;2015-08-27T08:18:49.903;;;;;704848.0;32244019.0;2;52;;;
34545;34545;;;"<p>You should just be able to use the <code>savefig</code> method of <code>sns_plot</code> directly.</p>

<pre><code>sns_plot.savefig(""output.png"")
</code></pre>

<p>For clarity with your code if you did want to access the matplotlib figure that <code>sns_plot</code> resides in then you can get it directly with</p>

<pre><code>fig = sns_plot.fig
</code></pre>

<p>In this case there is no <code>get_figure</code> method as your code assumes.</p>
";;0;;2015-08-27T09:03:19.973;;32245025;2015-08-27T09:18:10.130;2015-08-27T09:18:10.130;;4293020.0;;4293020.0;32244753.0;2;9;;;
34546;34546;;;"

<p>Remove the <code>get_figure</code> and just use <code>sns_plot.savefig('output.png')</code></p>

<pre class=""lang-py prettyprint-override""><code>df = sns.load_dataset('iris')
sns_plot = sns.pairplot(df, hue='species', size=2.5)
sns_plot.savefig(""output.png"")
</code></pre>
";;2;;2015-08-27T09:03:20.310;;32245026;2015-08-27T09:03:20.310;;;;;5205141.0;32244753.0;2;34;;;
34722;34722;;;"<p>I want to little bit change answer by Wes, because version 0.16.2 need set <code>as_index=False</code>. If you don't set it, you get empty dataframe.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation"" rel=""noreferrer"">Source</a>:</p>

<blockquote>
  <p>Aggregation functions will not return the groups that you are aggregating over if they are named columns, when <code>as_index=True</code>, the default. The grouped columns will be the indices of the returned object.</p>
  
  <p>Passing <code>as_index=False</code> will return the groups that you are aggregating over, if they are named columns.</p>
  
  <p>Aggregating functions are ones that reduce the dimension of the returned objects, for example: <code>mean</code>, <code>sum</code>, <code>size</code>, <code>count</code>, <code>std</code>, <code>var</code>, <code>sem</code>, <code>describe</code>, <code>first</code>, <code>last</code>, <code>nth</code>, <code>min</code>, <code>max</code>. This is what happens when you do for example <code>DataFrame.sum()</code> and get back a <code>Series</code>.  </p>
  
  <p>nth can act as a reducer or a filter, see <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-nth"" rel=""noreferrer"">here</a>.</p>
</blockquote>

<pre><code>import pandas as pd

df1 = pd.DataFrame({""Name"":[""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""],
                    ""City"":[""Seattle"",""Seattle"",""Portland"",""Seattle"",""Seattle"",""Portland""]})
print df1
#
#       City     Name
#0   Seattle    Alice
#1   Seattle      Bob
#2  Portland  Mallory
#3   Seattle  Mallory
#4   Seattle      Bob
#5  Portland  Mallory
#
g1 = df1.groupby([""Name"", ""City""], as_index=False).count()
print g1
#
#                  City  Name
#Name    City
#Alice   Seattle      1     1
#Bob     Seattle      2     2
#Mallory Portland     2     2
#        Seattle      1     1
#
</code></pre>

<p>EDIT:</p>

<p>In version <code>0.17.1</code> and later you can use <code>subset</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html"" rel=""noreferrer""><code>count</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a> with parameter <code>name</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html"" rel=""noreferrer""><code>size</code></a>:</p>

<pre><code>print df1.groupby([""Name"", ""City""], as_index=False ).count()
#IndexError: list index out of range

print df1.groupby([""Name"", ""City""]).count()
#Empty DataFrame
#Columns: []
#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]

print df1.groupby([""Name"", ""City""])[['Name','City']].count()
#                  Name  City
#Name    City                
#Alice   Seattle      1     1
#Bob     Seattle      2     2
#Mallory Portland     2     2
#        Seattle      1     1

print df1.groupby([""Name"", ""City""]).size().reset_index(name='count')
#      Name      City  count
#0    Alice   Seattle      1
#1      Bob   Seattle      2
#2  Mallory  Portland      2
#3  Mallory   Seattle      1
</code></pre>

<p>Differences between <code>count</code> and <code>size</code> in <a class='doc-link' href=""https://stackoverflow.com/documentation/pandas/1822/grouping-data/6874/aggregating-by-size-and-count#t=201607220906502658034"">SO Documentation</a>.</p>
";;3;;2015-08-31T08:48:05.397;;32307259;2016-07-22T09:07:54.877;2017-05-23T12:10:48.913;;-1.0;;2901002.0;10373660.0;2;54;;;
34780;34780;;;"<p>If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...</p>

<pre><code>columns = df.columns
columns = [row.replace(""$"","""") for row in columns]
df.rename(columns=dict(zip(columns, things)), inplace=True)
df.head() #to validate the output
</code></pre>

<p>Best way? IDK. A way - yes.</p>

<p>A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory &amp; execution time. @kadee, @kaitlyn, &amp; @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of .000 and .001 seconds for all the answers. Moral: my answer above likely isn't the 'Best' way.</p>

<pre><code>import pandas as pd
import cProfile, pstats, re

old_names = ['$a', '$b', '$c', '$d', '$e']
new_names = ['a', 'b', 'c', 'd', 'e']
col_dict = {'$a': 'a', '$b': 'b','$c':'c','$d':'d','$e':'e'}

df = pd.DataFrame({'$a':[1,2], '$b': [10,20],'$c':['bleep','blorp'],'$d':[1,2],'$e':['texa$','']})

df.head()

def eumiro(df,nn):
    df.columns = nn
    #This direct renaming approach is duplicated in methodology in several other answers: 
    return df

def lexual1(df):
    return df.rename(columns=col_dict)

def lexual2(df,col_dict):
    return df.rename(columns=col_dict, inplace=True)

def Panda_Master_Hayden(df):
    return df.rename(columns=lambda x: x[1:], inplace=True)

def paulo1(df):
    return df.rename(columns=lambda x: x.replace('$', ''))

def paulo2(df):
    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)

def migloo(df,on,nn):
    return df.rename(columns=dict(zip(on, nn)), inplace=True)

def kadee(df):
    return df.columns.str.replace('$','')

def awo(df):
    columns = df.columns
    columns = [row.replace(""$"","""") for row in columns]
    return df.rename(columns=dict(zip(columns, '')), inplace=True)

def kaitlyn(df):
    df.columns = [col.strip('$') for col in df.columns]
    return df

print 'eumiro'
cProfile.run('eumiro(df,new_names)')
print 'lexual1'
cProfile.run('lexual1(df)')
print 'lexual2'
cProfile.run('lexual2(df,col_dict)')
print 'andy hayden'
cProfile.run('Panda_Master_Hayden(df)')
print 'paulo1'
cProfile.run('paulo1(df)')
print 'paulo2'
cProfile.run('paulo2(df)')
print 'migloo'
cProfile.run('migloo(df,old_names,new_names)')
print 'kadee'
cProfile.run('kadee(df)')
print 'awo'
cProfile.run('awo(df)')
print 'kaitlyn'
cProfile.run('kaitlyn(df)')
</code></pre>
";;2;;2015-09-01T02:24:17.883;;32322596;2016-09-07T02:24:19.983;2016-09-07T02:24:19.983;;2917381.0;;2917381.0;11346283.0;2;14;;;
34863;34863;;;"<p>As explained on another answer using <code>pandas.DataFrame()</code> directly here will not act as you think.</p>

<p>What you can do is use <code>pandas.DataFrame.from_dict</code>with <strong><code>orient='index'</code></strong>: </p>

<pre><code>In[7]: pandas.DataFrame.from_dict({u'2012-06-08': 388,
 u'2012-06-09': 388,
 u'2012-06-10': 388,
 u'2012-06-11': 389,
 u'2012-06-12': 389,
 u'2012-06-13': 389,
 u'2012-06-14': 389,
 u'2012-06-15': 389,
 u'2012-06-16': 389,
 u'2012-06-17': 389,
 u'2012-06-18': 390,
 u'2012-06-19': 390,
 u'2012-06-20': 390,
 u'2012-06-21': 390,
 u'2012-06-22': 390,
 u'2012-06-23': 390,
 u'2012-06-24': 390,
 u'2012-06-25': 391,
 u'2012-06-26': 391,
 u'2012-06-27': 391,
 u'2012-06-28': 391,
 u'2012-06-29': 391,
 u'2012-06-30': 391,
 u'2012-07-01': 391,
 u'2012-07-02': 392,
 u'2012-07-03': 392,
 u'2012-07-04': 392,
 u'2012-07-05': 392,
 u'2012-07-06': 392}, orient='index')
Out[7]: 
              0
2012-06-13  389
2012-06-16  389
2012-06-12  389
2012-07-03  392
2012-07-02  392
2012-06-29  391
2012-06-30  391
2012-07-01  391
2012-06-15  389
2012-06-08  388
2012-06-09  388
2012-07-05  392
2012-07-04  392
2012-06-14  389
2012-07-06  392
2012-06-17  389
2012-06-20  390
2012-06-21  390
2012-06-22  390
2012-06-23  390
2012-06-11  389
2012-06-10  388
2012-06-26  391
2012-06-27  391
2012-06-28  391
2012-06-24  390
2012-06-19  390
2012-06-18  390
2012-06-25  391
</code></pre>
";;2;;2015-09-02T03:07:48.990;;32344037;2015-09-02T03:07:48.990;;;;;508907.0;18837262.0;2;42;;;
34953;34953;;;"<p>Wes' code didn't work for me.  But the DatetimeIndex function (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html"" rel=""nofollow noreferrer"">docs</a>) did:</p>

<pre><code>times = pd.DatetimeIndex(data.datetime_col)
grouped = df.groupby([times.hour, times.minute])
</code></pre>

<p>The DatetimeIndex object is a representation of times in pandas.  The first line creates a array of the datetimes.  The second line uses this array to get the hour and minute data for all of the rows, allowing the data to be grouped (<a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""nofollow noreferrer"">docs</a>) by these values.</p>
";;0;;2015-09-03T03:00:52.813;;32366268;2017-08-23T18:48:23.643;2017-08-23T18:48:23.643;;5294981.0;;5294981.0;16266019.0;2;16;;;
35069;35069;;;"<p><a href=""https://github.com/pydata/pandas/issues/7053"" rel=""noreferrer"">Since 0.14.1</a>, you can now do <code>nlargest</code> and <code>nsmallest</code> on a <code>groupby</code> object:</p>

<pre><code>In [23]: df.groupby('id')['value'].nlargest(2)
Out[23]: 
id   
1   2    3
    1    2
2   6    4
    5    3
3   7    1
4   8    1
dtype: int64
</code></pre>

<p>There's a slight weirdness that you get the original index in there as well, but this might be really useful depending on what your original index <em>was</em>.</p>

<p>If you're not interested in it, you can do <code>.reset_index(level=1, drop=True)</code> to get rid of it altogether.</p>

<p>(Note: <a href=""https://github.com/pydata/pandas/pull/10393"" rel=""noreferrer"">From 0.17.1</a> you'll be able to do this on a DataFrameGroupBy too but for now it only works with <code>Series</code> and <code>SeriesGroupBy</code>.)</p>
";;0;;2015-09-04T12:14:13.553;;32397818;2015-09-04T12:14:13.553;;;;;2071807.0;20069009.0;2;50;;;
35074;35074;;;"<p>To avoid using a private method you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html?highlight=dtypes#pandas.DataFrame.select_dtypes"" rel=""nofollow noreferrer"">select_dtypes</a>, where you can either include or exclude the dtypes you want. </p>

<p>Ran into it on <a href=""https://github.com/pydata/pandas/issues/5775"" rel=""nofollow noreferrer"">this post</a> on the exact same thing. </p>

<p>Or in your case, specifically:<br>
<code>source.select_dtypes(['number']) or source.select_dtypes([np.number]</code></p>
";;3;;2015-09-04T13:55:33.357;;32399908;2017-04-27T11:21:27.257;2017-04-27T11:21:27.257;;2468113.0;;2468113.0;12725417.0;2;14;;;
35081;35081;;;"<p>Just as the error suggests , <code>pandas.read_csv</code> needs a file-like object as the first argument.</p>

<p>If you want to read the csv from a string, you can use <a href=""https://docs.python.org/3/library/io.html#io.StringIO"" rel=""nofollow noreferrer""><code>io.StringIO</code></a> (Python 3.x) or <a href=""https://docs.python.org/2/library/stringio.html#StringIO.StringIO"" rel=""nofollow noreferrer""><code>StringIO.StringIO</code> (Python 2.x)</a> . </p>

<p>Also, for the URL - <a href=""https://github.com/cs109/2014_data/blob/master/countries.csv"" rel=""nofollow noreferrer"">https://github.com/cs109/2014_data/blob/master/countries.csv</a> - you are getting back <code>html</code> response , not raw csv, you should use the url given by the <code>Raw</code> link in the github page for getting raw csv response , which is - <a href=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv</a></p>

<p>Example -</p>

<pre><code>import pandas as pd
import io
import requests
url=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""
s=requests.get(url).content
c=pd.read_csv(io.StringIO(s.decode('utf-8')))
</code></pre>

<h2>Update</h2>

<p>From pandas <code>0.19.2</code> you can now just <a href=""https://stackoverflow.com/a/41880513/2071807"">pass the url directly</a>.</p>
";;4;;2015-09-04T14:50:24.840;;32400969;2017-07-27T13:48:11.897;2017-07-27T13:48:11.897;;2071807.0;;795990.0;32400867.0;2;32;;;
35087;35087;;;"<p>As I commented you need to use a StringIO  object and decode i.e <code>c=pd.read_csv(io.StringIO(s.decode(""utf-8"")))</code> if using requests, you need to decode as .content returns <em>bytes</em> if you used .text you would just need to pass s as is <code>s = requests.get(url).text</code> c = <code>pd.read_csv(StringIO(s))</code>. </p>

<p>A simpler approach is to pass the correct url of the <em>raw</em> data directly to <code>read_csv</code>, you <strong>don't</strong> have to pass a file like object, you can pass a url so you don't need requests at all:</p>

<pre><code>c = pd.read_csv(""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"")

print(c)
</code></pre>

<p>Output:</p>

<pre><code>                              Country         Region
0                             Algeria         AFRICA
1                              Angola         AFRICA
2                               Benin         AFRICA
3                            Botswana         AFRICA
4                             Burkina         AFRICA
5                             Burundi         AFRICA
6                            Cameroon         AFRICA
..................................
</code></pre>

<p>From the  <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">docs</a>:</p>

<p><strong>filepath_or_buffer</strong> :</p>

<blockquote>
  <p>string or file handle / StringIO
  The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.csv</p>
</blockquote>
";;3;;2015-09-04T15:06:02.640;;32401251;2015-09-04T15:20:06.073;2015-09-04T15:20:06.073;;2141635.0;;2141635.0;32400867.0;2;6;;;
35235;35235;;;"<p>Given that all the dataframes have the same columns, you can simply <code>concat</code> them:</p>

<pre><code>import pandas as pd
df = pd.concat(list_of_dataframes)
</code></pre>
";;2;;2015-09-07T18:17:24.497;;32444187;2015-09-07T18:17:24.497;;;;;1453822.0;32444138.0;2;31;;;
35326;35326;;;"<p>I think this a really good question, in Hive you would use <code>EXPLODE</code>, I think there is a case to be made that Pandas should include this functionality by default. You could explode your list column like this:</p>

<pre><code>import numpy as np

df = pd.DataFrame({'listcol':[[1,2,3],[4,5,6]]})
X = pd.concat([pd.DataFrame(v, index=np.repeat(k,len(v))) 
            for k,v in df.listcol.to_dict().items()])    
</code></pre>

<p>Then you can use <code>pd.merge</code> to join this back to your original dataframe as @helpanderr suggested in the comment to your original question.</p>
";;0;;2015-09-09T00:27:41.963;;32469151;2015-09-09T00:27:41.963;;;;;839957.0;32468402.0;2;7;;;
35331;35331;;;"<p>In the code below, I first reset the index to make the row iteration easier.  </p>

<p>I create a list of lists where each element of the outer list is a row of the target DataFrame and each element of the inner list is one of the columns.  This nested list will ultimately be concatenated to create the desired DataFrame.</p>

<p>I use a <code>lambda</code> function together with list iteration to create a row for each element of the <code>nearest_neighbors</code> paired with the relevant <code>name</code> and <code>opponent</code>.  </p>

<p>Finally, I create a new DataFrame from this list (using the original column names and setting the index back to <code>name</code> and <code>opponent</code>).</p>

<pre><code>df = (pd.DataFrame({'name': ['A.J. Price'] * 3, 
                    'opponent': ['76ers', 'blazers', 'bobcats'], 
                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})
      .set_index(['name', 'opponent']))

&gt;&gt;&gt; df
                                                    nearest_neighbors
name       opponent                                                  
A.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]
           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]
           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]

df.reset_index(inplace=True)
rows = []
_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) 
                         for nn in row.nearest_neighbors], axis=1)
df_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])

&gt;&gt;&gt; df_new
                    nearest_neighbors
name       opponent                  
A.J. Price 76ers          Zach LaVine
           76ers           Jeremy Lin
           76ers        Nate Robinson
           76ers                Isaia
           blazers        Zach LaVine
           blazers         Jeremy Lin
           blazers      Nate Robinson
           blazers              Isaia
           bobcats        Zach LaVine
           bobcats         Jeremy Lin
           bobcats      Nate Robinson
           bobcats              Isaia
</code></pre>

<p><strong>EDIT JUNE 2017</strong></p>

<p>An alternative method is as follows:</p>

<pre><code>&gt;&gt;&gt; (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), 
             id_vars=['name', 'opponent'],
             value_name='nearest_neighbors')
     .set_index(['name', 'opponent'])
     .drop('variable', axis=1)
     .dropna()
     .sort_index()
     )
</code></pre>
";;0;;2015-09-09T03:36:02.723;;32470490;2017-06-22T05:45:56.240;2017-06-22T05:45:56.240;;2411802.0;;2411802.0;32468402.0;2;22;;;
35398;35398;;;"<p>This topic is two years old but seems like a probable place for people to land when they search for comparisons of Pandas and data.table</p>

<p>Since both of these have evolved over time, I want to post a relatively newer comparison (from 2014) here for the interested users: <a href=""https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping"" rel=""noreferrer"">https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping</a></p>

<p>It would be interesting to know if Wes and/or Matt (who, by the way, are creators of Pandas and data.table respectively and have both commented above) have any news to add here as well.</p>

<p>-- UPDATE --</p>

<p>A comment posted below by jangorecki contains a link that I think is very useful: <a href=""https://github.com/szilard/benchm-databases"" rel=""noreferrer"">https://github.com/szilard/benchm-databases</a></p>

<p><a href=""https://i.stack.imgur.com/dAnZO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dAnZO.png"" alt=""https://github.com/szilard/benchm-databases/blob/master/plot.png""></a></p>

<p>This graph depicts the average times of aggregation and join operations for different technologies (<strong>lower = faster</strong>; comparison last updated in Sept 2016). It was really educational for me.</p>

<p>Going back to the question, <code>R DT key</code> and <code>R DT</code> refer to the keyed/unkeyed flavors of R's data.table and happen to be faster in this benchmark than Python's Pandas (<code>Py pandas</code>). </p>
";;3;;2015-09-09T22:09:14.963;;32489918;2017-04-19T22:55:26.137;2017-04-19T22:55:26.137;;1601756.0;;1527248.0;8991709.0;2;24;;;
35503;35503;;;"<pre><code>df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})
df['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)
</code></pre>

<p>Yields this dataframe</p>

<pre><code>   Year quarter  period
0  2014      q1  2014q1
1  2015      q2  2015q2
</code></pre>

<p>This method generalizes to an arbitrary number of string columns by replacing <code>df[['Year', 'quarter']]</code> with any column slice of your dataframe, e.g. <code>df.iloc[:,0:2].apply(lambda x: ''.join(x), axis=1)</code>.</p>
";;4;;2015-09-11T17:36:18.113;;32529152;2016-02-02T22:25:24.210;2016-02-02T22:25:24.210;;2133371.0;;2133371.0;19377969.0;2;57;;;
35536;35536;;;"<p><strong>The Challenge</strong> One of the most challenging aspects of responding to SO questions is the time it takes to recreate the problem (including the data).  Questions which don't have a clear way to reproduce the data are less likely to be answered.  Given that you are taking the time to write a question and you have an issue that you'd like help with, you can easily help yourself by providing data that others can then use to help solve your problem.</p>

<p>The instructions provided by @Andy for writing good Pandas questions are an excellent place to start.  For more information, refer to <a href=""https://stackoverflow.com/help/how-to-ask"">how to ask</a> and how to create <a href=""https://stackoverflow.com/help/mcve"">Minimal, Complete, and Verifiable examples</a>.</p>

<p><strong>Please clearly state your question upfront.</strong>  After taking the time to write your question and any sample code, try to read it and provide an 'Executive Summary' for your reader which summarizes the problem and clearly states the question.</p>

<p><em>Original question</em>:</p>

<blockquote>
  <p>I have this data... </p>
  
  <p>I want to do this... </p>
  
  <p>I want my result to look like this... </p>
  
  <p>However, when I try to do [this], I get the following problem...</p>
  
  <p>I've tried to find solutions by doing [this] and [that].</p>
  
  <p>How do I fix it?</p>
</blockquote>

<p>Depending on the amount of data, sample code and error stacks provided, the reader needs to go a long way before understanding what the problem is.  Try restating your question so that the question itself is on top, and then provide the necessary details.</p>

<p><em>Revised Question</em>:</p>

<blockquote>
  <p><strong>Qustion:</strong>  How can I do [this]? </p>
  
  <p>I've tried to find solutions by doing [this] and [that].</p>
  
  <p>When I've tried to do [this], I get the following problem...</p>
  
  <p>I'd like my final results to look like this...</p>
  
  <p>Here is some minimal code that can reproduce my problem...</p>
  
  <p>And here is how to recreate my sample data:
      <code>df = pd.DataFrame({'A': [...], 'B': [...], ...})</code></p>
</blockquote>

<p><strong>PROVIDE SAMPLE DATA IF NEEDED!!!</strong></p>

<p>Sometimes just the head or tail of the DataFrame is all that is needed.  You can also use the methods proposed by @JohnE to create larger datasets that can be reproduced by others.  Using his example to generate a 100 row DataFrame of stock prices:</p>

<pre><code>stocks = pd.DataFrame({ 
    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),
    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),
    'price':(np.random.randn(100).cumsum() + 10) })
</code></pre>

<p>If this was your actual data, you may just want to include the head and/or tail of the dataframe as follows (be sure to anonymize any sensitive data):</p>

<pre><code>&gt;&gt;&gt; stocks.head(5).to_dict()
{'date': {0: Timestamp('2011-01-01 00:00:00'),
  1: Timestamp('2011-01-01 00:00:00'),
  2: Timestamp('2011-01-01 00:00:00'),
  3: Timestamp('2011-01-01 00:00:00'),
  4: Timestamp('2011-01-02 00:00:00')},
 'price': {0: 10.284260107718254,
  1: 11.930300761831457,
  2: 10.93741046217319,
  3: 10.884574289565609,
  4: 11.78005850418319},
 'ticker': {0: 'aapl', 1: 'aapl', 2: 'aapl', 3: 'aapl', 4: 'aapl'}}

&gt;&gt;&gt; pd.concat([stocks.head(), stocks.tail()], ignore_index=True).to_dict()
{'date': {0: Timestamp('2011-01-01 00:00:00'),
  1: Timestamp('2011-01-01 00:00:00'),
  2: Timestamp('2011-01-01 00:00:00'),
  3: Timestamp('2011-01-01 00:00:00'),
  4: Timestamp('2011-01-02 00:00:00'),
  5: Timestamp('2011-01-24 00:00:00'),
  6: Timestamp('2011-01-25 00:00:00'),
  7: Timestamp('2011-01-25 00:00:00'),
  8: Timestamp('2011-01-25 00:00:00'),
  9: Timestamp('2011-01-25 00:00:00')},
 'price': {0: 10.284260107718254,
  1: 11.930300761831457,
  2: 10.93741046217319,
  3: 10.884574289565609,
  4: 11.78005850418319,
  5: 10.017209045035006,
  6: 10.57090128181566,
  7: 11.442792747870204,
  8: 11.592953372130493,
  9: 12.864146419530938},
 'ticker': {0: 'aapl',
  1: 'aapl',
  2: 'aapl',
  3: 'aapl',
  4: 'aapl',
  5: 'msft',
  6: 'msft',
  7: 'msft',
  8: 'msft',
  9: 'msft'}}
</code></pre>

<p>You may also want to provide a description of the DataFrame (using only the relevant columns).  This makes it easier for others to check the data types of each column and identify other common errors (e.g. dates as string vs. datetime64 vs. object):</p>

<pre><code>stocks.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 100 entries, 0 to 99
Data columns (total 3 columns):
date      100 non-null datetime64[ns]
price     100 non-null float64
ticker    100 non-null object
dtypes: datetime64[ns](1), float64(1), object(1)
</code></pre>

<p><strong>NOTE:  If your DataFrame has a MultiIndex:</strong></p>

<p>If your DataFrame has a multiindex, you must first reset before calling <code>to_dict</code>.  You then need to recreate the index using <code>set_index</code>:</p>

<pre><code># MultiIndex example.  First create a MultiIndex DataFrame.
df = stocks.set_index(['date', 'ticker'])
&gt;&gt;&gt; df
    price
date       ticker           
2011-01-01 aapl    10.284260
           aapl    11.930301
           aapl    10.937410
           aapl    10.884574
2011-01-02 aapl    11.780059
...

# After resetting the index and passing the DataFrame to `to_dict`, make sure to use 
# `set_index` to restore the original MultiIndex.  This DataFrame can then be restored.

d = df.reset_index().to_dict()
df_new = pd.DataFrame(d).set_index(['date', 'ticker'])
&gt;&gt;&gt; df_new.head()
                       price
date       ticker           
2011-01-01 aapl    10.284260
           aapl    11.930301
           aapl    10.937410
           aapl    10.884574
2011-01-02 aapl    11.780059
</code></pre>
";;0;;2015-09-12T07:06:37.250;;32536193;2015-09-13T12:33:58.070;2017-05-23T12:18:21.853;;-1.0;;2411802.0;20109391.0;2;7;;;
35615;35615;;;"<pre><code>for column in df:
    print(df[column])
</code></pre>
";;3;;2015-09-14T06:42:27.347;;32558621;2015-09-14T06:42:27.347;;;;;992687.0;28218698.0;2;87;;;
35724;35724;;;"<p>If you are facing same problem as mine. Here is the solution which works for me.</p>

<ol>
<li>Uninstall every python and anaconda. </li>
<li>Download anaconda from here ""<a href=""http://continuum.io/downloads"" rel=""noreferrer"">http://continuum.io/downloads</a>"" and only install        it (no other python is needed).</li>
<li>Open spyder and import.</li>
<li><p>If you get any error, type in command prompt</p>

<p>pip install module_name</p></li>
</ol>

<p>I hope it will work for you too</p>
";;2;;2015-09-15T10:42:55.927;;32583988;2015-09-15T10:42:55.927;;;;;5275540.0;32565302.0;2;6;;;
35754;35754;;;"<p>You just specify converters.  I created an excel spreadsheet of the following structure:</p>

<pre><code>names   ages
bob     05
tom     4
suzy    3
</code></pre>

<p>Where the ""ages"" column is formatted as strings.  To load: </p>

<pre><code>import pandas as pd

df = pd.read_excel('Book1.xlsx',sheetname='Sheet1',header=0,converters={'names':str,'ages':str})
&gt;&gt;&gt; df
       names ages
   0   bob   05
   1   tom   4
   2   suzy  3
</code></pre>
";;3;;2015-09-15T17:06:27.830;;32591786;2017-01-15T14:03:09.263;2017-01-15T14:03:09.263;;6207849.0;;1678467.0;32591466.0;2;27;;;
35755;35755;;;"<p>The read_excel() function has a converters argument, where you can apply functions to input in certain columns.  You can use this to keep them as strings.
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"" rel=""noreferrer"">Documentation</a>:</p>

<blockquote>
  <p>Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the Excel cell content, and return the transformed content.</p>
</blockquote>

<p>Example code:</p>

<pre><code>pandas.read_excel(my_file, converters = {my_str_column: str})
</code></pre>
";;2;;2015-09-15T17:07:12.193;;32591799;2015-09-15T17:07:12.193;;;;;5294981.0;32591466.0;2;6;;;
35801;35801;;;"<p>With pandas version <code>0.16.x</code>, there is now a <code>DataFrame.sample</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"">method built-in</a>:</p>

<pre><code>import pandas

df = pandas.DataFrame(data)

# Randomly sample 70% of your dataframe
df_0.7 = df.sample(frac=0.7)

# Randomly sample 7 elements from your dataframe
df_7 = df.sample(n=7)
</code></pre>

<p>For either approach above, you can get the rest of the rows by doing:</p>

<pre><code>df_rest = df.loc[~df.index.isin(df_0.7.index)]
</code></pre>
";;2;;2015-09-16T10:57:30.303;;32606673;2015-09-17T10:46:08.573;2015-09-17T10:46:08.573;;943773.0;;943773.0;15923826.0;2;66;;;
35919;35919;;;"<p>I had a similar question regarding the interaction between excel and python (in particular, pandas), and I was referred to this question.</p>

<p>Thanks to some pointers by stackoverflow community, I found a package called <a href=""http://docs.xlwings.org/quickstart.html#interact-with-excel-from-python"" rel=""nofollow noreferrer"">xlwings</a> that seems to cover a lot of the functionalities HaPsantran required.</p>

<p>To use the OP's example:</p>

<p>Working with an existing excel file, you can drop an anchor in the data block (Sheet3) you want to import to pandas by naming it in excel and do: </p>

<pre><code># opened an existing excel file
</code></pre>

<p><code>wb = Workbook(Existing_file)</code></p>

<pre><code># Find in the excel file a named cell and reach the boundary of the cell block (boundary defined by empty column / row) and read the cell 
</code></pre>

<p><code>df = Range(Anchor).table.value</code></p>

<pre><code># import pandas and manipulate the data block
df = pd.DataFrame(df) # into Pandas DataFrame
df['sum'] = df.sum(axis= 1)

# write back to Sheet3
Range(Anchor).value = df.values
</code></pre>

<h1>tested that this implementation didn't temper existing formula in the excel file</h1>

<p>Let me know if this solves your problem and if there's anything I can help.</p>

<p>Big kudos to the developer of xlwings, they made this possible.</p>

<hr>

<p>Below is an update to my earlier answer after further question from @jamzsabb, and to reflect a changed API after xlwings updated to >= 0.9.0.</p>

<pre><code>import xlwings as xw
import pandas as pd
target_df = xw.Range('A7').options(pd.DataFrame, expand='table').value # only do this if the 'A7' cell (the cell within area of interest) is in active worksheet
#otherwise do:
#sht = xw.Book(r'path to your xlxs file\name_of_file.xlsx`).sheets['name of sheet']
#target_df = sht.Range('A7').options(pd.DataFrame, expand='table').value # you can also change 'A7' to any name that you've given to a cell like 'interest_table`
</code></pre>
";;4;;2015-09-18T06:33:01.970;;32645303;2017-01-23T01:54:39.117;2017-01-23T01:54:39.117;;3588314.0;;3588314.0;28142420.0;2;7;;;
35961;35961;;;"<p>+1 for using <code>at</code> or <code>iat</code> for scalar operations. Example benchmark:</p>

<pre><code>In [1]: import numpy, pandas
   ...: df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))
   ...: dictionary = df.to_dict()

In [2]: %timeit value = dictionary[5][5]
The slowest run took 34.06 times longer than the fastest. This could mean that an intermediate result is being cached 
1000000 loops, best of 3: 310 ns per loop

In [4]: %timeit value = df.loc[5, 5]
10000 loops, best of 3: 104 s per loop

In [5]: %timeit value = df.iloc[5, 5]
10000 loops, best of 3: 98.8 s per loop

In [6]: %timeit value = df.iat[5, 5]
The slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 9.58 s per loop

In [7]: %timeit value = df.at[5, 5]
The slowest run took 6.59 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 9.26 s per loop
</code></pre>

<p>It seems using <code>at</code> (<code>iat</code>) is about 10 times faster than <code>loc</code> (<code>iloc</code>).</p>
";;0;;2015-09-18T18:34:57.490;;32658847;2017-07-13T16:45:08.993;2017-07-13T16:45:08.993;;439325.0;;439325.0;22084338.0;2;9;;;
35975;35975;;;"<pre><code>print df.to_string(index=False)
</code></pre>
";;0;;2015-09-18T23:09:38.137;;32662331;2015-09-18T23:39:07.823;2015-09-18T23:39:07.823;;906523.0;;5352325.0;24644656.0;2;48;;;
36022;36022;;;"<p>While <code>iterrows()</code> is a good option, sometimes <code>itertuples()</code> can be much faster:</p>

<pre><code>df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})

%timeit [row.a * 2 for idx, row in df.iterrows()]
# =&gt; 10 loops, best of 3: 50.3 ms per loop

%timeit [row[1] * 2 for row in df.itertuples()]
# =&gt; 1000 loops, best of 3: 541 s per loop
</code></pre>
";;4;;2015-09-20T13:52:48.830;;32680162;2016-06-01T09:00:01.160;2016-06-01T09:00:01.160;;1054939.0;;1054939.0;16476924.0;2;65;;;
36077;36077;;;"<p>This won't win a code golf competition, and borrows from the previous answers - but clearly shows how the key is added, and how the join works. This creates 2 new data frames from lists, then adds the key to do the cartesian product on.</p>

<p>My use case was that I needed a list of all store IDs on for each week in my list. So, I created a list of all the weeks I wanted to have, then a list of all the store IDs I wanted to map them against.</p>

<p>The merge I chose left, but would be semantically the same as inner in this setup. You can see this <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra"" rel=""nofollow noreferrer"">in the documentation on merging</a>, which states it does a Cartesian product if key combination appears more than once in both tables - which is what we set up.</p>

<pre><code>days = pd.DataFrame({'date':list_of_days})
stores = pd.DataFrame({'store_id':list_of_stores})
stores['key'] = 0
days['key'] = 0
days_and_stores = days.merge(stores, how='left', on = 'key')
days_and_stores.drop('key',1, inplace=True)
</code></pre>
";;3;;2015-09-21T16:43:28.537;;32700453;2017-04-18T14:08:48.620;2017-04-18T14:08:48.620;;1457012.0;;1457012.0;13269890.0;2;13;;;
36230;36230;;;"<p>Pandas plotting can accept any extra keyword arguments from the respective matplotlib function. So for completeness from the comments of others here, this is how one would do it:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(100,2), columns=list('AB'))

df.hist(normed=1)
</code></pre>

<p>Also, for direct comparison this may be a good way as well:</p>

<pre><code>df.plot(kind='hist', normed=1, bins=20, stacked=False, alpha=.5)
</code></pre>
";;0;;2015-09-23T20:02:57.183;;32748510;2015-09-23T20:02:57.183;;;;;2055968.0;17874063.0;2;8;;;
36244;36244;;;"<p>Stata's <code>encode</code> command starts with a string variable and creates a new integer variable with labels mapped to the original string variable.  The direct analog of this in pandas would now be the categorical variable type which became a full-fledged part of pandas starting in 0.15 (which was released after this question was originally asked and answered).</p>

<p>See documentation <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.2/categorical.html"">here</a>.</p>

<p>To demonstrate for this example, the Stata command would be something like:</p>

<pre><code>encode cat, generate(cat2)
</code></pre>

<p>whereas the pandas command would be:</p>

<pre><code>x['cat2'] = x['cat'].astype('category')

  cat  val cat2
0   A   10    A
1   A   20    A
2   B   30    B
</code></pre>

<p>Just as Stata does with <code>encode</code>, the data are stored as integers, but display as strings in the default output.</p>

<p>You can verify this by using the categorical accessor <code>cat</code> to see the underlying integer.  (And for that reason you probably don't want to use 'cat' as a column name.)</p>

<pre><code>x['cat2'].cat.codes

0    0
1    0
2    1
</code></pre>
";;2;;2015-09-23T22:01:13.363;;32750237;2015-09-23T22:33:12.653;2015-09-23T22:33:12.653;;1820446.0;;3877338.0;20619851.0;2;6;;;
36248;36248;;;"<p>The <code>agg</code> function will do this for you.  Pass the columns and function as a dict with column, output:</p>

<pre><code>df.groupby(['Country', 'Item_Code']).agg({'Y1961': np.sum, 'Y1962': [np.sum, np.mean]})  # Added example for two output columns from a single input column
</code></pre>

<p>This will display only the group by columns, and the specified aggregate columns.  In this example I included two agg functions applied to 'Y1962'.</p>

<p>To get exactly what you hoped to see, included the other columns in the group by, and apply sums to the Y variables in the frame:</p>

<pre><code>df.groupby(['Code', 'Country', 'Item_Code', 'Item', 'Ele_Code', 'Unit']).agg({'Y1961': np.sum, 'Y1962': np.sum, 'Y1963': np.sum})
</code></pre>
";;2;;2015-09-24T00:04:50.473;;32751357;2015-09-24T00:11:46.153;2015-09-24T00:11:46.153;;1563909.0;;1563909.0;32751229.0;2;8;;;
36249;36249;;;"<p>You can select the columns of a groupby:</p>

<pre><code>In [11]: df.groupby(['Country', 'Item_Code'])[[""Y1961"", ""Y1962"", ""Y1963""]].sum()
Out[11]:
                       Y1961  Y1962  Y1963
Country     Item_Code
Afghanistan 15            10     20     30
            25            10     20     30
Angola      15            30     40     50
            25            30     40     50
</code></pre>

<p><em>Note that the list passed must be a subset of the columns otherwise you'll see a KeyError.</em></p>
";;1;;2015-09-24T00:12:27.013;;32751412;2015-09-24T00:12:27.013;;;;;1240268.0;32751229.0;2;31;;;
36261;36261;;;"<p><a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html"" rel=""noreferrer""><code>numpy.random.randint</code></a> accepts a third argument (<code>size</code>) , in which you can specify the size of the output array. You can use this to create your <code>DataFrame</code> -</p>

<pre><code>df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))
</code></pre>

<p>Here - <code>np.random.randint(0,100,size=(100, 4))</code> - creates an output array of size <code>(100,4)</code> with random integer elements between <code>[0,100)</code> .</p>

<hr>

<p>Demo -</p>

<pre><code>In [22]: df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))

In [23]: df
Out[23]:
     A   B   C   D
0   45  88  44  92
1   62  34   2  86
2   85  65  11  31
3   74  43  42  56
4   90  38  34  93
5    0  94  45  10
6   58  23  23  60
..  ..  ..  ..  ..
</code></pre>
";;0;;2015-09-24T02:20:46.730;;32752318;2015-09-24T02:20:46.730;;;;;795990.0;32752292.0;2;33;;;
36278;36278;;;"<p>dtypes is a Pandas Series. 
That means it contains index &amp; values attributes.
If you only need the column names:</p>

<pre><code>headers = df.dtypes.index
</code></pre>

<p>it will return a list containing the column names of ""df"" dataframe.</p>
";;0;;2015-09-24T11:40:57.207;;32760406;2015-09-24T11:40:57.207;;;;;1326106.0;24901766.0;2;9;;;
36280;36280;;;"<p>It doesn't work because:</p>

<ol>
<li>the second argument for <code>withColumn</code> should be a <code>Column</code> not a collection. <code>np.array</code> won't work here</li>
<li>when you pass <code>""index in indexes""</code> as a SQL expression to <code>where</code> <code>indexes</code> is out of scope and it is not resolved as a valid identifier</li>
</ol>

<p><strong>PySpark >= 1.4.0</strong></p>

<p><s>You can add row numbers using respective window function and query using <code>Column.isin</code> method or properly formated query string:</p>

<pre class=""lang-py prettyprint-override""><code>from pyspark.sql.functions import col, rowNumber
from pyspark.sql.window import Window

w = Window.orderBy()
indexed = df.withColumn(""index"", rowNumber().over(w))

# Using DSL
indexed.where(col(""index"").isin(set(indexes)))

# Using SQL expression
indexed.where(""index in ({0})"".format("","".join(str(x) for x in indexes)))
</code></pre>

<p></s></p>

<p>It looks like window functions called without <code>PARTITION BY</code> clause move all data to the single partition so above may be not the best solution after all.</p>

<blockquote>
  <p>Any faster and simpler way to deal with it?</p>
</blockquote>

<p>Not really. Spark DataFrames don't support random row access. </p>

<p><code>PairedRDD</code> can be accessed using <code>lookup</code> method which is relatively fast if data is partitioned using <code>HashPartitioner</code>. There is also <a href=""https://github.com/amplab/spark-indexedrdd"" rel=""nofollow"">indexed-rdd</a> project which supports efficient lookups. </p>

<p><strong>Edit</strong>:</p>

<p>Independent  of PySpark version you can try something like this:</p>

<pre><code>from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, LongType

row = Row(""char"")
row_with_index = Row(""char"", ""index"")

df = sc.parallelize(row(chr(x)) for x in range(97, 112)).toDF()
df.show(5)

## +----+
## |char|
## +----+
## |   a|
## |   b|
## |   c|
## |   d|
## |   e|
## +----+
## only showing top 5 rows

# This part is not tested but should work and save some work later
schema  = StructType(
    df.schema.fields[:] + [StructField(""index"", LongType(), False)])

indexed = (df.rdd # Extract rdd
    .zipWithIndex() # Add index
    .map(lambda ri: row_with_index(*list(ri[0]) + [ri[1]])) # Map to rows
    .toDF(schema)) # It will work without schema but will be more expensive

# inSet in Spark &lt; 1.3
indexed.where(col(""index"").isin(indexes))
</code></pre>
";;1;;2015-09-24T12:18:20.063;;32761138;2016-10-07T19:35:44.917;2016-10-07T19:35:44.917;;-1.0;;1560062.0;32760888.0;2;11;;;
36284;36284;;;"<p>Take the following example, it is easier to understand.</p>

<pre><code>|            | 13:00:00 | 13:30:00 | 14:00:00 | 14:30:00 | 15:00:00 | 15:30:00 | 16:00:00 | 16:30:00 | 
|------------|----------|----------|----------|----------|----------|----------|----------|----------| 
| 2011-01-01 | 2054     | 2071     | 2060     | 2054     | 2042     | 2064     | 2043     | 2089     | 
| 2011-01-02 | 2096     | 2038     | 2079     | 2052     | 2056     | 2092     | 2007     | 2008     | 
| 2011-01-03 | 2002     | 2083     | 2077     | 2087     | 2097     | 2079     | 2046     | 2078     | 
| 2011-01-04 | 2011     | 2063     | 2014     | 2094     | 2052     | 2041     | 2026     | 2077     | 
| 2011-01-05 | 2045     | 2056     | 2001     | 2061     | 2061     | 2061     | 2094     | 2068     | 
| 2011-01-06 | 2035     | 2043     | 2069     | 2006     | 2066     | 2067     | 2021     | 2012     | 
| 2011-01-07 | 2031     | 2036     | 2057     | 2043     | 2098     | 2010     | 2020     | 2016     | 
| 2011-01-08 | 2065     | 2025     | 2046     | 2024     | 2015     | 2011     | 2065     | 2013     | 
| 2011-01-09 | 2019     | 2036     | 2082     | 2009     | 2083     | 2009     | 2097     | 2046     | 
| 2011-01-10 | 2097     | 2060     | 2073     | 2003     | 2028     | 2012     | 2029     | 2011     | 
</code></pre>

<p>Let say we want to find the min from (2, b) to (6, d) <strong>for each row</strong>.</p>

<p>We can just fill the undesired data of the first and the last row by np.inf.</p>

<pre><code>df.loc[""2011-01-07"", :datetime.time(15, 0)] = np.inf
df.loc[""2011-01-10"", datetime.time(13, 30):] = np.inf
</code></pre>

<p>you get</p>

<pre><code>|            | 13:00:00 | 13:30:00 | 14:00:00 | 14:30:00 | 15:00:00 | 15:30:00 | 16:00:00 | 16:30:00 | 
|------------|----------|----------|----------|----------|----------|----------|----------|----------| 
| 2011-01-01 | 2054.0   | 2071.0   | 2060.0   | 2054.0   | 2042.0   | 2064.0   | 2043.0   | 2089.0   | 
| 2011-01-02 | 2096.0   | 2038.0   | 2079.0   | 2052.0   | 2056.0   | 2092.0   | 2007.0   | 2008.0   | 
| 2011-01-03 | 2002.0   | 2083.0   | 2077.0   | 2087.0   | 2097.0   | 2079.0   | 2046.0   | 2078.0   | 
| 2011-01-04 | 2011.0   | 2063.0   | 2014.0   | 2094.0   | 2052.0   | 2041.0   | 2026.0   | 2077.0   | 
| 2011-01-05 | 2045.0   | 2056.0   | 2001.0   | 2061.0   | 2061.0   | 2061.0   | 2094.0   | 2068.0   | 
| 2011-01-06 | 2035.0   | 2043.0   | 2069.0   | 2006.0   | 2066.0   | 2067.0   | 2021.0   | 2012.0   | 
| 2011-01-07 | inf      | inf      | inf      | inf      | inf      | 2010.0   | 2020.0   | 2016.0   | 
| 2011-01-08 | 2065.0   | 2025.0   | 2046.0   | 2024.0   | 2015.0   | 2011.0   | 2065.0   | 2013.0   | 
| 2011-01-09 | 2019.0   | 2036.0   | 2082.0   | 2009.0   | 2083.0   | 2009.0   | 2097.0   | 2046.0   | 
| 2011-01-10 | 2097.0   | inf      | inf      | inf      | inf      | inf      | inf      | inf      | 
</code></pre>

<p>In order to get the result:</p>

<pre><code>df.loc[""2011-01-07"": ""2011-01-10"", :].idxmin(axis=1)

2011-01-07    15:30:00
2011-01-08    15:30:00
2011-01-09    14:30:00
2011-01-10    13:00:00
Freq: D, dtype: object
</code></pre>
";;7;;2015-09-24T14:15:40.670;;32763707;2015-09-24T15:41:46.203;2015-09-24T15:41:46.203;;2347543.0;;2347543.0;31866802.0;2;6;;;
36287;36287;;;"<p>You can first stack the DataFrame to create a series and then index slice it as required and take the min. For example:</p>

<pre><code>first, last = ('2011-01-07', datetime.time(15)), ('2011-01-10', datetime.time(13, 30))
df.stack().loc[first: last].min()
</code></pre>

<p>The result of <code>df.stack</code> is a <code>Series</code> with a <code>MultiIndex</code> where the inner level is composed of the original columns. We then slice using <code>tuple</code> pairs with the start and end date and times. 
If you're going to be doing lots of such operations then you should consider assigning <code>df.stack()</code> to some variable. You might then consider changing the index to a proper <code>DatetimeIndex</code>. Then you can work with both the time series and the grid format as required. </p>

<p>Here's another method which avoids stacking and is a lot faster on DataFrames of the size you're actually working with (as a one-off; slicing the stacked <code>DataFrame</code> is a lot faster once it's stacked so if you're doing many of these operations you should stack and convert the index).<br>
It's less general as it works with <code>min</code> and <code>max</code> but not with, say, <code>mean</code>. It gets the <code>min</code> of the subset of the first and last rows and the <code>min</code> of the rows in between (if any) and takes the <code>min</code> of these three candidates. </p>

<pre><code>first_row = df.index.get_loc(first[0])
last_row = df.index.get_loc(last[0])
if first_row == last_row:
    result = df.loc[first[0], first[1]: last[1]].min()
elif first_row &lt; last_row:
    first_row_min = df.loc[first[0], first[1]:].min()
    last_row_min = df.loc[last[0], :last[1]].min()
    middle_min = df.iloc[first_row + 1:last_row].min().min()
    result = min(first_row_min, last_row_min, middle_min)
else: 
    raise ValueError('first row must be &lt;= last row')
</code></pre>

<p>Note that if <code>first_row + 1 == last_row</code> then <code>middle_min</code> is <code>nan</code> but the result is still correct as long as <code>middle_min</code> doesn't come first in the call to <code>min</code>. </p>
";;0;;2015-09-24T15:04:47.470;;32764796;2015-10-01T10:32:51.073;2015-10-01T10:32:51.073;;3868428.0;;3868428.0;31866802.0;2;9;;;
36318;36318;;;"<p>A hacky way, but should be fast, is to concat the shifted DataFrames:</p>

<pre><code>In [11]: df.shift(1)
Out[11]:
            13:00:00  13:30:00  14:00:00  14:30:00  15:00:00  15:30:00  16:00:00  16:30:00
2011-01-01       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN
2011-01-02      2054      2071      2060      2054      2042      2064      2043      2089
2011-01-03      2096      2038      2079      2052      2056      2092      2007      2008
2011-01-04      2002      2083      2077      2087      2097      2079      2046      2078
2011-01-05      2011      2063      2014      2094      2052      2041      2026      2077
2011-01-06      2045      2056      2001      2061      2061      2061      2094      2068
2011-01-07      2035      2043      2069      2006      2066      2067      2021      2012
2011-01-08      2031      2036      2057      2043      2098      2010      2020      2016
2011-01-09      2065      2025      2046      2024      2015      2011      2065      2013
2011-01-10      2019      2036      2082      2009      2083      2009      2097      2046

In [12]: df.shift(2).iloc[:, 4:]
Out[12]:
            15:00:00  15:30:00  16:00:00  16:30:00
2011-01-01       NaN       NaN       NaN       NaN
2011-01-02       NaN       NaN       NaN       NaN
2011-01-03      2042      2064      2043      2089
2011-01-04      2056      2092      2007      2008
2011-01-05      2097      2079      2046      2078
2011-01-06      2052      2041      2026      2077
2011-01-07      2061      2061      2094      2068
2011-01-08      2066      2067      2021      2012
2011-01-09      2098      2010      2020      2016
2011-01-10      2015      2011      2065      2013

In [13]: pd.concat([df.iloc[:, :1], df.shift(1), df.shift(2).iloc[:, 4:]], axis=1)
Out[13]:
            13:00:00  13:00:00  13:30:00  14:00:00  14:30:00  15:00:00  15:30:00  16:00:00  16:30:00  15:00:00  15:30:00  16:00:00  16:30:00
2011-01-01      2054       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN
2011-01-02      2096      2054      2071      2060      2054      2042      2064      2043      2089       NaN       NaN       NaN       NaN
2011-01-03      2002      2096      2038      2079      2052      2056      2092      2007      2008      2042      2064      2043      2089
2011-01-04      2011      2002      2083      2077      2087      2097      2079      2046      2078      2056      2092      2007      2008
2011-01-05      2045      2011      2063      2014      2094      2052      2041      2026      2077      2097      2079      2046      2078
2011-01-06      2035      2045      2056      2001      2061      2061      2061      2094      2068      2052      2041      2026      2077
2011-01-07      2031      2035      2043      2069      2006      2066      2067      2021      2012      2061      2061      2094      2068
2011-01-08      2065      2031      2036      2057      2043      2098      2010      2020      2016      2066      2067      2021      2012
2011-01-09      2019      2065      2025      2046      2024      2015      2011      2065      2013      2098      2010      2020      2016
2011-01-10      2097      2019      2036      2082      2009      2083      2009      2097      2046      2015      2011      2065      2013
</code></pre>

<p>and take the minimum across the columns (making sure you discard the columns which are too early or too late on a given day:</p>

<pre><code>In [14]: pd.concat([df.iloc[:, :1], df.shift(1), df.shift(2).iloc[:, 4:]], axis=1).min(1)
Out[14]:
2011-01-01    2054
2011-01-02    2042
2011-01-03    2002
2011-01-04    2002
2011-01-05    2011
2011-01-06    2001
2011-01-07    2006
2011-01-08    2010
2011-01-09    2010
2011-01-10    2009
Freq: D, dtype: float64
</code></pre>

<p>You can do this more efficiently, but more noisily, by taking the minimum of each shifted DataFrame before concatting:</p>

<pre><code>In [21]: pd.concat([df.iloc[:, :1].min(1),
                    df.shift(1).min(1),
                    df.shift(2).iloc[:, 4:].min(1)],
                   axis=1).min(1)
Out[21]:
2011-01-01    2054
2011-01-02    2042
2011-01-03    2002
2011-01-04    2002
2011-01-05    2011
2011-01-06    2001
2011-01-07    2006
2011-01-08    2010
2011-01-09    2010
2011-01-10    2009
Freq: D, dtype: float64
</code></pre>

<p><em>Either will be significantly faster than looping through days.</em></p>
";;4;;2015-09-24T20:58:53.050;;32770800;2015-09-24T20:58:53.050;;;;;1240268.0;31866802.0;2;6;;;
36329;36329;;;"<p>To complement <a href=""https://stackoverflow.com/questions/30026815/add-multiple-columns-to-pandas-dataframe-from-function/30027273#30027273"">John Galt's answer</a>:  </p>

<p>Depending on the task that is performed by <code>lambdafun</code>, you may get some speedup by storing the result of <code>apply</code> in a new <code>DataFrame</code> and then joining with the original: </p>

<pre><code>lambdafunc = lambda x: pd.Series([x['mytime'].hour,
                                  x['mydate'].isocalendar()[1],
                                  x['mydate'].weekday()])

newcols = df.apply(lambdafunc, axis=1)
newcols.columns = ['hour', 'weekday', 'weeknum']
newdf = df.join(newcols) 
</code></pre>

<p>Even if you do not see a speed improvement, I would recommend using the <code>join</code>.  You will be able to avoid the (always annoying) <code>SettingWithCopyWarning</code> that may pop up when assigning directly on the columns:</p>

<pre><code>SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
</code></pre>
";;0;;2015-09-25T00:53:03.280;;32773145;2015-09-25T00:53:03.280;2017-05-23T12:17:45.237;;-1.0;;1476240.0;30026815.0;2;9;;;
36357;36357;;;"<p>I'm using pandas 0.16.2. This has better performance on my large dataset:</p>

<pre><code>data.groupby(data.date.dt.year)
</code></pre>

<p>Using the <code>dt</code> option and playing around with <code>weekofyear</code>, <code>dayofweek</code> etc. becomes far easier.</p>
";;2;;2015-09-25T13:55:49.410;;32783825;2016-12-08T19:58:16.510;2016-12-08T19:58:16.510;;2662901.0;;3450487.0;11391969.0;2;27;;;
36409;36409;;;"<h2>tl;dr</h2>

<p>If you just want to count the number of rows per group, do:</p>

<pre><code>df.groupby(key_columns).size()
</code></pre>

<p>where <code>key_columns</code> is the column or list of columns you are grouping by. For example <code>key_columns = ['col1','col2']</code></p>

<p><br></p>

<h3>A simple example:</h3>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame([['a', 1],
                           ['b', 2],
                           ['c', 3],
                           ['a', 4],
                           ['b', 5]], 
                          columns=['col1', 'col2'])

In [3]: counts = df.groupby('col1').size(); counts
Out[3]: 
col1
a    2
b    2
c    1
dtype: int64
</code></pre>

<p>Note that <code>counts</code> is a pandas Series:</p>

<pre><code>In [4]: type(counts)
Out[4]: pandas.core.series.Series
</code></pre>

<p>If you want the results as a pandas Dataframe, do the following:</p>

<pre><code>In [5]: counts_df = pd.DataFrame(df.groupby('col1').size().rename('counts'))

In [6]: counts_df
Out[6]: 
      counts
col1        
a          2
b          2
c          1

In [7]: type(counts_df)
Out[7]: pandas.core.frame.DataFrame
</code></pre>

<p><br></p>

<hr>

<p>In what follows I will elaborate some more.</p>

<h3>Setup some test data</h3>

<pre><code>In[1]:
import numpy as np
import pandas as pd 

keys = np.array([
        ['A', 'B'],
        ['A', 'B'],
        ['A', 'B'],
        ['A', 'B'],
        ['C', 'D'],
        ['C', 'D'],
        ['C', 'D'],
        ['E', 'F'],
        ['E', 'F'],
        ['G', 'H'] 
        ])

df = pd.DataFrame(np.hstack([keys,np.random.randn(10,4).round(2)]), 
                  columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])

df[['col3', 'col4', 'col5', 'col6']] = \
    df[['col3', 'col4', 'col5', 'col6']].astype(float)
</code></pre>

<p><br>
Below we show the data types and data for the test dataframe:</p>

<pre><code>In [2]: df.dtypes
Out[2]:
col1     object
col2     object
col3    float64
col4    float64
col5    float64
col6    float64
dtype: object

In [3]: df
Out[3]:
  col1 col2  col3  col4  col5  col6
0    A    B  1.50 -1.70 -0.46 -0.30
1    A    B  0.04 -0.22 -0.91  2.43
2    A    B  0.25 -1.00 -0.78  0.46
3    A    B  2.66 -1.56 -0.30 -0.44
4    C    D -1.05  1.04 -0.31 -0.88
5    C    D -0.19 -1.08  0.31 -0.91
6    C    D -1.34 -1.83 -2.06 -2.09
7    E    F  1.83  1.56  0.86 -0.70
8    E    F  0.87 -1.03 -2.59 -1.35
9    G    H -0.13  0.53 -0.40 -1.64
</code></pre>

<p><br> </p>

<p>Now, suppose you want to get the <code>mean</code> and the <code>count</code> for some of the columns. Let's go ahead and run a simple aggreagation (<code>agg</code>) to do this:</p>

<h3>One <code>count</code> per aggregated column</h3>

<pre><code>In [8]: df[['col1', 'col2', 'col3', 'col4']]\
            .groupby(['col1', 'col2']).agg(['mean', 'count'])
Out[8]:
             col3            col4      
             mean count      mean count
col1 col2                              
A    B     1.1125     4 -1.120000     4
C    D    -0.8600     3 -0.623333     3
E    F     1.3500     2  0.265000     2
G    H    -0.1300     1  0.530000     1
</code></pre>

<p>It is kind of annoying that you get one <code>count</code> column for each of the columns aggregated.  If all of your data is valid (i.e., you do not have any <code>NaN</code> cells) then all of the <code>count</code> columns will be redundant.  </p>

<p><br>    </p>

<h3>One <code>count</code> per group</h3>

<p>To end up with a single <code>count</code> column, we can save the <code>groupby</code> results to a variable, and use it separately to calculate the mean, and to get the size of each group. </p>

<p>We then <code>join</code> the <code>means</code> with the <code>counts</code> (renaming the columns along the  way for clarity) to end up with a single dataframe:</p>

<pre><code>In [9]: groupby_object = df[['col1', 'col2', 'col3', 'col4']]\
            .groupby(['col1', 'col2'])

In [10]: groupby_object.agg('mean')\
             .rename(columns = lambda x: x + ' mean')\
             .join(pd.DataFrame(groupby_object.size(), 
                                columns=['counts']))
Out[10]:
           col3 mean  col4 mean  counts
col1 col2                              
A    B        1.1125  -1.120000       4
C    D       -0.8600  -0.623333       3
E    F        1.3500   0.265000       2
G    H       -0.1300   0.530000       1
</code></pre>

<p><br>
<strong>Disclaimer:</strong></p>

<p>If some of the columns that you are aggregating have null values, then you really want to be looking at the group sizes independently for each aggregated column. Otherwise you may be misled as to how many records are actually being used to calculate the mean.</p>
";;2;;2015-09-26T19:34:51.527;;32801170;2016-07-29T16:52:01.607;2016-07-29T16:52:01.607;;1476240.0;;1476240.0;19384532.0;2;215;;;
36411;36411;;;"<p>The ignore_index option is working in your example, you just need to know that it is ignoring the <em>axis of concatenation</em> which in your case is the columns. (Perhaps a better name would be ignore_labels.) If you want the concatenation to ignore the index labels, then your axis variable has to be set to 0 (the default).</p>
";;1;;2015-09-26T20:53:49.487;;32801924;2015-09-26T21:22:10.587;2015-09-26T21:22:10.587;;4602248.0;;4602248.0;32801806.0;2;8;;;
36413;36413;;;"<p>If I understood you correctly, this is what you would like to do.</p>

<pre><code>import pandas as pd

df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3'],
                    'D': ['D0', 'D1', 'D2', 'D3']},
                    index=[0, 2, 3,4])

df2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],
                    'C': ['C4', 'C5', 'C6', 'C7'],
                    'D2': ['D4', 'D5', 'D6', 'D7']},
                    index=[ 4, 5, 6 ,7])


df1.reset_index(drop=True, inplace=True)
df2.reset_index(drop=True, inplace=True)

df = pd.concat( [df1, df2], axis=1) 
</code></pre>

<p>Which gives:</p>

<pre><code>    A   B   D   A1  C   D2
0   A0  B0  D0  A4  C4  D4
1   A1  B1  D1  A5  C5  D5
2   A2  B2  D2  A6  C6  D6
3   A3  B3  D3  A7  C7  D7
</code></pre>

<p>Actually, I would have expected that <code>df = pd.concat(dfs,axis=1,ignore_index=True)</code> gives the same result.</p>

<p>This is the excellent explanation from <strong>jreback</strong>:</p>

<blockquote>
  <p><code>ignore_index=True</code> ignores, meaning doesnt align on the joining axis. it simply pastes them together in the order that they are passed, then reassigns a range for the actual index (e.g. <code>range(len(index))</code>)
  so the difference between joining on non-overlapping indexes (assume <code>axis=1</code> in the example), is that with <code>ignore_index=False</code> (the default), you get the concat of the indexes, and with <code>ignore_index=True</code> you get a range.</p>
</blockquote>
";;3;;2015-09-26T21:05:10.463;;32802014;2015-09-27T18:48:15.797;2015-09-27T18:48:15.797;;2272172.0;;2272172.0;32801806.0;2;18;;;
36558;36558;;;"<p>If you don't want to create a new dataframe, or if your dataframe has more columns than just the ones you want to split, you could:</p>

<pre><code>df[""flips""], df[""row_name""] = zip(*df[""row""].str.split().tolist())
del df[""row""]  
</code></pre>
";;2;;2015-09-29T17:52:36.873;;32850652;2015-09-29T18:08:43.013;2015-09-29T18:08:43.013;;4945132.0;;4945132.0;14745022.0;2;12;;;
36752;36752;;;"<p>Drop multiple columns like this:</p>

<pre><code>cols = [1,2,4,5,12]
df.drop(df.columns[cols],axis=1,inplace=True)
</code></pre>
";;2;;2015-10-02T14:10:43.470;;32909107;2016-02-07T20:25:27.440;2016-02-07T20:25:27.440;;3130926.0;;3130926.0;20297317.0;2;17;;;
36891;36891;;;"<p><code>df.columns.values</code> also give us the column names</p>
";;0;;2015-10-05T08:43:29.097;;32944421;2015-10-05T08:43:29.097;;;;;1930402.0;18022845.0;2;6;;;
36937;36937;;;"<p>Transposing is inefficient for large DataFrames.  Here is an alternative:</p>

<pre><code>def duplicate_columns(frame):
    groups = frame.columns.to_series().groupby(frame.dtypes).groups
    dups = []
    for t, v in groups.items():
        dcols = frame[v].to_dict(orient=""list"")

        vs = dcols.values()
        ks = dcols.keys()
        lvs = len(vs)

        for i in range(lvs):
            for j in range(i+1,lvs):
                if vs[i] == vs[j]: 
                    dups.append(ks[i])
                    break

    return dups       
</code></pre>

<p>Use it like this:</p>

<pre><code>dups = duplicate_columns(frame)
frame = frame.drop(dups, axis=1)
</code></pre>

<p><strong>Edit</strong></p>

<p>A memory efficient version that treats nans like any other value:</p>

<pre><code>from pandas.core.common import array_equivalent

def duplicate_columns(frame):
    groups = frame.columns.to_series().groupby(frame.dtypes).groups
    dups = []

    for t, v in groups.items():

        cs = frame[v].columns
        vs = frame[v]
        lcs = len(cs)

        for i in range(lcs):
            ia = vs.iloc[:,i].values
            for j in range(i+1, lcs):
                ja = vs.iloc[:,j].values
                if array_equivalent(ia, ja):
                    dups.append(cs[i])
                    break

    return dups
</code></pre>
";;5;;2015-10-06T03:16:35.100;;32961145;2015-10-06T18:15:09.840;2015-10-06T18:15:09.840;;759442.0;;759442.0;14984119.0;2;8;;;
36959;36959;;;"<p><code>df.memory_usage()</code> will return how much each column occupies:</p>

<pre><code>&gt;&gt;&gt; df.memory_usage()

Row_ID            20906600
Household_ID      20906600
Vehicle           20906600
Calendar_Year     20906600
Model_Year        20906600
...
</code></pre>

<p>To include indexes, pass <code>index=True</code>.</p>

<p>So to get overall memory consumption:</p>

<pre><code>&gt;&gt;&gt; df.memory_usage(index=True).sum()
731731000
</code></pre>
";;3;;2015-10-06T12:34:11.840;;32970117;2015-10-06T12:34:11.840;;;;;519379.0;18089667.0;2;24;;;
37050;37050;;;"<p>If you want to read a zipped or a tar.gz file into pandas dataframe, the read_csv methods includes this particular implementation.     </p>

<pre><code>df = pd.read_csv(filename.tar.gz, compression='gzip', header=0, sep=',', quotechar='""')
</code></pre>

<p>compression : {gzip, bz2, infer, None}, default infer
For on-the-fly decompression of on-disk data. If infer, then use gzip or bz2 if filepath_or_buffer is a string ending in .gz or .bz2, respectively, and no decompression otherwise. Set to None for no decompression.
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>
";;3;;2015-10-07T13:30:12.870;;32993553;2015-10-07T13:30:12.870;;;;;5109871.0;18885175.0;2;41;;;
37083;37083;;;"<p>Another possibility is to use numpy.maximum(). This is more straight-forward to read in my opinion.</p>

<pre><code>import pandas as pd
import numpy as np
df['value'] = np.maximum(df['value'], 0)
</code></pre>

<p>And it's also 2-3X faster.</p>

<pre><code>In [21]: df = pd.DataFrame({'value': np.arange(-1000000,1000000)})
In [22]: %timeit df.loc[df['value']&lt;0,'value'] = 0
10 loops, best of 3: 27.3 ms per loop

In [23]: df = pd.DataFrame({'value': np.arange(-1000000,1000000)})
In [24]: %timeit df['value'] = np.maximum(df['value'], 0)
100 loops, best of 3: 9.42 ms per loop
</code></pre>
";;1;;2015-10-07T19:38:26.177;;33000983;2015-10-14T18:54:42.613;2015-10-14T18:54:42.613;;5420054.0;;5420054.0;17068269.0;2;9;;;
37106;37106;;;"<p>user3817518: ""Please also share if there is another way to put many dataframes into excel using the built-in df.to_excel functionality !!""</p>

<p>Here's my attempt:</p>

<p>Easy way to put together a lot of dataframes on just one sheet or across multiple tabs. Let me know if this works!</p>

<p>-- To test, just run the sample dataframes and the second and third portion of code.</p>

<h3>Sample dataframes</h3>

<pre><code>import pandas as pd
import numpy as np

# Sample dataframes    
randn = np.random.randn
df = pd.DataFrame(randn(15, 20))
df1 = pd.DataFrame(randn(10, 5))
df2 = pd.DataFrame(randn(5, 10))
</code></pre>

<h3>Put multiple dataframes into one xlsx sheet</h3>

<pre><code># funtion
def multiple_dfs(df_list, sheets, file_name, spaces):
    writer = pd.ExcelWriter(file_name,engine='xlsxwriter')   
    row = 0
    for dataframe in df_list:
        dataframe.to_excel(writer,sheet_name=sheets,startrow=row , startcol=0)   
        row = row + len(dataframe.index) + spaces + 1
    writer.save()

# list of dataframes
dfs = [df,df1,df2]

# run function
multiple_dfs(dfs, 'Validation', 'test1.xlsx', 1)
</code></pre>

<h3>Put multiple dataframes across separate tabs/sheets</h3>

<pre><code># function
def dfs_tabs(df_list, sheet_list, file_name):
    writer = pd.ExcelWriter(file_name,engine='xlsxwriter')   
    for dataframe, sheet in zip(df_list, sheet_list):
        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0)   
    writer.save()

# list of dataframes and sheet names
dfs = [df, df1, df2]
sheets = ['df','df1','df2']    

# run function
dfs_tabs(dfs, sheets, 'multi-test.xlsx')
</code></pre>
";;6;;2015-10-07T23:44:08.570;;33004253;2016-03-04T05:48:25.187;2016-03-04T05:48:25.187;;5266712.0;;5266712.0;32957441.0;2;10;;;
37121;37121;;;"<p>Just be careful when using bdate_range or BDay() - the name might mislead you to think that it is a range of business days, whereas in reality it's just calendar days with weekends stripped out (ie. it doesn't take holidays into account).</p>
";;0;;2015-10-08T09:23:03.253;;33011646;2015-10-08T09:23:03.253;;;;;1933152.0;13019719.0;2;9;;;
37153;37153;;;"<p>in pandas 16.2, I had to do <code>pd.DataFrame.from_records(d)</code> to get this to work. </p>
";;5;;2015-10-08T15:59:32.687;;33020669;2015-10-08T15:59:32.687;;;;;3681451.0;20638006.0;2;27;;;
37214;37214;;;"<p>Code:</p>

<pre><code>df = pd.DataFrame({'a':list('tuhimerisabhain')})
df.a.value_counts()

&gt;&gt;&gt; df.a.value_counts()
i    3
h    2
a    2
n    1
b    1
m    1
r    1
t    1
e    1
u    1
s    1
</code></pre>
";;2;;2015-10-09T14:02:42.160;;33040290;2017-07-05T16:41:32.350;2017-07-05T16:41:32.350;;6885902.0;;4416255.0;22391433.0;2;9;;;
37266;37266;;;"<p>Pickle works good!</p>

<pre><code>import pandas as pd
df.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl
df1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df
</code></pre>
";;1;;2015-10-10T04:52:42.097;;33050438;2015-12-01T10:44:45.603;2015-12-01T10:44:45.603;;5173219.0;;5173219.0;17098654.0;2;8;;;
37276;37276;;;"<p>Let's first create the example pandas dataframe:</p>

<pre><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: index = pd.Index(['01/01/2012','01/01/2012','01/01/2012','01/02/2012','01/02/2012'], name='Date')

In [4]: df = pd.DataFrame({'ID':[100,101,102,201,202],'wt':[.5,.75,1,.5,1],'value':[60,80,100,100,80]},index=index)
</code></pre>

<p>Then, the average of 'wt' weighted by 'value' and grouped by the index is obtained as:</p>

<pre><code>In [5]: df.groupby(df.index).apply(lambda x: np.average(x.wt, weights=x.value))
Out[5]: 
Date
01/01/2012    0.791667
01/02/2012    0.722222
dtype: float64
</code></pre>

<p>Alternatively, one can also define a function:</p>

<pre><code>In [5]: def grouped_weighted_avg(values, weights, by):
   ...:     return (values * weights).groupby(by).sum() / weights.groupby(by).sum()

In [6]: grouped_weighted_avg(values=df.wt, weights=df.value, by=df.index)
Out[6]: 
Date
01/01/2012    0.791667
01/02/2012    0.722222
dtype: float64
</code></pre>
";;3;;2015-10-10T13:09:16.770;;33054358;2015-10-10T14:51:29.830;2015-10-10T14:51:29.830;;1551810.0;;1551810.0;26205922.0;2;9;;;
37387;37387;;;"<p>You should read the documentation here: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html</a></p>

<p>What you are looking for is a left join. The default option is an inner join. You can change this behavior by passing a different how argument:</p>

<pre><code>df1.merge(df2,how='left', left_on='Column1', right_on='ColumnA')
</code></pre>
";;3;;2015-10-12T17:29:39.820;;33086953;2015-10-13T04:59:54.513;2015-10-13T04:59:54.513;;3329265.0;;3329265.0;33086881.0;2;13;;;
37397;37397;;;"<p>If you're sure the index row values are the same then to avoid the index alignment order then just call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index"" rel=""noreferrer""><code>reset_index()</code></a>, this will reset your index values back to start from <code>0</code>:</p>

<pre><code>df_c = pd.concat([df_a.reset_index(drop=True), df_b], axis=1)
</code></pre>
";;0;;2015-10-12T19:04:47.507;;33088410;2015-10-12T19:37:28.493;2015-10-12T19:37:28.493;;704848.0;;704848.0;33088010.0;2;20;;;
37468;37468;;;"<p>The difference is due to different normalizations.  Scipy by default does not correct for bias, whereas pandas does.</p>

<p>You can tell scipy to correct for bias by passing the <code>bias=False</code> argument:</p>

<pre><code>&gt;&gt;&gt; x = pandas.Series(np.random.randn(10))
&gt;&gt;&gt; stats.skew(x)
-0.17644348972413657
&gt;&gt;&gt; x.skew()
-0.20923623968879457
&gt;&gt;&gt; stats.skew(x, bias=False)
-0.2092362396887948
&gt;&gt;&gt; stats.kurtosis(x)
0.6362620964462327
&gt;&gt;&gt; x.kurtosis()
2.0891062062174464
&gt;&gt;&gt; stats.kurtosis(x, bias=False)
2.089106206217446
</code></pre>

<p>There does not appear to be a way to tell pandas to remove the bias correction.</p>
";;0;;2015-10-13T17:46:05.893;;33109272;2015-10-13T17:46:05.893;;;;;1427416.0;33109107.0;2;16;;;
37550;37550;;;"<p>You mention you are using Pandas (in your title).  If so, there is no need to use an external library, you can just use <code>to_datetime</code></p>

<pre><code>&gt;&gt;&gt; pandas.to_datetime('today')
Timestamp('2015-10-14 00:00:00')
</code></pre>

<p>This will always return today's date at midnight, irrespective of the actual time (but guess what the argument <code>now</code> will do). </p>
";;2;;2015-10-14T18:51:45.517;;33133356;2015-10-14T18:51:45.517;;;;;230446.0;21738566.0;2;33;;;
37628;37628;;;"<p>Try using <code>df.loc[row_index,col_indexer] = value</code></p>
";;1;;2015-10-15T13:32:55.250;;33149986;2015-10-16T13:31:32.757;2015-10-16T13:31:32.757;;2039244.0;;5449812.0;13842088.0;2;7;;;
37688;37688;;;"<pre><code>df = pd.read_csv('somefile.csv', low_memory=False)
</code></pre>

<p>This should solve the issue. I got exactly the same error, when reading 1.8M rows from a CSV.</p>
";;2;;2015-10-16T03:12:36.323;;33161955;2016-01-13T08:32:12.280;2016-01-13T08:32:12.280;;3730397.0;;5452173.0;24251219.0;2;7;;;
37701;37701;;;"<p>You can <em>reset</em> the index using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a> to get back a default index of 1, 2, ..., n (and use <code>drop=True</code> to indicate you want to drop the existing index instead of adding it as a column to your dataframe):</p>

<pre><code>In [19]: df2 = df2.reset_index(drop=True)

In [20]: df2
Out[20]:
   x  y
0  0  0
1  0  1
2  0  2
3  1  0
4  1  1
5  1  2
6  2  0
7  2  1
8  2  2
</code></pre>
";;0;;2015-10-16T08:32:23.440;;33165860;2017-03-25T20:53:00.583;2017-03-25T20:53:00.583;;653364.0;;653364.0;33165734.0;2;21;;;
37708;37708;;;"<pre><code>pip install -U matplotlib
</code></pre>

<p>worked for me.</p>

<p>Thanks joris!</p>
";;2;;2015-10-16T10:20:10.780;;33168054;2015-10-16T10:20:10.780;;;;;992687.0;33159634.0;2;20;;;
37765;37765;;;"<p>Try:</p>

<pre><code>sub2['income'].fillna((sub2['income'].mean()), inplace=True)
</code></pre>
";;0;;2015-10-16T20:18:30.597;;33178896;2015-10-16T20:30:12.237;2015-10-16T20:30:12.237;;55075.0;;5455333.0;18689823.0;2;9;;;
38021;38021;;;"<blockquote>
  <p>It's not smart enough to realize it's still a ""vector"" in math terms.</p>
</blockquote>

<p>Say rather that it's smart enough to recognize a difference in dimensionality. :-)</p>

<p>I think the simplest thing you can do is select that row positionally using <code>iloc</code>, which gives you a Series with the columns as the new index and the values as the values:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([list(range(5))], columns=[""a{}"".format(i) for i in range(5)])
&gt;&gt;&gt; df
   a0  a1  a2  a3  a4
0   0   1   2   3   4
&gt;&gt;&gt; df.iloc[0]
a0    0
a1    1
a2    2
a3    3
a4    4
Name: 0, dtype: int64
&gt;&gt;&gt; type(_)
&lt;class 'pandas.core.series.Series'&gt;
</code></pre>
";;3;;2015-10-20T21:21:08.170;;33247007;2015-10-20T21:21:08.170;;;;;487339.0;33246771.0;2;10;;;
38034;38034;;;"<p>You can use <code>apply</code> for this, and it's a bit neater:</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(1)

df = pd.DataFrame(np.random.randn(4,4)* 4 + 3)

          0         1         2         3
0  9.497381  0.552974  0.887313 -1.291874
1  6.461631 -6.206155  9.979247 -0.044828
2  4.276156  2.002518  8.848432 -5.240563
3  1.710331  1.463783  7.535078 -1.399565

df.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))

          0         1         2         3
0  0.515087  0.133967 -0.651699  0.135175
1  0.125241 -0.689446  0.348301  0.375188
2 -0.155414  0.310554  0.223925 -0.624812
3 -0.484913  0.244924  0.079473  0.114448
</code></pre>

<p>Also, it works nicely with <code>groupby</code>, if you select the relevant columns:</p>

<pre><code>df['grp'] = ['A', 'A', 'B', 'B']

          0         1         2         3 grp
0  9.497381  0.552974  0.887313 -1.291874   A
1  6.461631 -6.206155  9.979247 -0.044828   A
2  4.276156  2.002518  8.848432 -5.240563   B
3  1.710331  1.463783  7.535078 -1.399565   B


df.groupby(['grp'])[[0,1,2,3]].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))

     0    1    2    3
0  0.5  0.5 -0.5 -0.5
1 -0.5 -0.5  0.5  0.5
2  0.5  0.5  0.5 -0.5
3 -0.5 -0.5 -0.5  0.5
</code></pre>
";;0;;2015-10-21T03:10:46.473;;33250288;2016-01-26T22:30:38.810;2016-01-26T22:30:38.810;;210945.0;;210945.0;12525722.0;2;19;;;
38054;38054;;;"<p>You can do this by making a <a href=""http://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.twinx"" rel=""noreferrer""><code>twinx</code></a> axes for the frequencies. You can switch the two y axes around so the frequencies stay on the left and the counts on the right, but without having to recalculate the counts axis (here we use <a href=""http://matplotlib.org/api/axis_api.html#matplotlib.axis.YAxis.tick_right"" rel=""noreferrer""><code>tick_left()</code></a> and <a href=""http://matplotlib.org/api/axis_api.html#matplotlib.axis.YAxis.tick_right"" rel=""noreferrer""><code>tick_right()</code></a> to move the ticks and <a href=""http://matplotlib.org/api/axis_api.html#matplotlib.axis.YAxis.set_label_position"" rel=""noreferrer""><code>set_label_position</code></a> to move the axis labels</p>

<p>You can then set the ticks using the <a href=""http://matplotlib.org/api/ticker_api.html#ticker"" rel=""noreferrer""><code>matplotlib.ticker</code></a> module, specifically <a href=""http://matplotlib.org/api/ticker_api.html#matplotlib.ticker.MultipleLocator"" rel=""noreferrer""><code>ticker.MultipleLocator</code></a> and <a href=""http://matplotlib.org/api/ticker_api.html#matplotlib.ticker.LinearLocator"" rel=""noreferrer""><code>ticker.LinearLocator</code></a>.</p>

<p>As for your annotations, you can get the x and y locations for all 4 corners of the bar with <code>patch.get_bbox().get_points()</code>. This, along with setting the horizontal and vertical alignment correctly, means you don't need to add any arbitrary offsets to the annotation location.</p>

<p>Finally, you need to turn the grid off for the twinned axis, to prevent grid lines showing up on top of the bars (<a href=""http://matplotlib.org/api/axis_api.html#matplotlib.axis.Axis.grid"" rel=""noreferrer""><code>ax2.grid(None)</code></a>)</p>

<p>Here is a working script:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.ticker as ticker

# Some random data
dfWIM = pd.DataFrame({'AXLES': np.random.normal(8, 2, 5000).astype(int)})
ncount = len(dfWIM)

plt.figure(figsize=(12,8))
ax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])
plt.title('Distribution of Truck Configurations')
plt.xlabel('Number of Axles')

# Make twin axis
ax2=ax.twinx()

# Switch so count axis is on right, frequency on left
ax2.yaxis.tick_left()
ax.yaxis.tick_right()

# Also switch the labels over
ax.yaxis.set_label_position('right')
ax2.yaxis.set_label_position('left')

ax2.set_ylabel('Frequency [%]')

for p in ax.patches:
    x=p.get_bbox().get_points()[:,0]
    y=p.get_bbox().get_points()[1,1]
    ax.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), 
            ha='center', va='bottom') # set the alignment of the text

# Use a LinearLocator to ensure the correct number of ticks
ax.yaxis.set_major_locator(ticker.LinearLocator(11))

# Fix the frequency range to 0-100
ax2.set_ylim(0,100)
ax.set_ylim(0,ncount)

# And use a MultipleLocator to ensure a tick spacing of 10
ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))

# Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars
ax2.grid(None)

plt.savefig('snscounter.pdf')
</code></pre>

<p><a href=""https://i.stack.imgur.com/UYIv2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UYIv2.png"" alt=""enter image description here""></a></p>
";;5;;2015-10-21T12:14:27.530;;33259038;2016-07-28T15:56:42.443;2016-07-28T15:56:42.443;;588071.0;;588071.0;33179122.0;2;14;;;
38121;38121;;;"<p>You can use groupby's <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html""><code>size</code></a>:</p>

<pre><code>In [11]: df.groupby([""Group"", ""Size""]).size()
Out[11]:
Group     Size
Moderate  Medium    1
          Small     1
Short     Small     2
Tall      Large     1
dtype: int64

In [12]: df.groupby([""Group"", ""Size""]).size().reset_index(name=""Time"")
Out[12]:
      Group    Size  Time
0  Moderate  Medium     1
1  Moderate   Small     1
2     Short   Small     2
3      Tall   Large     1
</code></pre>
";;0;;2015-10-22T00:44:50.733;;33271634;2015-10-22T00:44:50.733;;;;;1240268.0;33271098.0;2;28;;;
38321;38321;;;"<p>There is an easy way to do this relaying on shapely functions.
First, you need to get the exterior ring of the polygon, and project the 
point to the ring. It is mandatory to get the exterior as a LinearRing since polygons
do not have the projection function. Opposed to intuition, this gives a distance, the distance
from the first point of the ring to the point in the ring closest to 
the given point. Then, you just use that distance to get the point with 
the interpolate function. See the code below.</p>

<pre><code>from shapely.geometry import Polygon, Point, LinearRing

poly = Polygon([(0, 0), (2,8), (14, 10), (6,1)])
point = Point(12,4)

pol_ext = LinearRing(poly.exterior.coords)
d = pol_ext.project(point)
p = pol_ext.interpolate(d)
closest_point_coords = list(p.coords)[0]
</code></pre>

<p>It is important to mention that this method only works if you know the 
point is outside the exterior of the polygon. If the point is inside one 
of its interior rings, you need to adapt the code for that situation. </p>

<p>If the polygon do not has interior rings, the code will work even for points inside the polygon. That is because we are in fact working with the exterior ring as a line string, and ignoring whether the line string comes from  a polygon or not. </p>

<p>It is easy to extend this code to the general case of computing the distance of any point (inside or outside of the polygon) to the closest point in the polygon boundary. You only need to compute the closest point (and distance) from the point to all line rings: the exterior ring, and each interior ring of the polygon. Then, you just keep the minimum of those.</p>

<p><a href=""https://i.stack.imgur.com/mB9wC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mB9wC.jpg"" alt=""enter image description here""></a></p>
";;5;;2015-10-24T22:30:16.430;;33324058;2017-04-03T11:46:27.027;2017-04-03T11:46:27.027;;1527176.0;;1527176.0;33311616.0;2;12;;;
38368;38368;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html#pandas.core.groupby.GroupBy.size""><code>size</code></a> includes <code>NaN</code> values, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html#pandas.core.groupby.GroupBy.count""><code>count</code></a> does not:</p>

<pre><code>In [46]:
df = pd.DataFrame({'a':[0,0,1,2,2,2], 'b':[1,2,3,4,np.NaN,4], 'c':np.random.randn(6)})
df

Out[46]:
   a   b         c
0  0   1  1.067627
1  0   2  0.554691
2  1   3  0.458084
3  2   4  0.426635
4  2 NaN -2.238091
5  2   4  1.256943

In [48]:
print(df.groupby(['a'])['b'].count())
print(df.groupby(['a'])['b'].size())

a
0    2
1    1
2    2
Name: b, dtype: int64

a
0    2
1    1
2    3
dtype: int64 
</code></pre>
";;0;;2015-10-26T13:13:57.047;;33346694;2015-10-26T13:13:57.047;;;;;704848.0;33346591.0;2;15;;;
38490;38490;;;"<p>Thought i should add here, that if you want to access rows or columns to loop through them, you do this:</p>

<pre><code>import pandas as pd

# open the file
xlsx = pd.ExcelFile(PATH\FileName.xlsx)

# get the first sheet as an object
sheet1 = xlsx.parse(0)

# get the first column as a list you can loop through
# where the is 0 in the code below change to the row or column number you want    
column = sheet1.icol(0).real

# get the first row as a list you can loop through
row = sheet1.irow(0).real
</code></pre>
";;1;;2015-10-27T17:35:51.733;;33374834;2015-10-27T17:35:51.733;;;;;3571614.0;17063458.0;2;10;;;
38495;38495;;;"<p>If you want to set options temporarily for display one large df, you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html"">option_context</a>:</p>

<pre><code>with pd.option_context('display.max_rows', 999, 'display.max_columns', 5):
    print df
</code></pre>

<p>Option values are restored automatically when you exit the <code>with</code> block. </p>
";;1;;2015-10-27T18:04:34.610;;33375383;2015-10-27T18:04:34.610;;;;;2901002.0;11707586.0;2;26;;;
38527;38527;;;"<p>To get a list of all the duplicated entries with Pandas version 0.17, you can simply set 'keep = False' in the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"">duplicated</a> function.</p>

<pre><code>frame[frame.duplicated(['key1','key2'],keep=False)]

    key1  key2  data
0     1     2     5
1     2     2     6
3     1     2     6
4     2     2     1
6     2     2     2
7     2     2     8
</code></pre>
";;0;;2015-10-28T00:58:58.280;;33381151;2015-10-28T01:09:33.897;2015-10-28T01:09:33.897;;3915498.0;;3915498.0;26244309.0;2;7;;;
38528;38528;;;"<p>With Pandas version 0.17, you can set 'keep = False' in the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"" rel=""noreferrer"">duplicated</a> function to get all the duplicate items.</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame(['a','b','c','d','a','b'])

In [3]: df
Out[3]: 
       0
    0  a
    1  b
    2  c
    3  d
    4  a
    5  b

In [4]: df[df.duplicated(keep=False)]
Out[4]: 
       0
    0  a
    1  b
    4  a
    5  b
</code></pre>
";;0;;2015-10-28T01:10:16.800;;33381246;2015-10-28T01:10:16.800;;;;;3915498.0;14657241.0;2;11;;;
38541;38541;;;"<p>The 'duplicated' method works for dataframes and for series. Just select on those rows which aren't marked as having a duplicate index:</p>

<pre><code>df[~df.index.duplicated()]
</code></pre>
";;2;;2015-10-28T09:31:17.317;;33387356;2015-10-28T09:31:17.317;;;;;5497487.0;22918212.0;2;12;;;
38594;38594;;;"<p>If you use accepted answer, you'll lose your column names, as shown in the accepted answer <em>example</em>, and described in the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow noreferrer""><em>documentation</em></a> (emphasis added):</p>

<blockquote>
  <p>The resulting axis will be labeled 0, ..., n - 1. This is useful if you are concatenating objects where the concatenation axis does <strong>not</strong> have meaningful indexing information.</p>
</blockquote>

<p>It looks like your column names (<code>'Name column'</code>) are meaningful.</p>

<p>You can use <code>pandas.concat</code>, but <strong>don't</strong> <code>ignore_index</code> (its default value is false; so you can omit it altogether):</p>

<pre><code>import pandas

original = pandas.DataFrame({
    'Age':[10, 12, 13], 
    'Gender':['M','F','F']})

data = ['Nate A', 'Jessie A', 'Daniel H', 'John D']
additional = pandas.DataFrame({'Name': data})

new = pandas.concat([original, additional], axis=1) 
# Identical:
# new = pandas.concat([original, additional], ignore_index=False, axis=1) 

print(new.head())

#          Age        Gender        Name
#0          10             M      Nate A
#1          12             F    Jessie A
#2          13             F    Daniel H
#3         NaN           NaN      John D
</code></pre>

<p>Notice how John D does not have an Age or a Gender.</p>
";;0;;2015-10-29T01:00:25.053;;33404243;2017-08-14T15:26:42.830;2017-08-14T15:26:42.830;;1175496.0;;1175496.0;27126511.0;2;14;;;
38933;38933;;;"<p>As of v0.14 you can use holiday calendars.</p>

<pre>
from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import CustomBusinessDay

us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())
print pd.DatetimeIndex(start='2010-01-01',end='2010-01-15', freq=us_bd)
</pre>

<p>returns:</p>

<pre>
DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',
               '2010-01-08', '2010-01-11', '2010-01-12', '2010-01-13',
               '2010-01-14', '2010-01-15'],
              dtype='datetime64[ns]', freq='C')
</pre>
";;0;;2015-11-02T15:08:18.013;;33480745;2015-11-02T15:08:18.013;;;;;805030.0;13019719.0;2;10;;;
39226;39226;;;"<p>If you need an arbitrary sequence instead of sorted sequence, you could do:</p>

<pre><code>sequence = ['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]
your_dataframe = your_dataframe.reindex(columns=sequence)
</code></pre>

<p>I tested this in 2.7.10 and it worked for me.</p>
";;0;;2015-11-05T21:48:39.667;;33555435;2015-11-05T22:08:35.103;2015-11-05T22:08:35.103;;880772.0;;5451875.0;11067027.0;2;8;;;
39276;39276;;;"<p>Although there are already some answers I found a nice comparison in which they tried several ways to serialize Pandas DataFrames: <a href=""http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/"" rel=""noreferrer"">Efficiently Store Pandas DataFrames</a> [Edit: page has been deleted, but still available on <a href=""https://web.archive.org/web/20151120080623/http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/"" rel=""noreferrer"">web.archive.org</a>]. </p>

<p>They compare:</p>

<ul>
<li>pickle: original ASCII data format</li>
<li>cPickle, a C library</li>
<li>pickle-p2: uses the newer binary format</li>
<li>json: standardlib json library</li>
<li>json-no-index: like json, but without index</li>
<li>msgpack: binary JSON alternative</li>
<li>CSV</li>
<li>hdfstore: HDF5 storage format</li>
</ul>

<p>In their experiment they serialize a DataFrame of 1,000,000 rows with the two columns tested separately: one with text data, the other with numbers. Their disclaimer says:</p>

<blockquote>
  <p>You should not trust that what follows generalizes to your data. You should look at your own data and run benchmarks yourself</p>
</blockquote>

<p>The source code for the test which they refer to is available <a href=""https://gist.github.com/mrocklin/4f6d06a2ccc03731dd5f"" rel=""noreferrer"">online</a>. Since this code did not work directly I made some minor changes, which you can get here: <a href=""https://gist.github.com/agoldhoorn/ee3bec427dec5bfabb2c"" rel=""noreferrer"">serialize.py</a>
 I got the following results:</p>

<p><a href=""https://i.stack.imgur.com/T9JEL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T9JEL.png"" alt=""time comparison results""></a></p>

<p>They also mention that with the conversion of text data to <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.2/generated/pandas.core.categorical.Categorical.html"" rel=""noreferrer"">categorical</a> data the the serialization is much faster. In their test about 10 times as fast (also see the test code).</p>

<p><strong>Edit</strong>: The higher times for pickle than csv can be explained by the data format used. By default <a href=""https://docs.python.org/2/library/pickle.html#data-stream-format"" rel=""noreferrer""><code>pickle</code></a> uses a printable ASCII representation, which generates larger data sets. As can be seen from the graph however, pickle using the newer binary data format (version 2, <code>pickle-p2</code>) has much lower load times.</p>

<p>Some other references:</p>

<ul>
<li>In the question <a href=""https://softwarerecs.stackexchange.com/questions/7463/fastest-python-library-to-read-a-csv-file"">Fastest Python library to read a CSV file</a> there is a very detailed <a href=""https://softwarerecs.stackexchange.com/a/7510/18147"">answer</a> which compares different libraries to read csv files with a benchmark. The result is that for reading csv files <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html"" rel=""noreferrer""><code>numpy.fromfile</code></a> is the fastest.</li>
<li>Another <a href=""https://gist.github.com/justinfx/3174062"" rel=""noreferrer"">serialization test</a>
shows <a href=""https://pypi.python.org/pypi/msgpack-python"" rel=""noreferrer"">msgpack-python</a>, <a href=""https://pypi.python.org/pypi/ujson"" rel=""noreferrer"">ujson</a>, and cPickle to be the quickest in serializing.</li>
</ul>
";;4;;2015-11-06T15:24:10.203;;33570065;2016-04-05T13:22:18.580;2017-04-13T12:50:02.277;;-1.0;;1771479.0;17098654.0;2;22;;;
39333;39333;;;"<p>Now you can do <code>df['column'].dt.date</code></p>

<p>Note that for datetime objects, if you don't see the hour when they're all 00:00:00, that's not pandas. That's iPython notebook trying to make things look pretty.  </p>
";;0;;2015-11-07T00:22:59.540;;33577649;2015-11-07T00:22:59.540;;;;;3681451.0;16852911.0;2;11;;;
39365;39365;;;"<pre><code>sns.set_style(""whitegrid"", {'axes.grid' : False})
</code></pre>

<p>Note that the style can be whichever valid one that you choose.</p>

<p>For a nice article on this, please refer to <a href=""http://gree2.github.io/python/2015/05/05/python-seaborn-tutorial-controlling-figure-aesthetics/"" rel=""noreferrer"">this site</a>.</p>
";;1;;2015-11-08T03:04:47.447;;33590284;2015-11-08T04:25:57.353;2015-11-08T04:25:57.353;;1402846.0;;5379775.0;26868304.0;2;11;;;
39769;39769;;;"<pre><code>df['MyColumnName'] = df['MyColumnName'].astype('float64') 
</code></pre>
";;1;;2015-11-13T06:28:27.473;;33687073;2016-07-09T06:13:02.790;2016-07-09T06:13:02.790;;3222797.0;;1209842.0;16729483.0;2;9;;;
40101;40101;;;"<p>On much larger datasets, I found that <code>.apply()</code> is few orders slower than <code>pd.DataFrame(df['b'].values.tolist())</code></p>

<p>This performance issue was closed in GitHub, although I do not agree with this decision:</p>

<p><a href=""https://github.com/pandas-dev/pandas/issues/11615"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/11615</a></p>
";;2;;2015-11-17T17:58:08.523;;33763855;2017-03-14T18:07:25.657;2017-03-14T18:07:25.657;;2230844.0;;2230844.0;29550414.0;2;10;;;
40121;40121;;;"<p>New in version 0.16.1:</p>

<pre><code>sample_dataframe = your_dataframe.sample(n=how_many_rows_you_want)
</code></pre>

<p>doc here: <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sample.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sample.html</a></p>
";;2;;2015-11-17T22:53:28.787;;33768634;2016-04-26T15:52:39.280;2016-04-26T15:52:39.280;;1504411.0;;1504411.0;12190874.0;2;12;;;
40148;40148;;;"<p>try using apply function. </p>

<pre><code>df['quantity'] = df['quantity'].apply(lambda x: x*-1)
</code></pre>
";;1;;2015-11-18T10:32:59.687;;33777664;2015-11-18T10:32:59.687;;;;;2933686.0;33768122.0;2;15;;;
40155;40155;;;"<p>Here's the answer after a bit of research:</p>

<pre><code>df.loc[:,'quantity'] *= -1 #seems to prevent SettingWithCopyWarning 
</code></pre>
";;1;;2015-11-18T12:50:22.863;;33780558;2015-11-18T12:50:22.863;;;;;5133552.0;33768122.0;2;23;;;
40160;40160;;;"<p>I had the same question as you did! I found an easy way of getting the inverse of quantile using scipy.</p>

<pre><code>#libs required
from scipy import stats
import pandas as pd
import numpy as np

#generate ramdom data with same seed (to be reproducible)
np.random.seed(seed=1)
df = pd.DataFrame(np.random.uniform(0,1,(10)), columns=['a'])

#quantile function
x = df.quantile(0.5)[0]

#inverse of quantile
stats.percentileofscore(df['a'],x)
</code></pre>
";;1;;2015-11-18T14:09:29.043;;33782239;2015-11-18T14:09:29.043;;;;;2072615.0;26489134.0;2;15;;;
40161;40161;;;"<p>Jean PA's solution is the simplest, most correct one for this question. Writing this as an answer since I don't have the rep to comment.</p>

<p>For constructing a histogram straight from pandas, some of the args are passed on to the matplotlib.hist method anyway, so:</p>

<pre><code>results.val1.hist(bins = 120, log = True)
</code></pre>

<p>Would produce what you need.</p>
";;0;;2015-11-18T14:17:31.690;;33782409;2015-11-18T14:17:31.690;;;;;2583933.0;21033720.0;2;6;;;
40184;40184;;;"<p>As already hinted at, isin requires columns and indices to be the same for a match. If match should only be on row contents, one way to get the mask for filtering the rows present is to convert the rows to a (Multi)Index:</p>

<pre><code>In [77]: df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]})
In [78]: df2 = pandas.DataFrame(data = {'col1' : [1, 3, 4], 'col2' : [10, 12, 13]})
In [79]: df1.loc[~df1.set_index(list(df1.columns)).index.isin(df2.set_index(list(df2.columns)).index)]
Out[79]:
   col1  col2
1     2    11
4     5    14
</code></pre>

<p>If index should be taken into account, set_index has keyword argument append to append columns to existing index. If columns do not line up, list(df.columns) can be replaced with column specifications to align the data.</p>

<pre><code>pandas.MultiIndex.from_tuples(list(df&lt;N&gt;.to_records(index = False)))
</code></pre>

<p>could alternatively be used to create the indices, though I doubt this is more efficient.</p>
";;0;;2015-11-18T17:37:15.170;;33786696;2016-09-08T13:57:52.580;2016-09-08T13:57:52.580;;4870883.0;;4870883.0;28901683.0;2;6;;;
40233;40233;;;"<p>In pandas <code>0.17.0</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html""><code>convert_objects</code></a> raise a warning:</p>

<blockquote>
  <p>FutureWarning: convert_objects is deprecated.  Use the data-type
  specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.</p>
</blockquote>

<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.to_numeric.html""><code>pd.to_numeric</code></a> method and apply it for the dataframe with arg <code>coerce</code>.</p>

<pre><code>df1 = df.apply(pd.to_numeric, args=('coerce',))
</code></pre>

<p>or may be in more appropriate way:</p>

<pre><code>df1 = df.apply(pd.to_numeric, errors='coerce')
</code></pre>

<p><strong>EDIT</strong></p>

<p>That method only valid for pandas version >= <code>0.17.0</code>,  from <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#other-enhancements"">docs what's new in pandas 0.17.0</a>:</p>

<blockquote>
  <p>pd.to_numeric is a new function to coerce strings to numbers (possibly with coercion) <a href=""https://github.com/pydata/pandas/issues/11133"">(GH11133)</a></p>
</blockquote>
";;3;;2015-11-19T05:25:35.140;;33795876;2016-07-18T13:38:02.743;2016-07-18T13:38:02.743;;1534017.0;;4542359.0;18434208.0;2;31;;;
40238;40238;;;"<p>Set column max width using:</p>

<pre><code>pd.set_option('max_colwidth', 800)
</code></pre>

<p>This particular statement sets max width to 800px, per column.</p>
";;0;;2015-11-19T08:43:35.197;;33798922;2015-11-19T08:43:35.197;;;;;3713939.0;11707586.0;2;10;;;
40284;40284;;;"<p>There is a python parquet reader that works relatively well: <a href=""https://github.com/jcrobak/parquet-python"" rel=""nofollow noreferrer"">https://github.com/jcrobak/parquet-python</a></p>

<p>It will create python objects and then you will have to move them to a Pandas DataFrame so the process will be slower than <code>pd.read_csv</code> for example.</p>

<p>Update: since the time I answered this there has been a lot of work on this look at Apache Arrow for a better read and write of parquet. Also: <a href=""http://wesmckinney.com/blog/python-parquet-multithreading/"" rel=""nofollow noreferrer"">http://wesmckinney.com/blog/python-parquet-multithreading/</a></p>
";;4;;2015-11-19T20:46:29.577;;33814054;2017-05-11T22:04:53.763;2017-05-11T22:04:53.763;;427978.0;;427978.0;33813815.0;2;11;;;
40385;40385;;;"<p>As of pandas 0.17.0, <code>DataFrame.sort()</code> is deprecated, and set to be removed in a future version of pandas. The way to sort a dataframe by its values is now is <code>DataFrame.sort_values</code></p>

<p>As such, the answer to your question would now be</p>

<pre><code>df.sort_values(['b', 'c'], ascending=[True, False], inplace=True)
</code></pre>
";;1;;2015-11-20T23:11:35.423;;33837592;2015-11-20T23:11:35.423;;;;;1748679.0;17141558.0;2;16;;;
40477;40477;;;"<p>Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.</p>

<p>This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.</p>

<p>Instead , we can do this neatly in a single statement by using list comprehension like below:</p>

<pre><code>df.columns = [col.strip('$') for col in df.columns]
</code></pre>

<p>(<code>strip</code> method in Python strips the given character from beginning and end of the string.)</p>
";;1;;2015-11-23T13:56:10.337;;33872824;2017-07-05T13:19:35.273;2017-07-05T13:19:35.273;;2664350.0;;3533960.0;11346283.0;2;9;;;
40480;40480;;;"<p>is that ok ?</p>

<p><a href=""https://i.stack.imgur.com/adDLV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/adDLV.png"" alt=""enter image description here""></a></p>

<pre><code>import pandas as pd
from math import pi
from datetime import datetime as dt
from bokeh.io import output_file
from bokeh.charts import show
from bokeh.models import DatetimeTickFormatter
from bokeh.plotting import figure

df = pd.DataFrame(data=[1,2,3],
                  index=[dt(2015, 1, 1), dt(2015, 1, 2), dt(2015, 1, 3)],
                  columns=['foo'])
p = figure(plot_width=400, plot_height=400)
p.line(df.index, df['foo'])
p.xaxis.formatter=DatetimeTickFormatter(
        hours=[""%d %B %Y""],
        days=[""%d %B %Y""],
        months=[""%d %B %Y""],
        years=[""%d %B %Y""],
    )
p.xaxis.major_label_orientation = pi/4
output_file('myplot.html')
show(p)
</code></pre>
";;3;;2015-11-23T14:17:31.070;;33873209;2017-04-03T06:10:24.083;2017-04-03T06:10:24.083;;337065.0;;3581357.0;33869292.0;2;16;;;
40617;40617;;;"<p>Although the @silvado answer is good if you change <code>df.map(str)</code> to <code>df.astype(str)</code> it will be faster:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})

In [131]: %timeit df[""Year""].map(str)
10000 loops, best of 3: 132 us per loop

In [132]: %timeit df[""Year""].astype(str)
10000 loops, best of 3: 82.2 us per loop
</code></pre>
";;0;;2015-11-25T10:25:15.657;;33913961;2015-11-25T10:25:15.657;;;;;4542359.0;19377969.0;2;9;;;
40752;40752;;;"<p>You could use parameters <code>keep_default_na</code> and <code>na_values</code> to set all NA values by hand <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">docs</a>:</p>

<pre><code>import pandas as pd
from io import StringIO

data = """"""
PDB CHAIN SP_PRIMARY RES_BEG RES_END PDB_BEG PDB_END SP_BEG SP_END
5d8b N P60490 1 146 1 146 1 146
5d8b NA P80377 _ 126 1 126 1 126
5d8b O P60491 1 118 1 118 1 118
""""""

df = pd.read_csv(StringIO(data), sep=' ', keep_default_na=False, na_values=['_'])

In [130]: df
Out[130]:
    PDB CHAIN SP_PRIMARY  RES_BEG  RES_END  PDB_BEG  PDB_END  SP_BEG  SP_END
0  5d8b     N     P60490        1      146        1      146       1     146
1  5d8b    NA     P80377      NaN      126        1      126       1     126
2  5d8b     O     P60491        1      118        1      118       1     118

In [144]: df.CHAIN.apply(type)
Out[144]:
0    &lt;class 'str'&gt;
1    &lt;class 'str'&gt;
2    &lt;class 'str'&gt;
Name: CHAIN, dtype: object
</code></pre>

<p><strong>EDIT</strong></p>

<p>All default <code>NA</code> values from <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#na-values"" rel=""noreferrer"">na-values</a>:</p>

<blockquote>
  <p>The default NaN recognized values are <code>['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A','N/A', 'NA', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan']</code>. Although a 0-length string '' is not included in the default NaN values list, it is still treated as a missing value.</p>
</blockquote>
";;1;;2015-11-27T07:30:05.330;;33952294;2015-11-27T09:26:59.623;2015-11-27T09:26:59.623;;4542359.0;;4542359.0;33952142.0;2;13;;;
40761;40761;;;"<p>Did you try assigning it back to the column?</p>

<pre><code>df['column'] = df['column'].astype('str') 
</code></pre>

<p>Referring to this <a href=""https://stackoverflow.com/q/21018654/3231320"">question</a>, the pandas dataframe stores the pointers to the strings and hence it is of type 
'object'. As per the <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html"" rel=""nofollow noreferrer"">docs</a> ,You could try:   </p>

<pre><code>df['column_new'] = df['column'].str.split(',') 
</code></pre>
";;5;;2015-11-27T12:51:31.533;;33957850;2015-11-27T15:37:57.640;2017-05-23T12:18:17.367;;-1.0;;3231320.0;33957720.0;2;8;;;
40787;40787;;;"<p>As of version 17.0, you can format with the <code>dt</code> accessor:</p>

<pre><code>dates.dt.strftime('%Y-%m-%d')
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/whatsnew.html#whatsnew-0170-strftime"">Reference</a></p>
";;0;;2015-11-28T03:34:18.003;;33967359;2015-11-28T03:34:18.003;;;;;690430.0;30132282.0;2;9;;;
40849;40849;;;"<p>Real simple just use </p>

<pre><code>df.columns = ['Name1', 'Name2', 'Name3'...]
</code></pre>

<p>and it will assign the column names by the order you put them</p>
";;0;;2015-11-29T19:22:47.913;;33986975;2015-11-29T19:22:47.913;;;;;5548422.0;11346283.0;2;9;;;
40876;40876;;;"<p>You could use <code>regex</code> parameter of <code>replace</code> method to achieve that:</p>

<pre><code>misc['product_desc'] = misc['product_desc'].replace(to_replace='\n', value='', regex=True)
</code></pre>
";;2;;2015-11-30T07:03:08.363;;33993078;2015-11-30T07:03:08.363;;;;;4542359.0;14345739.0;2;6;;;
40904;40904;;;"<p>Lets use the titanic dataset from the seaborn package as an example</p>

<pre><code># Load dataset (pip install seaborn)
&gt;&gt; import seaborn.apionly as sns
&gt;&gt; titanic = sns.load_dataset('titanic')
</code></pre>

<blockquote>
  <h1>using the column names</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.loc[:,['sex','age','fare']]
</code></pre>

<blockquote>
  <h1>using the column indices</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.iloc[:,[2,3,6]]
</code></pre>

<blockquote>
  <h1>using ix</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.ix[:,[sex,age,fare]]
</code></pre>

<p>or</p>

<pre><code>&gt;&gt; titanic.ix[:,[2,3,6]]
</code></pre>

<blockquote>
  <h1>using the reindex method</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.reindex(columns=['sex','age','fare'])
</code></pre>
";;0;;2015-11-30T11:32:15.847;;33997632;2016-10-11T08:07:42.300;2016-10-11T08:07:42.300;;786326.0;;786326.0;10665889.0;2;39;;;
40964;40964;;;"<p>The <code>FailedPreconditionError</code> arises because the program is attempting to read a variable (named <code>""Variable_1""</code>) before it has been initialized. In TensorFlow, all variables must be explicitly initialized, by running their ""initializer"" operations. For convenience, you can run all of the variable initializers in the current session by executing the following statement before your training loop:</p>

<pre><code>tf.initialize_all_variables().run()
</code></pre>

<p>Note that this answer assumes that, as in the question, you are using <code>tf.InteractiveSession</code>, which allows you to run operations without specifying a session. For non-interactive uses, it is more common to use <code>tf.Session</code>, and initialize as follows:</p>

<pre><code>init_op = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init_op)
</code></pre>
";;0;;2015-12-01T05:15:19.060;;34013098;2015-12-01T05:15:19.060;;;;;3574081.0;34001922.0;2;33;;;
41275;41275;;;"<p>You could use dataframe method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.notnull.html"" rel=""noreferrer"">notnull</a> or inverse of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html"" rel=""noreferrer"">isnull</a>, or <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.isnan.html"" rel=""noreferrer"">numpy.isnan</a>:</p>

<pre><code>In [332]: df[df.EPS.notnull()]
Out[332]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN


In [334]: df[~df.EPS.isnull()]
Out[334]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN


In [347]: df[~np.isnan(df.EPS)]
Out[347]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN
</code></pre>
";;1;;2015-12-04T07:01:56.243;;34082664;2015-12-04T07:01:56.243;;;;;4542359.0;13413590.0;2;15;;;
41309;41309;;;"<p>You can use <code>names</code> directly in the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html""><code>read_csv</code></a> </p>

<blockquote>
  <p>names : array-like, default None List of column names to use. If file
  contains no header row, then you should explicitly pass header=None</p>
</blockquote>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t', 
                  names = [""Sequence"", ""Start"", ""End"", ""Coverage""])
</code></pre>

<p>The line below will not work as you expect. <code>Cov</code> is already a dataframe, assuming it really has 4 columns when it's being read from the file.</p>

<pre><code>Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])
</code></pre>
";;2;;2015-12-04T15:43:00.480;;34092032;2016-06-08T10:08:52.697;2016-06-08T10:08:52.697;;4542359.0;;4805990.0;34091877.0;2;46;;;
41316;41316;;;"<p>Alternatively you could read you csv with <code>header=None</code> and then add it with <code>df.columns</code>:</p>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t', header=None)
Cov.columns = [""Sequence"", ""Start"", ""End"", ""Coverage""]
</code></pre>
";;0;;2015-12-04T17:27:21.973;;34094058;2016-03-25T21:55:48.587;2016-03-25T21:55:48.587;;4542359.0;;4542359.0;34091877.0;2;25;;;
41348;41348;;;"<p>It's interesting but <code>df.columns.values.tolist()</code> is almost 3 times faster then <code>df.columns.tolist()</code> but I thought that they are the same:</p>

<pre><code>In [97]: %timeit df.columns.values.tolist()
100000 loops, best of 3: 2.97 s per loop

In [98]: %timeit df.columns.tolist()
10000 loops, best of 3: 9.67 s per loop
</code></pre>
";;0;;2015-12-04T21:41:53.610;;34097939;2015-12-04T21:41:53.610;;;;;4542359.0;19482970.0;2;7;;;
41402;41402;;;"<p>I think the easiest way to plot this data with all the lines on the same graph is to pivot it such that each ""template"" value is a column:</p>

<pre><code>pivoted = pandas.pivot_table(data, values='score', columns='template', index='date')
# Now there will be an index column for date and value columns for 0,1,2,3,4
pivoted.plot()
</code></pre>
";;1;;2015-12-05T20:34:54.947;;34110546;2015-12-05T20:34:54.947;;;;;101247.0;24080275.0;2;11;;;
41487;41487;;;"<p>This works in pandas 0.16.2</p>

<pre><code>In[23]: print df
        name          day   no
0      Jack       Monday    10
1      Jack      Tuesday    20
2      Jack      Tuesday    10
3      Jack    Wednesday    50
4      Jill       Monday    40
5      Jill    Wednesday   110
In[24]: df['no_cumulative'] = df.groupby(['name'])['no'].apply(lambda x: x.cumsum())
In[25]: print df
        name          day   no  no_cumulative
0      Jack       Monday    10             10
1      Jack      Tuesday    20             30
2      Jack      Tuesday    10             40
3      Jack    Wednesday    50             90
4      Jill       Monday    40             40
5      Jill    Wednesday   110            150
</code></pre>
";;1;;2015-12-07T10:03:00.320;;34130874;2015-12-07T10:03:00.320;;;;;5192992.0;22650833.0;2;15;;;
41728;41728;;;"<p>What about?</p>

<pre><code>df.columns.values[2] = ""new_name""
</code></pre>
";;1;;2015-12-10T02:41:10.453;;34192820;2015-12-10T02:41:10.453;;;;;4451192.0;20868394.0;2;18;;;
42040;42040;;;"<pre><code>pd.DataFrame(df.to_records()) # multiindex become columns and new index is integers only
</code></pre>
";;1;;2015-12-14T08:00:21.093;;34262133;2015-12-14T08:00:21.093;;;;;4438747.0;14507794.0;2;35;;;
42075;42075;;;"<p>This is much easier in pandas now with <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.drop_duplicates.html"">drop_duplicates</a> and the keep parameter.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"":[""foo"", ""foo"", ""foo"", ""bar""], ""B"":[0,1,1,1], ""C"":[""A"",""A"",""B"",""A""]})
df.drop_duplicates(subset=['A', 'C'], keep=False)
</code></pre>
";;2;;2015-12-14T16:38:02.883;;34272155;2015-12-14T16:38:02.883;;;;;4096199.0;23667369.0;2;39;;;
42098;42098;;;"<p>Since version <code>0.15.0</code> this can now be easily done using <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dt-accessor"" rel=""noreferrer""><code>.dt</code></a> to access just the date component:</p>

<pre><code>df['just_date'] = df['dates'].dt.date
</code></pre>
";;6;;2015-12-14T22:07:34.700;;34277514;2015-12-14T22:07:34.700;;;;;704848.0;16176996.0;2;50;;;
42125;42125;;;"<p>I would use HDF5/pytables as follows:</p>

<ol>
<li>Keep the data as a python list ""as long as possible"".</li>
<li>Append your results to that list.</li>
<li>When it gets ""big"":

<ul>
<li>push to HDF5 Store using pandas io (and an appendable table).</li>
<li>clear the list.</li>
</ul></li>
<li>Repeat.</li>
</ol>

<p>In fact, the function I define uses a list for each ""key"" so that you can store multiple DataFrames to the HDF5 Store in the same process.</p>

<hr>

<p>We define a function which you call with each row <code>d</code>:</p>

<pre><code>CACHE = {}
STORE = 'store.h5'   # Note: another option is to keep the actual file open

def process_row(d, key, max_len=5000, _cache=CACHE):
    """"""
    Append row d to the store 'key'.

    When the number of items in the key's cache reaches max_len,
    append the list of rows to the HDF5 store and clear the list.

    """"""
    # keep the rows for each key separate.
    lst = _cache.setdefault(key, [])
    if len(lst) &gt;= max_len:
        store_and_clear(lst, key)
    lst.append(d)

def store_and_clear(lst, key):
    """"""
    Convert key's cache list to a DataFrame and append that to HDF5.
    """"""
    df = pd.DataFrame(lst)
    with pd.HDFStore(STORE) as store:
        store.append(key, df)
    lst.clear()
</code></pre>

<p><em>Note: we use the with statement to automatically close the store after each write. It <strong>may</strong> be faster to keep it open, but if so <a href=""http://www.pytables.org/usersguide/tutorials.html#creating-a-new-table"">it's recommended that you flush regularly (closing flushes)</a>. Also note it may be more readable to have used a <a href=""https://docs.python.org/2/library/collections.html#collections.deque"">collections deque</a> rather than a list, but the performance of a list will be slightly better here.</em></p>

<p>To use this you call as:</p>

<pre><code>process_row({'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0},
            key=""df"")
</code></pre>

<p><em>Note: ""df"" is the stored <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hierarchical-keys"">key</a> used in the pytables store.</em></p>

<p>Once the job has finished ensure you <code>store_and_clear</code> the remaining cache:</p>

<pre><code>for k, lst in CACHE.items():  # you can instead use .iteritems() in python 2
    store_and_clear(lst, k)
</code></pre>

<p>Now your complete DataFrame is available via:</p>

<pre><code>with pd.HDFStore(STORE) as store:
    df = store[""df""]                    # other keys will be store[key]
</code></pre>

<h3>Some comments:</h3>

<ul>
<li>5000 can be adjusted, try with some smaller/larger numbers to suit your needs.</li>
<li><a href=""https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt"">List append is O(1)</a>, DataFrame append is O(<code>len(df)</code>).</li>
<li>Until you're doing stats or data-munging you don't need pandas, use what's fastest.</li>
<li>This code works with multiple key's (data points) coming in.</li>
<li>This is very little code, and we're staying in vanilla python list and then pandas dataframe...</li>
</ul>

<hr>

<p>Additionally, to get the up to date reads you could define a get method which stores and clears <em>before</em> reading. In this way you would get the most up to date data:</p>

<pre><code>def get_latest(key, _cache=CACHE):
    store_and_clear(_cache[key], key)
    with pd.HDFStore(STORE) as store:
        return store[key]
</code></pre>

<p>Now when you access with:</p>

<pre><code>df = get_latest(""df"")
</code></pre>

<p>you'll get the latest ""df"" available.</p>

<hr>

<p>Another option is <em>slightly</em> more involved: define a custom table in vanilla pytables, see the <a href=""http://www.pytables.org/usersguide/tutorials.html#creating-a-new-table"">tutorial</a>.</p>

<p><em>Note: You need to know the field-names to create the <a href=""http://www.pytables.org/usersguide/tutorials.html#declaring-a-column-descriptor"">column descriptor</a>.</em></p>
";;5;;2015-12-15T06:22:12.697;;34282362;2015-12-15T17:12:50.963;2015-12-15T17:12:50.963;;1240268.0;;1240268.0;16740887.0;2;7;;;
42178;42178;;;"<p>I would suggest using the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.Index.duplicated.html"" rel=""noreferrer"">duplicated</a> method on the Pandas Index itself:</p>

<pre><code>df3 = df3[~df3.index.duplicated(keep='first')]
</code></pre>

<p>While all the other methods work, the <a href=""https://stackoverflow.com/a/14900065/3622349"">currently accepted answer</a> is by far the least performant for the provided example. Furthermore, while the groupby method is only slightly less performant, I find the duplicated method to be more readable.</p>

<p>Using the sample data provided:</p>

<pre><code>&gt;&gt;&gt; %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')
1000 loops, best of 3: 1.54 ms per loop

&gt;&gt;&gt; %timeit df3.groupby(df3.index).first()
1000 loops, best of 3: 580 s per loop

&gt;&gt;&gt; %timeit df3[~df3.index.duplicated(keep='first')]
1000 loops, best of 3: 307 s per loop
</code></pre>

<p>Note that you can keep the last element by changing the keep argument.</p>

<p>It should also be noted that this method works with <code>MultiIndex</code> as well (using df1 as specified in Paul's example):</p>

<pre><code>&gt;&gt;&gt; %timeit df1.groupby(level=df1.index.names).last()
1000 loops, best of 3: 771 s per loop

&gt;&gt;&gt; %timeit df1[~df1.index.duplicated(keep='last')]
1000 loops, best of 3: 365 s per loop
</code></pre>
";;4;;2015-12-15T19:25:20.687;;34297689;2017-05-04T20:52:58.917;2017-05-23T12:03:05.203;;-1.0;;3622349.0;13035764.0;2;117;;;
42237;42237;;;"<p>Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.</p>

<pre><code>t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})
B = []
C = []
A = time.time()
for i,r in t.iterrows():
    C.append((r['a'], r['b']))
B.append(time.time()-A)

C = []
A = time.time()
for ir in t.itertuples():
    C.append((ir[1], ir[2]))    
B.append(time.time()-A)

C = []
A = time.time()
for r in zip(t['a'], t['b']):
    C.append((r[0], r[1]))
B.append(time.time()-A)

print B
</code></pre>

<p>Result:</p>

<pre><code>[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]
</code></pre>

<p>This is probably not the best way to measure the time consumption but it's quick for me.</p>

<p>Here are some pros and cons IMHO:</p>

<ul>
<li>.iterrows(): return index and row items in separate variables, but significantly slower</li>
<li>.itertuples(): faster than .iterrows(), but return index together with row items, ir[0] is the index</li>
<li>zip: quickest, but no access to index of the row</li>
</ul>
";;3;;2015-12-16T11:39:44.767;;34311080;2015-12-16T11:39:44.767;;;;;2142098.0;7837722.0;2;43;;;
42324;42324;;;"<p>Suppose you have two dataframes, df_1 and df_2 having multiple fields(column_names) and you want to find the only those entries in df_1 that are not in df_2 on the basis of some fields(e.g. fields_x, fields_y), follow the following steps.</p>

<p>Step1.Add a column key1 and key2 to df_1 and df_2 respectively.</p>

<p>Step2.Merge the dataframes as shown below. field_x and field_y are our desired columns.</p>

<p>Step3.Select only those rows from df_1 where key1 is not equal to key2.</p>

<p>Step4.Drop key1 and key2.</p>

<p>This method will solve your problem and works fast even with big data sets. I have tried it for dataframes with more than 1,000,000 rows.</p>

<pre><code>df_1['key1'] = 1
df_2['key2'] = 1
df_1 = pd.merge(df_1, df_2, on=['field_x', 'field_y'], how = 'left')
df_1 = df_1[~(df_1.key2 == df_1.key1)]
df_1 = df_1.drop(['key1','key2'], axis=1)
</code></pre>
";;1;;2015-12-17T11:50:03.533;;34333886;2015-12-17T12:43:12.437;2015-12-17T12:43:12.437;;1432801.0;;5628904.0;28901683.0;2;6;;;
42458;42458;;;"<p>Due to popular demand, <code>tqdm</code> has added support for <code>pandas</code>. Unlike the other answers, this <strong>will not noticeably slow pandas down</strong> -- here's an example for <code>DataFrameGroupBy.progress_apply</code>:</p>

<pre><code>import pandas as pd
import numpy as np
from tqdm import tqdm, tqdm_pandas

df = pd.DataFrame(np.random.randint(0, int(1e8), (10000, 1000)))

# Create and register a new `tqdm` instance with `pandas`
# (can use tqdm_gui, optional kwargs, etc.)
tqdm_pandas(tqdm())

# Now you can use `progress_apply` instead of `apply`
df.groupby(0).progress_apply(lambda x: x**2)
</code></pre>

<p>In case you're interested in how this works (and how to modify it for your own callbacks), see the <a href=""https://github.com/tqdm/tqdm/tree/master/examples"">examples on github</a>, the <a href=""https://pypi.python.org/pypi/tqdm"">full documentation on pypi</a>, or import the module and run <code>help(tqdm)</code>.</p>

<p><strong>EDIT</strong></p>

<hr>

<p>To directly answer the original question, replace:</p>

<pre><code>df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)
</code></pre>

<p>with:</p>

<pre><code>from tqdm import tqdm, tqdm_pandas
tqdm_pandas(tqdm())
df_users.groupby(['userID', 'requestDate']).progress_apply(feature_rollup)
</code></pre>
";;7;;2015-12-18T23:36:34.753;;34365537;2015-12-20T20:36:20.840;2015-12-20T20:36:20.840;;3896283.0;;3896283.0;18603270.0;2;34;;;
42587;42587;;;"<p>have you tried the <code>display</code> command?</p>

<pre><code>from IPython.display import display
display(salaries.head())
display(teams.head())
</code></pre>
";;3;;2015-12-21T18:50:50.390;;34402423;2015-12-21T18:58:18.643;2015-12-21T18:58:18.643;;4601386.0;;4601386.0;34398054.0;2;32;;;
42775;42775;;;"<p>Based on the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"" rel=""noreferrer"">official documentation</a>:</p>

<pre><code>dummies = pd.get_dummies(df['Category']).rename(columns=lambda x: 'Category_' + str(x))
df = pd.concat([df, dummies], axis=1)
df = df.drop(['Category'], inplace=True, axis=1)
</code></pre>

<p>There is also a nice <a href=""http://fastml.com/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/"" rel=""noreferrer"">post</a> in the FastML blog.</p>
";;1;;2015-12-24T21:07:15.503;;34457902;2015-12-24T21:07:15.503;;;;;1093289.0;11587782.0;2;8;;;
42796;42796;;;"<p>As of pandas v15.0, use the parameter, <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.describe.html"" rel=""nofollow noreferrer""><strong><code>DataFrame.describe(include = 'all')</code></strong></a> to get a summary of all the columns when the dataframe has <em>mixed column types</em>. The default behavior is to only provide a summary for the numerical columns.</p>

<p><strong>Example:</strong></p>

<pre><code>In[1]:

df = pd.DataFrame({'$a':['a', 'b', 'c', 'd', 'a'], '$b': np.arange(5)})
df.describe(include = 'all')

Out[1]:

        $a    $b
count   5   5.000000
unique  4   NaN
top     a   NaN
freq    2   NaN
mean    NaN 2.000000
std     NaN 1.581139
min     NaN 0.000000
25%     NaN 1.000000
50%     NaN 2.000000
75%     NaN 3.000000
max     NaN 4.000000
</code></pre>

<p>The numerical columns will have NaNs for summary statistics pertaining to objects (strings) and vice versa. </p>

<p><strong>Summarizing only numerical or object columns</strong></p>

<ol>
<li>To call <code>describe()</code> on just the numerical columns use <code>describe(include = [np.number])</code> </li>
<li><p>To call <code>describe()</code> on just the objects (strings) using <code>describe(include = ['O'])</code>.</p>

<pre><code>In[2]:

df.describe(include = [np.number])

Out[3]:

         $b
count   5.000000
mean    2.000000
std     1.581139
min     0.000000
25%     1.000000
50%     2.000000
75%     3.000000
max     4.000000

In[3]:

df.describe(include = ['O'])

Out[3]:

    $a
count   5
unique  4
top     a
freq    2
</code></pre></li>
</ol>
";;1;;2015-12-25T21:13:56.367;;34466473;2017-08-01T21:55:09.340;2017-08-01T21:55:09.340;;4561314.0;;1435522.0;24524104.0;2;13;;;
43040;43040;;;"<p>You can use the following command to filter only numeric columns</p>

<pre><code>df._get_numeric_data()
</code></pre>

<p>Example</p>

<pre><code>In [32]: data
Out[32]:
   A  B
0  1  s
1  2  s
2  3  s
3  4  s

In [33]: data._get_numeric_data()
Out[33]:
   A
0  1
1  2
2  3
3  4
</code></pre>
";;0;;2015-12-30T13:00:51.570;;34530065;2016-03-17T07:12:53.560;2016-03-17T07:12:53.560;;1645853.0;;1645853.0;25039626.0;2;25;;;
43045;43045;;;"<p>You can transform either column 'a' or 'b' so they are both either float64 or bool. However, an easier solution that preserves the data type of your features is this:</p>

<pre><code>DF3 = DF[(DF['a'] == 0) |  (DF['b'] == 0)]
</code></pre>

<p>A common operation is the use of boolean vectors to filter the data. The operators are: | for or, &amp; for and, and ~ for not. <strong><em>These must be grouped by using parentheses.</em></strong></p>
";;1;;2015-12-30T14:36:59.137;;34531543;2015-12-30T23:35:12.700;2015-12-30T23:35:12.700;;1290147.0;;1290147.0;34531416.0;2;22;;;
43112;43112;;;"<p>As of 17.0 <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.from_csv.html""><code>from_csv</code></a> is discouraged.</p>

<p>Use <code>pd.read_csv(fpath, sep='\t')</code> or <code>pd.read_table(fpath)</code>.</p>
";;0;;2015-12-31T16:13:36.940;;34548894;2016-01-07T15:42:04.623;2016-01-07T15:42:04.623;;690430.0;;690430.0;9652832.0;2;22;;;
43131;43131;;;"<pre><code>list(data_set.itertuples(index=False))
</code></pre>

<p>As of 17.1, the above will return a list of namedtuples.</p>
";;0;;2015-12-31T21:57:12.910;;34551914;2015-12-31T21:57:12.910;;;;;690430.0;9758450.0;2;16;;;
43135;43135;;;"<p>Running Python 3.4 on a mac</p>

<p>New pyvenv</p>

<pre><code>pip install pandas
pip install lxml
pip install html5lib
pip install BeautifulSoup4
</code></pre>

<p>Then ran your example....</p>

<pre><code>import pandas as pd
import html5lib
f_states=   pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') 
</code></pre>

<p>All works...</p>
";;5;;2016-01-01T10:05:21.403;;34555201;2016-01-01T10:05:21.403;;;;;3257992.0;34555135.0;2;16;;;
43199;43199;;;"<p>A nice addition is the ability to <strong>drop columns only if they exist</strong>, this way you can cover more use cases, and it will only drop the existing columns from the labels passed to it: </p>

<p>simply add <strong>errors='ignore'</strong> ,e.g:</p>

<pre><code>df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')
</code></pre>

<ul>
<li>this is new from pandas 0.16.1, docs are <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a></li>
</ul>
";;0;;2016-01-03T12:29:49.343;;34576537;2016-01-03T12:29:49.343;;;;;2923704.0;13411544.0;2;31;;;
43229;43229;;;"<p>As the question was updated to ask for the difference between <code>sort_values</code> (as <code>sort</code> is deprecated) and <code>sort_index</code>, the answer of @mathdan is no longer reflecting the current state with the latest pandas version (>= 0.17.0).</p>

<ul>
<li><code>sort_values</code> is meant to sort <strong>by the values of columns</strong></li>
<li><code>sort_index</code> is meant to sort <strong>by the index labels</strong> (or a specific level of the index, or the column labels when <code>axis=1</code>)</li>
</ul>

<p>Previously, <code>sort</code> (deprecated starting from pandas 0.17.0) and <code>sort_index</code> where indeed almost identical (both methods could sort by both columns and index). But this confusing situation has been solved in 0.17.0.<br>
For an overview of the changes in the sorting API, see <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#changes-to-sorting-api"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#changes-to-sorting-api</a></p>
";;0;;2016-01-04T13:29:37.097;;34592295;2016-12-06T16:01:17.137;2016-12-06T16:01:17.137;;653364.0;;653364.0;19332171.0;2;12;;;
43301;43301;;;"<p>Note that it may be important to use the ""inplace"" command when you want to do the drop in line. </p>

<pre><code>df.drop(df.index[[1,3]], inplace=True)
</code></pre>

<p>Because your original question is not returning anything, this command should be used.
<a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html"">http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html</a></p>
";;0;;2016-01-05T14:28:26.223;;34614046;2016-01-05T14:28:26.223;;;;;3155053.0;14661701.0;2;54;;;
43521;43521;;;"<p>There is a way of doing this and it actually looks similar to R</p>

<pre><code>new = old[['A', 'C', 'D']].copy()
</code></pre>

<p>Here you are just selecting the columns you want from the original data frame and creating a variable for those. If you want to modify the new dataframe at all you'll probably want to use <code>.copy()</code> to avoid a <code>SettingWithCopyWarning</code>.</p>

<p>An alternative method is to use <code>filter</code> which will create a copy by default:</p>

<pre><code>new = old.filter(['A','B','D'], axis=1)
</code></pre>

<p>Finally, depending on the number of columns in your original dataframe, it might be more succinct to express this using a <code>drop</code> (this will also create a copy by default):</p>

<pre><code>new = old.drop('B', axis=1)
</code></pre>
";;0;;2016-01-08T17:51:14.147;;34683105;2016-12-21T23:39:53.930;2016-12-21T23:39:53.930;;839957.0;;3639023.0;34682828.0;2;38;;;
43543;43543;;;"<p>To answer my own question, this functionality has been added to pandas in the meantime. Starting <strong>from pandas 0.15.0</strong>, you can use <code>tz_localize(None)</code> to remove the timezone resulting in local time.<br>
See the whatsnew entry: <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#timezone-handling-improvements"">http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#timezone-handling-improvements</a></p>

<p>So with my example from above:</p>

<pre><code>In [4]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=2, freq='H',
                          tz= ""Europe/Brussels"")

In [5]: t
Out[5]: DatetimeIndex(['2013-05-18 12:00:00+02:00', '2013-05-18 13:00:00+02:00'],
                       dtype='datetime64[ns, Europe/Brussels]', freq='H')
</code></pre>

<p>using <code>tz_localize(None)</code> removes the timezone information resulting in <strong>naive local time</strong>:</p>

<pre><code>In [6]: t.tz_localize(None)
Out[6]: DatetimeIndex(['2013-05-18 12:00:00', '2013-05-18 13:00:00'], 
                      dtype='datetime64[ns]', freq='H')
</code></pre>

<p>Further, you can also use <code>tz_convert(None)</code> to remove the timezone information but converting to UTC, so yielding <strong>naive UTC time</strong>:</p>

<pre><code>In [7]: t.tz_convert(None)
Out[7]: DatetimeIndex(['2013-05-18 10:00:00', '2013-05-18 11:00:00'], 
                      dtype='datetime64[ns]', freq='H')
</code></pre>

<hr>

<p>This is much <strong>more performant</strong> than the <code>datetime.replace</code> solution:</p>

<pre><code>In [31]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10000, freq='H',
                           tz=""Europe/Brussels"")

In [32]: %timeit t.tz_localize(None)
1000 loops, best of 3: 233 s per loop

In [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
10 loops, best of 3: 99.7 ms per loop
</code></pre>
";;1;;2016-01-08T22:57:13.760;;34687479;2016-01-08T22:57:13.760;;;;;653364.0;16628819.0;2;18;;;
43776;43776;;;"<p>Looks like @CarstenKnig <a href=""https://stackoverflow.com/a/16399202/623735"">found the right way</a>:</p>

<pre><code>df.hist(bins=20, weights=np.ones_like(df[df.columns[0]]) * 100. / len(df))
</code></pre>
";;2;;2016-01-13T01:34:43.230;;34756965;2017-04-20T19:42:27.527;2017-05-23T12:26:10.267;;-1.0;;623735.0;17874063.0;2;6;;;
43898;43898;;;"<p>Use of <code>sort</code> can result in warning message. See <a href=""https://github.com/bokeh/bokeh/issues/3014"">github</a> discussion.
So you might wanna use <code>sort_values</code>, docs <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.sort_values.html"">here</a></p>

<p>Then your code can look like this:</p>

<pre><code>df = df.sort_values(by=['c1','c2'], ascending=[False,True])
</code></pre>
";;2;;2016-01-14T12:58:01.823;;34790248;2016-01-14T12:58:01.823;;;;;3441597.0;17618981.0;2;17;;;
43917;43917;;;"<p>just use <code>replace</code>:</p>

<pre><code>In [106]:
df.replace('N/A',np.NaN)

Out[106]:
    x    y
0  10   12
1  50   11
2  18  NaN
3  32   13
4  47   15
5  20  NaN
</code></pre>

<p>What you're trying is called chain indexing: <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</a></p>

<p>You can use <code>loc</code> to ensure you operate on the original dF:</p>

<pre><code>In [108]:
df.loc[df['y'] == 'N/A','y'] = np.nan
df

Out[108]:
    x    y
0  10   12
1  50   11
2  18  NaN
3  32   13
4  47   15
5  20  NaN
</code></pre>
";;5;;2016-01-14T16:02:26.473;;34794112;2016-01-14T16:02:26.473;;;;;704848.0;34794067.0;2;20;;;
44002;44002;;;"<p>To add a row with column-totals:</p>

<pre><code>df.loc['Total']= df.sum()
</code></pre>
";;0;;2016-01-15T13:35:18.103;;34812293;2016-01-15T13:35:18.103;;;;;4862862.0;20804673.0;2;22;;;
44211;44211;;;"<p>Let's first look at the difference between <code>.map(str.strip)</code> and <code>.str.strip()</code> (second and third case).<br>
Therefore, you need to understand what <code>str.strip()</code> does under the hood: it actually does some <code>map(str.strip)</code>, but using a custom <code>map</code> function that will handle missing values.<br>
So given that <code>.str.strip()</code> <strong>does more</strong> than <code>.map(str.strip)</code>, it is to be expected that this method will always be slower (and as you have shown, in your case 2x slower).</p>

<p>Using the <code>.str.strip()</code> method has it advantages in the automatic NaN handling (or handling of other non-string values). Suppose the 'id' column contains a NaN value:</p>

<pre><code>In [4]: df['id'].map(str.strip)
...
TypeError: descriptor 'strip' requires a 'str' object but received a 'float'

In [5]: df['id'].str.strip()
Out[5]:
0                   NaN
1                as asd
2        asdsa asdasdas
              ...
29997              asds
29998            as asd
29999    asdsa asdasdas
Name: id, dtype: object
</code></pre>

<p>As @EdChum points out, you can indeed use <code>map(str.strip)</code> <em>if</em> you are sure you don't have any NaN values if this performance difference is important.</p>

<hr>

<p>Coming back to the other difference of <code>fcr['id'].astype(str).map(str.strip)</code>. If you already know that the values inside the series are strings, doing the <code>astype(str)</code> call is of course superfluous. And it is this call that explains the difference:</p>

<pre><code>In [74]: %timeit df['id'].astype(str).map(str.strip)
100 loops, best of 3: 10.5 ms per loop

In [75]: %timeit df['id'].astype(str)
100 loops, best of 3: 5.25 ms per loop

In [76]: %timeit df['id'].map(str.strip)
100 loops, best of 3: 5.18 ms per loop
</code></pre>

<p>Note that in the case you have non-string values (NaN, numeric values, ...), using <code>.str.strip()</code> and <code>.astype(str).map(str)</code> will <em>not</em> yield the same result:</p>

<pre><code>In [11]: s = pd.Series(['  a', 10])

In [12]: s.astype(str).map(str.strip)
Out[12]:
0     a
1    10
dtype: object

In [13]: s.str.strip()
Out[13]:
0      a
1    NaN
dtype: object
</code></pre>

<p>As you can see, <code>.str.strip()</code> will return non-string values as NaN, instead of converting them to strings.</p>
";;0;;2016-01-18T20:41:33.657;;34863702;2016-01-18T20:54:08.000;2016-01-18T20:54:08.000;;653364.0;;653364.0;34862336.0;2;11;;;
44260;44260;;;"<p>The more idiomatic way to do this with pandas is to use the <code>.sample</code> method of your dataframe, i.e.</p>

<pre><code>df.sample(frac=1)
</code></pre>

<p>The <code>frac</code> keyword argument specifies the fraction of rows to return in the random sample, so <code>frac=1</code> means return all rows (in random order).</p>

<p><strong>Note:</strong>
<em>If you wish to shuffle your dataframe in-place and reset the index, you could do e.g.</em></p>

<pre><code>df = df.sample(frac=1).reset_index(drop=True)
</code></pre>

<p><em>Here, specifying <code>drop=True</code> prevents <code>.reset_index</code> from creating a column containing the old index entries.</em></p>
";;0;;2016-01-19T14:49:17.730;;34879805;2016-01-27T09:18:11.330;2016-01-27T09:18:11.330;;2123555.0;;2123555.0;29576430.0;2;140;;;
44282;44282;;;"<p>You can do this as a post processing step using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html""><code>to_datetime</code></a> and passing arg <code>unit='ms'</code>:</p>

<pre><code>In [5]:
df['UNIXTIME'] = pd.to_datetime(df['UNIXTIME'], unit='ms')
df

Out[5]:
   RUN                UNIXTIME  VALUE
0    1 2015-11-10 13:05:02.320     10
1    2 2015-11-10 13:05:02.364     20
2    3 2015-11-10 13:05:22.364     42
</code></pre>
";;5;;2016-01-19T18:01:58.737;;34883876;2016-01-19T18:01:58.737;;;;;704848.0;34883101.0;2;15;;;
44323;44323;;;"<p>You can <code>apply</code> on your column range with a <code>lambda</code> that calls <code>corr</code> and pass the <code>Series</code> <code>'special_col'</code>:</p>

<pre><code>In [126]:
df[df.columns[1:-1]].apply(lambda x: x.corr(df['special_col']))

Out[126]:
stem1    0.500000
stem2   -0.500000
stem3   -0.999945
b1       0.500000
b2       0.500000
b3      -0.500000
dtype: float64
</code></pre>

<p><strong>Timings</strong></p>

<p>Actually the other method is quicker so I expect it to scale better:</p>

<pre><code>In [130]:
%timeit df[df.columns[1:-1]].apply(lambda x: x.corr(df['special_col']))
%timeit df[df.columns[1:]].corr()['special_col']

1000 loops, best of 3: 1.75 ms per loop
1000 loops, best of 3: 836 s per loop
</code></pre>
";;3;;2016-01-20T09:56:29.000;;34896797;2016-01-20T09:57:48.770;2016-01-20T09:57:48.770;;704848.0;;704848.0;34896455.0;2;9;;;
44324;44324;;;"<p><s>Note there is a mistake in your data, there special col is all 3, so no correlation can be computed.</s></p>

<p>If you remove the column selection in the end you'll get a correlation matrix of all other columns you are analysing. The last [:-1] is to remove correlation of 'special_col' with itself.</p>

<pre><code>In [15]: data[data.columns[1:]].corr()['special_col'][:-1]
Out[15]: 
stem1    0.500000
stem2   -0.500000
stem3   -0.999945
b1       0.500000
b2       0.500000
b3      -0.500000
Name: special_col, dtype: float64
</code></pre>

<p>If you are interested in speed, this is slightly faster on my machine:</p>

<pre><code>In [33]: np.corrcoef(data[data.columns[1:]].T)[-1][:-1]
Out[33]: 
array([ 0.5       , -0.5       , -0.99994535,  0.5       ,  0.5       ,
       -0.5       ])

In [34]: %timeit np.corrcoef(data[data.columns[1:]].T)[-1][:-1]
1000 loops, best of 3: 437 s per loop

In [35]: %timeit data[data.columns[1:]].corr()['special_col']
1000 loops, best of 3: 526 s per loop
</code></pre>

<p>But obviously, it returns an array, not a pandas series/DF.</p>
";;2;;2016-01-20T09:58:13.710;;34896835;2016-01-20T10:06:33.577;2016-01-20T10:06:33.577;;1304161.0;;1304161.0;34896455.0;2;7;;;
44326;44326;;;"<p>Why not just do:</p>

<pre><code>In [34]: df.corr().iloc[:-1,-1]
Out[34]:
stem1    0.500000
stem2   -0.500000
stem3   -0.999945
b1       0.500000
b2       0.500000
b3      -0.500000
Name: special_col, dtype: float64
</code></pre>

<p>or:</p>

<pre><code>In [39]: df.corr().ix['special_col', :-1]
Out[39]:
stem1    0.500000
stem2   -0.500000
stem3   -0.999945
b1       0.500000
b2       0.500000
b3      -0.500000
Name: special_col, dtype: float64
</code></pre>

<p><strong>Timings</strong></p>

<pre><code>In [35]: %timeit df.corr().iloc[-1,:-1]
1000 loops, best of 3: 576 us per loop

In [40]: %timeit df.corr().ix['special_col', :-1]
1000 loops, best of 3: 634 us per loop

In [36]: %timeit df[df.columns[1:]].corr()['special_col']
1000 loops, best of 3: 968 us per loop

In [37]: %timeit df[df.columns[1:-1]].apply(lambda x: x.corr(df['special_col']))
100 loops, best of 3: 2.12 ms per loop
</code></pre>
";;1;;2016-01-20T10:19:46.427;;34897294;2016-01-20T10:19:46.427;;;;;4542359.0;34896455.0;2;6;;;
44415;44415;;;"<p>You could use <a href=""https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjEkvbtobrKAhXJkSwKHVRRAxgQFggcMAA&amp;url=http%3A%2F%2Fpandas.pydata.org%2Fpandas-docs%2Fversion%2F0.17.1%2Fgenerated%2Fpandas.DataFrame.apply.html&amp;usg=AFQjCNEV-3rhym9xF5j7u78SpzSm6Ra4PA"" rel=""noreferrer""><code>apply</code></a> for your columns with checking <code>dtype</code> whether it's <code>numeric</code> or not by checking <a href=""http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.dtype.kind.html"" rel=""noreferrer""><code>dtype.kind</code></a>:</p>

<pre><code>res = df.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))

print(res)
     A      B     City   Name
0  1.0   0.25  Seattle   Jack
1  2.1   0.00       SF    Sue
2  0.0   0.00       LA      .
3  4.7   4.00       OC    Bob
4  5.6  12.20        .  Alice
5  6.8  14.40        .   John
</code></pre>
";;1;;2016-01-21T06:21:56.920;;34916691;2016-01-21T06:49:08.173;2016-01-21T06:49:08.173;;4542359.0;;4542359.0;34913590.0;2;7;;;
44595;44595;;;"<p>Given a sample dataframe <code>df</code> as:</p>

<pre><code>a,b
1,2
2,3
3,4
4,5
</code></pre>

<p>what you want is:</p>

<pre><code>df['a'] = df['a'].apply(lambda x: x + 1)
</code></pre>

<p>that returns:</p>

<pre><code>   a  b
0  2  2
1  3  3
2  4  4
3  5  5
</code></pre>
";;0;;2016-01-23T10:15:49.057;;34962199;2017-02-21T21:54:51.293;2017-02-21T21:54:51.293;;362951.0;;2699288.0;34962104.0;2;67;;;
44597;44597;;;"<p>You don't need a function at all. You can work on a whole column directly.</p>

<p>Example data:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': [100, 1000], 'b': [200, 2000], 'c': [300, 3000]})
&gt;&gt;&gt; df

      a     b     c
0   100   200   300
1  1000  2000  3000
</code></pre>

<p>Half all the values in column <code>a</code>:</p>

<pre><code>&gt;&gt;&gt; df.a = df.a / 2
&gt;&gt;&gt; df

     a     b     c
0   50   200   300
1  500  2000  3000
</code></pre>
";;0;;2016-01-23T10:58:30.313;;34962592;2017-08-22T16:18:07.417;2017-08-22T16:18:07.417;;2422648.0;;837534.0;34962104.0;2;9;;;
44743;44743;;;"<p>With approach explained by EdChum above, the values in the list are shown as rows. To show the values of lists as columns in DataFrame instead, simply use transpose() as following:</p>

<pre><code>table = [[1 , 2], [3, 4]]
df = DataFrame(table)
df = df.transpose()
df.columns = ['Heading1', 'Heading2']
</code></pre>

<p>The output then is:</p>

<pre><code>      Heading1  Heading2
0         1        3
1         2        4
</code></pre>
";;0;;2016-01-25T15:59:49.170;;34996876;2017-05-11T14:21:29.533;2017-05-11T14:21:29.533;;5553237.0;;5553237.0;19112398.0;2;34;;;
45081;45081;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.slice.html"" rel=""noreferrer""><code>str.slice</code></a> for that:</p>

<pre><code>df.columns = df.columns.str.slice(1)
</code></pre>
";;0;;2016-01-28T17:31:39.437;;35068123;2016-01-28T17:31:39.437;;;;;4542359.0;11346283.0;2;8;;;
45169;45169;;;"<p>A bit old but I will post this for other people. </p>

<p>What you want can be achieved, but you probably shouldn't want it ;)
Pandas supports hierarchical indexes for both rows and columns. 
In Python 2.7.x ...</p>

<pre><code>from StringIO import StringIO

raw = '''Salesman  Height   product      price
  Knut      6        bat          5
  Knut      6        ball         1
  Knut      6        wand         3
  Steve     5        pen          2'''
dff = pd.read_csv(StringIO(raw), sep='\s+')

print dff.set_index(['Salesman', 'Height', 'product']).unstack('product')
</code></pre>

<p>Produces a probably more convenient representation than what you were looking for</p>

<pre><code>                price             
product          ball bat pen wand
Salesman Height                   
Knut     6          1   5 NaN    3
Steve    5        NaN NaN   2  NaN
</code></pre>

<p>The advantage of using set_index and unstacking vs a single function as pivot is that you can break the operations down into clear small steps, which simplifies debugging. </p>
";;1;;2016-01-29T15:15:09.587;;35087831;2016-01-29T15:15:09.587;;;;;2240326.0;22798934.0;2;9;;;
45260;45260;;;"<p>You can mark their attributes as dynamically generated using <code>generated-members</code> option.</p>

<p>E.g. for pandas:</p>

<pre><code>generated-members=pandas.*
</code></pre>
";;7;;2016-01-30T20:28:12.070;;35106735;2016-01-30T20:28:12.070;;;;;4863572.0;33961756.0;2;14;;;
45368;45368;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_loc.html"">Index.get_loc</a> instead.</p>

<p>Reusing @unutbu's set up code, you'll achieve the same results.</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np


&gt;&gt;&gt; df = pd.DataFrame(np.arange(1,7).reshape(2,3),
                  columns = list('abc'),
                  index=pd.Series([2,5], name='b'))
&gt;&gt;&gt; df
   a  b  c
b
2  1  2  3
5  4  5  6
&gt;&gt;&gt; df.index.get_loc(5)
1
</code></pre>
";;2;;2016-02-01T19:35:48.007;;35138807;2016-02-01T19:35:48.007;;;;;2014591.0;18199288.0;2;18;;;
45373;45373;;;"<p>In the past, I've used Linux's pair of venerable <a href=""http://man7.org/linux/man-pages/man1/sort.1.html"" rel=""noreferrer""><code>sort</code></a> and <a href=""http://linux.die.net/man/1/split"" rel=""noreferrer""><code>split</code></a> utilities, to sort massive files that choked pandas.</p>

<p>I don't want to disparage the other answer on this page. However, since your data is text format (as you indicated in the comments), I think it's a tremendous complication to start transferring it into other formats (HDF, SQL, etc.), for something that GNU/Linux utilities have been solving very efficiently for the last 30-40 years.</p>

<hr>

<p>Say your file is called <code>stuff.csv</code>, and looks like this:</p>

<pre><code>4.9,3.0,1.4,0.6
4.8,2.8,1.3,1.2
</code></pre>

<p>Then the following command will sort it by the 3rd column:</p>

<pre><code>sort --parallel=8 -t . -nrk3 stuff.csv
</code></pre>

<p>Note that the number of threads here is set to 8.</p>

<hr>

<p>The above will work with files that fit into the main memory. When your file is too large, you would first split it into a number of parts. So</p>

<pre><code>split -l 100000 stuff.csv stuff
</code></pre>

<p>would split the file into files of length at most 100000 lines. </p>

<p>Now you would sort each file individually, as above. Finally, you would use <a href=""https://en.wikipedia.org/wiki/Merge_sort"" rel=""noreferrer"">mergesort</a>, again through (waith for it...) <code>sort</code>:</p>

<pre><code>sort -m sorted_stuff_* &gt; final_sorted_stuff.csv
</code></pre>

<hr>

<p>Finally, if your file is not in CSV (say it is a <code>tgz</code> file), then you should find a way to pipe a CSV version of it into <code>split</code>.</p>
";;1;;2016-02-01T20:36:12.100;;35139819;2016-02-02T07:15:27.530;2016-02-02T07:15:27.530;;3510736.0;;3510736.0;21271727.0;2;9;;;
45620;45620;;;"<p>I realize this question is quite old, but in the latest version of pandas there is an easy way to do exactly this. Column names (which are strings) <strong>can</strong> be sliced in whatever manner you like.</p>

<pre><code>columns = ['b', 'c']
df1 = pd.DataFrame(df, columns=columns)
</code></pre>
";;0;;2016-02-04T14:05:35.083;;35203149;2016-02-04T14:05:35.083;;;;;3616299.0;11285613.0;2;16;;;
45624;45624;;;"<p>Previous answer is not correct in my experience, you can't pass it a simple string, needs to be a datetime object. So:</p>

<pre><code>import datetime 
df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]
</code></pre>
";;2;;2016-02-04T14:29:12.307;;35203658;2016-02-04T14:29:12.307;;;;;2516989.0;22898824.0;2;21;;;
45687;45687;;;"<p>I had this problem as well but perhaps for a different reason. I had some trailing commas in my CSV that were adding an additional column that pandas was attempting to read. Using the following works but it simply ignores the bad lines:</p>

<pre><code>data = pd.read_csv('file1.csv', error_bad_lines=False)
</code></pre>

<p>If you want to keep the lines an ugly kind of hack for handling the errors is to do something like the following:</p>

<pre><code>line     = []
expected = []
saw      = []     
cont     = True 

while cont == True:     
    try:
        data = pd.read_csv('file1.csv',skiprows=line)
        cont = False
    except Exception as e:    
        errortype = e.message.split('.')[0].strip()                                
        if errortype == 'Error tokenizing data':                        
           cerror      = e.message.split(':')[1].strip().replace(',','')
           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]
           expected.append(int(nums[0]))
           saw.append(int(nums[2]))
           line.append(int(nums[1])-1)
         else:
           cerror      = 'Unknown'
           print 'Unknown Error - 222'

if line != []:
    # Handle the errors however you want
</code></pre>

<p>I proceeded to write a script to reinsert the lines into the DataFrame since the bad lines will be given by the variable 'line' in the above code. This can all be avoided by simply using the csv reader. Hopefully the pandas developers can make it easier to deal with this situation in the future.</p>
";;0;;2016-02-04T22:16:44.623;;35212740;2016-02-04T22:16:44.623;;;;;5885544.0;18039057.0;2;10;;;
45707;45707;;;"<pre><code>df.ix[:, df.columns != col]
</code></pre>

<p>where <code>col</code> is the name of the column to leave out.</p>
";;0;;2016-02-05T08:40:10.423;;35219658;2017-07-28T08:01:28.753;2017-07-28T08:01:28.753;;1991051.0;;1991051.0;20230326.0;2;19;;;
45807;45807;;;"<p>Change data type of DataFrame column:</p>

<p>To int:</p>

<p><code>df.column_name = df.column_name.astype(np.int64)</code></p>

<p>To str:</p>

<p><code>df.column_name = df.column_name.astype(str)</code></p>
";;2;;2016-02-06T12:24:52.560;;35240942;2016-04-16T08:18:44.133;2016-04-16T08:18:44.133;;2398521.0;;5891940.0;17950374.0;2;32;;;
45835;45835;;;"<p>I agree with Joris; it seems like you should be doing this differently, like with <a href=""http://docs.scipy.org/doc/numpy-1.10.1/user/basics.rec.html"" rel=""nofollow noreferrer"">numpy record arrays</a>. Modifying ""option 2"" from <a href=""https://stackoverflow.com/a/21647198/943773"">this great answer</a>, you could do it like this:</p>

<pre><code>import pandas
import numpy

dtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]
values = numpy.zeros(20, dtype=dtype)
index = ['Row'+str(i) for i in range(1, len(values)+1)]

df = pandas.DataFrame(values, index=index)
</code></pre>
";;0;;2016-02-06T19:15:08.960;;35245297;2016-02-06T19:15:08.960;2017-05-23T12:26:21.290;;-1.0;;943773.0;20763012.0;2;8;;;
45837;45837;;;"<p>Seems to work for me without the <code>StringIO</code>:</p>

<pre><code>test = pd.read_csv('https://docs.google.com/spreadsheets/d/' + 
                   '0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc' +
                   '/export?gid=0&amp;format=csv',
                   # Set first column as rownames in data frame
                   index_col=0,
                   # Parse column values to datetime
                   parse_dates=['Quradate']
                  )
test.head(5)  # Same result as @TomAugspurger
</code></pre>

<p>BTW, including the <code>?gid=</code> enables importing different sheets, find the gid in the URL.</p>
";;4;;2016-02-06T20:23:08.223;;35246041;2016-09-07T06:41:39.090;2016-09-07T06:41:39.090;;4599228.0;;1840471.0;19611729.0;2;16;;;
46003;46003;;;"<p>I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the <code>query()</code> method in v0.13 and I much prefer it. For your question, you could do <code>df.query('col == val')</code></p>

<p>Reproduced from <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query"">http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query</a></p>

<pre><code>In [167]: n = 10

In [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))

In [169]: df
Out[169]: 
          a         b         c
0  0.687704  0.582314  0.281645
1  0.250846  0.610021  0.420121
2  0.624328  0.401816  0.932146
3  0.011763  0.022921  0.244186
4  0.590198  0.325680  0.890392
5  0.598892  0.296424  0.007312
6  0.634625  0.803069  0.123872
7  0.924168  0.325076  0.303746
8  0.116822  0.364564  0.454607
9  0.986142  0.751953  0.561512

# pure python
In [170]: df[(df.a &lt; df.b) &amp; (df.b &lt; df.c)]
Out[170]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607

# query
In [171]: df.query('(a &lt; b) &amp; (b &lt; c)')
Out[171]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607
</code></pre>

<p>You can also access variables in the environment by prepending an <code>@</code>.</p>

<pre><code>exclude = ('red', 'orange')
df.query('color not in @exclude')
</code></pre>
";;2;;2016-02-09T01:36:49.217;;35282530;2016-02-09T01:36:49.217;;;;;3533440.0;17071871.0;2;24;;;
46111;46111;;;"<p>The solution for me was to change 'rows=>index' and 'cols=>columns'):</p>

<p>From:</p>

<pre><code>mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')
</code></pre>

<p>to:</p>

<pre><code>mean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')
</code></pre>
";;4;;2016-02-10T14:45:14.693;;35318270;2016-02-10T14:52:08.750;2016-02-10T14:52:08.750;;1033422.0;;1033422.0;35318269.0;2;18;;;
46340;46340;;;"<p>You can do it that way:</p>

<pre><code># for Python 2
df.index = df.index.map(unicode) 

# for Python 3 (the unicode type does not exist and is replaced by str)
df.index = df.index.map(str)
</code></pre>

<p>As for why you would proceed differently from when you'd convert from int to float, that's a peculiarity of numpy (the library on which pandas is based).</p>

<p>Every numpy array has a <em>dtype</em>, which is basically the <strong>machine</strong> type of its elements : in that manner, <strong>numpy deals directly with native types</strong>, not with Python objects, which explains how it is so fast. So when you are changing the dtype from int64 to float64, numpy will cast each element in the C code.</p>

<p>There's also a special dtype : <em>object</em>, that will basically provide a pointer toward a Python object.</p>

<p>If you want strings, you thus have to use the <em>object</em> dtype. But using <code>.astype(object)</code> would not give you the answer you were looking for : it would instead create an index with <em>object</em> dtype, but put Python float objects inside.</p>

<p>Here, by using map, we convert the index to strings with the appropriate function: numpy gets the string objects and understand that the index has to have an <em>object</em> dtype, because that's the only dtype that can accomodate strings.</p>
";;2;;2016-02-12T17:31:49.330;;35368792;2016-12-11T11:53:02.647;2016-12-11T11:53:02.647;;1122474.0;;1122474.0;35368645.0;2;21;;;
46412;46412;;;"<p>In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:</p>

<pre><code>df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], 
        axis=1, inplace=True)
</code></pre>
";;0;;2016-02-13T21:58:33.920;;35385805;2016-11-22T15:15:42.863;2016-11-22T15:15:42.863;;2411802.0;;2411802.0;13411544.0;2;19;;;
46423;46423;;;"<pre><code>df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]}
</code></pre>

<p>If your new list of columns is in the same order as the existing columns, the assignment is simple:</p>

<pre><code>new_cols = ['a', 'b', 'c', 'd', 'e']
df.columns = new_cols
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you had a dictionary keyed on old column names to new column names, you could do the following:</p>

<pre><code>d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}
df.columns.map(lambda col: d[col])
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you don't have a list or dictionary mapping, you could strip the leading <code>$</code> symbol via a list comprehension:</p>

<pre><code>df.columns = [col[1:] if col[0] == '$' else col for col in df]
</code></pre>
";;0;;2016-02-14T00:31:53.873;;35387028;2016-02-14T00:31:53.873;;;;;2411802.0;11346283.0;2;10;;;
46424;46424;;;"<blockquote>
  <p>I would like to add a new column, 'e', to the existing data frame and do not change anything in the data frame. (The series always got the same length as a dataframe.) </p>
</blockquote>

<p>I assume that the index values in <code>e</code> match those in <code>df1</code>.</p>

<p>The easiest way to initiate a new column named <code>e</code>, and assign it the values from your series <code>e</code>:</p>

<pre><code>df['e'] = e.values
</code></pre>

<p><strong>assign (Pandas 0.16.0+)</strong></p>

<p>As of Pandas 0.16.0, you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html"" rel=""noreferrer""><code>assign</code></a>, which assigns new columns to a DataFrame and returns a new object (a copy) with all the original columns in addition to the new ones.</p>

<pre><code>df1 = df1.assign(e=e.values)
</code></pre>

<p>As per <a href=""https://stackoverflow.com/questions/42101382/pandas-dataframe-assign-arguments"">this example</a> (which also includes the source code of the <code>assign</code> function), you can also include more than one column:</p>

<pre><code>df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
&gt;&gt;&gt; df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())
   a  b  mean_a  mean_b
0  1  3     1.5     3.5
1  2  4     1.5     3.5
</code></pre>

<p>In context with your example: </p>

<pre><code>np.random.seed(0)
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])
mask = df1.applymap(lambda x: x &lt;-0.7)
df1 = df1[-mask.any(axis=1)]
sLength = len(df1['a'])
e = pd.Series(np.random.randn(sLength))

&gt;&gt;&gt; df1
          a         b         c         d
0  1.764052  0.400157  0.978738  2.240893
2 -0.103219  0.410599  0.144044  1.454274
3  0.761038  0.121675  0.443863  0.333674
7  1.532779  1.469359  0.154947  0.378163
9  1.230291  1.202380 -0.387327 -0.302303

&gt;&gt;&gt; e
0   -1.048553
1   -1.420018
2   -1.706270
3    1.950775
4   -0.509652
dtype: float64

df1 = df1.assign(e=e.values)

&gt;&gt;&gt; df1
          a         b         c         d         e
0  1.764052  0.400157  0.978738  2.240893 -1.048553
2 -0.103219  0.410599  0.144044  1.454274 -1.420018
3  0.761038  0.121675  0.443863  0.333674 -1.706270
7  1.532779  1.469359  0.154947  0.378163  1.950775
9  1.230291  1.202380 -0.387327 -0.302303 -0.509652
</code></pre>

<p>The description of this new feature when it was first introduced can be found <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dataframe-assign"" rel=""noreferrer"">here</a>.</p>
";;3;;2016-02-14T00:49:58.643;;35387129;2017-05-11T19:17:24.240;2017-05-23T12:18:32.893;;-1.0;;2411802.0;12555323.0;2;50;;;
46543;46543;;;"<p>You can group and then unstack.</p>

<pre><code>&gt;&gt;&gt; df.groupby(['year', 'month', 'item'])['value'].sum().unstack('item')
item        item 1  item 2
year month                
2004 1          33     250
     2          44     224
     3          41     268
     4          29     232
     5          57     252
     6          61     255
     7          28     254
     8          15     229
     9          29     258
     10         49     207
     11         36     254
     12         23     209
</code></pre>

<p>Or use <code>pivot_table</code>:</p>

<pre><code>&gt;&gt;&gt; df.pivot_table(values='value', index=['year', 'month'], columns='item')
item        item 1  item 2
year month                
2004 1          33     250
     2          44     224
     3          41     268
     4          29     232
     5          57     252
     6          61     255
     7          28     254
     8          15     229
     9          29     258
     10         49     207
     11         36     254
     12         23     209
</code></pre>
";;0;;2016-02-15T17:44:27.973;;35415751;2016-02-15T17:44:27.973;;;;;2411802.0;35414625.0;2;17;;;
46546;46546;;;"<p>I believe if you include <code>item</code> in your MultiIndex, then you can just unstack:</p>

<pre><code>df.set_index(['year', 'month', 'item']).unstack(level=-1)
</code></pre>

<p>This yields:</p>

<pre class=""lang-none prettyprint-override""><code>                value      
item       item 1 item 2
year month              
2004 1         21    277
     2         43    244
     3         12    262
     4         80    201
     5         22    287
     6         52    284
     7         90    249
     8         14    229
     9         52    205
     10        76    207
     11        88    259
     12        90    200
</code></pre>

<p>It's a bit faster than using <code>pivot_table</code>, and about the same speed or slightly slower than using <code>groupby</code>.</p>
";;1;;2016-02-15T18:00:23.873;;35416021;2016-02-15T18:00:23.873;;;;;3100515.0;35414625.0;2;8;;;
46696;46696;;;"<p><a href=""https://stackoverflow.com/users/5014134/jianxun-li"">Jianxun</a>'s solution did the job for me but broke the y value indicator at the bottom left of the window. </p>

<p>I ended up using <code>FuncFormatter</code>instead (and also stripped the uneccessary trailing zeroes as suggested <a href=""https://stackoverflow.com/questions/14997799/most-pythonic-way-to-print-at-most-some-number-of-decimal-places"">here</a>):</p>

<pre><code>import pandas as pd
import numpy as np
from matplotlib.ticker import FuncFormatter

df = pd.DataFrame(np.random.randn(100,5))

ax = df.plot()
ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) 
</code></pre>

<p>Generally speaking I'd recommend using <code>FuncFormatter</code> for label formatting: it's reliable, and versatile. </p>

<p><a href=""https://i.stack.imgur.com/uKf1z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uKf1z.png"" alt=""enter image description here""></a></p>
";;3;;2016-02-17T01:39:40.573;;35446404;2016-04-14T14:21:49.860;2017-05-23T12:18:20.013;;-1.0;;5622825.0;31357611.0;2;29;;;
46727;46727;;;"<p>You can take the Axes object out after plotting and perform <code>.grid(False)</code> on both axes.</p>

<pre class=""lang-py prettyprint-override""><code># Gets the axes object out after plotting
ax = data.plot(...)

# Turns off grid on the left Axis.
ax.grid(False)

# Turns off grid on the secondary (right) Axis.
ax.right_ax(False)
</code></pre>
";;2;;2016-02-17T10:19:56.887;;35453836;2016-02-17T10:19:56.887;;;;;4055552.0;26868304.0;2;12;;;
46812;46812;;;"<p>I found it to be a simple version being outdated or mismatch and was fixed with: </p>

<pre><code>pip install --upgrade numpy
pip install --upgrade scipy
pip install --upgrade pandas
</code></pre>

<p>Or might work with the one liner:</p>

<pre><code>pip install --upgrade numpy scipy pandas
</code></pre>
";;2;;2016-02-18T08:42:17.903;;35476539;2016-06-13T06:32:13.530;2016-06-13T06:32:13.530;;2356861.0;;2356861.0;17709641.0;2;24;;;
46962;46962;;;"<p>Sometimes it's easier to do all the appending outside of pandas, then, just create the DataFrame in one shot.</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; list=[['a','b']]
&gt;&gt;&gt; list.append(['e','f'])
&gt;&gt;&gt; df=pd.DataFrame(list,columns=['col1','col2'])
   col1 col2
0    a    b
1    e    f
</code></pre>
";;0;;2016-02-19T15:40:37.103;;35509134;2016-02-19T15:40:37.103;;;;;904032.0;26309962.0;2;17;;;
47037;47037;;;"<p>suppose df is your dataframe then:</p>

<pre><code>Count_Row=df.shape[0] #gives number of row count
Count_Col=df.shape[1] #gives number of col count
</code></pre>
";;0;;2016-02-20T13:30:05.467;;35523946;2016-02-20T13:35:41.743;2016-02-20T13:35:41.743;;5282071.0;;5282071.0;15943769.0;2;50;;;
47073;47073;;;"<p>Pandas random sample will also work </p>

<pre><code>train=df.sample(frac=0.8,random_state=200)
test=df.drop(train.index)
</code></pre>
";;3;;2016-02-21T01:28:55.530;;35531218;2016-02-21T01:28:55.530;;;;;5826590.0;24147278.0;2;87;;;
47261;47261;;;"<p>You can append a single row as a dictionary using the <code>ignore_index</code> option.</p>

<pre><code>&gt;&gt;&gt; f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})
&gt;&gt;&gt; f
  Animal Color
0    cow  blue
1  horse   red
&gt;&gt;&gt; f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)
  Animal  Color
0    cow   blue
1  horse    red
2  mouse  black
</code></pre>
";;2;;2016-02-23T16:43:07.890;;35583219;2016-02-23T16:43:07.890;;;;;1120370.0;10715965.0;2;17;;;
47409;47409;;;"<p>Why don't you just use .to_frame if both have the same indexes?</p>

<pre><code>a.to_frame().join(b.to_frame())
</code></pre>
";;0;;2016-02-25T00:39:45.190;;35616082;2016-02-25T01:00:03.453;2016-02-25T01:00:03.453;;1659677.0;;3399944.0;18062135.0;2;9;;;
47422;47422;;;"<p>There is now an official guide on how to subclass Pandas data structures, which includes DataFrame as well as Series.</p>

<p>The guide is available here: <a href=""http://pandas.pydata.org/pandas-docs/stable/internals.html#subclassing-pandas-data-structures"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/internals.html#subclassing-pandas-data-structures</a> </p>

<p>The guide mentions this subclassed DataFrame from the Geopandas project as a good example: <a href=""https://github.com/geopandas/geopandas/blob/master/geopandas/geodataframe.py"" rel=""noreferrer"">https://github.com/geopandas/geopandas/blob/master/geopandas/geodataframe.py</a></p>

<p>As in HYRY's answer, it seems there are two things you're trying to accomplish: </p>

<ol>
<li>When calling methods on an instance of your class, return instances of the correct type (your type). For this, you can just add the <code>_constructor</code> property which should return your type.</li>
<li>Adding attributes which will be attached to copies of your object. To do this, you need to store the names of these attributes in a list, as the special <code>_metadata</code> attribute.</li>
</ol>

<p>Here's an example:</p>

<pre><code>class SubclassedDataFrame(DataFrame):
    _metadata = ['added_property']
    added_property = 1  # This will be passed to copies

    @property
    def _constructor(self):
        return SubclassedDataFrame
</code></pre>
";;0;;2016-02-25T06:22:31.910;;35619846;2016-02-25T06:22:31.910;;;;;1772977.0;22155951.0;2;6;;;
47839;47839;;;"<p>Pandas allows you to plot tables using matplotlib (details <a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html#plotting-tables"">here</a>).
Usually this plots the table directly onto a plot (with axes and everything) which is not what you want.  However, these can be removed first:</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd
from pandas.tools.plotting import table

ax = plt.subplot(111, frame_on=False) # no visible frame
ax.xaxis.set_visible(False)  # hide the x axis
ax.yaxis.set_visible(False)  # hide the y axis

table(ax, df)  # where df is your data frame

plt.savefig('mytable.png')
</code></pre>

<p>The output might not be the prettiest but you can find additional arguments for the table() function <a href=""http://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.table"">here</a>. 
Also thanks to <a href=""http://matplotlib.1069221.n5.nabble.com/Draw-only-table-without-XY-Axis-td19546.html"">this post</a> for info on how to remove axes in matplotlib.</p>

<hr>

<h2>EDIT:</h2>

<p>Here is a (admittedly quite hacky) way of simulating multi-indexes when plotting using the method above.  If you have a multi-index data frame called df that looks like:</p>

<pre><code>first  second
bar    one       1.991802
       two       0.403415
baz    one      -1.024986
       two      -0.522366
foo    one       0.350297
       two      -0.444106
qux    one      -0.472536
       two       0.999393
dtype: float64
</code></pre>

<p>First reset the indexes so they become normal columns</p>

<pre><code>df = df.reset_index() 
df
    first second       0
0   bar    one  1.991802
1   bar    two  0.403415
2   baz    one -1.024986
3   baz    two -0.522366
4   foo    one  0.350297
5   foo    two -0.444106
6   qux    one -0.472536
7   qux    two  0.999393
</code></pre>

<p>Remove all duplicates from the higher order multi-index columns by setting them to an empty string (in my example I only have duplicate indexes in ""first""):</p>

<pre><code>df.ix[df.duplicated('first') , 'first'] = ''
df
  first second         0
0   bar    one  1.991802
1          two  0.403415
2   baz    one -1.024986
3          two -0.522366
4   foo    one  0.350297
5          two -0.444106
6   qux    one -0.472536
7          two  0.999393
</code></pre>

<p>Change the column names over your ""indexes"" to the empty string </p>

<pre><code>new_cols = df.columns.values
new_cols[:2] = '',''  # since my index columns are the two left-most on the table
df.columns = new_cols 
</code></pre>

<p>Now call the table function but set all the row labels in the table to the empty string (this makes sure the actual indexes of your plot are not displayed):</p>

<pre><code>table(ax, df, rowLabels=['']*df.shape[0], loc='center')
</code></pre>

<p>et voila:</p>

<p><a href=""https://i.stack.imgur.com/lXofB.png""><img src=""https://i.stack.imgur.com/lXofB.png"" alt=""enter image description here""></a></p>

<p>Your not-so-pretty but totally functional multi-indexed table.</p>
";;4;;2016-03-01T05:03:44.257;;35715029;2016-03-02T01:03:04.940;2016-03-02T01:03:04.940;;5992438.0;;5992438.0;35634238.0;2;9;;;
48063;48063;;;"<p>Since pandas 0.17.0, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html""><code>sort</code></a> is deprecated and replaced by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html""><code>sort_values</code></a>:</p>

<pre><code>df.sort_values(['Peak', 'Weeks'], ascending=[True, False], inplace=True)
</code></pre>
";;0;;2016-03-03T09:38:20.797;;35768306;2016-03-03T09:38:20.797;;;;;1389680.0;13636592.0;2;18;;;
48131;48131;;;"<p>Here is another method using <code>.loc</code> to replace inf with nan on a Series:</p>

<pre><code>s.loc[(~np.isfinite(s)) &amp; s.notnull()] = np.nan
</code></pre>

<p>So, in response to the original question:</p>

<pre><code>df = pd.DataFrame(np.ones((3, 3)), columns=list('ABC'))

for i in range(3): 
    df.iat[i, i] = np.inf

df
          A         B         C
0       inf  1.000000  1.000000
1  1.000000       inf  1.000000
2  1.000000  1.000000       inf

df.sum()
A    inf
B    inf
C    inf
dtype: float64

df.apply(lambda s: s[np.isfinite(s)].dropna()).sum()
A    2
B    2
C    2
dtype: float64
</code></pre>
";;0;;2016-03-03T21:52:22.923;;35783766;2016-03-04T00:20:30.070;2016-03-04T00:20:30.070;;2411802.0;;2411802.0;17477979.0;2;8;;;
48139;48139;;;"<p>Sampling randomizes, so just sample the entire data frame.</p>

<pre><code>df.sample(frac=1)
</code></pre>
";;0;;2016-03-03T22:51:38.233;;35784666;2016-03-03T22:51:38.233;;;;;1120370.0;15772009.0;2;57;;;
48397;48397;;;"<p>The method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.cat.html#pandas.Series.str.cat""><code>cat()</code> of the <code>.str</code> accessor</a> works really well for this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[""2014"", ""q1""], 
...                    [""2015"", ""q3""]],
...                   columns=('Year', 'Quarter'))
&gt;&gt;&gt; print(df)
   Year Quarter
0  2014      q1
1  2015      q3
&gt;&gt;&gt; df['Period'] = df.Year.str.cat(df.Quarter)
&gt;&gt;&gt; print(df)
   Year Quarter  Period
0  2014      q1  2014q1
1  2015      q3  2015q3
</code></pre>

<p><code>cat()</code> even allows you to add a separator so, for example, suppose you only have integers for year and period, you can do this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[2014, 1],
...                    [2015, 3]],
...                   columns=('Year', 'Quarter'))
&gt;&gt;&gt; print(df)
   Year Quarter
0  2014       1
1  2015       3
&gt;&gt;&gt; df['Period'] = df.Year.astype(str).str.cat(df.Quarter.astype(str), sep='q')
&gt;&gt;&gt; print(df)
   Year Quarter  Period
0  2014       1  2014q1
1  2015       3  2015q3
</code></pre>
";;2;;2016-03-07T18:04:04.660;;35850749;2016-03-07T18:04:04.660;;;;;1273938.0;19377969.0;2;47;;;
48602;48602;;;"<p>If you have a time-axis, and you have Pandas imported as pd, you can use:</p>

<pre><code>ax.axvline(pd.to_datetime('2015-11-01'), color='r', linestyle='--', lw=2)
</code></pre>

<p>For multiple lines:</p>

<pre><code>xposition = [pd.to_datetime('2010-01-01'), pd.to_datetime('2015-12-31')]
for xc in xposition:
    ax.axvline(x=xc, color='k', linestyle='-')
</code></pre>
";;0;;2016-03-09T20:59:44.467;;35902487;2016-11-23T20:09:01.090;2016-11-23T20:09:01.090;;445131.0;;601059.0;19213789.0;2;11;;;
48883;48883;;;"<p>You can also use <code>pd.DataFrame.from_records</code> which is more convenient when you already have the dictionary in hand:</p>

<pre><code>df = pd.DataFrame.from_records([{ 'A':a,'B':b }])
</code></pre>

<p>You can also set index, if you want, by:</p>

<pre><code>df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')
</code></pre>
";;3;;2016-03-13T13:26:17.427;;35970794;2016-03-13T13:26:17.427;;;;;586057.0;17839973.0;2;9;;;
48998;48998;;;"<p><code>np.isnan</code> can be applied to NumPy arrays of native dtype (such as np.float64):</p>

<pre><code>In [99]: np.isnan(np.array([np.nan, 0], dtype=np.float64))
Out[99]: array([ True, False], dtype=bool)
</code></pre>

<p>but raises TypeError when applied to object arrays:</p>

<pre><code>In [96]: np.isnan(np.array([np.nan, 0], dtype=object))
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<hr>

<p>Since you have Pandas, you could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html""><code>pd.isnull</code></a> instead -- it can accept NumPy arrays of object or native dtypes:</p>

<pre><code>In [97]: pd.isnull(np.array([np.nan, 0], dtype=float))
Out[97]: array([ True, False], dtype=bool)

In [98]: pd.isnull(np.array([np.nan, 0], dtype=object))
Out[98]: array([ True, False], dtype=bool)
</code></pre>

<p>Note that <code>None</code> is also considered a null value in object arrays.</p>
";;1;;2016-03-15T01:38:22.013;;36001191;2016-03-15T01:44:12.210;2016-03-15T01:44:12.210;;190597.0;;190597.0;36000993.0;2;21;;;
49038;49038;;;"<p>open the console and type the code below.</p>

<p>Code:</p>

<pre><code>import pandas as pd
pd.__version__
</code></pre>

<blockquote>
  <p>**Its double underscore before and after the word ""version"".</p>
</blockquote>

<p>Output:</p>

<pre><code>'0.14.1'
</code></pre>
";;0;;2016-03-15T13:11:03.927;;36012306;2016-03-15T13:11:03.927;;;;;6037997.0;20612645.0;2;7;;;
49041;49041;;;"<p>From version <code>0.18.0</code> you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename_axis.html""><code>rename_axis</code></a>:</p>

<pre><code>print df
             Column 1
Index Title          
Apples            1.0
Oranges           2.0
Puppies           3.0
Ducks             4.0
</code></pre>

<p>The new functionality works well in method chains.</p>

<pre><code>print df.rename_axis('foo')
         Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0
</code></pre>

<p>You can also rename column names with parameter <code>axis</code>:</p>

<pre><code>print df
Col Name     Column 1
Index Title          
Apples            1.0
Oranges           2.0
Puppies           3.0
Ducks             4.0
</code></pre>



<pre><code>print df.rename_axis('foo').rename_axis(""bar"", axis=""columns"")
bar      Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0

print df.rename_axis('foo').rename_axis(""bar"", axis=1)
bar      Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0
</code></pre>
";;0;;2016-03-15T14:12:42.043;;36013757;2016-03-15T14:12:42.043;;;;;2901002.0;18022845.0;2;21;;;
49043;49043;;;"<p>From version <code>0.18.0</code> you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename_axis.html"" rel=""noreferrer""><code>rename_axis</code></a>:</p>

<pre><code>print df
         Column 1
foo              
Apples          1
Oranges         2
Puppies         3
Ducks           4

print df.index.name
foo


print df.rename_axis(None)
         Column 1
Apples          1
Oranges         2
Puppies         3
Ducks           4

print df.rename_axis(None).index.name
None
</code></pre>
";;0;;2016-03-15T14:37:13.043;;36014326;2016-03-15T14:37:13.043;;;;;2901002.0;29765548.0;2;9;;;
49177;49177;;;"<p>Use of a lamba function this time with string.format().  </p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': ['q1', 'q2']})
print df
df['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
print df

  Quarter  Year
0      q1  2014
1      q2  2015
  Quarter  Year YearQuarter
0      q1  2014      2014q1
1      q2  2015      2015q2
</code></pre>

<p>This allows you to work with non-strings and reformat values as needed.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': [1, 2]})
print df.dtypes
print df

df['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}q{}'.format(x[0],x[1]), axis=1)
print df

Quarter     int64
Year       object
dtype: object
   Quarter  Year
0        1  2014
1        2  2015
   Quarter  Year YearQuarter
0        1  2014      2014q1
1        2  2015      2015q2
</code></pre>
";;0;;2016-03-16T16:43:15.060;;36041831;2016-03-16T16:57:26.003;2016-03-16T16:57:26.003;;5403087.0;;5403087.0;19377969.0;2;12;;;
49238;49238;;;"<p>You can also do a nested np.where()</p>

<pre><code>df['Age_group'] = np.where(df.Age&lt;18, 'under 18',
                           np.where(df.Age&lt;40,'under 40', '&gt;40'))
</code></pre>
";;1;;2016-03-17T11:57:14.783;;36059898;2016-03-17T11:57:14.783;;;;;5781120.0;21733893.0;2;8;;;
49321;49321;;;"<p>This will give you the second row of each group (zero indexed, nth(0) is the same as first()):</p>

<pre><code>df.groupby('id').nth(1) 
</code></pre>

<p>Documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#taking-the-nth-row-of-each-group"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/groupby.html#taking-the-nth-row-of-each-group</a></p>
";;2;;2016-03-18T00:03:33.897;;36073837;2016-03-18T00:03:33.897;;;;;2548596.0;20067636.0;2;13;;;
49325;49325;;;"<p>You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group. </p>

<pre><code>In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)

Out[35]: 
   count     job source
4      7   sales      E
2      6   sales      C
1      4   sales      B
5      5  market      A
8      4  market      D
6      3  market      B
</code></pre>
";;2;;2016-03-18T01:20:50.350;;36074520;2017-07-05T10:54:22.157;2017-07-05T10:54:22.157;;4463240.0;;4463240.0;27842613.0;2;24;;;
49349;49349;;;"<p>You can now just call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.strip.html#pandas.Series.str.strip""><code>.str.strip</code></a> on the columns if you're using a recent version:</p>

<pre><code>In [5]:
df = pd.DataFrame(columns=['Year', 'Month ', 'Value'])
print(df.columns.tolist())
df.columns = df.columns.str.strip()
df.columns.tolist()

['Year', 'Month ', 'Value']
Out[5]:
['Year', 'Month', 'Value']
</code></pre>
";;0;;2016-03-18T10:56:29.453;;36082588;2016-05-13T08:12:28.397;2016-05-13T08:12:28.397;;704848.0;;704848.0;21606987.0;2;13;;;
49392;49392;;;"<p>Here is an algorithm that doesn't require counting the number of lines in the file beforehand, so you only need to read the file once.</p>

<p>Say you want m samples. First, the algorithm keeps the first m samples. When it sees the i-th sample (i > m), with probability m/i, the algorithm uses the sample to randomly replace an already selected sample.</p>

<p>By doing so, for any i > m, we always have a subset of m samples randomly selected from the first i samples.</p>

<p>See code below:</p>

<pre><code>import random

n_samples = 10
samples = []

for i, line in enumerate(f):
    if i &lt; n_samples:
        samples.append(line)
    elif random.random() &lt; n_samples * 1. / (i+1):
            samples[random.randint(0, n_samples-1)] = line
</code></pre>
";;0;;2016-03-18T18:05:44.377;;36091388;2016-03-18T18:11:13.717;2016-03-18T18:11:13.717;;359416.0;;359416.0;22258491.0;2;7;;;
49662;49662;;;"<pre><code>df.columns = ['a', 'b', 'c', 'd', 'e']
</code></pre>

<p>It will replace the existing names with the names you provide, in the order you provide.</p>

<p>You can also assign them by index like this:</p>

<pre><code>df.columns.values[2] = 'c'    #renames the 2nd column to 'c'
</code></pre>
";;0;;2016-03-22T08:59:12.740;;36149967;2016-12-12T06:29:12.353;2016-12-12T06:29:12.353;;445131.0;;5440236.0;11346283.0;2;41;;;
49778;49778;;;"<p>For those coming from Google (etc.) such as myself:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html"" rel=""noreferrer""><code>convert_objects</code></a> has been deprecated - if you use it, you get a warning like this one: </p>

<pre><code>FutureWarning: convert_objects is deprecated.  Use the data-type specific converters 
pd.to_datetime, pd.to_timedelta and pd.to_numeric.
</code></pre>

<p>You should do something like the following:</p>

<ul>
<li><code>df =</code><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html"" rel=""noreferrer""><code>df.astype(np.float)</code></a> </li>
<li><code>df[""A""] =</code><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer""><code>pd.to_numeric(df[""A""])</code></a></li>
</ul>
";;0;;2016-03-23T17:02:54.473;;36184396;2016-07-29T00:06:22.713;2016-07-29T00:06:22.713;;6455166.0;;3160855.0;21197774.0;2;35;;;
49809;49809;;;"<p>There is now, two years after the question, an 'out-of-core' pandas equivalent: <a href=""http://dask.pydata.org/en/latest/"" rel=""noreferrer"">dask</a>. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.</p>
";;2;;2016-03-23T20:30:53.543;;36188131;2016-03-23T20:30:53.543;;;;;2115409.0;14262433.0;2;32;;;
49972;49972;;;"<p>Try to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html"" rel=""noreferrer"">isnull()</a>:</p>

<pre><code>In [97]: df
Out[97]:
     a    b  c
0  NaN  7.0  0
1  0.0  NaN  4
2  2.0  NaN  4
3  1.0  7.0  0
4  1.0  3.0  9
5  7.0  4.0  9
6  2.0  6.0  9
7  9.0  6.0  4
8  3.0  0.0  9
9  9.0  0.0  1

In [98]: pd.isnull(df).sum() &gt; 0
Out[98]:
a     True
b     True
c    False
dtype: bool
</code></pre>

<p>or as @root proposed clearer version:</p>

<pre><code>In [5]: df.isnull().any()
Out[5]:
a     True
b     True
c    False
dtype: bool

In [7]: df.columns[df.isnull().any()].tolist()
Out[7]: ['a', 'b']
</code></pre>

<p>to select a subset - all columns containing at least one <code>NaN</code> value:</p>

<pre><code>In [31]: df.loc[:, df.isnull().any()]
Out[31]:
     a    b
0  NaN  7.0
1  0.0  NaN
2  2.0  NaN
3  1.0  7.0
4  1.0  3.0
5  7.0  4.0
6  2.0  6.0
7  9.0  6.0
8  3.0  0.0
9  9.0  0.0
</code></pre>
";;3;;2016-03-25T18:54:05.870;;36226137;2017-04-27T08:35:28.620;2017-04-27T08:35:28.620;;5741205.0;;5741205.0;36226083.0;2;45;;;
50036;50036;;;"<pre><code>w.female.replace(to_replace=dict(female=1, male=0), inplace=True)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html"" rel=""noreferrer"">pandas.DataFrame.replace() docs</a>.</p>
";;0;;2016-03-26T15:08:46.670;;36236885;2016-03-26T15:08:46.670;;;;;4279.0;23307301.0;2;12;;;
50114;50114;;;"<p>Just to make this more clear for novice pandas programmers, here is a concrete example,</p>

<pre><code>pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) 
</code></pre>

<p>Here we select a complaint from complaints table (sqlalchemy model is Complaint) with id = 2</p>
";;0;;2016-03-28T07:08:00.653;;36257640;2016-03-28T07:08:00.653;;;;;4618272.0;29525808.0;2;8;;;
50393;50393;;;"<p>This is a few months late, but I have created <a href=""https://github.com/matplotlib/matplotlib/pull/6251"">PR#6251</a> with matplotlib to add a new <code>PercentFormatter</code> class. With this class you just need one line to reformat your axis (two if you count the import of <code>matplotlib.ticker</code>):</p>

<pre><code>import ...
import matplotlib.ticker as mtick

ax = df['myvar'].plot(kind='bar')
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
</code></pre>

<p><code>PercentFormatter()</code> accepts three arguments, <code>max</code>, <code>decimals</code>, <code>symbol</code>. <code>max</code> allows you to set the value that corresponds to 100% on the axis. This is nice if you have data from 0.0 to 1.0 and you want to display it from 0% to 100%. Just do <code>PercentFormatter(1.0)</code>.</p>

<p>The other two parameters allow you to set the number of digits after the decimal point and the symbol. They default to <code>None</code> and <code>'%'</code>, respectively. <code>decimals=None</code> will automatically set the number of decimal points based on how much of the axes you are showing.</p>
";;4;;2016-03-30T21:16:37.090;;36319915;2016-04-02T02:54:40.180;2016-04-02T02:54:40.180;;2988730.0;;2988730.0;31357611.0;2;12;;;
50657;50657;;;"<p>You can use <code>iteritems()</code>:</p>

<pre><code>for name, values in df.iteritems():
    print '{name}: {value}'.format(name=name, value=values[0])
</code></pre>
";;0;;2016-04-02T11:31:47.997;;36372667;2016-04-02T11:31:47.997;;;;;5483098.0;28218698.0;2;18;;;
50664;50664;;;"<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html"" rel=""noreferrer"">docs for v0.18.0</a>, if you're running on a terminal (ie not iPython notebook, qtconsole or IDLE), it's a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:</p>

<pre><code>pd.set_option('display.large_repr', 'truncate')
pd.set_option('display.max_columns', 0)
</code></pre>
";;0;;2016-04-02T13:26:39.270;;36373866;2016-04-02T13:26:39.270;;;;;682515.0;11707586.0;2;6;;;
50852;50852;;;"<p>An alternative to <a href=""https://stackoverflow.com/a/21232849/3888455"">darindaCoder's answer</a>:</p>

<pre><code>path = r'C:\DRO\DCL_rawdata_files'                     # use your path
all_files = glob.glob(os.path.join(path, ""*.csv""))     # advisable to use os.path.join as this makes concatenation OS independent

df_from_each_file = (pd.read_csv(f) for f in all_files)
concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)
# doesn't create a list, nor does it append to one
</code></pre>
";;8;;2016-04-05T02:47:11.073;;36416258;2016-10-06T20:07:27.437;2017-05-23T10:31:16.803;;-1.0;;3888455.0;20906474.0;2;97;;;
50912;50912;;;"<p>If you want to apply to all columns you can use:</p>

<pre><code>df.apply(pd.value_counts)
</code></pre>

<p>This will apply a column based aggregation function (in this case value_counts) to each of the columns.</p>
";;3;;2016-04-05T18:30:11.140;;36434248;2016-04-05T18:30:11.140;;;;;2117998.0;22391433.0;2;27;;;
51102;51102;;;"<p>I am not sure if previous versions of <code>pandas</code> prevented this but now the following snippet works perfectly for me and produces exactly what you want without having to use <code>apply</code></p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler


&gt;&gt;&gt; scaler = MinMaxScaler()

&gt;&gt;&gt; dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],
                           'B':[103.02,107.26,110.35,114.23,114.68],
                           'C':['big','small','big','small','small']})

&gt;&gt;&gt; dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])

&gt;&gt;&gt; dfTest
          A         B      C
0  0.000000  0.000000    big
1  0.926219  0.363636  small
2  0.935335  0.628645    big
3  1.000000  0.961407  small
4  0.938495  1.000000  small
</code></pre>
";;0;;2016-04-07T11:48:04.410;;36475297;2016-06-20T09:04:33.377;2016-06-20T09:04:33.377;;4068678.0;;4068678.0;24645153.0;2;24;;;
51299;51299;;;"<p>If I'm not mistaken, the following does what was asked without the memory problems of the transpose solution and with fewer lines than @kalu 's function, keeping the first of any similarly named columns.</p>

<pre><code>Cols = list(df.columns)
for i,item in enumerate(df.columns):
    if item in df.columns[:i]: Cols[i] = ""toDROP""
df.columns = Cols
df = df.drop(""toDROP"",1)
</code></pre>
";;0;;2016-04-09T05:53:28.073;;36513262;2016-04-09T05:53:28.073;;;;;2048531.0;14984119.0;2;9;;;
51336;51336;;;"<p>It's the index column, pass <code>index=False</code> to not write it out, see the <a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.DataFrame.to_csv.html"">docs</a></p>

<p>Example:</p>

<pre><code>In [37]:
df = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))
pd.read_csv(io.StringIO(df.to_csv()))

Out[37]:
   Unnamed: 0         a         b         c
0           0  0.109066 -1.112704 -0.545209
1           1  0.447114  1.525341  0.317252
2           2  0.507495  0.137863  0.886283
3           3  1.452867  1.888363  1.168101
4           4  0.901371 -0.704805  0.088335
</code></pre>

<p>compare with:</p>

<pre><code>In [38]:
pd.read_csv(io.StringIO(df.to_csv(index=False)))

Out[38]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335
</code></pre>

<p>You could also optionally tell <code>read_csv</code> that the first column is the index column by passing <code>index_col=0</code>:</p>

<pre><code>In [40]:
pd.read_csv(io.StringIO(df.to_csv()), index_col=0)

Out[40]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335
</code></pre>
";;1;;2016-04-09T15:50:11.727;;36519122;2016-04-09T16:16:29.247;2016-04-09T16:16:29.247;;704848.0;;704848.0;36519086.0;2;34;;;
51433;51433;;;"<h3>Original Solution: Incorrect Usage of <code>collections.OrderedDict</code></h3>

<p>In my original solution, I proposed to use <code>OrderedDict</code> from the <code>collections</code> package in python's standard library.</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt;
&gt;&gt;&gt; foo = np.array( [ 1, 2, 3 ] )
&gt;&gt;&gt; bar = np.array( [ 4, 5, 6 ] )
&gt;&gt;&gt;
&gt;&gt;&gt; pd.DataFrame( OrderedDict( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } ) )

   foo  bar
0    1    4
1    2    5
2    3    6
</code></pre>

<h3>Right Solution: Passing Key-Value Tuple Pairs for Order Preservation</h3>

<p>However, as noted, if a normal dictionary is passed to <code>OrderedDict</code>, the order may still not be preserved since the order is randomized when constructing the dictionary. However, a work around is to convert a list of key-value tuple pairs into an <code>OrderedDict</code>, as suggested from <a href=""https://stackoverflow.com/questions/15711755/converting-dict-to-ordereddict"">this SO post</a>:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt;
&gt;&gt;&gt; a = np.array( [ 1, 2, 3 ] )
&gt;&gt;&gt; b = np.array( [ 4, 5, 6 ] )
&gt;&gt;&gt; c = np.array( [ 7, 8, 9 ] )
&gt;&gt;&gt;
&gt;&gt;&gt; pd.DataFrame( OrderedDict( { 'a': pd.Series(a), 'b': pd.Series(b), 'c': pd.Series(c) } ) )

   a  c  b
0  1  7  4
1  2  8  5
2  3  9  6

&gt;&gt;&gt; pd.DataFrame( OrderedDict( (('a', pd.Series(a)), ('b', pd.Series(b)), ('c', pd.Series(c))) ) )

   a  b  c
0  1  4  7
1  2  5  8
2  3  6  9
</code></pre>
";;3;;2016-04-11T03:47:34.417;;36539513;2016-04-11T04:00:12.200;2017-05-23T10:31:33.227;;-1.0;;5550138.0;36539396.0;2;9;;;
51566;51566;;;"<p>When reading to and from your csv file include the argument index=False so for example</p>

<pre><code> df.to_csv(filename ,  index = False)
</code></pre>

<p>and to read from the csv</p>

<pre><code>df.read_csv(filename ,  index = False)  
</code></pre>

<p>This should prevent the issue so you don't need to fix it later.</p>
";;1;;2016-04-12T11:31:31.830;;36572039;2016-04-12T11:31:31.830;;;;;2033214.0;20107570.0;2;53;;;
51675;51675;;;"<p>This is much simple and easy way.</p>

<pre><code>import pandas
df = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')
# or using sheet index starting 0
df = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname=2)
</code></pre>

<p>check out documentation full details
<a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.read_excel.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.read_excel.html</a></p>
";;1;;2016-04-13T06:51:37.860;;36590692;2016-04-13T06:51:37.860;;;;;1328978.0;17063458.0;2;34;;;
52068;52068;;;"<p>Another way to do this is to reassign <code>df</code> based on a cross section of <code>df</code>, using the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html"">.xs</a> method.</p>

<pre><code>&gt;&gt;&gt; df

    a
    b   c
0   1   2
1   3   4

&gt;&gt;&gt; df = df.xs('a', axis=1, drop_level=True)

    # 'a' : key on which to get cross section
    # axis=1 : get cross section of column
    # drop_level=True : returns cross section without the multilevel index

&gt;&gt;&gt; df

    b   c
0   1   2
1   3   4
</code></pre>
";;0;;2016-04-17T21:57:21.957;;36682678;2016-04-17T21:57:21.957;;;;;3407933.0;22233488.0;2;15;;;
52089;52089;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""noreferrer""><code>loc</code></a> to get series which satisfying your condition and then <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html"" rel=""noreferrer""><code>iloc</code></a> to get first element:</p>

<pre><code>In [2]: df
Out[2]:
    A  B
0  p1  1
1  p1  2
2  p3  3
3  p2  4

In [3]: df.loc[df['B'] == 3, 'A']
Out[3]:
2    p3
Name: A, dtype: object

In [4]: df.loc[df['B'] == 3, 'A'].iloc[0]
Out[4]: 'p3'
</code></pre>
";;1;;2016-04-18T04:32:11.027;;36685531;2016-04-18T04:32:11.027;;;;;4542359.0;36684013.0;2;17;;;
52115;52115;;;"<p>I think you need first reset default header style, then you can change it:</p>

<pre><code>pd.core.format.header_style = None
</code></pre>

<p>All together:</p>

<pre><code>import pandas as pd

data = pd.DataFrame({'test_data': [1,2,3,4,5]})
writer = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')

pd.core.format.header_style = None

data.to_excel(writer, sheet_name='test', index=False)

workbook  = writer.book
worksheet = writer.sheets['test']

font_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})
header_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})

worksheet.set_column('A:A', None, font_fmt)
worksheet.set_row(0, None, header_fmt)

writer.save()
</code></pre>

<p>Explaining by <a href=""https://stackoverflow.com/questions/36694313/pandas-xlsxwriter-format-header/36694513?noredirect=1#comment60978389_36694513"">jmcnamara</a>, thank you:</p>

<p>In Excel a cell format overrides a row format overrides a column format.The <code>pd.core.format.header_style</code> is converted to a format and is applied to each cell in the header. As such the default cannot be overridden by <code>set_row()</code>. Setting <code>pd.core.format.header_style</code> to <code>None</code> means that the header cells don't have a user defined format and thus it can be overridden by <code>set_row()</code>.</p>

<p>EDIT: In version <code>0.18.1</code> you have to change </p>

<pre><code>pd.core.format.header_style = None
</code></pre>

<p>to:</p>

<pre><code>pd.formats.format.header_style = None
</code></pre>

<p>thanks <a href=""https://stackoverflow.com/questions/36694313/pandas-xlsxwriter-format-header/36694513?noredirect=1#comment62459317_36694513"">krvkir</a>.</p>
";;3;;2016-04-18T12:50:07.893;;36694513;2016-05-27T10:42:53.230;2017-05-23T11:54:41.213;;-1.0;;2901002.0;36694313.0;2;13;;;
52188;52188;;;"<p>you can also use <code>sort_values()</code>, <code>groupby()</code> and finally <code>cumcount() + 1</code>:</p>

<pre><code>df['RN'] = df.sort_values(['data1','data2'], ascending=[True,False]) \
             .groupby(['key1']) \
             .cumcount() + 1
print(df)
</code></pre>

<p>yields:</p>

<pre><code>   data1  data2 key1  RN
0      1      1    a   1
1      2     10    a   2
2      2      2    a   3
3      3      3    b   1
4      3     30    a   4
</code></pre>

<p>PS tested with pandas 0.18</p>
";;4;;2016-04-18T21:18:39.583;;36704460;2016-04-18T21:18:39.583;;;;;5741205.0;17775935.0;2;14;;;
52216;52216;;;"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.nlargest.html"" rel=""noreferrer""><code>nlargest</code></a> (New in <code>pandas</code> version <code>0.17.0</code>):</p>

<pre><code>print df
   0  Bytes  Client             Ip
0  1      1    1000   192.168.10.2
1  0      0    2000  192.168.10.12
2  2      2     500   192.168.10.4
3  3      3     159  192.168.10.56

print df.nlargest(3, 'Client')
   0  Bytes  Client             Ip
1  0      0    2000  192.168.10.12
0  1      1    1000   192.168.10.2
2  2      2     500   192.168.10.4
</code></pre>
";;0;;2016-04-19T05:50:14.513;;36709634;2016-04-19T05:50:14.513;;;;;2901002.0;16958499.0;2;10;;;
52219;52219;;;"<p>if you are using Jupyter Notebook, How about....</p>

<pre><code> %%timeit
 df.isnull().any().any()
</code></pre>

<p>or</p>

<pre><code> %timeit 
 df.isnull().values.sum()
</code></pre>

<p>or, are there anywhere NaNs in the data, if yes, where?</p>

<pre><code> df.isnull().any()
</code></pre>
";;0;;2016-04-19T06:23:08.170;;36710126;2016-04-19T06:23:08.170;;;;;4026992.0;26266362.0;2;10;;;
52290;52290;;;"<p>A sign of a good environment is many choices, so I'll add this from <a href=""http://blaze.readthedocs.org/en/latest/"" rel=""noreferrer"">Anaconda Blaze</a>, really using <a href=""https://pypi.python.org/pypi/odo/0.3.2"" rel=""noreferrer"">Odo</a></p>

<pre><code>import blaze as bz
import pandas as pd

df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':[2,4,6,8,10]})

for chunk in bz.odo(df, target=bz.chunks(pd.DataFrame), chunksize=2):
    # Do stuff with chunked dataframe
</code></pre>
";;0;;2016-04-19T17:41:17.103;;36725677;2016-04-19T17:41:17.103;;;;;4601439.0;25699439.0;2;6;;;
52783;52783;;;"<p>maybe this is most strightforward (most pythonic i guess):</p>

<pre><code>out.apply(pd.Series)
</code></pre>

<p>if you would want to rename the columns to something more meaningful, than:</p>

<pre><code>out.columns=['Kstats','Pvalue']
</code></pre>

<p>if you do not want the default name for the index:</p>

<pre><code>out.index.name=None
</code></pre>
";;1;;2016-04-23T21:40:15.413;;36816769;2016-04-23T21:40:15.413;;;;;4695774.0;22799300.0;2;10;;;
53146;53146;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.DataFrame.merge.html"" rel=""noreferrer""><code>merge</code></a> the 2 dfs using method 'outer' and pass param <code>indicator=True</code> this will tell you whether the rows are present in both/left only/right only, you can then filter the merged df after:</p>

<pre><code>In [22]:
merged = df1.merge(df2, indicator=True, how='outer')
merged[merged['_merge'] == 'right_only']

Out[22]:
  Buyer  Quantity      _merge
3  Carl         2  right_only
4  Mark         1  right_only
</code></pre>
";;3;;2016-04-27T15:03:01.577;;36893675;2016-04-27T15:23:32.190;2016-04-27T15:23:32.190;;704848.0;;704848.0;36891977.0;2;20;;;
53219;53219;;;"<p>yet another ways to do this:</p>

<pre><code>df['period'] = df['Year'].astype(str) + df['quarter']
</code></pre>

<p>or bit slower:</p>

<pre><code>df['period'] = df[['Year','quarter']].astype(str).sum(axis=1)
</code></pre>

<p>Let's test it on 200K rows DF:</p>

<pre><code>In [250]: df
Out[250]:
   Year quarter
0  2014      q1
1  2015      q2

In [251]: df = pd.concat([df] * 10**5)

In [252]: df.shape
Out[252]: (200000, 2)
</code></pre>

<p><strong>UPDATE:</strong> new timings using Pandas 0.19.0</p>

<p><strong>Timing</strong> without CPU/GPU optimization (sorted from fastest to slowest):</p>

<pre><code>In [107]: %timeit df['Year'].astype(str) + df['quarter']
10 loops, best of 3: 131 ms per loop

In [106]: %timeit df['Year'].map(str) + df['quarter']
10 loops, best of 3: 161 ms per loop

In [108]: %timeit df.Year.str.cat(df.quarter)
10 loops, best of 3: 189 ms per loop

In [109]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 567 ms per loop

In [110]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 584 ms per loop

In [111]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
1 loop, best of 3: 24.7 s per loop
</code></pre>

<p><strong>Timing</strong> using CPU/GPU optimization:</p>

<pre><code>In [113]: %timeit df['Year'].astype(str) + df['quarter']
10 loops, best of 3: 53.3 ms per loop

In [114]: %timeit df['Year'].map(str) + df['quarter']
10 loops, best of 3: 65.5 ms per loop

In [115]: %timeit df.Year.str.cat(df.quarter)
10 loops, best of 3: 79.9 ms per loop

In [116]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 230 ms per loop

In [117]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 230 ms per loop

In [118]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
1 loop, best of 3: 9.38 s per loop
</code></pre>
";;7;;2016-04-28T10:02:08.850;;36911306;2016-10-10T17:44:27.647;2016-10-10T17:44:27.647;;5741205.0;;5741205.0;19377969.0;2;49;;;
53273;53273;;;"<p>The <code>or</code> and <code>and</code> python statements require <code>truth</code>-values. For <code>pandas</code> these are considered ambiguous so you should use ""bitwise"" <code>|</code> (or) or <code>&amp;</code> (and) operations:</p>

<pre><code>result = result[(result['var']&gt;0.25) | (result['var']&lt;-0.25)]
</code></pre>

<p>These are overloaded for these kind of datastructures to yield the element-wise <code>or</code> (or <code>and</code>).</p>

<hr>

<p>Just to add some more explanation to this statement:</p>

<p>The exception is thrown when you want to get the <code>bool</code> of a <code>pandas.Series</code>:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; x = pd.Series([1])
&gt;&gt;&gt; bool(x)
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>What you hit was a place where the operator <strong>implicitly</strong> converted the operands to <code>bool</code> (you used <code>or</code> but it also happens for <code>and</code>, <code>if</code> and <code>while</code>):</p>

<pre><code>&gt;&gt;&gt; x or x
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; x and x
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; if x:
...     print('fun')
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; while x:
...     print('fun')
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>Besides these 4 statements there are several python functions that hide some <code>bool</code> calls (like <code>any</code>, <code>all</code>, <code>filter</code>, ...) these are normally not problematic with <code>pandas.Series</code> but for completeness I wanted to mention these.</p>

<hr>

<p>In your case the exception isn't really helpful, because it doesn't mention the <strong>right alternatives</strong>. For <code>and</code> and <code>or</code> you can use (if you want element-wise comparisons):</p>

<ul>
<li><p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_or.html""><code>numpy.logical_or</code></a>:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.logical_or(x, y)
</code></pre>

<p>or simply the <code>|</code> operator:</p>

<pre><code>&gt;&gt;&gt; x | y
</code></pre></li>
<li><p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_and.html""><code>numpy.logical_and</code></a>:</p>

<pre><code>&gt;&gt;&gt; np.logical_and(x, y)
</code></pre>

<p>or simply the <code>&amp;</code> operator:</p>

<pre><code>&gt;&gt;&gt; x &amp; y
</code></pre></li>
</ul>

<p>If you're using the operators then make sure you set your parenthesis correctly because of <a href=""https://docs.python.org/reference/expressions.html#operator-precedence"">the operator precedence</a>.</p>

<p>There are <a href=""https://docs.scipy.org/doc/numpy/reference/routines.logic.html"">several logical numpy functions</a> which <em>should</em> work on <code>pandas.Series</code>.</p>

<hr>

<p>The alternatives mentioned in the Exception are more suited if you encountered it when doing <code>if</code> or <code>while</code>. I'll shortly explain each of these:</p>

<ul>
<li><p>If you want to check if your Series is <strong>empty</strong>:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([])
&gt;&gt;&gt; x.empty
True
&gt;&gt;&gt; x = pd.Series([1])
&gt;&gt;&gt; x.empty
False
</code></pre>

<p>Python normally interprets the <code>len</code>gth of containers (like <code>list</code>, <code>tuple</code>, ...) as truth-value if it has no explicit boolean interpretation. So if you want the python-like check, you could do: <code>if x.size</code> or <code>if not x.empty</code> instead of <code>if x</code>.</p></li>
<li><p>If your <code>Series</code> contains <strong>one and only one</strong> boolean value:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([100])
&gt;&gt;&gt; (x &gt; 50).bool()
True
&gt;&gt;&gt; (x &lt; 50).bool()
False
</code></pre></li>
<li><p>If you want to check the <strong>first and only item</strong> of your Series (like <code>.bool()</code> but works even for not boolean contents):</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([100])
&gt;&gt;&gt; x.item()
100
</code></pre></li>
<li><p>If you want to check if <strong>all</strong> or <strong>any</strong> item is not-zero, not-empty or not-False:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([0, 1, 2])
&gt;&gt;&gt; x.all()   # because one element is zero
False
&gt;&gt;&gt; x.any()   # because one (or more) elements are non-zero
True
</code></pre></li>
</ul>
";;0;;2016-04-28T17:54:30.553;;36922103;2017-01-01T18:50:10.940;2017-01-01T18:50:10.940;;5393381.0;;5393381.0;36921951.0;2;48;;;
53274;53274;;;"<p>For boolean logic, use <code>&amp;</code> and <code>|</code>.</p>

<pre><code>np.random.seed(0)
df = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))

&gt;&gt;&gt; df
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
2  0.950088 -0.151357 -0.103219
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863

&gt;&gt;&gt; df.loc[(df.C &gt; 0.25) | (df.C &lt; -0.25)]
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863
</code></pre>

<p>To see what is happening, you get a column of booleans for each comparison, e.g.</p>

<pre><code>df.C &gt; 0.25
0     True
1    False
2    False
3     True
4     True
Name: C, dtype: bool
</code></pre>

<p>When you have multiple criteria, you will get multiple columns returned.  This is why the the join logic is ambiguous.  Using <code>and</code> or <code>or</code> treats each column separately, so you first need to reduce that column to a single boolean value.  For example, to see if any value or all values in each of the columns is True.</p>

<pre><code># Any value in either column is True?
(df.C &gt; 0.25).any() or (df.C &lt; -0.25).any()
True

# All values in either column is True?
(df.C &gt; 0.25).all() or (df.C &lt; -0.25).all()
False
</code></pre>

<p>One convoluted way to achieve the same thing is to zip all of these columns together, and perform the appropriate logic.</p>

<pre><code>&gt;&gt;&gt; df[[any([a, b]) for a, b in zip(df.C &gt; 0.25, df.C &lt; -0.25)]]
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863
</code></pre>

<p>For more details, refer to <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">Boolean Indexing</a> in the docs.</p>
";;0;;2016-04-28T18:15:59.950;;36922486;2016-04-28T18:23:59.073;2016-04-28T18:23:59.073;;2411802.0;;2411802.0;36921951.0;2;7;;;
53489;53489;;;"<p>Simply do:</p>

<pre><code>grouped_df = df.groupby('A')

for key, item in grouped_df:
    print grouped_df.get_group(key), ""\n\n""
</code></pre>

<p>This also works,</p>

<pre><code>grouped_df = df.groupby('A')    
gb = grouped_df.groups

for key, values in gb.iteritems():
    print df.ix[values], ""\n\n""
</code></pre>

<p><strong>For selective key grouping:</strong> Insert the keys you want inside the key_list_from_gb, in following, using gb.keys(): For Example,</p>

<pre><code>gb = grouped_df.groups
gb.keys()

key_list_from_gb = [key1, key2, key3]

for key, values in gb.iteritems():
    if key in key_list_from_gb:
        print df.ix[values], ""\n""
</code></pre>
";;2;;2016-04-30T06:59:52.403;;36951842;2016-04-30T06:59:52.403;;;;;5424918.0;22691010.0;2;20;;;
53504;53504;;;"<p>As of version 0.11.0, columns <em>can be</em> sliced in the manner you tried using the <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#selection-choices"" rel=""noreferrer""><code>.loc</code></a> indexer: </p>

<pre><code>df.loc[:, 'C':'E']
</code></pre>

<p>returns columns <code>C</code> through <code>E</code>.</p>

<hr>

<p>A demo on a randomly generated DataFrame:</p>

<pre><code>import pandas as pd
import numpy as np
np.random.seed(5)
df = pd.DataFrame(np.random.randint(100, size=(100, 6)), 
                  columns=list('ABCDEF'), 
                  index=['R{}'.format(i) for i in range(100)])
df.head()

Out: 
     A   B   C   D   E   F
R0  99  78  61  16  73   8
R1  62  27  30  80   7  76
R2  15  53  80  27  44  77
R3  75  65  47  30  84  86
R4  18   9  41  62   1  82
</code></pre>

<p>To get the columns from C to E (note that unlike integer slicing, 'E' is included in the columns):</p>

<pre><code>df.loc[:, 'C':'E']

Out: 
      C   D   E
R0   61  16  73
R1   30  80   7
R2   80  27  44
R3   47  30  84
R4   41  62   1
R5    5  58   0
...
</code></pre>

<p>Same works for selecting rows based on labels. Get the rows 'R6' to 'R10' from those columns:</p>

<pre><code>df.loc['R6':'R10', 'C':'E']

Out: 
      C   D   E
R6   51  27  31
R7   83  19  18
R8   11  67  65
R9   78  27  29
R10   7  16  94
</code></pre>

<p><code>.loc</code> also accepts a boolean array so you can select the columns whose corresponding entry in the array is <code>True</code>. For example, <code>df.columns.isin(list('BCD'))</code> returns <code>array([False,  True,  True,  True, False, False], dtype=bool)</code> - True if the column name is in the list <code>['B', 'C', 'D']</code>; False, otherwise.</p>

<pre><code>df.loc[:, df.columns.isin(list('BCD'))]

Out: 
      B   C   D
R0   78  61  16
R1   27  30  80
R2   53  80  27
R3   65  47  30
R4    9  41  62
R5   78   5  58
...
</code></pre>
";;0;;2016-04-30T12:39:08.817;;36955053;2017-04-29T16:14:38.213;2017-04-29T16:14:38.213;;2285236.0;;2285236.0;11285613.0;2;24;;;
53516;53516;;;"<p>Slight variation:</p>

<pre><code>w.female.replace(['male', 'female'], [1, 0], inplace=True)
</code></pre>
";;0;;2016-04-30T16:34:40.357;;36957431;2016-04-30T16:34:40.357;;;;;5878756.0;23307301.0;2;6;;;
53521;53521;;;"<p>from version 0.16.1 you can do </p>

<pre><code>df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')
</code></pre>
";;1;;2016-04-30T18:57:48.920;;36958937;2016-10-21T21:20:09.157;2016-10-21T21:20:09.157;;1218980.0;;3156200.0;13411544.0;2;33;;;
53746;53746;;;"<p>The actual question posed, missed by most answers here is:</p>

<h3>Why can't I use <code>del df.column_name</code>?</h3>

<p>At first we need to understand the problem, which requires us to dive into <a href=""http://www.rafekettler.com/magicmethods.html""><em>python magic methods</em></a>.</p>

<p>As Wes points out in his answer <code>del df['column']</code> maps to the python <em>magic method</em> <code>df.__delitem__('column')</code> which is <a href=""https://github.com/pydata/pandas/blob/c6110e25b3eceb2f25022c2aa9ccea03c0b8b359/pandas/core/generic.py#L1580"">implemented in pandas to drop the column</a></p>

<p>However, as pointed out in the link above about <a href=""http://www.rafekettler.com/magicmethods.html""><em>python magic methods</em></a>:</p>

<blockquote>
  <p>In fact, <strong>del</strong> should almost never be used because of the precarious circumstances under which it is called; use it with caution!</p>
</blockquote>

<p>You could argue that <code>del df['column_name']</code> should not be used or encouraged, and thereby <code>del df.column_name</code> should not even be considered.</p>

<p>However, in theory, <code>del df.column_name</code> could be implemeted to work in pandas using <a href=""http://www.rafekettler.com/magicmethods.html#access"">the <em>magic method <code>__delattr__</code></em></a>. This does however introduce certain problems, problems which the <code>del df['column_name']</code> implementation already has, but in lesser degree.</p>

<h2>Example Problem</h2>

<p>What if I define a column in a dataframe called ""dtypes"" or ""columns"".</p>

<p>Then assume I want to delete these columns.</p>

<p><code>del df.dtypes</code> would make the <code>__delattr__</code> method confused as if it should delete the ""dtypes"" attribute or the ""dtypes"" column.</p>

<h2>Architectural questions behind this problem</h2>

<ol>
<li>Is a dataframe a
collection of <em>columns</em>?</li>
<li>Is a dataframe a collection of <em>rows</em>?</li>
<li>Is a column an <em>attribute</em> of a dataframe?</li>
</ol>

<h3>Pandas answers:</h3>

<ol>
<li>Yes, in all ways</li>
<li>No, but if you want it to be, you can use the <code>.ix</code>, <code>.loc</code> or <code>.iloc</code> methods.</li>
<li>Maybe, do you want to <em>read</em> data? Then <strong>yes</strong>, <em>unless</em> the name of the attribute is already taken by another attribute belonging to the dataframe. Do you want to <em>modify</em> data? Then <strong>no</strong>.</li>
</ol>

<h1>TLDR;</h1>

<p>You cannot do <code>del df.column_name</code> because pandas has a quite wildly grown architecture that needs to be reconsidered in order for this kind of <em>cognitive dissonance</em> not to occur to its users.</p>

<h3>Protip:</h3>

<p>Don't use df.column_name, It may be pretty, but it causes <em>cognitive dissonance</em></p>

<h3>Zen of Python quotes that fits in here:</h3>

<p>There are multiple ways of deleting a column.</p>

<blockquote>
  <p>There should be one-- and preferably only one --obvious way to do it.</p>
</blockquote>

<p>Columns are sometimes attributes but sometimes not.</p>

<blockquote>
  <p>Special cases aren't special enough to break the rules.</p>
</blockquote>

<p>Does <code>del df.dtypes</code> delete the dtypes attribute or the dtypes column?</p>

<blockquote>
  <p>In the face of ambiguity, refuse the temptation to guess.</p>
</blockquote>
";;1;;2016-05-03T09:48:51.933;;37000877;2016-05-03T10:01:14.760;2016-05-03T10:01:14.760;;3730397.0;;3730397.0;13411544.0;2;44;;;
53808;53808;;;"<p>Here are results of my read and write comparison for the DF (shape: 4000000 x 6, size in memory 183.1 MB, size of uncompressed CSV - 492 MB).</p>

<p>Comparison for the following storage formats: (<code>CSV</code>, <code>CSV.gzip</code>, <code>Pickle</code>, <code>HDF5</code> [various compression]):</p>

<pre><code>                  read_s  write_s  size_ratio_to_CSV
storage
CSV               17.900    69.00              1.000
CSV.gzip          18.900   186.00              0.047
Pickle             0.173     1.77              0.374
HDF_fixed          0.196     2.03              0.435
HDF_tab            0.230     2.60              0.437
HDF_tab_zlib_c5    0.845     5.44              0.035
HDF_tab_zlib_c9    0.860     5.95              0.035
HDF_tab_bzip2_c5   2.500    36.50              0.011
HDF_tab_bzip2_c9   2.500    36.50              0.011
</code></pre>

<p>reading</p>

<p><a href=""https://i.stack.imgur.com/f7liH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f7liH.png"" alt=""enter image description here""></a></p>

<p>writing/saving</p>

<p><a href=""https://i.stack.imgur.com/yM1NB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yM1NB.png"" alt=""enter image description here""></a></p>

<p>file size ratio in relation to uncompressed CSV file</p>

<p><a href=""https://i.stack.imgur.com/2DDyv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2DDyv.png"" alt=""enter image description here""></a></p>

<p><strong>RAW DATA:</strong></p>

<p>CSV:</p>

<pre><code>In [68]: %timeit df.to_csv(fcsv)
1 loop, best of 3: 1min 9s per loop

In [74]: %timeit pd.read_csv(fcsv)
1 loop, best of 3: 17.9 s per loop
</code></pre>

<p>CSV.gzip:</p>

<pre><code>In [70]: %timeit df.to_csv(fcsv_gz, compression='gzip')
1 loop, best of 3: 3min 6s per loop

In [75]: %timeit pd.read_csv(fcsv_gz)
1 loop, best of 3: 18.9 s per loop
</code></pre>

<p>Pickle:</p>

<pre><code>In [66]: %timeit df.to_pickle(fpckl)
1 loop, best of 3: 1.77 s per loop

In [72]: %timeit pd.read_pickle(fpckl)
10 loops, best of 3: 173 ms per loop
</code></pre>

<p>HDF (<code>format='fixed'</code>) [Default]:</p>

<pre><code>In [67]: %timeit df.to_hdf(fh5, 'df')
1 loop, best of 3: 2.03 s per loop

In [73]: %timeit pd.read_hdf(fh5, 'df')
10 loops, best of 3: 196 ms per loop
</code></pre>

<p>HDF (<code>format='table'</code>):</p>

<pre><code>In [37]: %timeit df.to_hdf('D:\\temp\\.data\\37010212_tab.h5', 'df', format='t')
1 loop, best of 3: 2.6 s per loop

In [38]: %timeit pd.read_hdf('D:\\temp\\.data\\37010212_tab.h5', 'df')
1 loop, best of 3: 230 ms per loop
</code></pre>

<p>HDF (<code>format='table', complib='zlib', complevel=5</code>):</p>

<pre><code>In [40]: %timeit df.to_hdf('D:\\temp\\.data\\37010212_tab_compress_zlib5.h5', 'df', format='t', complevel=5, complib='zlib')
1 loop, best of 3: 5.44 s per loop

In [41]: %timeit pd.read_hdf('D:\\temp\\.data\\37010212_tab_compress_zlib5.h5', 'df')
1 loop, best of 3: 854 ms per loop
</code></pre>

<p>HDF (<code>format='table', complib='zlib', complevel=9</code>):</p>

<pre><code>In [36]: %timeit df.to_hdf('D:\\temp\\.data\\37010212_tab_compress_zlib9.h5', 'df', format='t', complevel=9, complib='zlib')
1 loop, best of 3: 5.95 s per loop

In [39]: %timeit pd.read_hdf('D:\\temp\\.data\\37010212_tab_compress_zlib9.h5', 'df')
1 loop, best of 3: 860 ms per loop
</code></pre>

<p>HDF (<code>format='table', complib='bzip2', complevel=5</code>):</p>

<pre><code>In [42]: %timeit df.to_hdf('D:\\temp\\.data\\37010212_tab_compress_bzip2_l5.h5', 'df', format='t', complevel=5, complib='bzip2')
1 loop, best of 3: 36.5 s per loop

In [43]: %timeit pd.read_hdf('D:\\temp\\.data\\37010212_tab_compress_bzip2_l5.h5', 'df')
1 loop, best of 3: 2.5 s per loop
</code></pre>

<p>HDF (<code>format='table', complib='bzip2', complevel=9</code>):</p>

<pre><code>In [42]: %timeit df.to_hdf('D:\\temp\\.data\\37010212_tab_compress_bzip2_l9.h5', 'df', format='t', complevel=9, complib='bzip2')
1 loop, best of 3: 36.5 s per loop

In [43]: %timeit pd.read_hdf('D:\\temp\\.data\\37010212_tab_compress_bzip2_l9.h5', 'df')
1 loop, best of 3: 2.5 s per loop
</code></pre>

<p>PS i can't test <code>feather</code> on my <em>Windows</em> notebook</p>

<p>DF info:</p>

<pre><code>In [49]: df.shape
Out[49]: (4000000, 6)

In [50]: df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 4000000 entries, 0 to 3999999
Data columns (total 6 columns):
a    datetime64[ns]
b    datetime64[ns]
c    datetime64[ns]
d    datetime64[ns]
e    datetime64[ns]
f    datetime64[ns]
dtypes: datetime64[ns](6)
memory usage: 183.1 MB

In [41]: df.head()
Out[41]:
                    a                   b                   c  \
0 1970-01-01 00:00:00 1970-01-01 00:01:00 1970-01-01 00:02:00
1 1970-01-01 00:01:00 1970-01-01 00:02:00 1970-01-01 00:03:00
2 1970-01-01 00:02:00 1970-01-01 00:03:00 1970-01-01 00:04:00
3 1970-01-01 00:03:00 1970-01-01 00:04:00 1970-01-01 00:05:00
4 1970-01-01 00:04:00 1970-01-01 00:05:00 1970-01-01 00:06:00

                    d                   e                   f
0 1970-01-01 00:03:00 1970-01-01 00:04:00 1970-01-01 00:05:00
1 1970-01-01 00:04:00 1970-01-01 00:05:00 1970-01-01 00:06:00
2 1970-01-01 00:05:00 1970-01-01 00:06:00 1970-01-01 00:07:00
3 1970-01-01 00:06:00 1970-01-01 00:07:00 1970-01-01 00:08:00
4 1970-01-01 00:07:00 1970-01-01 00:08:00 1970-01-01 00:09:00
</code></pre>

<p>File sizes:</p>

<pre><code>{ .data }   ls -lh 37010212.*                                                                          /d/temp/.data
-rw-r--r-- 1 Max None 492M May  3 22:21 37010212.csv
-rw-r--r-- 1 Max None  23M May  3 22:19 37010212.csv.gz
-rw-r--r-- 1 Max None 214M May  3 22:02 37010212.h5
-rw-r--r-- 1 Max None 184M May  3 22:02 37010212.pickle
-rw-r--r-- 1 Max None 215M May  4 10:39 37010212_tab.h5
-rw-r--r-- 1 Max None 5.4M May  4 10:46 37010212_tab_compress_bzip2_l5.h5
-rw-r--r-- 1 Max None 5.4M May  4 10:51 37010212_tab_compress_bzip2_l9.h5
-rw-r--r-- 1 Max None  17M May  4 10:42 37010212_tab_compress_zlib5.h5
-rw-r--r-- 1 Max None  17M May  4 10:36 37010212_tab_compress_zlib9.h5
</code></pre>

<p><strong>Conclusion:</strong></p>

<p><code>Pickle</code> and <code>HDF5</code> are much faster, but <code>HDF5</code> is more convenient - you can store multiple tables/frames inside, you can read your data conditionally (look at <code>where</code> parameter in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_hdf.html"" rel=""nofollow noreferrer"">read_hdf()</a>), you can also store your data compressed (<code>zlib</code> - is faster, <code>bzip2</code> - provides better compression ratio), etc.</p>

<p>PS if you can build/use <code>feather-format</code> - it should be even faster compared to <code>HDF5</code> and <code>Pickle</code></p>

<p><strong>PPS:</strong> don't use Pickle for big data frames, as you may end up with <a href=""https://stackoverflow.com/questions/37553397/python-applymap-taking-time-to-run"">SystemError: error return without exception set</a> error message. It's also described <a href=""https://github.com/pydata/pandas/issues/12712"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/pydata/pandas/issues/3699"" rel=""nofollow noreferrer"">here</a>.</p>
";;1;;2016-05-03T18:45:37.427;;37012035;2016-06-05T22:05:53.170;2017-05-23T12:25:54.457;;-1.0;;5741205.0;37010212.0;2;19;;;
53959;53959;;;"<p>To convert a pandas dataframe (df) to a numpy ndarray, use this code:</p>

<p><code>df=df.values</code></p>

<p>df now becomes a numpy ndarray.</p>
";;2;;2016-05-05T05:29:51.800;;37043071;2016-05-05T05:29:51.800;;;;;4666038.0;13187778.0;2;65;;;
54082;54082;;;"<p>The by far the simplest approach is:</p>

<pre><code>yourdf.drop(['columnheading1', 'columnheading2'], axis=1, inplace=True)
</code></pre>
";;4;;2016-05-06T10:08:57.603;;37069701;2016-05-07T10:21:43.070;2016-05-07T10:21:43.070;;5853731.0;;5853731.0;28538536.0;2;27;;;
54254;54254;;;"<p>You can also pass to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">DataFrame.drop</a> the <em>label itself</em> (instead of Series of index labels):</p>

<pre><code>In[17]: df
Out[17]: 
            a         b         c         d         e
one  0.456558 -2.536432  0.216279 -1.305855 -0.121635
two -1.015127 -0.445133  1.867681  2.179392  0.518801

In[18]: df.drop('one')
Out[18]: 
            a         b         c         d         e
two -1.015127 -0.445133  1.867681  2.179392  0.518801
</code></pre>

<p>Which is equivalent to:</p>

<pre><code>In[19]: df.drop(df.index[[0]])
Out[19]: 
            a         b         c         d         e
two -1.015127 -0.445133  1.867681  2.179392  0.518801
</code></pre>
";;2;;2016-05-08T08:28:42.613;;37097791;2016-05-08T08:28:42.613;;;;;6025342.0;14661701.0;2;21;;;
54274;54274;;;"<p>In version <code>0.18.1</code> you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"" rel=""noreferrer""><code>to_datetime</code></a>, but:</p>

<ul>
<li>names of columns have to be <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>minute</code> and <code>second</code>:</li>
<li>minimal columns are <code>year</code>, <code>month</code> and <code>day</code></li>
</ul>

<p>Sample:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'year': [2015, 2016],
                   'month': [2, 3],
                    'day': [4, 5],
                    'hour': [2, 3],
                    'minute': [10, 30],
                    'second': [21,25]})

print df
   day  hour  minute  month  second  year
0    4     2      10      2      21  2015
1    5     3      30      3      25  2016

print pd.to_datetime(df[['year', 'month', 'day']])
0   2015-02-04
1   2016-03-05
dtype: datetime64[ns]

print pd.to_datetime(df[['year', 'month', 'day', 'hour']])
0   2015-02-04 02:00:00
1   2016-03-05 03:00:00
dtype: datetime64[ns]

print pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])
0   2015-02-04 02:10:00
1   2016-03-05 03:30:00
dtype: datetime64[ns]

print pd.to_datetime(df)
0   2015-02-04 02:10:21
1   2016-03-05 03:30:25
dtype: datetime64[ns]
</code></pre>

<p>Another solution is convert to <code>dictionary</code>:</p>

<pre><code>print df
   M  D     Y  Apples  Oranges
0  5  6  1990      12        3
1  5  7  1990      14        4
2  5  8  1990      15       34
3  5  9  1990      23       21

print pd.to_datetime(dict(year=df.Y, month=df.M, day=df.D))
0   1990-05-06
1   1990-05-07
2   1990-05-08
3   1990-05-09
dtype: datetime64[ns]
</code></pre>
";;0;;2016-05-08T18:06:27.143;;37103131;2016-05-08T18:06:27.143;;;;;2901002.0;19350806.0;2;12;;;
54793;54793;;;"<p>If you don't mind importing the <code>sklearn</code> library, I would recommend the method talked on <a href=""https://web.archive.org/web/20160520170701/http://chrisalbon.com:80/python/pandas_normalize_column.html"" rel=""noreferrer"">this</a> blog.</p>

<pre><code>import pandas as pd
from sklearn import preprocessing

data = {'score': [234,24,14,27,-74,46,73,-18,59,160]}
df = pd.DataFrame(data)
df

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df)
df_normalized = pd.DataFrame(np_scaled)
df_normalized
</code></pre>
";;3;;2016-05-13T00:48:06.180;;37199623;2017-02-06T23:33:43.820;2017-02-06T23:33:43.820;;554319.0;;309830.0;12525722.0;2;34;;;
54909;54909;;;"<p>Since pandas represents timestamps in nanosecond resolution, the timespan that can be represented using a 64-bit integer is limited to approximately 584 years</p>

<pre><code>pd.Timestamp.min
Out[54]: Timestamp('1677-09-22 00:12:43.145225')

In [55]: pd.Timestamp.max
Out[55]: Timestamp('2262-04-11 23:47:16.854775807')
</code></pre>

<p>And your value is out of this range 2262-05-01 00:00:00 and hence the outofbounds error</p>

<p>Straight out of: <a href=""http://pandas-docs.github.io/pandas-docs-travis/timeseries.html#timestamp-limitations"" rel=""noreferrer"">http://pandas-docs.github.io/pandas-docs-travis/timeseries.html#timestamp-limitations</a></p>
";;3;;2016-05-14T12:29:02.760;;37226672;2016-05-14T12:29:02.760;;;;;786326.0;32888124.0;2;12;;;
55198;55198;;;"<p>Another succinct way of doing this is <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.clip.html"" rel=""noreferrer"">pandas.DataFrame.clip</a>.</p>

<p>For example:</p>

<pre><code>In [20]: df = DataFrame({'a': [-1, 100, -2]})

In [21]: df
Out[21]: 
     a
0   -1
1  100
2   -2

In [22]: df.clip(lower=0)
Out[22]: 
     a
0    0
1  100
2    0
</code></pre>

<p>There's also <code>df.clip_lower(0)</code>.</p>
";;0;;2016-05-17T17:41:20.877;;37282998;2016-05-17T17:41:20.877;;;;;782063.0;27759084.0;2;7;;;
55254;55254;;;"<p>Firstly, easiest way to one hot encode: use Sklearn.</p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a></p>

<p>Secondly, I don't think using pandas to one hot encode is that simple (unconfirmed though)</p>

<p><a href=""https://stackoverflow.com/questions/11587782/creating-dummy-variables-in-pandas-for-python"">Creating dummy variables in pandas for python</a></p>

<p>Lastly, is it necessary for you to one hot encode? One hot encoding exponentially increases the number of features, drastically increasing the run time of any classifier or anything else you are going to run. Especially when each categorical feature has many levels. Instead you can do dummy coding.</p>

<p>Using dummy encoding usually works well, for much less run time and complexity. A wise prof once told me, 'Less is More'. </p>

<p>Here's the code for my custom encoding function if you want.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder

#Auto encodes any dataframe column of type category or object.
def dummyEncode(df):
        columnsToEncode = list(df.select_dtypes(include=['category','object']))
        le = LabelEncoder()
        for feature in columnsToEncode:
            try:
                df[feature] = le.fit_transform(df[feature])
            except:
                print('Error encoding '+feature)
        return df
</code></pre>

<p>EDIT: Comparison to be clearer:</p>

<p>One-hot encoding: convert n levels to n-1 columns.</p>

<pre><code>Index  Animal         Index  cat  mouse
  1     dog             1     0     0
  2     cat       --&gt;   2     1     0
  3    mouse            3     0     1
</code></pre>

<p>You can see how this will explode your memory if you have many different types (or levels) in your categorical feature. Keep in mind, this is just ONE column.</p>

<p>Dummy Coding:</p>

<pre><code>Index  Animal         Index  Animal
  1     dog             1      0   
  2     cat       --&gt;   2      1 
  3    mouse            3      2
</code></pre>

<p>Convert to numerical representations instead. Greatly saves feature space, at the cost of a bit of accuracy.</p>
";;10;;2016-05-18T07:46:34.593;;37293283;2016-05-18T09:59:01.910;2017-05-23T12:26:42.257;;-1.0;;5766416.0;37292872.0;2;9;;;
55405;55405;;;"<p>From pandas version 0.18+ filtering a series can also be done as below</p>

<pre><code>    test = {
383:    3.000000,
663:    1.000000,
726:    1.000000,
737:    9.000000,
833:    8.166667
}

pd.Series(test).where(lambda x : x!=1).dropna()
</code></pre>

<p>Checkout:
<a href=""http://pandas.pydata.org/pandas-docs/version/0.18.1/whatsnew.html#method-chaininng-improvements"">http://pandas.pydata.org/pandas-docs/version/0.18.1/whatsnew.html#method-chaininng-improvements</a></p>
";;1;;2016-05-19T15:17:05.660;;37327506;2016-05-19T15:17:05.660;;;;;3450487.0;28272137.0;2;11;;;
55516;55516;;;"<p>Simply use this for printing entire frames:</p>

<pre><code>pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
</code></pre>

<p>Also you can create a pretty-print function with context manager, like in examples above.</p>

<p>For full list of useful options, see:</p>

<pre><code>pd.describe_option('display')
</code></pre>

<p>Few examples of use:
<a href=""http://pandas.pydata.org/pandas-docs/stable/options.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/options.html</a></p>
";;2;;2016-05-20T13:18:54.233;;37347783;2017-03-24T15:51:21.013;2017-03-24T15:51:21.013;;3338479.0;;3338479.0;19124601.0;2;14;;;
55725;55725;;;"<p>To index-based access to the pandas table, one can also consider <em>numpy.as_array</em> option to convert the table to Numpy array as</p>

<pre><code>np_df = df.as_matrix()
</code></pre>

<p>and then</p>

<pre><code>np_df[i] 
</code></pre>

<p>would work.</p>
";;0;;2016-05-23T06:53:06.950;;37384347;2016-05-23T06:53:06.950;;;;;5970782.0;16096627.0;2;6;;;
55973;55973;;;"<p>Using transpose and reindex</p>

<pre><code>import pandas as pd

cats = ['a', 'b', 'c']
df = pd.DataFrame({'cat': ['a', 'b', 'a']})

dummies = pd.get_dummies(df, prefix='', prefix_sep='')
dummies = dummies.T.reindex(cats).T.fillna(0)

print dummies

    a    b    c
0  1.0  0.0  0.0
1  0.0  1.0  0.0
2  1.0  0.0  0.0
</code></pre>
";;5;;2016-05-25T02:49:08.253;;37426982;2016-05-25T02:49:08.253;;;;;2336654.0;37425961.0;2;8;;;
56053;56053;;;"<p>There is a new index method called <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.difference.html""><code>difference</code></a>. It returns the original columns, with the columns passed as argument removed. </p>

<pre><code>df2 = df[df.columns.difference(['B', 'D'])]
</code></pre>

<p>Here, the output is used to filter out columns <code>B</code> and <code>D</code> from <code>df</code>.</p>
";;4;;2016-05-25T15:12:27.937;;37441204;2016-08-16T08:34:48.393;2016-08-16T08:34:48.393;;5276797.0;;5276797.0;14940743.0;2;45;;;
56059;56059;;;"<p>If you need to know how many ""1 or more"" rows have NaNs:</p>

<pre><code>df.isnull().T.any().T.sum()
</code></pre>

<p>Or if you need to pull out these rows and examine them:</p>

<pre><code>nan_rows = df[df.isnull().T.any().T]
</code></pre>
";;0;;2016-05-25T16:17:45.180;;37442692;2016-05-25T16:17:45.180;;;;;623735.0;29530232.0;2;11;;;
56114;56114;;;"<p>I wasn't fully satisfied with some other GUIs, so I created my own, which I'm now maintaining <a href=""https://github.com/bluenote10/PandasDataFrameGUI"" rel=""noreferrer"">on Github</a>. Example:</p>

<p><a href=""https://i.stack.imgur.com/rtB25.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rtB25.png"" alt=""enter image description here""></a></p>

<p>Apart from the basic table + plot functionality, I wanted to have a specific way to filter data:</p>

<ul>
<li>select a column to filter from a combo box</li>
<li>write an ""underscore expression"" to filter on that column using arbitrary Python code. For example: <code>_ &gt; 0</code> to filter positive values only, or more complex expressions like <code>(_ &gt;= date(2016, 1, 1)) &amp; (_ &lt;= date(2016, 1, 31))</code> e.g. for datetime columns.</li>
</ul>
";;4;;2016-05-25T21:01:08.440;;37447530;2016-05-25T21:01:08.440;;;;;1804173.0;10636024.0;2;17;;;
56149;56149;;;"<blockquote>
  <p>is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s?</p>
</blockquote>

<p>Yes, there is! Pandas has a special type of Series just for <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"" rel=""noreferrer"">categorical data</a>. One of the attributes of this series is the possible categories, which <code>get_dummies</code> takes into account. Here's an example:</p>

<pre><code>In [1]: import pandas as pd

In [2]: cat=pd.Series(list('aba'),index=range(1,4))

In [3]: cat=cat.astype('category',categories=list('abc'))

In [4]: cat
Out[4]: 
1    a
2    b
3    a
dtype: category
Categories (3, object): [a, b, c]
</code></pre>

<p>Then, <code>get_dummies</code> will do exactly what you want!</p>

<pre><code>In [5]: pd.get_dummies(cat)
Out[5]: 
   a  b  c
1  1  0  0
2  0  1  0
3  1  0  0
</code></pre>

<p>There are a bunch of other ways to create a categorical <code>Series</code> or <code>DataFrame</code>, this is just the one I find most convenient. You can read about all of them in <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"" rel=""noreferrer"">the pandas documentation</a>.</p>

<p><strong>EDIT:</strong></p>

<p>I haven't followed the exact versioning, but there was a <a href=""https://github.com/pydata/pandas/issues/10627"" rel=""noreferrer"">bug</a> in how pandas treats sparse matrices, at least until version 0.17.0. It was corrected by version 0.18.1. </p>

<p>For version 0.17.0, if you try to do this with the <code>sparse=True</code> option with a <code>DataFrame</code>, the column of zeros for the missing dummy variable will be a column of <code>NaN</code>, and it will be converted to dense.</p>
";;4;;2016-05-26T04:53:38.597;;37451867;2016-06-25T17:25:51.830;2016-06-25T17:25:51.830;;3358599.0;;3358599.0;37425961.0;2;15;;;
56161;56161;;;"<h1>Why it does not work</h1>

<p>There is no datetime dtype to be set for read_csv as csv files can only contain strings, integers and floats.</p>

<p>Setting a dtype to datetime will make pandas interpret the datetime as an object, meaning you will end up with a string.</p>

<h1>Pandas way of solving this</h1>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pandas.read_csv()</code></a> function has a keyword argument called <code>parse_dates</code></p>

<p>Using this you can on the fly convert strings, floats or integers into datetimes using the default <code>date_parser</code> (<code>dateutil.parser.parser</code>)</p>

<pre><code>headers = ['col1', 'col2', 'col3', 'col4']
dtypes = {'col1': 'str', 'col2': 'str', 'col3': 'str', 'col4': 'float'}
parse_dates = ['col1', 'col2']
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes, parse_dates=parse_dates)
</code></pre>

<p>This will cause pandas to read <code>col1</code> and <code>col2</code> as strings, which they most likely are (""2016-05-05"" etc.) and after having read the string, the date_parser for each column will act upon that string and give back whatever that function returns.</p>

<h1>Defining your own date parsing function:</h1>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pandas.read_csv()</code></a> function <strong>also</strong> has a keyword argument called <code>date_parser</code></p>

<p>Setting this to a lambda function will make that particular function be used for the parsing of the dates.</p>

<h3>GOTCHA WARNING</h3>

<p>You have to give it the function, not the execution of the function, thus this is <strong>Correct</strong></p>

<pre><code>date_parser = pd.datetools.to_datetime
</code></pre>

<p>This is <strong>incorrect</strong>:</p>

<pre><code>date_parser = pd.datetools.to_datetime()
</code></pre>
";;2;;2016-05-26T07:11:49.710;;37453925;2016-05-26T07:20:49.613;2016-05-26T07:20:49.613;;3730397.0;;3730397.0;21269399.0;2;60;;;
56755;56755;;;"<p>It means:</p>

<pre><code>'O'     (Python) objects
</code></pre>

<p><a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/arrays.dtypes.html"" rel=""nofollow noreferrer"">Source</a>.</p>

<blockquote>
  <p>The first character specifies the kind of data and the remaining characters specify the number of bytes per item, except for Unicode, where it is interpreted as the number of characters. The item size must correspond to an existing type, or an error will be raised. The supported kinds are
  to an existing type, or an error will be raised. The supported kinds are:</p>
</blockquote>

<pre><code>'b'       boolean
'i'       (signed) integer
'u'       unsigned integer
'f'       floating-point
'c'       complex-floating point
'O'       (Python) objects
'S', 'a'  (byte-)string
'U'       Unicode
'V'       raw data (void)
</code></pre>

<p>Another <a href=""https://stackoverflow.com/a/42672574/2901002"">answer</a> helps if need check <code>type</code>s.</p>
";;0;;2016-06-01T07:27:02.127;;37562101;2017-04-30T04:50:25.590;2017-05-23T11:54:44.827;;-1.0;;2901002.0;37561991.0;2;13;;;
56873;56873;;;"<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'ItemQty': {0: 3, 1: 25}, 
                   'Seatblocks': {0: '2:218:10:4,6', 1: '1:13:36:1,12 1:13:37:1,13'}, 
                   'ItemExt': {0: 60, 1: 300}, 
                   'CustomerName': {0: 'McCartney, Paul', 1: 'Lennon, John'}, 
                   'CustNum': {0: 32363, 1: 31316}, 
                   'Item': {0: 'F04', 1: 'F01'}}, 
                    columns=['CustNum','CustomerName','ItemQty','Item','Seatblocks','ItemExt'])

print (df)
   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt
0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60
1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300
</code></pre>

<p>Another similar solution with chaining is use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rename.html"" rel=""noreferrer""><code>rename</code></a>:</p>

<pre><code>print (df.drop('Seatblocks', axis=1)
             .join
             (
             df.Seatblocks
             .str
             .split(expand=True)
             .stack()
             .reset_index(drop=True, level=1)
             .rename('Seatblocks')           
             ))

   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks
0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6
1    31316     Lennon, John       25  F01      300  1:13:36:1,12
1    31316     Lennon, John       25  F01      300  1:13:37:1,13
</code></pre>

<hr>

<p>If in column are <strong>NOT</strong> <code>NaN</code> values, the fastest solution is use <code>list</code> comprehension with <code>DataFrame</code> constructor:</p>

<pre><code>df = pd.DataFrame(['a b c']*100000, columns=['col'])

In [141]: %timeit (pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))))
1 loop, best of 3: 211 ms per loop

In [142]: %timeit (pd.DataFrame(df.col.str.split().tolist()))
10 loops, best of 3: 87.8 ms per loop

In [143]: %timeit (pd.DataFrame(list(df.col.str.split())))
10 loops, best of 3: 86.1 ms per loop

In [144]: %timeit (df.col.str.split(expand=True))
10 loops, best of 3: 156 ms per loop

In [145]: %timeit (pd.DataFrame([ x.split() for x in df['col'].tolist()]))
10 loops, best of 3: 54.1 ms per loop
</code></pre>

<p>But if column contains <code>NaN</code> only works <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""noreferrer""><code>str.split</code></a> with parameter <code>expand=True</code> which return <code>DataFrame</code> (<a href=""http://pandas.pydata.org/pandas-docs/stable/text.html#splitting-and-replacing-strings"" rel=""noreferrer"">documentation</a>), and it explain why it is slowier:</p>

<pre><code>df = pd.DataFrame(['a b c']*10, columns=['col'])
df.loc[0] = np.nan
print (df.head())
     col
0    NaN
1  a b c
2  a b c
3  a b c
4  a b c

print (df.col.str.split(expand=True))
     0     1     2
0  NaN  None  None
1    a     b     c
2    a     b     c
3    a     b     c
4    a     b     c
5    a     b     c
6    a     b     c
7    a     b     c
8    a     b     c
9    a     b     c
</code></pre>
";;2;;2016-06-02T12:35:40.437;;37592047;2016-09-14T08:47:08.010;2016-09-14T08:47:08.010;;2901002.0;;2901002.0;17116814.0;2;8;;;
56947;56947;;;"<p>I think as_index=False should do the trick.</p>

<pre><code>df.groupby(['Name','Type','ID'], as_index=False).count()
</code></pre>
";;0;;2016-06-02T22:06:18.107;;37602937;2016-06-02T22:06:18.107;;;;;1451649.0;31569549.0;2;8;;;
57127;57127;;;"<p>Method 1:</p>

<pre><code>df.xs('A', level='Col', axis=1)
</code></pre>

<p>for more refer to <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#cross-section"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/advanced.html#cross-section</a></p>

<p>Method 2:</p>

<pre><code>df.loc[:, (slice(None), 'A')]
</code></pre>

<p><strong>Caveat:</strong> this method requires the labels to be sorted. for more refer to <a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#the-need-for-sortedness-with-multiindex"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/advanced.html#the-need-for-sortedness-with-multiindex</a></p>
";;1;;2016-06-05T16:13:45.587;;37644056;2016-06-05T16:13:45.587;;;;;2725109.0;25189575.0;2;11;;;
57148;57148;;;"<p>Just multiply your Dataframe by 1 (int)</p>

<pre><code>[1]: data = pd.DataFrame([[True, False, True], [False, False, True]])
[2]: print data
          0      1     2
     0   True  False  True
     1   False False  True

[3]: print data*1
         0  1  2
     0   1  0  1
     1   0  0  1
</code></pre>
";;0;;2016-06-05T21:54:35.933;;37647160;2016-06-05T21:54:35.933;;;;;3142393.0;17383094.0;2;11;;;
57186;57186;;;"<p>Also useful and very succinct:</p>

<p><code>df[df.index.duplicated()]</code></p>

<p>Note that this only returns one of the duplicated rows, so to see all the duplicated rows you'll want this:</p>

<p><code>df[df.index.duplicated(keep=False)]</code></p>
";;0;;2016-06-06T10:17:52.457;;37654890;2016-06-06T11:12:05.590;2016-06-06T11:12:05.590;;5209935.0;;5209935.0;20199129.0;2;7;;;
57188;57188;;;"<p>As others have said, you've probably got duplicate values in your original index. To find them do this:</p>

<p><code>df[df.index.duplicated()]</code></p>
";;0;;2016-06-06T10:26:39.810;;37655063;2016-06-06T10:26:39.810;;;;;5209935.0;27236275.0;2;36;;;
57493;57493;;;"<p>Don't use <code>ix</code>. It's <a href=""https://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#deprecate-ix"" rel=""noreferrer"">deprecated</a>. The most readable and idiomatic way of doing this is <code>df.drop()</code>:</p>

<pre><code>&gt;&gt;&gt; df

          a         b         c         d
0  0.175127  0.191051  0.382122  0.869242
1  0.414376  0.300502  0.554819  0.497524
2  0.142878  0.406830  0.314240  0.093132
3  0.337368  0.851783  0.933441  0.949598

&gt;&gt;&gt; df.drop('b', axis=1)

          a         c         d
0  0.175127  0.382122  0.869242
1  0.414376  0.554819  0.497524
2  0.142878  0.314240  0.093132
3  0.337368  0.933441  0.949598
</code></pre>

<p>Note that by default, <code>.drop()</code> does not operate inplace; despite the ominous name, <code>df</code> is unharmed by this process. If you want to permanently remove <code>b</code> from <code>df</code>, do <code>df.drop('b', inplace=True)</code>.</p>

<p><code>df.drop()</code> also accepts a list of labels, e.g. <code>df.drop(['a', 'b'], axis=1)</code> will drop column <code>a</code> and <code>b</code>.</p>
";;1;;2016-06-09T05:38:42.073;;37717675;2017-07-01T01:17:21.583;2017-07-01T01:17:21.583;;409879.0;;409879.0;29763620.0;2;39;;;
57643;57643;;;"<p>Another solution is use use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.droplevel.html""><code>MultiIndex.droplevel</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#changes-to-rename""><code>rename_axis</code></a> (new in <code>pandas</code> <code>0.18.0</code>):</p>

<pre><code>import pandas as pd

cols = pd.MultiIndex.from_arrays([['basic_amt']*4,
                                     ['NSW','QLD','VIC','All']], 
                                     names = [None, 'Faculty'])
idx = pd.Index(['All', 'Full Time', 'Part Time'])

df = pd.DataFrame([(1,1,2,4),
                   (0,1,0,1),
                   (1,0,2,3)], index = idx, columns=cols)

print (df)
          basic_amt            
Faculty         NSW QLD VIC All
All               1   1   2   4
Full Time         0   1   0   1
Part Time         1   0   2   3

df.columns = df.columns.droplevel(0)
#pandas 0.18.0 and higher
df = df.rename_axis(None, axis=1)
#pandas bellow 0.18.0
#df.columns.name = None

print (df)
           NSW  QLD  VIC  All
All          1    1    2    4
Full Time    0    1    0    1
Part Time    1    0    2    3

print (df.columns)
Index(['NSW', 'QLD', 'VIC', 'All'], dtype='object')
</code></pre>

<p>If need both column names use <code>list</code> comprehension:</p>

<pre><code>df.columns = ['_'.join(col) for col in df.columns]
print (df)
           basic_amt_NSW  basic_amt_QLD  basic_amt_VIC  basic_amt_All
All                    1              1              2              4
Full Time              0              1              0              1
Part Time              1              0              2              3

print (df.columns)
Index(['basic_amt_NSW', 'basic_amt_QLD', 'basic_amt_VIC', 'basic_amt_All'], dtype='object')
</code></pre>
";;0;;2016-06-10T13:05:44.890;;37749078;2016-06-10T13:05:44.890;;;;;2901002.0;14189695.0;2;8;;;
57695;57695;;;"<p>I use tia (<a href=""https://github.com/bpsmith/tia/blob/master/examples/datamgr.ipynb"" rel=""nofollow noreferrer"">https://github.com/bpsmith/tia/blob/master/examples/datamgr.ipynb</a>)</p>

<p>It already downloads data as a panda dataframe from bloomberg. 
You can download history for multiple tickers in one single call and even download some bloombergs reference data (Central Bank date meetings, holidays for a certain country, etc)</p>

<p>And you just install it with pip. 
This link is full of examples but to download historical data is as easy as:</p>

<pre><code>import pandas as pd
import tia.bbg.datamgr as dm

mgr = dm.BbgDataManager()
sids = mgr['MSFT US EQUITY', 'IBM US EQUITY', 'CSCO US EQUITY']
df = sids.get_historical('PX_LAST', '1/1/2014', '11/12/2014')
</code></pre>

<p>and df is a pandas dataframe. </p>

<p>Hope it helps</p>
";;0;;2016-06-10T22:06:53.963;;37757763;2017-08-10T06:40:59.193;2017-08-10T06:40:59.193;;2333545.0;;5235265.0;19387868.0;2;6;;;
57846;57846;;;"<p>Another way to do this is to introduce a minimal amount of noise, which will artificially create unique bin edges. Here's an example:</p>

<pre><code>a = pd.Series(range(100) + ([0]*20))

def jitter(a_series, noise_reduction=1000000):
    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))

# and now this works by adding a little noise
a_deciles = pd.qcut(a + jitter(a), 10, labels=False)
</code></pre>

<p>we can recreate the original error using something like this:</p>

<pre><code>a_deciles = pd.qcut(a, 10, labels=False)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 173, in qcut
    precision=precision, include_lowest=True)
  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 192, in _bins_to_cuts
    raise ValueError('Bin edges must be unique: %s' % repr(bins))
ValueError: Bin edges must be unique: array([  0.        ,   0.        ,   0.        ,   3.8       ,
        11.73333333,  19.66666667,  27.6       ,  35.53333333,
        43.46666667,  51.4       ,  59.33333333,  67.26666667,
        75.2       ,  83.13333333,  91.06666667,  99.        ])
</code></pre>
";;1;;2016-06-13T15:41:19.533;;37793940;2016-06-13T18:16:53.927;2016-06-13T18:16:53.927;;2636317.0;;2636317.0;20158597.0;2;8;;;
57947;57947;;;"<p>Extending <a href=""https://stackoverflow.com/a/12510334/5741205"">@Def_Os's</a> answer with an actual demo...</p>

<p>As @Def_Os has already said - using <a href=""https://pandas-datareader.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Pandas Datareader</a> makes this task a real fun</p>

<pre><code>In [12]: from pandas_datareader import data
</code></pre>

<p>pulling all available historical data for <code>AAPL</code> starting from <code>1980-01-01</code></p>

<pre><code>#In [13]: aapl = data.DataReader('AAPL', 'yahoo', '1980-01-01')

# yahoo api is inconsistent for getting historical data, please use google instead.
In [13]: aapl = data.DataReader('AAPL', 'google', '1980-01-01')
</code></pre>

<p>first 5 rows</p>

<pre><code>In [14]: aapl.head()
Out[14]:
                 Open       High     Low   Close     Volume  Adj Close
Date
1980-12-12  28.750000  28.875000  28.750  28.750  117258400   0.431358
1980-12-15  27.375001  27.375001  27.250  27.250   43971200   0.408852
1980-12-16  25.375000  25.375000  25.250  25.250   26432000   0.378845
1980-12-17  25.875000  25.999999  25.875  25.875   21610400   0.388222
1980-12-18  26.625000  26.750000  26.625  26.625   18362400   0.399475
</code></pre>

<p>last 5 rows</p>

<pre><code>In [15]: aapl.tail()
Out[15]:
                 Open       High        Low      Close    Volume  Adj Close
Date
2016-06-07  99.250000  99.870003  98.959999  99.029999  22366400  99.029999
2016-06-08  99.019997  99.559998  98.680000  98.940002  20812700  98.940002
2016-06-09  98.500000  99.989998  98.459999  99.650002  26419600  99.650002
2016-06-10  98.529999  99.349998  98.480003  98.830002  31462100  98.830002
2016-06-13  98.690002  99.120003  97.099998  97.339996  37612900  97.339996
</code></pre>

<p>save all data as CSV file</p>

<pre><code>In [16]: aapl.to_csv('d:/temp/aapl_data.csv')
</code></pre>

<p>d:/temp/aapl_data.csv - 5 first rows</p>

<pre><code>Date,Open,High,Low,Close,Volume,Adj Close
1980-12-12,28.75,28.875,28.75,28.75,117258400,0.431358
1980-12-15,27.375001,27.375001,27.25,27.25,43971200,0.408852
1980-12-16,25.375,25.375,25.25,25.25,26432000,0.378845
1980-12-17,25.875,25.999999,25.875,25.875,21610400,0.38822199999999996
1980-12-18,26.625,26.75,26.625,26.625,18362400,0.399475
...
</code></pre>
";;0;;2016-06-14T08:30:56.797;;37806819;2017-06-09T07:59:18.630;2017-06-09T07:59:18.630;;6879949.0;;5741205.0;12433076.0;2;10;;;
58200;58200;;;"<h2>Let's start with some sample data</h2>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame([[1, 'a'], [2, 'b'], [3, 'c']],
   ...:                   columns=['num', 'char'])

In [3]: df
Out[3]: 
   num char
0    1    a
1    2    b
2    3    c

In [4]: df.dtypes
Out[4]: 
num      int64
char    object
dtype: object
</code></pre>

<h3>Now let's use a simple <code>DataFrame</code> initialization using the columns of the original <code>DataFrame</code> but providing no data:</h3>

<pre><code>In [5]: empty_copy_1 = pd.DataFrame(data=None, columns=df.columns)

In [6]: empty_copy_1
Out[6]: 
Empty DataFrame
Columns: [num, char]
Index: []

In [7]: empty_copy_1.dtypes
Out[7]: 
num     object
char    object
dtype: object
</code></pre>

<p>As you can see, the column data types are not the same as in our original <code>DataFrame</code>.  </p>

<h3>So, if you want to preserve the column <code>dtype</code>...</h3>

<p>If you want to preserve the column data types you need to construct the <code>DataFrame</code> one <code>Series</code> at a time </p>

<pre><code>In [8]: empty_copy_2 = pd.DataFrame.from_items([
   ...:     (name, pd.Series(data=None, dtype=series.dtype))
   ...:     for name, series in df.iteritems()])

In [9]: empty_copy_2
Out[9]: 
Empty DataFrame
Columns: [num, char]
Index: []

In [10]: empty_copy_2.dtypes
Out[10]: 
num      int64
char    object
dtype: object
</code></pre>
";;1;;2016-06-16T06:14:27.720;;37851232;2016-11-11T16:17:46.517;2016-11-11T16:17:46.517;;1476240.0;;1476240.0;27467730.0;2;6;;;
58472;58472;;;"<p><code>pandas.merge()</code> is the underlying function used for all merge/join behavior. </p>

<p>DataFrames provide the <code>pandas.DataFrame.merge()</code> and <code>pandas.DataFrame.join()</code> methods as a convenient way to access the capabilities of <code>pandas.merge()</code>. For example, <code>df1.merge(right=df2, ...)</code> is equivalent to <code>pandas.merge(left=df1, right=df2, ...)</code>. </p>

<p>These are the main differences between <code>df.join()</code> and <code>df.merge()</code>:</p>

<ol>
<li>lookup on right table: <code>df1.join(df2)</code> always joins via the index of <code>df2</code>, but <code>df1.merge(df2)</code> can join to one or more columns of <code>df2</code> (default) or to the index of <code>df2</code> (with <code>right_index=True</code>). </li>
<li>lookup on left table: by default, <code>df1.join(df2)</code> uses the index of <code>df1</code> and <code>df1.merge(df2)</code> uses column(s) of <code>df1</code>. That can be overridden by specifying <code>df1.join(df2, on=key_or_keys)</code> or <code>df1.merge(df2, right_index=True)</code>. </li>
<li>left vs inner join: <code>df1.join(df2)</code> does a left join by default (keeps all rows of <code>df1</code>), but <code>df.merge</code> does an inner join by default (returns only matching rows of <code>df1</code> and <code>df2</code>).</li>
</ol>

<p>So, the generic approach is to use <code>pandas.merge(df1, df2)</code> or <code>df1.merge(df2)</code>. But for a number of common situations (keeping all rows of <code>df1</code> and joining to  an index in <code>df2</code>), you can save some typing by using <code>df1.join(df2)</code> instead.</p>

<p>Some notes on these issues from the documentation at <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging"">http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging</a>:</p>

<blockquote>
  <p><code>merge</code> is a function in the pandas namespace, and it is also
  available as a DataFrame instance method, with the calling DataFrame
  being implicitly considered the left object in the join.</p>
  
  <p>The related <code>DataFrame.join</code> method, uses <code>merge</code> internally for the
  index-on-index and index-on-column(s) joins, but joins on indexes by
  default rather than trying to join on common columns (the default
  behavior for <code>merge</code>). If you are joining on index, you may wish to
  use <code>DataFrame.join</code> to save yourself some typing.</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>These two function calls are completely equivalent:</p>

<pre><code>left.join(right, on=key_or_keys)
pd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)
</code></pre>
</blockquote>
";;0;;2016-06-17T22:51:58.390;;37891437;2016-06-17T23:00:01.017;2016-06-17T23:00:01.017;;3830997.0;;3830997.0;22676081.0;2;56;;;
59102;59102;;;"<p><em>TLDR: do not append items to a series one by one, better extend with an ordered collection</em></p>

<p>I think the question in its current form is a bit tricky. And the accepted answer does answer the question. But the more I use pandas, the more I understand that it's a bad idea to append items to a Series one by one. I'll try to explain why for pandas beginners.</p>

<p>You might think that appending data to a given Series might allow you to reuse some resources, but in reality a Series is just a container that stores a relation between an index and a values array. Each is a numpy.array under the hood, and the index is immutable. When you add to Series an item with a label that is missing in the index, a new index with size n+1 is created, and a new values values array of the same size. That means that when you append items one by one, you create two more arrays of the n+1 size on each step.</p>

<p>By the way, you can not append a new item by position (you will get an IndexError) and the label in an index does not have to be unique, that is when you assign a value with a label, you assign the value to all existing items with the the label, and a new row is not appended in this case. This might lead to subtle bugs.</p>

<p>The moral of the story is that you should not append data one by one, you should better extend with an ordered collection. The problem is that you can not extend a Series inplace. That is why it is better to organize your code so that you don't need to update a specific instance of a Series by reference.</p>

<p>If you create labels yourself and they are increasing, the easiest way is to add new items to a dictionary, then create a new Series from the dictionary (it sorts the keys) and append the Series to an old one. If the keys are not increasing, then you will need to create two separate lists for the new labels and the new values.</p>

<p>Below are some code samples:</p>

<pre><code>In [1]: import pandas as pd
In [2]: import numpy as np

In [3]: s = pd.Series(np.arange(4)**2, index=np.arange(4))

In [4]: s
Out[4]:
0    0
1    1
2    4
3    9
dtype: int64

In [6]: id(s.index), id(s.values)
Out[6]: (4470549648, 4470593296)
</code></pre>

<p>When we update an existing item, the index and the values array stay the same (if you do not change the type of the value)</p>

<pre><code>In [7]: s[2] = 14  

In [8]: id(s.index), id(s.values)
Out[8]: (4470549648, 4470593296)
</code></pre>

<p>But when you add a new item, a new index and a new values array is generated:</p>

<pre><code>In [9]: s[4] = 16

In [10]: s
Out[10]:
0     0
1     1
2    14
3     9
4    16
dtype: int64

In [11]: id(s.index), id(s.values)
Out[11]: (4470548560, 4470595056)
</code></pre>

<p>That is if you are going to append several items, collect them in a dictionary, create a Series, append it to the old one and save the result:</p>

<pre><code>In [13]: new_items = {item: item**2 for item in range(5, 7)}

In [14]: s2 = pd.Series(new_items)

In [15]: s2  # keys are guaranteed to be sorted!
Out[15]:
5    25
6    36
dtype: int64

In [16]: s = s.append(s2); s
Out[16]:
0     0
1     1
2    14
3     9
4    16
5    25
6    36
dtype: int64
</code></pre>
";;0;;2016-06-23T13:16:57.263;;37992805;2016-06-23T13:16:57.263;;;;;68998.0;13331518.0;2;15;;;
59331;59331;;;"<p>For getting rows use</p>

<pre><code>df.index
</code></pre>

<p>and for columns use</p>

<pre><code>df.columns
</code></pre>

<p>You can always use <code>len(func)</code> for getting the count of list, hence you can use
<code>len(df.index)</code> for getting the number of rows.</p>

<p>But keep in mind, as stated by @root, using <code>shape[0] and shape[1]</code> for getting the number of rows and columns, respectively, is a faster option.</p>
";;0;;2016-06-25T05:23:38.267;;38025280;2016-12-14T18:55:42.587;2016-12-14T18:55:42.587;;4230591.0;;2234161.0;15943769.0;2;7;;;
59367;59367;;;"<p>I use the <code>suffixes</code> option in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""noreferrer""><code>.merge()</code></a>:</p>

<pre><code>dfNew = df.merge(df2, left_index=True, right_index=True,
                 how='outer', suffixes=('', '_y'))
</code></pre>

<p>You can then filter the columns based on the flag ""_y"", i.e. delete them.</p>
";;0;;2016-06-26T00:13:15.310;;38034085;2016-07-20T00:48:49.173;2016-07-20T00:48:49.173;;3621464.0;;3962284.0;19125091.0;2;15;;;
59485;59485;;;"<p>I think a list comprehension is your best bet for the most Pythonic (and fastest) way to do this:</p>

<pre><code>[int(i.days) for i in (df.B - df.A)]
</code></pre>

<ol>
<li>i will return the timedelta(e.g. '-58 days')</li>
<li>i.days will return this value as a long integer value(e.g. -58L)</li>
<li>int(i.days) will give you the -58 you seek.</li>
</ol>
";;1;;2016-06-27T13:17:28.113;;38055014;2017-05-10T18:47:33.207;2017-05-10T18:47:33.207;;6342575.0;;6342575.0;22132525.0;2;7;;;
59751;59751;;;"<p>Manually, you can use <code>pd.DataFrame</code> constructor, giving a numpy array (<code>data</code>) and a list of the names of the columns (<code>columns</code>).
To have everything in one DataFrame, you can concatenate the features and the target into one numpy array with <code>np.c_[...]</code> (note the <code>[]</code>):</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

# save load_iris() sklearn dataset to iris
# if you'd like to check dataset type use: type(load_iris())
# if you'd like to view list of attributes use: dir(load_iris())
iris = load_iris()

# np.c_ is the numpy concatenate function
# which is used to concat iris['data'] and iris['target'] arrays 
# for pandas column argument: concat iris['feature_names'] list
# and string list (in this case one string); you can make this anything you'd like..  
# the original dataset would probably call this ['Species']
data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                     columns= iris['feature_names'] + ['target'])
</code></pre>
";;4;;2016-06-29T13:26:16.777;;38105540;2016-12-07T23:57:07.640;2016-12-07T23:57:07.640;;5503872.0;TomDLT;6156647.0;38105539.0;2;30;;;
59926;59926;;;"<p>You can select your desired columns and do it by assignment:</p>

<pre><code>df[['a', 'b']] = df[['a','b']].fillna(value=0)
</code></pre>

<p>The resulting output is as expected:</p>

<pre><code>     a    b    c
0  1.0  4.0  NaN
1  2.0  5.0  NaN
2  3.0  0.0  7.0
3  0.0  6.0  8.0
</code></pre>
";;10;;2016-06-30T22:09:23.560;;38134049;2016-06-30T22:19:27.140;2016-06-30T22:19:27.140;;3339965.0;;3339965.0;38134012.0;2;26;;;
60078;60078;;;"<p>Another way to set the column types is to first construct a numpy record array with your desired types, fill it out and then pass it to a DataFrame constructor.</p>

<pre><code>import pandas as pd
import numpy as np    

x = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)])
df = pd.DataFrame(x)

df.dtypes -&gt;

x      uint8
y    float64
</code></pre>
";;0;;2016-07-02T04:49:52.623;;38156594;2016-07-02T04:49:52.623;;;;;2512851.0;21197774.0;2;6;;;
60220;60220;;;"<p>You can also use <a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.issubdtype.html"" rel=""noreferrer""><code>np.issubdtype</code></a> with <code>np.number</code>. Consider the following DataFrame:</p>

<pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [1.0, 2.0, 3.0], 
                   'C': [1j, 2j, 3j], 'D': ['a', 'b', 'c']})
df
Out: 
   A    B   C  D
0  1  1.0  1j  a
1  2  2.0  2j  b
2  3  3.0  3j  c

df.dtypes
Out: 
A         int64
B       float64
C    complex128
D        object
dtype: object
</code></pre>

<hr>

<pre><code>np.issubdtype(df['A'].dtype, np.number)
Out: True

np.issubdtype(df['B'].dtype, np.number)
Out: True

np.issubdtype(df['C'].dtype, np.number)
Out: True

np.issubdtype(df['D'].dtype, np.number)
Out: False
</code></pre>

<p>For multiple columns you can use np.vectorize:</p>

<pre><code>is_number = np.vectorize(lambda x: np.issubdtype(x, np.number))
is_number(df.dtypes)
Out: array([ True,  True,  True, False], dtype=bool)
</code></pre>

<p>And for selection, pandas now has <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html"" rel=""noreferrer""><code>select_dtypes</code></a>:</p>

<pre><code>df.select_dtypes(include=[np.number])
Out: 
   A    B   C
0  1  1.0  1j
1  2  2.0  2j
2  3  3.0  3j
</code></pre>
";;0;;2016-07-04T13:17:50.233;;38185759;2016-11-23T20:32:16.053;2016-11-23T20:32:16.053;;2285236.0;;2285236.0;19900202.0;2;14;;;
60518;60518;;;"<p>To convert the string to an actual dict, you can do <code>df['Pollutant Levels'].map(eval)</code>. Afterwards, the solution below can be used to convert the dict to different columns.</p>

<hr>

<p>Using a small example, you can use <code>.apply(pd.Series)</code>:</p>

<pre><code>In [2]: df = pd.DataFrame({'a':[1,2,3], 'b':[{'c':1}, {'d':3}, {'c':5, 'd':6}]})

In [3]: df
Out[3]:
   a                   b
0  1           {u'c': 1}
1  2           {u'd': 3}
2  3  {u'c': 5, u'd': 6}

In [4]: df['b'].apply(pd.Series)
Out[4]:
     c    d
0  1.0  NaN
1  NaN  3.0
2  5.0  6.0
</code></pre>

<p>To combine it with the rest of the dataframe, you can <code>concat</code> the other columns with the above result:</p>

<pre><code>In [7]: pd.concat([df.drop(['b'], axis=1), df['b'].apply(pd.Series)], axis=1)
Out[7]:
   a    c    d
0  1  1.0  NaN
1  2  NaN  3.0
2  3  5.0  6.0
</code></pre>

<hr>

<p>Using your code, this also works if I leave out the <code>iloc</code> part:</p>

<pre><code>In [15]: pd.concat([df.drop('b', axis=1), pd.DataFrame(df['b'].tolist())], axis=1)
Out[15]:
   a    c    d
0  1  1.0  NaN
1  2  NaN  3.0
2  3  5.0  6.0
</code></pre>
";;6;;2016-07-06T18:51:37.493;;38231651;2016-07-06T21:29:37.567;2016-07-06T21:29:37.567;;653364.0;;653364.0;38231591.0;2;21;;;
60670;60670;;;"<h3>Note:</h3>

<p>Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.</p>

<pre><code>import numpy as np
import pandas as pd

def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(df.index)
    m = len(df)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = df.ix[perm[:train_end]]
    validate = df.ix[perm[train_end:validate_end]]
    test = df.ix[perm[validate_end:]]
    return train, validate, test
</code></pre>

<h3>Demonstration</h3>

<pre><code>np.random.seed([3,1415])
df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/ThpsQ.png""><img src=""https://i.stack.imgur.com/ThpsQ.png"" alt=""enter image description here""></a></p>

<pre><code>train, validate, test = train_validate_test_split(df)

train
</code></pre>

<p><a href=""https://i.stack.imgur.com/XNRBT.png""><img src=""https://i.stack.imgur.com/XNRBT.png"" alt=""enter image description here""></a></p>

<pre><code>validate
</code></pre>

<p><a href=""https://i.stack.imgur.com/PpyC8.png""><img src=""https://i.stack.imgur.com/PpyC8.png"" alt=""enter image description here""></a></p>

<pre><code>test
</code></pre>

<p><a href=""https://i.stack.imgur.com/U6CaT.png""><img src=""https://i.stack.imgur.com/U6CaT.png"" alt=""enter image description here""></a></p>
";;1;;2016-07-07T16:47:10.180;;38251063;2016-07-07T17:01:30.127;2016-07-07T17:01:30.127;;2336654.0;;2336654.0;38250710.0;2;18;;;
60671;60671;;;"<p>Numpy solution (thanks to <a href=""https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-development-and-test/38251213?noredirect=1#comment63923795_38251213"">root</a> for the randomizing hint) - we will split our data set into the following parts: (60% - train set, 20% - validation set, 20% - test set):</p>

<pre><code>In [305]: train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])

In [306]: train
Out[306]:
          A         B         C         D         E
0  0.046919  0.792216  0.206294  0.440346  0.038960
2  0.301010  0.625697  0.604724  0.936968  0.870064
1  0.642237  0.690403  0.813658  0.525379  0.396053
9  0.488484  0.389640  0.599637  0.122919  0.106505
8  0.842717  0.793315  0.554084  0.100361  0.367465
7  0.185214  0.603661  0.217677  0.281780  0.938540

In [307]: validate
Out[307]:
          A         B         C         D         E
5  0.806176  0.008896  0.362878  0.058903  0.026328
6  0.145777  0.485765  0.589272  0.806329  0.703479

In [308]: test
Out[308]:
          A         B         C         D         E
4  0.521640  0.332210  0.370177  0.859169  0.401087
3  0.333348  0.964011  0.083498  0.670386  0.169619
</code></pre>

<p>PS [int(.6*len(df)), int(.8*len(df))] - is an <code>indices_or_sections</code> array for <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html"" rel=""noreferrer"">numpy.split()</a></p>

<p>Here is a small demo for <code>np.split()</code> usage - let's split 20-elements array into the following parts: 90%, 10%, 10%:</p>

<pre><code>In [45]: a = np.arange(1, 21)

In [46]: a
Out[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])

In [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])
Out[47]:
[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),
 array([17, 18]),
 array([19, 20])]
</code></pre>
";;9;;2016-07-07T16:56:12.720;;38251213;2016-09-16T18:37:17.760;2017-05-23T11:54:54.010;;-1.0;;5741205.0;38250710.0;2;35;;;
60731;60731;;;"<p>The disparity in these %timeit results</p>

<pre><code>In [273]: %timeit df1[df1['letter'] == 'ben']
10 loops, best of 3: 36.1 ms per loop

In [274]: %timeit df2[df2['letter'] == 'ben']
10 loops, best of 3: 108 ms per loop
</code></pre>

<p>also shows up in the <em>pure NumPy</em> equality comparisons:</p>

<pre><code>In [275]: %timeit df1['letter'].values == 'ben'
10 loops, best of 3: 24.1 ms per loop

In [276]: %timeit df2['letter'].values == 'ben'
10 loops, best of 3: 96.5 ms per loop
</code></pre>

<p>Under the hood, Pandas' <code>df1['letter'] == 'ben'</code> <a href=""https://github.com/pydata/pandas/blob/master/pandas/lib.pyx#L786"" rel=""noreferrer"">calls a Cython
function</a>
which loops through the values of the underlying NumPy array,
<code>df1['letter'].values</code>. It is essentially doing the same thing as
<code>df1['letter'].values == 'ben'</code> but with different handling of NaNs.  </p>

<p>Moreover, notice that simply accessing the items in <code>df1['letter']</code> in
sequential order can be done more quickly than doing the same for <code>df2['letter']</code>:</p>

<pre><code>In [11]: %timeit [item for item in df1['letter']]
10 loops, best of 3: 49.4 ms per loop

In [12]: %timeit [item for item in df2['letter']]
10 loops, best of 3: 124 ms per loop
</code></pre>

<p>The difference in times within each of these three sets of <code>%timeit</code> tests are
roughly the same. I think that is because they all share the same cause.</p>

<p>Since the <code>letter</code> column holds strings, the NumPy arrays <code>df1['letter'].values</code> and
<code>df2['letter'].values</code> have dtype <code>object</code> and therefore they hold
pointers to the memory location of the arbitrary Python objects (in this case strings).</p>

<p>Consider the memory location of the strings stored in the DataFrames, <code>df1</code> and
<code>df2</code>. In CPython the <code>id</code> returns the memory location of the object:</p>

<pre><code>memloc = pd.DataFrame({'df1': list(map(id, df1['letter'])),
                       'df2': list(map(id, df2['letter'])), })

               df1              df2
0  140226328244040  140226299303840
1  140226328243088  140226308389048
2  140226328243872  140226317328936
3  140226328243760  140226230086600
4  140226328243368  140226285885624
</code></pre>

<p>The strings in <code>df1</code> (after the first dozen or so) tend to appear sequentially
in memory, while sorting causes the strings in <code>df2</code> (taken in order) to be
scattered in memory:</p>

<pre><code>In [272]: diffs = memloc.diff(); diffs.head(30)
Out[272]: 
         df1         df2
0        NaN         NaN
1     -952.0   9085208.0
2      784.0   8939888.0
3     -112.0 -87242336.0
4     -392.0  55799024.0
5     -392.0   5436736.0
6      952.0  22687184.0
7       56.0 -26436984.0
8     -448.0  24264592.0
9      -56.0  -4092072.0
10    -168.0 -10421232.0
11 -363584.0   5512088.0
12      56.0 -17433416.0
13      56.0  40042552.0
14      56.0 -18859440.0
15      56.0 -76535224.0
16      56.0  94092360.0
17      56.0  -4189368.0
18      56.0     73840.0
19      56.0  -5807616.0
20      56.0  -9211680.0
21      56.0  20571736.0
22      56.0 -27142288.0
23      56.0   5615112.0
24      56.0  -5616568.0
25      56.0   5743152.0
26      56.0 -73057432.0
27      56.0  -4988200.0
28      56.0  85630584.0
29      56.0  -4706136.0
</code></pre>

<p>Most of the strings in <code>df1</code> are 56 bytes apart:</p>

<pre><code>In [14]: 
In [16]: diffs['df1'].value_counts()
Out[16]: 
 56.0           986109
 120.0           13671
-524168.0          215
-56.0                1
-12664712.0          1
 41136.0             1
-231731080.0         1
Name: df1, dtype: int64

In [20]: len(diffs['df1'].value_counts())
Out[20]: 7
</code></pre>

<p>In contrast the strings in <code>df2</code> are scattered all over the place:</p>

<pre><code>In [17]: diffs['df2'].value_counts().head()
Out[17]: 
-56.0     46
 56.0     44
 168.0    39
-112.0    37
-392.0    35
Name: df2, dtype: int64

In [19]: len(diffs['df2'].value_counts())
Out[19]: 837764
</code></pre>

<p>When these objects (strings) are located sequentially in memory, their values
can be retrieved more quickly. This is why the equality comparisons performed by
<code>df1['letter'].values == 'ben'</code> can be done faster than those in <code>df2['letter'].values
== 'ben'</code>. <em>The lookup time is smaller</em>.</p>

<p>This memory accessing issue also explains why there is no disparity in the
<code>%timeit</code> results for the <code>value</code> column.</p>

<pre><code>In [5]: %timeit df1[df1['value'] == 0]
1000 loops, best of 3: 1.8 ms per loop

In [6]: %timeit df2[df2['value'] == 0]
1000 loops, best of 3: 1.78 ms per loop
</code></pre>

<p><code>df1['value']</code> and <code>df2['value']</code> are NumPy arrays of dtype <code>float64</code>. Unlike object
arrays, their values are packed together contiguously in memory. Sorting <code>df1</code>
with <code>df2 = df1.sort_values('letter')</code> causes the values in <code>df2['value']</code> to be
reordered, but since the values are <em>copied</em> into a new NumPy array, the values
are located sequentially in memory. So accessing the values in <code>df2['value']</code> can
be done just as quickly as those in <code>df1['value']</code>.</p>
";;0;;2016-07-08T03:13:17.503;;38258390;2016-07-08T11:32:39.120;2016-07-08T11:32:39.120;;190597.0;;190597.0;38254067.0;2;8;;;
60859;60859;;;"<p>In a newer version of pandas (0.17 and up), you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer"">to_numeric</a> function. It allows you to convert the whole dataframe or just individual columns. It also gives you an ability to select how to treat stuff that can't be converted to numeric values:</p>

<pre><code>import pandas as pd
s = pd.Series(['1.0', '2', -3])
pd.to_numeric(s)
s = pd.Series(['apple', '1.0', '2', -3])
pd.to_numeric(s, errors='ignore')
pd.to_numeric(s, errors='coerce')
</code></pre>
";;1;;2016-07-09T05:46:03.850;;38278787;2016-07-09T05:46:03.850;;;;;1090562.0;16729483.0;2;19;;;
61285;61285;;;"<p>I feel the best option will be to use the direct checks rather than using loc function:</p>

<pre><code>df = df[(df['date'] &gt; '2000-6-1') &amp; (df['date'] &lt;= '2000-6-10')]
</code></pre>

<p>It works for me.</p>

<p>Major issue with loc function with a slice is that the limits should be present in the actual values, if not this will result in KeyError.</p>
";;0;;2016-07-13T01:00:33.113;;38341066;2016-07-13T01:00:33.113;;;;;5113822.0;29370057.0;2;9;;;
61325;61325;;;"<p>This is not an answer to the OP question but a toy example to illustrate the answer of @ShikharDua above which I found very useful. </p>

<p>While this fragment is trivial, in the actual data I had 1,000s of rows, and many columns, and I wished to be able to group by different columns and then perform the stats below for more than one taget column. So having a reliable method for building the data frame one row at a time was a great convenience. Thank you @ShikharDua ! </p>

<pre><code>import pandas as pd 

BaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],
                          'Territory'  : ['West','East','South','West','East','South'],
                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})
BaseData

columns = ['Customer','Num Unique Products', 'List Unique Products']

rows_list=[]
for name, group in BaseData.groupby('Customer'):
    RecordtoAdd={} #initialise an empty dict 
    RecordtoAdd.update({'Customer' : name}) #
    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      
    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   

    rows_list.append(RecordtoAdd)

AnalysedData = pd.DataFrame(rows_list)

print('Base Data : \n',BaseData,'\n\n Analysed Data : \n',AnalysedData)
</code></pre>
";;0;;2016-07-13T09:49:18.073;;38348167;2016-07-13T09:49:18.073;;;;;3250815.0;10715965.0;2;8;;;
61792;61792;;;"<p>This answer simply extends @Andy Hayden's, making it resilient to when numeric fields are <code>nan</code>, and wrapping it up into a function.</p>

<pre><code>import pandas as pd
import numpy as np


def diff_pd(df1, df2):
    """"""Identify differences between two pandas DataFrames""""""
    assert (df1.columns == df2.columns).all(), \
        ""DataFrame column names are different""
    if df1.equals(df2):
        return None
    else:
        # need to account for np.nan != np.nan returning True
        diff_mask = (df1 != df2) &amp; ~(df1.isnull() &amp; df2.isnull())
        ne_stacked = diff_mask.stack()
        changed = ne_stacked[ne_stacked]
        changed.index.names = ['id', 'col']
        difference_locations = np.where(diff_mask)
        changed_from = df1.values[difference_locations]
        changed_to = df2.values[difference_locations]
        return pd.DataFrame({'from': changed_from, 'to': changed_to},
                            index=changed.index)
</code></pre>

<p>So with your data (slightly edited to have a NaN in the score column):</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3:
    from StringIO import StringIO
else:
    from io import StringIO

DF1 = StringIO(""""""id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 ""He was late to class""
112  Nick   1.11                     False                ""Graduated""
113  Zoe    NaN                     True                  "" ""
"""""")
DF2 = StringIO(""""""id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 ""He was late to class""
112  Nick   1.21                     False                ""Graduated""
113  Zoe    NaN                     False                ""On vacation"" """""")
df1 = pd.read_table(DF1, sep='\s+', index_col='id')
df2 = pd.read_table(DF2, sep='\s+', index_col='id')
diff_pd(df1, df2)
</code></pre>

<p>Output:</p>

<pre><code>                from           to
id  col                          
112 score       1.11         1.21
113 isEnrolled  True        False
    Comment           On vacation
</code></pre>
";;0;;2016-07-17T13:12:43.213;;38421614;2016-07-17T13:12:43.213;;;;;2550114.0;17095101.0;2;6;;;
61894;61894;;;"<p>A version for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html"" rel=""noreferrer"">pandas Series</a>:</p>

<pre><code>import numpy as np

def balanced_subsample(y, size=None):

    subsample = []

    if size is None:
        n_smp = y.value_counts().min()
    else:
        n_smp = int(size / len(y.value_counts().index))

    for label in y.value_counts().index:
        samples = y[y == label].index.values
        index_range = range(samples.shape[0])
        indexes = np.random.choice(index_range, size=n_smp, replace=False)
        subsample += samples[indexes].tolist()

    return subsample
</code></pre>
";;0;;2016-07-18T15:14:42.333;;38440332;2016-07-18T15:14:42.333;;;;;41977.0;23455728.0;2;6;;;
62073;62073;;;"<h1>Diary of an Answerer</h1>

<p>My best advice for asking questions would be to play on the psychology of the people who answer questions.  Being one of those people, I can give insight into why I answer certain questions and why I don't answer others.</p>

<h3>Motivations</h3>

<p>I'm motivated to answer questions for several reasons</p>

<ol>
<li>Stackoverflow.com has been a tremendously valuable resource to me.  I wanted to give back.</li>
<li>In my efforts to give back, I've found this site to be an even more powerful resource than before.  Answering questions is a learning experience for me and I like to learn.  <a href=""https://stackoverflow.com/a/38444638/2336654"">Read this answer and comment from another vet</a>.  This kind of interaction makes me happy.</li>
<li>I like points!</li>
<li>See #3.</li>
<li>I like interesting problems.</li>
</ol>

<p>All my purest intentions are great and all, but I get that satisfaction if I answer 1 question or 30.  <strong>What drives my choices</strong> for which questions to answer has a huge component of point maximization.</p>

<p>I'll also spend time on interesting problems but that is few and far between and doesn't help an asker who needs a solution to a non-interesting question.  Your best bet to get me to answer a question is to serve that question up on a platter ripe for me to answer it with as little effort as possible.  If I'm looking at two questions and one has code I can copy paste to create all the variables I need... I'm taking that one!  I'll come back to the other one if I have time, maybe.</p>

<h3>Main Advice</h3>

<p>Make it easy for the people answering questions.</p>

<ul>
<li>Provide code that creates variables that are needed.</li>
<li>Minimize that code.  If my eyes glaze over as I look at the post, I'm on to the next question or getting back to whatever else I'm doing.</li>
<li>Think about what your asking and be specific.  We want to see what you've done because natural languages (English) are inexact and confusing.  Code samples of what you've tried help resolve inconsistencies in a natural language description.</li>
<li>PLEASE show what you expect!!!  I have to sit down and try things.  I almost never know the answer to a question without trying some things out.  If I don't see an example of what you're looking for, I might pass on the question because I don't feel like guessing.</li>
</ul>

<h3>Your reputation is more than just your reputation.</h3>

<p>I like points (I mentioned that above).  But those points aren't really really my reputation.  My real reputation is an amalgamation of what others on the site think of me.  I strive to be fair and honest and I hope others can see that.  What that means for an asker is, we remember the behaviors of askers.  If you don't select answers and upvote good answers, I remember.  If you behave in ways I don't like or in ways I do like, I remember.  This also plays into which questions I'll answer.</p>

<hr>

<p>Anyway, I can probably go on, but I'll spare all of you who actually read this.</p>
";;0;;2016-07-19T18:35:13.243;;38466059;2016-07-19T18:35:13.243;2017-05-23T11:54:58.497;;-1.0;;2336654.0;20109391.0;2;12;;;
62081;62081;;;"<p>You can also use a conditional lookup using <code>.loc</code> as seen here:</p>

<pre><code>df.loc[df[&lt;some_column_name&gt;] == &lt;condition&gt;, &lt;another_column_name&gt;] = &lt;value_to_add&gt;
</code></pre>

<p>where <code>&lt;some_column_name</code> is the column you want to check the <code>&lt;condition&gt;</code> variable against and <code>&lt;another_column_name&gt;</code> is the column you want to add to (can be a new column or one that already exists). <code>&lt;value_to_add&gt;</code> is the value you want to add to that column/row.</p>

<p>This example doesn't work precisely with the question at hand, but it might be useful for someone wants to add a specific value based on a condition.</p>
";;1;;2016-07-19T19:59:50.590;;38467449;2016-07-19T19:59:50.590;;;;;1224827.0;13842088.0;2;17;;;
62126;62126;;;"<p>Went with the good old stack trace. Learned a bit about pdb through Pycharm as well. Turns out what happens is the following:</p>

<p>1) </p>

<pre><code>cls.sum = _make_stat_function(
            'sum', name, name2, axis_descr,
            'Return the sum of the values for the requested axis',
            nanops.nansum)
</code></pre>

<p>Let's have a look at <code>_make_stat_function</code></p>

<p>2)</p>

<pre><code>def _make_stat_function(name, name1, name2, axis_descr, desc, f):
    @Substitution(outname=name, desc=desc, name1=name1, name2=name2,
                  axis_descr=axis_descr)
    @Appender(_num_doc)
    def stat_func(self, axis=None, skipna=None, level=None, numeric_only=None,
                  **kwargs):
        _validate_kwargs(name, kwargs, 'out', 'dtype')

        if skipna is None:
            skipna = True
        if axis is None:
            axis = self._stat_axis_number
        if level is not None:
            return self._agg_by_level(name, axis=axis, level=level,
                                      skipna=skipna)
        return self._reduce(f, name, axis=axis, skipna=skipna,
                            numeric_only=numeric_only)
</code></pre>

<p>The last line is key. It's kind of funny, as there are about 7 different <code>_reduces</code> within <code>pandas.core</code>. pdb says it's the one in <code>pandas.core.frame</code>. Let's take a look.</p>

<p>3)</p>

<pre><code>def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,
            filter_type=None, **kwds):
    axis = self._get_axis_number(axis)

    def f(x):
        return op(x, axis=axis, skipna=skipna, **kwds)

    labels = self._get_agg_axis(axis)

    # exclude timedelta/datetime unless we are uniform types
    if axis == 1 and self._is_mixed_type and self._is_datelike_mixed_type:
        numeric_only = True

    if numeric_only is None:
        try:
            values = self.values
            result = f(values)
        except Exception as e:

            # try by-column first
            if filter_type is None and axis == 0:
                try:

                    # this can end up with a non-reduction
                    # but not always. if the types are mixed
                    # with datelike then need to make sure a series
                    result = self.apply(f, reduce=False)
                    if result.ndim == self.ndim:
                        result = result.iloc[0]
                    return result
                except:
                    pass

            if filter_type is None or filter_type == 'numeric':
                data = self._get_numeric_data()
            elif filter_type == 'bool':
                data = self._get_bool_data()
            else:  # pragma: no cover
                e = NotImplementedError(""Handling exception with filter_""
                                        ""type %s not implemented."" %
                                        filter_type)
                raise_with_traceback(e)
            result = f(data.values)
            labels = data._get_agg_axis(axis)
    else:
        if numeric_only:
            if filter_type is None or filter_type == 'numeric':
                data = self._get_numeric_data()
            elif filter_type == 'bool':
                data = self._get_bool_data()
            else:  # pragma: no cover
                msg = (""Generating numeric_only data with filter_type %s""
                       ""not supported."" % filter_type)
                raise NotImplementedError(msg)
            values = data.values
            labels = data._get_agg_axis(axis)
        else:
            values = self.values
        result = f(values)

    if hasattr(result, 'dtype') and is_object_dtype(result.dtype):
        try:
            if filter_type is None or filter_type == 'numeric':
                result = result.astype(np.float64)
            elif filter_type == 'bool' and notnull(result).all():
                result = result.astype(np.bool_)
        except (ValueError, TypeError):

            # try to coerce to the original dtypes item by item if we can
            if axis == 0:
                result = com._coerce_to_dtypes(result, self.dtypes)

    return Series(result, index=labels)
</code></pre>

<p>Holy smokes, talk about an out of control function. Someone needs a refactoring! Let's zoom in on the trouble line(s):</p>

<pre><code>if hasattr(result, 'dtype') and is_object_dtype(result.dtype):
    try:
        if filter_type is None or filter_type == 'numeric':
            result = result.astype(np.float64)
</code></pre>

<p>And you better believe that last line gets executed. Here's some of the pdb trace:</p>

<pre><code>&gt; c:\users\matthew\anaconda2\lib\site-packages\pandas\core\frame.py(4801)_reduce()
-&gt; result = result.astype(np.float64)
(Pdb) l
4796                result = f(values)
4797    
4798            if hasattr(result, 'dtype') and is_object_dtype(result.dtype):
4799                try:
4800                    if filter_type is None or filter_type == 'numeric':
4801 -&gt;                     result = result.astype(np.float64)
4802                    elif filter_type == 'bool' and notnull(result).all():
4803                        result = result.astype(np.bool_)
4804                except (ValueError, TypeError):
4805    
4806                    # try to coerce to the original dtypes item by item if we can
</code></pre>

<p>If you're a non-believer, open up pandas.core.frame.py and put a <code>print ""OI""</code> right above line 4801. It should splat out to console :). Note I'm on Anaconda 2, windows.</p>

<p>I'm going to go with ""bug"", to answer your question.</p>
";;1;;2016-07-20T01:36:39.067;;38470963;2016-07-20T01:58:01.537;2016-07-20T01:58:01.537;;3691783.0;;3691783.0;38470550.0;2;17;;;
62247;62247;;;"<p>First make the list into a Series:</p>

<pre><code>se = pd.Series(mylist)
</code></pre>

<p>Then add the values to the DataFrame:</p>

<pre><code>df['new_col'] = se.values
</code></pre>
";;2;;2016-07-20T20:58:22.283;;38490727;2016-07-20T20:58:22.283;;;;;5405782.0;26666919.0;2;40;;;
62319;62319;;;"<blockquote>
  <blockquote>
    <p>Inserting <strong>data</strong> into panadas dataframe and <strong>providing coloumn name</strong>.   </p>
  </blockquote>
</blockquote>

<pre><code>import pandas as pd
df = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T
df.columns = [['Alphabet','Words']]
print(df)   #printing dataframe.
</code></pre>

<p><strong><em>This is our printed data:</em></strong>
<br><a href=""https://i.stack.imgur.com/BuKLt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BuKLt.png"" alt=""enter image description here""></a></p>

<p></p>

<blockquote>
  <blockquote>
    <p>For making a <strong><em>group of dataframe in pandas and counter</em></strong>, <br>
    You need to provide one more coloumn which counts the grouping, <strong>lets call that coloumn as, <em>""COUNTER""</em> in  dataframe</strong>.</p>
    
    <p>Like this:</p>
  </blockquote>
</blockquote>

<pre><code>df['COUNTER'] =1       #initially, set that counter to 1.
group_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function
print(group_data)
</code></pre>

<blockquote>
  <p><strong>OUTPUT:</strong></p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/EpdTH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EpdTH.png"" alt=""enter image description here""></a></p>

<blockquote>
  <blockquote>
    <p><strong><em>HOPE! IT HELPS</em></strong></p>
  </blockquote>
</blockquote>
";;0;;2016-07-21T11:53:17.417;;38503561;2016-07-22T07:28:24.537;2016-07-22T07:28:24.537;;5866942.0;;5866942.0;17679089.0;2;6;;;
62362;62362;;;"<p>It seems that in recent Pandas versions the way to go is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#assigning-new-columns-in-method-chains"" rel=""nofollow noreferrer"">df.assign</a>:</p>

<p><code>df1 = df1.assign(e=np.random.randn(sLength))</code></p>

<p>It doesn't produce SettingWithCopyWarning.</p>
";;0;;2016-07-21T17:35:37.920;;38510820;2017-04-27T22:30:28.410;2017-04-27T22:30:28.410;;2623899.0;;114795.0;12555323.0;2;21;;;
62577;62577;;;"<p>Try updating pip:</p>

<pre><code>pip install -U pip
</code></pre>
";;1;;2016-07-23T17:40:37.183;;38544742;2016-07-23T17:40:37.183;;;;;28035.0;26473681.0;2;8;;;
62835;62835;;;"<p>Try:</p>

<pre><code>(df['A'] + df['B']).where((df['A'] &lt; 0) | (df['B'] &gt; 0), df['A'] / df['B'])
</code></pre>

<p>The difference between the <code>numpy</code> <code>where</code> and <code>DataFrame</code> <code>where</code> is that the default values are supplied by the <code>DataFrame</code> that the <code>where</code> method is being called on (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html"">docs</a>).</p>

<p>I.e.</p>

<pre><code>np.where(m, A, B)
</code></pre>

<p>is roughly equivalent to</p>

<pre><code>A.where(m, B)
</code></pre>

<p>If you wanted a similar call signature using pandas, you could take advantage of <a href=""https://docs.python.org/3/tutorial/classes.html#method-objects"">the way method calls work in Python</a>:</p>

<pre><code>pd.DataFrame.where(cond=(df['A'] &lt; 0) | (df['B'] &gt; 0), self=df['A'] + df['B'], other=df['A'] / df['B'])
</code></pre>

<p>or without kwargs (Note: that the positional order of arguments is different from the <code>numpy</code> <code>where</code> <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"">argument order</a>):</p>

<pre><code>pd.DataFrame.where(df['A'] + df['B'], (df['A'] &lt; 0) | (df['B'] &gt; 0), df['A'] / df['B'])
</code></pre>
";;9;;2016-07-26T01:15:44.807;;38579700;2016-07-26T01:33:25.767;2016-07-26T01:33:25.767;;1953800.0;;1953800.0;38579532.0;2;13;;;
62844;62844;;;"<p>The cleanest solution would be use a <code>FunctionTransformer</code> to convert to dense: this will automatically implement the <code>fit</code>, <code>transform</code> and <code>fit_transform</code> methods as in David's answer. Additionally if I don't need special names for my pipeline steps, I like to use the <code>sklearn.pipeline.make_pipeline</code> convenience function to enable a more minimalist language for describing the model:</p>

<pre><code>pipeline = make_pipeline(
     CountVectorizer(), 
     FunctionTransformer(lambda x: x.todense(), accept_sparse=True), 
     RandomForestClassifier()
)
</code></pre>
";;2;;2016-07-26T03:32:00.407;;38580576;2016-10-04T21:22:06.553;2016-10-04T21:22:06.553;;839957.0;;839957.0;28384680.0;2;8;;;
63293;63293;;;"<p><code>OneHotEncoder</code> cannot process string values directly. If your nominal features are strings, then you need to first map them into integers.</p>

<p><code>pandas.get_dummies</code> is kind of the opposite. By default, it only converts string columns into one-hot representation, unless columns are specified. </p>
";;1;;2016-07-29T05:07:19.750;;38650886;2016-07-29T05:07:19.750;;;;;534298.0;36631163.0;2;10;;;
63660;63660;;;"<p>I found the easiest way is to use the <code>colormap</code> parameter in <code>.plot()</code> with one of the preset color gradients:</p>

<pre><code>df.plot(kind='bar', stacked=True, colormap='Paired')
</code></pre>

<p><a href=""https://i.stack.imgur.com/7BOTc.png""><img src=""https://i.stack.imgur.com/7BOTc.png"" alt=""enter image description here""></a></p>

<p>You can find a large <a href=""http://scipy.github.io/old-wiki/pages/Cookbook/Matplotlib/Show_colormaps"">list of preset colormaps here</a>.</p>

<p><a href=""https://i.stack.imgur.com/i5XZo.png""><img src=""https://i.stack.imgur.com/i5XZo.png"" alt=""colormaps""></a></p>
";;0;;2016-08-01T22:50:51.950;;38709267;2016-08-01T22:50:51.950;;;;;683407.0;11927715.0;2;17;;;
63696;63696;;;"<p>pandas tries really hard to make making dataframes convenient.  As such, it interprets lists and arrays as things you'd want to split into columns.  I'm not going to complain, this is almost always helpful.</p>

<p>I've done this one of two ways.</p>

<p><strong><em>Option 1</em></strong>:</p>

<pre><code># Only works with a non MultiIndex
# and its slow, so don't use it
df.T.apply(tuple).apply(list)
</code></pre>

<p><strong><em>Option 2</em></strong>:</p>

<pre><code>pd.Series(df.T.to_dict('list'))
</code></pre>

<p>Both give you:</p>

<pre><code>a    [1, 2, 3, 4]
b    [5, 6, 7, 8]
dtype: object
</code></pre>

<p>However <strong><em>Option 2</em></strong> scales better.</p>

<hr>

<h3>Timing</h3>

<p><strong>given <code>df</code></strong></p>

<p><a href=""https://i.stack.imgur.com/oJ0nk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oJ0nk.png"" alt=""enter image description here""></a></p>

<p><strong>much larger <code>df</code></strong></p>

<pre><code>from string import ascii_letters
letters = list(ascii_letters)
df = pd.DataFrame(np.random.choice(range(10), (52 ** 2, 52)),
                  pd.MultiIndex.from_product([letters, letters]),
                  letters)
</code></pre>

<p>Results for <code>df.T.apply(tuple).apply(list)</code> are erroneous because that solution doesn't work over a MultiIndex.</p>

<p><a href=""https://i.stack.imgur.com/X2c18.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/X2c18.png"" alt=""enter image description here""></a></p>
";;0;;2016-08-02T06:30:33.320;;38713212;2016-08-02T07:09:37.043;2016-08-02T07:09:37.043;;2336654.0;;2336654.0;38713200.0;2;8;;;
63698;63698;;;"<p>You can first convert <code>DataFrame</code> to <code>numpy array</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html""><code>values</code></a>, then convert to list and last create new <code>Series</code> with index from <code>df</code> if need faster solution:</p>

<pre><code>print (pd.Series(df.values.tolist(), index=df.index))
a    [1, 2, 3, 4]
b    [5, 6, 7, 8]
dtype: object
</code></pre>

<p>Timings with small DataFrame:</p>

<pre><code>In [76]: %timeit (pd.Series(df.values.tolist(), index=df.index))
1000 loops, best of 3: 295 s per loop

In [77]: %timeit pd.Series(df.T.to_dict('list'))
1000 loops, best of 3: 685 s per loop

In [78]: %timeit df.T.apply(tuple).apply(list)
1000 loops, best of 3: 958 s per loop
</code></pre>

<p>and with large:</p>

<pre><code>from string import ascii_letters
letters = list(ascii_letters)
df = pd.DataFrame(np.random.choice(range(10), (52 ** 2, 52)),
                  pd.MultiIndex.from_product([letters, letters]),
                  letters)

In [71]: %timeit (pd.Series(df.values.tolist(), index=df.index))
100 loops, best of 3: 2.06 ms per loop

In [72]: %timeit pd.Series(df.T.to_dict('list'))
1 loop, best of 3: 203 ms per loop

In [73]: %timeit df.T.apply(tuple).apply(list)
1 loop, best of 3: 506 ms per loop
</code></pre>
";;0;;2016-08-02T06:41:55.957;;38713387;2016-08-02T06:49:11.940;2016-08-02T06:49:11.940;;2901002.0;;2901002.0;38713200.0;2;17;;;
63927;63927;;;"<p>I just ran into this, and the pandas issue is still open, so I'm posting my workaround. Assuming <code>df</code> is my DataFrame and <code>dtype</code> is a dict mapping column names to types:</p>

<pre><code>for k, v in dtype.items():
    df[k] = df[k].astype(v)
</code></pre>

<p>(note: use <code>dtype.iteritems()</code> in python 2)</p>

<p>For the reference:</p>

<ul>
<li>The list of allowed data types (NumPy <code>dtypes</code>): <a href=""https://docs.scipy.org/doc/numpy-1.12.0/reference/arrays.dtypes.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy-1.12.0/reference/arrays.dtypes.html</a></li>
<li>Pandas also supports some other types. E.g., <code>category</code>: <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/categorical.html</a></li>
<li>The relevant GitHub issue: <a href=""https://github.com/pandas-dev/pandas/issues/9287"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/9287</a></li>
</ul>
";;2;;2016-08-03T17:46:08.557;;38750433;2017-08-15T10:45:08.647;2017-08-15T10:45:08.647;;420867.0;;643821.0;25610592.0;2;8;;;
63995;63995;;;"<p>@Pietrovismara's solution is correct but I'd just like to add: rather than having a separate line to add column names, it's possible to do this from pd.read_csv.</p>

<pre><code>df = pd.read_csv('output_list.txt', sep="" "", header=None, names=[""a"", ""b"", ""c""])
</code></pre>
";;0;;2016-08-04T03:25:02.080;;38757990;2016-08-04T03:25:02.080;;;;;2574823.0;21546739.0;2;12;;;
64114;64114;;;"<p>I know this question and answer has been chewed to death. But I referred to it for inspiration for one of the problem I was having . I was able to solve it using bits and pieces from different answers hence providing my response in case anyone needs it.</p>

<p>My method is generic wherein you can add additional delimiters by comma separating <code>delimiters=</code> variable and future-proof it.</p>

<p><strong>Working Code:</strong></p>

<pre><code>import pandas as pd
import re


df = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})

delimiters = '$'
matchPattern = '|'.join(map(re.escape, delimiters))
df.columns = [re.split(matchPattern, i)[1] for i in df.columns ]
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt;&gt;&gt; df
   $a  $b  $c  $d  $e
0   1   3   5   7   9
1   2   4   6   8  10

&gt;&gt;&gt; df
   a  b  c  d   e
0  1  3  5  7   9
1  2  4  6  8  10
</code></pre>
";;0;;2016-08-04T20:26:50.133;;38776854;2016-08-04T20:26:50.133;;;;;5936628.0;11346283.0;2;7;;;
64250;64250;;;"<p>You could override the CSS of the output code. It uses <code>flex-direction: column</code> by default. Try changing it to <code>row</code> instead. Here's an example:</p>

<pre><code>import pandas as pd
import numpy as np
from IPython.display import display, HTML

CSS = """"""
.output {
    flex-direction: row;
}
""""""

HTML('&lt;style&gt;{}&lt;/style&gt;'.format(CSS))
</code></pre>

<p><a href=""https://i.stack.imgur.com/RbIxj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RbIxj.png"" alt=""Jupyter image""></a></p>

<p>You could, of course, customize the CSS further as you wish.</p>

<p>If you wish to target only one cell's output, try using the <code>:nth-child()</code> selector. For example, this code will modify the CSS of the output of only the 5th cell in the notebook:</p>

<pre><code>CSS = """"""
div.cell:nth-child(5) .output {
    flex-direction: row;
}
""""""
</code></pre>
";;7;;2016-08-06T08:03:13.477;;38801975;2017-01-28T13:39:39.040;2017-01-28T13:39:39.040;;5912399.0;;5912399.0;38783027.0;2;18;;;
64798;64798;;;"<ul>
<li><p>Explicit is better than implicit. </p>

<p><code>df[boolean_mask]</code> selects rows where <code>boolean_mask</code> is True, but there is a corner case when you might not want it to: when <code>df</code> has boolean-valued column labels:</p>

<pre><code>In [229]: df = pd.DataFrame({True:[1,2,3],False:[3,4,5]}); df
Out[229]: 
   False  True 
0      3      1
1      4      2
2      5      3
</code></pre>

<p>You might want to use <code>df[[True]]</code> to select the <code>True</code> column. Instead it raises a <code>ValueError</code>:</p>

<pre><code>In [230]: df[[True]]
ValueError: Item wrong length 1 instead of 3.
</code></pre>

<p>In contrast, the following does not raise <code>ValueError</code> even though the structure of <code>df2</code> is almost the same:</p>

<pre><code>In [258]: df2 = pd.DataFrame({'A':[1,2,3],'B':[3,4,5]}); df2
Out[258]: 
   A  B
0  1  3
1  2  4
2  3  5

In [259]: df2[['B']]
Out[259]: 
   B
0  3
1  4
2  5
</code></pre>

<p>Also note that</p>

<pre><code>In [231]: df.loc[[True]]
Out[231]: 
   False  True 
0      3      1
</code></pre>

<p>Thus, <code>df[boolean_mask]</code> does not always behave the same as <code>df.loc[boolean_mask]</code>. Even though this is arguably an unlikely use case, I would recommend always using <code>df.loc[boolean_mask]</code> instead of <code>df[boolean_mask]</code> because the meaning of <code>df.loc</code>'s syntax is explicit. With <code>df.loc[indexer]</code> you know automatically that <code>df.loc</code> is selecting rows. In contrast, it is not clear if <code>df[indexer]</code> will select rows or columns (or raise <code>ValueError</code>) without knowing details about <code>indexer</code> and <code>df</code>.</p></li>
<li><p><code>df.loc[row_indexer, column_index]</code> can select rows <em>and</em> columns. <code>df[indexer]</code> can only select rows <em>or</em> columns depending on the type of values in <code>indexer</code> and the type of column values <code>df</code> has (again, are they boolean?). </p>

<pre><code>In [237]: df2.loc[[True,False,True], 'B']
Out[237]: 
0    3
2    5
Name: B, dtype: int64
</code></pre></li>
<li><p>When a slice is passed to <code>df.loc</code> the end-points are included in the range. When a slice is passed to <code>df[...]</code>, the slice is interpreted as a half-open interval:</p>

<pre><code>In [239]: df2.loc[1:2]
Out[239]: 
   A  B
1  2  4
2  3  5

In [271]: df2[1:2]
Out[271]: 
   A  B
1  2  4
</code></pre></li>
</ul>
";;1;;2016-08-11T02:08:40.420;;38886211;2016-08-11T03:18:34.440;2016-08-11T03:18:34.440;;190597.0;;190597.0;38886080.0;2;13;;;
64822;64822;;;"<p>there is a nicer alternative - use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental"" rel=""nofollow noreferrer"">query()</a> method:</p>

<pre><code>In [58]: df = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})

In [59]: df
Out[59]:
   closing_price
0            104
1             99
2             98
3             95
4            103
5            101
6            101
7             99
8             95
9             96

In [60]: df.query('99 &lt;= closing_price &lt;= 101')
Out[60]:
   closing_price
1             99
5            101
6            101
7             99
</code></pre>

<p><strong>UPDATE:</strong> answering the comment:</p>

<blockquote>
  <p>I like the syntax here but fell down when trying to combine with
  expresison; <code>df.query('(mean + 2 *sd) &lt;= closing_price &lt;=(mean + 2
  *sd)')</code></p>
</blockquote>

<pre><code>In [161]: qry = ""(closing_price.mean() - 2*closing_price.std())"" +\
     ...:       "" &lt;= closing_price &lt;= "" + \
     ...:       ""(closing_price.mean() + 2*closing_price.std())""
     ...:

In [162]: df.query(qry)
Out[162]:
   closing_price
0             97
1            101
2             97
3             95
4            100
5             99
6            100
7            101
8             99
9             95
</code></pre>
";;5;;2016-08-11T07:07:57.237;;38889524;2017-08-21T18:36:45.060;2017-08-21T18:36:45.060;;5741205.0;;5741205.0;31617845.0;2;6;;;
64882;64882;;;"<p>There's great information in these answers, but I'm adding my own to clearly summarize which methods work array-wise versus element-wise. jeremiahbuddha mostly did this but did not mention Series.apply.  I don't have the rep to comment.</p>

<ul>
<li><p><code>DataFrame.apply</code> operates on entire rows or columns at a time.</p></li>
<li><p><code>DataFrame.applymap</code>, <code>Series.apply</code>, and <code>Series.map</code> operate on one
element at time.</p></li>
</ul>

<p>There is a lot of overlap between the capabilities of <code>Series.apply</code> and <code>Series.map</code>, meaning that either one will work in most cases.  They do have some slight differences though, some of which were discussed in osa's answer.</p>
";;0;;2016-08-11T15:20:32.563;;38900352;2016-08-11T15:20:32.563;;;;;5405967.0;19798153.0;2;7;;;
64907;64907;;;"<p>You can use  <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html"" rel=""noreferrer""><code>sklearn.utils.shuffle()</code></a> (<a href=""https://github.com/scikit-learn/scikit-learn/issues/4008"" rel=""noreferrer"">requires</a> sklearn 0.16.1 or higher to support Pandas data frames):</p>

<pre><code># Generate data
import pandas as pd
df = pd.DataFrame({'A':range(5), 'B':range(5)})
print('df: {0}'.format(df))

# Shuffle Pandas data frame
import sklearn.utils
df = sklearn.utils.shuffle(df)
print('\n\ndf: {0}'.format(df))
</code></pre>

<p>outputs:</p>

<pre><code>df:    A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4


df:    A  B
1  1  1
0  0  0
3  3  3
4  4  4
2  2  2
</code></pre>

<p>Then you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>df.reset_index()</code></a> to reset the index column, if needs to be:</p>

<pre><code>df = df.reset_index(drop=True)
print('\n\ndf: {0}'.format(df)
</code></pre>

<p>outputs:</p>

<pre><code>df:    A  B
0  1  1
1  0  0
2  4  4
3  2  2
4  3  3
</code></pre>
";;0;;2016-08-11T17:40:15.687;;38902835;2016-08-11T17:40:15.687;;;;;395857.0;15772009.0;2;8;;;
65076;65076;;;"<p>You could do it easily with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.lower.html"" rel=""noreferrer""><code>str.lower</code></a> for <code>columns</code>:</p>

<pre><code>df.columns = df.columns.str.lower()
</code></pre>

<p>Example:</p>

<pre><code>In [63]: df
Out[63]: 
  country country isocode  year     XRAT         tcgdp
0  Canada             CAN  2001  1.54876  9.249094e+05
1  Canada             CAN  2002  1.56932  9.572999e+05
2  Canada             CAN  2003  1.40105  1.016902e+06

In [64]: df.columns = df.columns.str.lower()

In [65]: df
Out[65]: 
  country country isocode  year     xrat         tcgdp
0  Canada             CAN  2001  1.54876  9.249094e+05
1  Canada             CAN  2002  1.56932  9.572999e+05
2  Canada             CAN  2003  1.40105  1.016902e+06
</code></pre>
";;0;;2016-08-13T10:41:42.127;;38931854;2016-08-13T10:41:42.127;;;;;4542359.0;19726029.0;2;10;;;
65098;65098;;;"<p>The equivalent of</p>

<pre><code>df %&gt;% groupby(col1) %&gt;% summarize(col2_agg=max(col2), col3_agg=min(col3))
</code></pre>

<p>is</p>

<pre><code>df.groupby('col1').agg({'col2': 'max', 'col3': 'min'})
</code></pre>

<p>which returns </p>

<pre><code>      col2  col3
col1            
1        5    -5
2        9    -9
</code></pre>

<p>The returning object is a pandas.DataFrame with an index called <code>col1</code> and columns named <code>col2</code> and <code>col3</code>. By default, when you group your data pandas sets the grouping column(s) as index for efficient access and modification. However, if you don't want that, there are two alternatives to set <code>col1</code> as a column.</p>

<ul>
<li><p>Pass <code>as_index=False</code>: </p>

<pre><code>df.groupby('col1', as_index=False).agg({'col2': 'max', 'col3': 'min'})
</code></pre></li>
<li><p>Call <code>reset_index</code>: </p>

<pre><code>df.groupby('col1').agg({'col2': 'max', 'col3': 'min'}).reset_index()
</code></pre></li>
</ul>

<p>both yield</p>

<pre><code>col1  col2  col3           
   1     5    -5
   2     9    -9
</code></pre>

<p>You can also pass multiple functions to <code>groupby.agg</code>.</p>

<pre><code>agg_df = df.groupby('col1').agg({'col2': ['max', 'min', 'std'], 
                                 'col3': ['size', 'std', 'mean', 'max']})
</code></pre>

<p>This also returns a DataFrame but now it has a MultiIndex for columns.</p>

<pre><code>     col2               col3                   
      max min       std size       std mean max
col1                                           
1       5   1  1.581139    5  1.581139   -3  -1
2       9   0  3.535534    5  3.535534   -6   0
</code></pre>

<p>MultiIndex is very handy for selection and grouping. Here are some examples:</p>

<pre><code>agg_df['col2']  # select the second column
      max  min       std
col1                    
1       5    1  1.581139
2       9    0  3.535534

agg_df[('col2', 'max')]  # select the maximum of the second column
Out: 
col1
1    5
2    9
Name: (col2, max), dtype: int64

agg_df.xs('max', axis=1, level=1)  # select the maximum of all columns
Out: 
      col2  col3
col1            
1        5    -1
2        9     0
</code></pre>

<p>Earlier (before <a href=""http://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""nofollow noreferrer"">version 0.20.0</a>) it was possible to use dictionaries for renaming the columns in the <code>agg</code> call. For example</p>

<pre><code>df.groupby('col1')['col2'].agg({'max_col2': 'max'})
</code></pre>

<p>would return the maximum of the second column as <code>max_col2</code>:</p>

<pre><code>      max_col2
col1          
1            5
2            9
</code></pre>

<p>However, it was deprecated in favor of the rename method:</p>

<pre><code>df.groupby('col1')['col2'].agg(['max']).rename(columns={'max': 'col2_max'})

      col2_max
col1          
1            5
2            9
</code></pre>

<p>It can get verbose for a DataFrame like <code>agg_df</code> defined above. You can use a renaming function to flatten those levels in that case:</p>

<pre><code>agg_df.columns = ['_'.join(col) for col in agg_df.columns]

      col2_max  col2_min  col2_std  col3_size  col3_std  col3_mean  col3_max
col1                                                                        
1            5         1  1.581139          5  1.581139         -3        -1
2            9         0  3.535534          5  3.535534         -6         0
</code></pre>

<p>For operations like <code>groupby().summarize(newcolumn=max(col2 * col3))</code>, you can still use agg by first adding a new column with <code>assign</code>.</p>

<pre><code>df.assign(new_col=df.eval('col2 * col3')).groupby('col1').agg('max') 

      col2  col3  new_col
col1                     
1        5    -1       -1
2        9     0        0
</code></pre>

<p>This returns maximum for old and new columns but as always you can slice that.</p>

<pre><code>df.assign(new_col=df.eval('col2 * col3')).groupby('col1')['new_col'].agg('max')

col1
1   -1
2    0
Name: new_col, dtype: int64
</code></pre>

<p>With <code>groupby.apply</code> this would be shorter:</p>

<pre><code>df.groupby('col1').apply(lambda x: (x.col2 * x.col3).max())

col1
1   -1
2    0
dtype: int64
</code></pre>

<p>However, <code>groupby.apply</code> treats this as a custom function so it is not vectorized. Up to now, the functions we passed to <code>agg</code> ('min', 'max', 'min', 'size' etc.) are vectorized and these are aliases for those optimized functions. You can replace <code>df.groupby('col1').agg('min')</code> with <code>df.groupby('col1').agg(min)</code>, <code>df.groupby('col1').agg(np.min)</code> or <code>df.groupby('col1').min()</code> and they will all execute the same function. You will not see the same efficiency when you use custom functions.</p>

<p>Lastly, as of version 0.20, <code>agg</code> can be used on DataFrames directly, without having to group first. See examples <a href=""https://stackoverflow.com/a/43586132/2285236"">here</a>.</p>
";;5;;2016-08-13T18:18:21.130;;38935669;2017-08-08T11:45:38.830;2017-08-08T11:45:38.830;;2285236.0;;2285236.0;38935541.0;2;15;;;
65201;65201;;;"<p>The biggest clue is the rows are all being returned on one line. This indicates line terminators are being ignored or are not present. </p>

<p>You can specify the line terminator for csv_reader. If you are on a mac the lines created will end with <code>\r</code>rather than the linux standard <code>\n</code> or better still the suspenders and belt approach of windows with <code>\r\n</code>.</p>

<pre><code>pandas.read_csv(filename, sep='\t', lineterminator='\r')
</code></pre>

<p>You could also open all your data using the codecs package. This may increase robustness at the expense of document loading speed.</p>

<pre><code>import codecs

doc = codecs.open('document','rU','UTF-16') #(open for reading with ""universal"" type set)

df = pandas.read_csv(doc, sep='\t')
</code></pre>
";;0;;2016-08-15T08:35:14.623;;38951835;2017-08-05T15:13:38.097;2017-08-05T15:13:38.097;;786326.0;;3502465.0;27896214.0;2;10;;;
66086;66086;;;"<p>You can compute pairwise cosine similarity on the rows of a sparse matrix directly using sklearn.  As of version 0.17 it also supports sparse output:</p>

<pre><code>from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

A =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])
A_sparse = sparse.csr_matrix(A)

similarities = cosine_similarity(A_sparse)
print('pairwise dense output:\n {}\n'.format(similarities))

#also can output sparse matrices
similarities_sparse = cosine_similarity(A_sparse,dense_output=False)
print('pairwise sparse output:\n {}\n'.format(similarities_sparse))
</code></pre>

<p>Results:</p>

<pre><code>pairwise dense output:
[[ 1.          0.40824829  0.40824829]
[ 0.40824829  1.          0.33333333]
[ 0.40824829  0.33333333  1.        ]]

pairwise sparse output:
(0, 1)  0.408248290464
(0, 2)  0.408248290464
(0, 0)  1.0
(1, 0)  0.408248290464
(1, 2)  0.333333333333
(1, 1)  1.0
(2, 1)  0.333333333333
(2, 0)  0.408248290464
(2, 2)  1.0
</code></pre>

<p>If you want column-wise cosine similarities simply transpose your input matrix beforehand: </p>

<pre><code>A_sparse.transpose()
</code></pre>
";;0;;2016-08-23T14:45:15.900;;39104306;2016-08-23T14:45:15.900;;;;;6243513.0;17627219.0;2;19;;;
66166;66166;;;"<p>As the answer given by @pelson uses <code>set_color_cycle</code> and this is deprecated in Matplotlib 1.5, I thought it would be useful to have an updated version of his solution using <code>set_prop_cycle</code>:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

for i in range(3):
    plt.plot(np.arange(10) + i)

plt.gca().set_prop_cycle(None)

for i in range(3):
    plt.plot(np.arange(10, 0, -1) + i)

plt.show()
</code></pre>

<p>Remark also that I had to change <code>np.arange(10,1,-1)</code> to <code>np.arange(10,0,-1)</code>. The former gave an array of only 9 elements. This probably arises from using different Numpy versions. Mine is 1.10.2.</p>

<p><strong>EDIT</strong>: Removed the need to use <code>rcParams</code>. Thanks to @divenex for pointing that out in a comment.</p>
";;4;;2016-08-24T06:57:07.610;;39116381;2017-02-17T13:59:38.217;2017-02-17T13:59:38.217;;2783943.0;;2783943.0;24193174.0;2;6;;;
66285;66285;;;"<p>I use <code>groupby</code> and <code>size</code></p>

<pre><code>df.groupby(['id', 'group', 'term']).size().unstack(fill_value=0)
</code></pre>

<p><a href=""https://i.stack.imgur.com/uEOMQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uEOMQ.png"" alt=""enter image description here""></a></p>

<hr>

<h1>Timing</h1>

<p><a href=""https://i.stack.imgur.com/Qz8dM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Qz8dM.png"" alt=""enter image description here""></a></p>

<p><strong><em>1,000,000 rows</em></strong></p>

<pre><code>df = pd.DataFrame(dict(id=np.random.choice(100, 1000000),
                       group=np.random.choice(20, 1000000),
                       term=np.random.choice(10, 1000000)))
</code></pre>

<p><a href=""https://i.stack.imgur.com/FZ3GD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FZ3GD.png"" alt=""enter image description here""></a></p>
";;8;;2016-08-24T20:57:41.523;;39132900;2016-08-24T22:13:38.547;2016-08-24T22:13:38.547;;2336654.0;;2336654.0;39132742.0;2;10;;;
66410;66410;;;"<p>this should work:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,5,(6, 2)), columns=['col1','col2'])
&gt;&gt;&gt; df['ind1'] = list('AAABCC')
&gt;&gt;&gt; df['ind2'] = range(6)
&gt;&gt;&gt; df.set_index(['ind1','ind2'], inplace=True)
&gt;&gt;&gt; df

           col1  col2
ind1 ind2            
A    0        3     2
     1        2     0
     2        2     3
B    3        2     4
C    4        3     1
     5        0     0


&gt;&gt;&gt; df.groupby([df.index.get_level_values(0),'col1']).count()

           col2
ind1 col1      
A    2        2
     3        1
B    2        1
C    0        1
     3        1
</code></pre>

<p>I had the same problem using one of the columns from multiindex. with multiindex, you cannot use df.index.levels[0] since it has only distinct values from that particular index level and will be most likely of different size than whole dataframe...</p>

<p>check <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_level_values.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_level_values.html</a> - get_level_values ""Return vector of label values for requested level, equal to the length of the index""</p>
";;0;;2016-08-26T08:40:47.160;;39162006;2016-08-26T08:50:00.887;2016-08-26T08:50:00.887;;6728093.0;;6728093.0;30925079.0;2;23;;;
66530;66530;;;"<p>Instead of using <code>pd.TimeGrouper</code>, a combination of <code>groupby</code> then <code>resample</code> is more straightforward and linked to <a href=""https://stackoverflow.com/questions/18677271/grouping-daily-data-by-month-in-python-pandas-and-then-normalising"">from the pandas documentation to this SO post</a></p>

<pre><code>grouped = df.groupby('Location').resample('H')['Event'].count()
grouped.unstack(level=0).fillna(0)
</code></pre>

<p>Will output</p>

<pre><code>Location              HK  LDN
2014-08-25 21:00:00  1.0  1.0
2014-08-25 22:00:00  0.0  2.0
</code></pre>
";;0;;2016-08-27T22:34:22.773;;39186403;2016-08-27T22:34:22.773;2017-05-23T12:34:17.993;;-1.0;;3707607.0;32012012.0;2;9;;;
66576;66576;;;"<pre><code>df[df.columns.difference(['b'])]

Out: 
          a         c         d
0  0.427809  0.459807  0.333869
1  0.678031  0.668346  0.645951
2  0.996573  0.673730  0.314911
3  0.786942  0.719665  0.330833
</code></pre>
";;1;;2016-08-28T14:05:30.033;;39192113;2016-08-28T14:05:30.033;;;;;2285236.0;29763620.0;2;9;;;
66641;66641;;;"<p><strong>update</strong> - Look for another answer above from me titled <strong>Best way Found</strong>. It's really the best way :)</p>

<p>if you want the month year unique pair, using apply is pretty sleek.</p>

<pre><code>    df['mnth_yr'] = df['date_column'].apply(lambda x: x.strftime('%B-%Y'))     
</code></pre>

<p>outputs month-year in one column.</p>

<p>don't forget to first change the format to date-time before, I generally forget :|</p>

<pre><code>    df['date_column'] = pd.to_datetime(df['date_column'])
</code></pre>
";;0;;2016-08-29T12:28:37.073;;39206377;2017-07-21T05:17:12.517;2017-07-21T05:17:12.517;;3364687.0;;3364687.0;25146121.0;2;13;;;
66816;66816;;;"<p>In your case,</p>

<pre><code>df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)
</code></pre>

<p>will do exactly what you what.</p>

<p>In my case (general form):</p>

<pre><code>df = df.reindex_axis(sorted(df.columns), axis=1)
df = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)
</code></pre>
";;0;;2016-08-30T21:57:36.780;;39237712;2016-09-14T21:20:31.137;2016-09-14T21:20:31.137;;4803173.0;;4803173.0;13148429.0;2;19;;;
66877;66877;;;"<p>The dataframe.sort() method is - so my understanding - deprecated in pandas > 0.18. In order to solve your problem you should use dataframe.sort_values() instead:</p>

<pre><code>f.sort_values(by=[""c1"",""c2""], ascending=[False, True])
</code></pre>

<p>The output looks like this: </p>

<pre><code>    c1  c2
    3   10
    2   15
    2   30
    2   100
    1   20
</code></pre>
";;0;;2016-08-31T10:05:08.633;;39246607;2016-08-31T10:05:08.633;;;;;2672805.0;17618981.0;2;7;;;
66890;66890;;;"<p>I use this to grouby a Series by its values:</p>

<pre><code>grouped = s.groupby(s)
</code></pre>

<p>Or:</p>

<pre><code>grouped = s.groupby(lambda x: s[x])
</code></pre>
";;1;;2016-08-31T11:01:56.300;;39247824;2017-06-01T09:40:40.770;2017-06-01T09:40:40.770;;1201614.0;;1201614.0;33483670.0;2;20;;;
66907;66907;;;"<p>This is simple to do with <a href=""https://stanford.edu/~mwaskom/software/seaborn/index.html"" rel=""noreferrer"">Seaborn</a> (<code>pip install seaborn</code>) as a oneliner </p>

<p><code>sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)</code>
:</p>

<pre><code>import seaborn as sns
import pandas as pd
import numpy as np
np.random.seed(1974)

df = pd.DataFrame(
    np.random.normal(10, 1, 30).reshape(10, 3),
    index=pd.date_range('2010-01-01', freq='M', periods=10),
    columns=('one', 'two', 'three'))
df['key1'] = (4, 4, 4, 6, 6, 6, 8, 8, 8, 8)

sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/sDrhW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sDrhW.png"" alt=""enter image description here""></a></p>

<p>Here is the dataframe for reference:</p>

<p><a href=""https://i.stack.imgur.com/V7XYk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/V7XYk.png"" alt=""enter image description here""></a></p>

<p>Since you have three variable columns in your data, you may want to plot all pairwise dimensions with:</p>

<pre><code>sns.pairplot(vars=[""one"",""two"",""three""], data=df, hue=""key1"", size=5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/zOWFk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zOWFk.png"" alt=""enter image description here""></a></p>
";;0;;2016-08-31T13:44:54.077;;39251401;2017-05-05T13:53:35.730;2017-05-05T13:53:35.730;;5058588.0;;5058588.0;21654635.0;2;13;;;
66957;66957;;;"<p>A simple solution is:</p>

<pre><code>df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)
</code></pre>
";;1;;2016-08-31T21:39:46.173;;39259437;2016-08-31T21:39:46.173;;;;;6780703.0;13331698.0;2;12;;;
67043;67043;;;"<p>numpy is much faster than Pandas for this kind of things :</p>

<pre><code>numpy.percentile(df.a,95) # attention : the percentile is given in percent (5 = 5%)
</code></pre>

<p>is equivalent but 3 times faster than :</p>

<pre><code>df.a.quantile(.95)  # as you already noticed here it is "".95"" not ""95""
</code></pre>

<p>so for your code, it gives :</p>

<pre><code>df[df.a &lt; np.percentile(df.a,95)]
</code></pre>
";;0;;2016-09-01T16:02:54.840;;39276195;2016-09-01T16:02:54.840;;;;;4316500.0;18580461.0;2;7;;;
67132;67132;;;"<p><strong>Approach 1: You can use get_dummies on pandas dataframe.</strong></p>

<p>Example 1:</p>

<p><a href=""https://i.stack.imgur.com/kb7Bq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kb7Bq.png"" alt=""enter image description here""></a></p>

<p>Example 2:</p>

<p>The following will transform a given column into one hot. Use prefix to have multiple dummies.</p>

<p><a href=""https://i.stack.imgur.com/t5gGS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t5gGS.png"" alt=""enter image description here""></a></p>

<p><strong>Approach 2: Use Scikit-learn</strong></p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a></p>

<p><a href=""https://i.stack.imgur.com/eaTqF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eaTqF.png"" alt=""enter image description here""></a></p>
";;0;;2016-09-02T07:55:21.130;;39287161;2016-09-02T07:55:21.130;;;;;6633975.0;37292872.0;2;21;;;
67289;67289;;;"<p><code>round()</code> method was added for DatetimeIndex, Timestamp, TimedeltaIndex and Timedelta in pandas 0.18.0. Now we can do the following:</p>

<pre><code>In[114]: index = pd.DatetimeIndex([pd.Timestamp('2012-01-01 02:03:04.000'), pd.Timestamp('2012-01-01 02:03:04.002'), pd.Timestamp('20130712 02:03:04.500'), pd.Timestamp('2012-01-01 02:03:04.501')])

In[115]: index.values
Out[115]: 
array(['2012-01-01T02:03:04.000000000', '2012-01-01T02:03:04.002000000',
       '2013-07-12T02:03:04.500000000', '2012-01-01T02:03:04.501000000'], dtype='datetime64[ns]')

In[116]: index.round('S')
Out[116]: 
DatetimeIndex(['2012-01-01 02:03:04', '2012-01-01 02:03:04',
               '2013-07-12 02:03:04', '2012-01-01 02:03:05'],
              dtype='datetime64[ns]', freq=None)
</code></pre>

<p><code>round()</code> accepts frequency parameter. String aliases for it are listed <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"" rel=""noreferrer"">here</a>.</p>
";;0;;2016-09-03T19:02:03.680;;39310387;2016-09-03T19:02:03.680;;;;;1438906.0;13785932.0;2;7;;;
67497;67497;;;"<p>Since pandas has styling functionality now, you don't need JavaScript hacks anymore.
This is a pure pandas solution:</p>

<pre><code>import pandas as pd

df = []
df.append(dict(date='2016-04-01', sleep=11.2, calories=2740))
df.append(dict(date='2016-04-02', sleep=7.3, calories=3600))
df.append(dict(date='2016-04-03', sleep=8.3, calories=3500))

df = pd.DataFrame(df)

def highlight_last_row(s):
    return ['background-color: #FF0000' if i==len(s)-1 else '' for i in range(len(s))]

s = df.style.apply(highlight_last_row)
</code></pre>

<p><a href=""https://i.stack.imgur.com/z474p.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/z474p.png"" alt=""enter image description here""></a></p>
";;1;;2016-09-06T11:03:54.827;;39347475;2016-10-19T14:48:27.757;2016-10-19T14:48:27.757;;1509797.0;;1509797.0;18096748.0;2;16;;;
67566;67566;;;"<p>I'd use matplotlib table functionality, with some additional styling:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import six

df = pd.DataFrame()
df['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']
df['calories'] = [2200, 2100, 1500]
df['sleep hours'] = [2200, 2100, 1500]
df['gym'] = [True, False, False]

def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,
                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',
                     bbox=[0, 0, 1, 1], header_columns=0,
                     ax=None, **kwargs):
    if ax is None:
        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])
        fig, ax = plt.subplots(figsize=size)
        ax.axis('off')

    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)

    mpl_table.auto_set_font_size(False)
    mpl_table.set_fontsize(font_size)

    for k, cell in six.iteritems(mpl_table._cells):
        cell.set_edgecolor(edge_color)
        if k[0] == 0 or k[1] &lt; header_columns:
            cell.set_text_props(weight='bold', color='w')
            cell.set_facecolor(header_color)
        else:
            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])
    return ax

render_mpl_table(df, header_columns=0, col_width=2.0)
</code></pre>

<p><a href=""https://i.stack.imgur.com/F5xoF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F5xoF.png"" alt=""enter image description here""></a></p>
";;2;;2016-09-06T22:32:56.433;;39358752;2017-05-19T13:57:59.127;2017-05-19T13:57:59.127;;1509797.0;;1509797.0;19726663.0;2;8;;;
67570;67570;;;"<h1>TL;DR version:</h1>

<p>For the simple case of:</p>

<ul>
<li>I have a text column with a delimiter and I want two columns</li>
</ul>

<p>The simplest solution is:</p>

<pre><code>df['A'], df['B'] = df['AB'].str.split(' ', 1).str
</code></pre>

<p>Or you can create create a DataFrame with one column for each entry of the split automatically with:</p>

<pre><code>df['AB'].str.split(' ', 1, expand=True)
</code></pre>

<p>Notice how, in either case, the <code>.tolist()</code> method is not necessary. Neither is <code>zip()</code>.</p>

<h1>In detail:</h1>

<p><a href=""https://stackoverflow.com/a/21296915/1273938"">Andy Hayden's solution</a> is most excellent in demonstrating the power of the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html"" rel=""noreferrer""><code>str.extract()</code></a> method.</p>

<p>But for a simple split over a known separator (like, splitting by dashes, or splitting by whitespace), the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""noreferrer""><code>.str.split()</code></a> method is enough<sup>1</sup>. It operates on a column (Series) of strings, and returns a column (Series) of lists:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})
&gt;&gt;&gt; df

      AB
0  A1-B1
1  A2-B2
&gt;&gt;&gt; df['AB_split'] = df['AB'].str.split('-')
&gt;&gt;&gt; df

      AB  AB_split
0  A1-B1  [A1, B1]
1  A2-B2  [A2, B2]
</code></pre>

<p><sub>1: If you're unsure what the first two parameters of <code>.str.split()</code> do,
 I recommend the docs for the <a href=""https://docs.python.org/3.6/library/stdtypes.html#str.split"" rel=""noreferrer"">plain Python version of the method</a>.</sub></p>

<p>But how do you go from:</p>

<ul>
<li>a column containing two-element lists</li>
</ul>

<p>to:</p>

<ul>
<li>two columns, each containing the respective element of the lists?</li>
</ul>

<p>Well, we need to take a closer look at the <code>.str</code> attribute of a column.</p>

<p>It's a magical object that is used to collect methods that treat each element in a column as a string, and then apply the respective method in each element as efficient as possible:</p>

<pre><code>&gt;&gt;&gt; upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})
&gt;&gt;&gt; upper_lower_df

   U
0  A
1  B
2  C
&gt;&gt;&gt; upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()
&gt;&gt;&gt; upper_lower_df

   U  L
0  A  a
1  B  b
2  C  c
</code></pre>

<p>But it also has an ""indexing"" interface for getting each element of a string by its index:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str[0]

0    A
1    A
Name: AB, dtype: object

&gt;&gt;&gt; df['AB'].str[1]

0    1
1    2
Name: AB, dtype: object
</code></pre>

<p>Of course, this indexing interface of <code>.str</code> doesn't really care if each element it's indexing is actually a string, as long as it can be indexed, so:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str.split('-', 1).str[0]

0    A1
1    A2
Name: AB, dtype: object

&gt;&gt;&gt; df['AB'].str.split('-', 1).str[1]

0    B1
1    B2
Name: AB, dtype: object
</code></pre>

<p>Then, it's a simple matter of taking advantage of the Python tuple unpacking of iterables to do</p>

<pre><code>&gt;&gt;&gt; df['A'], df['B'] = df['AB'].str.split('-', 1).str
&gt;&gt;&gt; df

      AB  AB_split   A   B
0  A1-B1  [A1, B1]  A1  B1
1  A2-B2  [A2, B2]  A2  B2
</code></pre>

<p>Of course, getting a DataFrame out of splitting a column of strings is so useful that the <code>.str.split()</code> method can do it for you with the <code>expand=True</code> parameter:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str.split('-', 1, expand=True)

    0   1
0  A1  B1
1  A2  B2
</code></pre>

<p>So, another way of accomplishing what we wanted is to do:</p>

<pre><code>&gt;&gt;&gt; df = df[['AB']]
&gt;&gt;&gt; df

      AB
0  A1-B1
1  A2-B2

&gt;&gt;&gt; df.join(df['AB'].str.split('-', 1, expand=True).rename(columns={0:'A', 1:'B'}))

      AB   A   B
0  A1-B1  A1  B1
1  A2-B2  A2  B2
</code></pre>
";;4;;2016-09-06T22:52:23.940;;39358924;2017-03-30T14:47:08.650;2017-05-23T10:31:37.513;;-1.0;;1273938.0;14745022.0;2;81;;;
67597;67597;;;"<p>In order to get the interaction effect you're looking for, you must bin all the columns you care about, together.</p>

<p>The cleanest way I can think of doing this is to <code>stack</code> into a single <code>series</code> then use <code>pd.cut</code></p>

<p>Considering your sample <code>df</code></p>

<p><a href=""https://i.stack.imgur.com/8IjPO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8IjPO.png"" alt=""enter image description here""></a></p>

<pre><code>df_ = pd.cut(df[['A', 'B']].stack(), 5, labels=list(range(5))).unstack()
df_.columns = df_.columns.to_series() + 'bkt'
pd.concat([df, df_], axis=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/dlCEm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dlCEm.png"" alt=""enter image description here""></a></p>

<hr>

<p>Let's build a better example and look at a visualization using <code>seaborn</code></p>

<pre><code>df = pd.DataFrame(dict(A=(np.random.randn(10000) * 100 + 20).astype(int),
                       B=(np.random.randn(10000) * 100 - 20).astype(int)))

import seaborn as sns

df.index = df.index.to_series().astype(str).radd('city')

df_ = pd.cut(df[['A', 'B']].stack(), 30, labels=list(range(30))).unstack()
df_.columns = df_.columns.to_series() + 'bkt'

sns.jointplot(x=df_.Abkt, y=df_.Bbkt, kind=""scatter"", color=""k"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/Q2XnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q2XnW.png"" alt=""enter image description here""></a></p>

<hr>

<p>Or how about some data with some correlation</p>

<pre><code>mean, cov = [0, 1], [(1, .5), (.5, 1)]
data = np.random.multivariate_normal(mean, cov, 100000)
df = pd.DataFrame(data, columns=[""A"", ""B""])

df.index = df.index.to_series().astype(str).radd('city')

df_ = pd.cut(df[['A', 'B']].stack(), 30, labels=list(range(30))).unstack()
df_.columns = df_.columns.to_series() + 'bkt'

sns.jointplot(x=df_.Abkt, y=df_.Bbkt, kind=""scatter"", color=""k"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/YMo9M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YMo9M.png"" alt=""enter image description here""></a></p>

<hr>

<h3>Interactive <code>bokeh</code></h3>

<p>Without getting too complicated</p>

<pre><code>from bokeh.io import show, output_notebook, output_file

from bokeh.plotting import figure
from bokeh.layouts import row, column
from bokeh.models import ColumnDataSource, Select, CustomJS

output_notebook()

# generate random data
flips = np.random.choice((1, -1), (5, 5))
flips = np.tril(flips, -1) + np.triu(flips, 1) + np.eye(flips.shape[0])

half = np.ones((5, 5)) / 2
cov = (half + np.diag(np.diag(half))) * flips
mean = np.zeros(5)

data = np.random.multivariate_normal(mean, cov, 10000)
df = pd.DataFrame(data, columns=list('ABCDE'))

df.index = df.index.to_series().astype(str).radd('city')

# Stack and cut to get dependent relationships
b = 20
df_ = pd.cut(df.stack(), b, labels=list(range(b))).unstack()

# assign default columns x and y.  These will be the columns I set bokeh to read
df_[['x', 'y']] = df_.loc[:, ['A', 'B']]

source = ColumnDataSource(data=df_)

tools = 'box_select,pan,box_zoom,wheel_zoom,reset,resize,save'

p = figure(plot_width=600, plot_height=300)
p.circle('x', 'y', source=source, fill_color='olive', line_color='black', alpha=.5)

def gcb(like, n):
    code = """"""
    var data = source.get('data');
    var f = cb_obj.get('value');
    data['{0}{1}'] = data[f];
    source.trigger('change');
    """"""
    return CustomJS(args=dict(source=source), code=code.format(like, n))

xcb = CustomJS(
    args=dict(source=source),
    code=""""""
    var data = source.get('data');
    var colm = cb_obj.get('value');
    data['x'] = data[colm];
    source.trigger('change');
    """"""
)

ycb = CustomJS(
    args=dict(source=source),
    code=""""""
    var data = source.get('data');
    var colm = cb_obj.get('value');
    data['y'] = data[colm];
    source.trigger('change');
    """"""
)

options = list('ABCDE')
x_select = Select(options=options, callback=xcb, value='A')
y_select = Select(options=options, callback=ycb, value='B')


show(column(p, row(x_select, y_select)))
</code></pre>

<p><a href=""https://i.stack.imgur.com/RTu1E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RTu1E.png"" alt=""enter image description here""></a></p>
";;4;;2016-09-07T07:26:47.430;;39363715;2016-09-12T10:23:56.827;2016-09-12T10:23:56.827;;2336654.0;;2336654.0;39156545.0;2;6;;;
67606;67606;;;"<p>You can also use:</p>

<pre><code>dat1 = pd.concat([dat1, dat2], axis=1)
</code></pre>
";;0;;2016-09-07T10:00:07.277;;39366792;2016-09-07T10:00:07.277;;;;;897323.0;20602947.0;2;7;;;
67618;67618;;;"<p>You can use the df.iloc function as follows:</p>

<pre><code>for i in range(0, len(df)):
    print df.iloc[i]['c1'], df.iloc[i]['c2']
</code></pre>
";;1;;2016-09-07T12:56:04.117;;39370553;2016-11-07T09:09:10.507;2016-11-07T09:09:10.507;;6523882.0;;6523882.0;16476924.0;2;23;;;
67625;67625;;;"<p>To check if one <em>or more</em> columns all exist, you can use <a href=""https://docs.python.org/3.5/library/stdtypes.html?highlight=issubset#set.issubset"" rel=""nofollow noreferrer""><code>set.issubset</code></a>, as in:</p>

<pre><code>if set(['A','C']).issubset(df.columns):
   df['sum'] = df['A'] + df['C']                
</code></pre>

<p><strong>edit:</strong> As @brianpck points out in a comment, <code>set([])</code> can alternatively be constructed with curly braces:</p>

<pre><code>if {'A', 'C'}.issubset(df.columns):
</code></pre>

<p>See <a href=""https://stackoverflow.com/questions/17373161/using-curly-braces-to-initialize-set"">this question</a> for a discussion of the curly-braces syntax.</p>
";;2;;2016-09-07T13:55:51.020;;39371897;2017-07-10T13:57:34.433;2017-07-10T13:57:34.433;;2573061.0;;2573061.0;24870306.0;2;18;;;
67679;67679;;;"<p>Reducing memory usage in Python is difficult, because <a href=""http://effbot.org/pyfaq/why-doesnt-python-release-the-memory-when-i-delete-a-large-object.htm"" rel=""noreferrer"">Python does not actually release memory back to the operating system</a>. If you delete objects, then the memory is available to new Python objects, but not <code>free()</code>'d back to the system (<a href=""https://stackoverflow.com/q/15455048/509706"">see this question</a>).</p>

<p>If you stick to numeric numpy arrays, those are freed, but boxed objects are not.</p>

<pre><code>&gt;&gt;&gt; import os, psutil, numpy as np
&gt;&gt;&gt; def usage():
...     process = psutil.Process(os.getpid())
...     return process.get_memory_info()[0] / float(2 ** 20)
... 
&gt;&gt;&gt; usage() # initial memory usage
27.5 

&gt;&gt;&gt; arr = np.arange(10 ** 8) # create a large array without boxing
&gt;&gt;&gt; usage()
790.46875
&gt;&gt;&gt; del arr
&gt;&gt;&gt; usage()
27.52734375 # numpy just free()'d the array

&gt;&gt;&gt; arr = np.arange(10 ** 8, dtype='O') # create lots of objects
&gt;&gt;&gt; usage()
3135.109375
&gt;&gt;&gt; del arr
&gt;&gt;&gt; usage()
2372.16796875  # numpy frees the array, but python keeps the heap big
</code></pre>

<h2>Reducing the Number of Dataframes</h2>

<p>Python keep our memory at high watermark, but we can reduce the total number of dataframes we create. When modifying your dataframe, prefer <code>inplace=True</code>, so you don't create copies.</p>

<p>Another common gotcha is holding on to copies of previously created dataframes in ipython:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'foo': [1,2,3,4]})

In [3]: df + 1
Out[3]: 
   foo
0    2
1    3
2    4
3    5

In [4]: df + 2
Out[4]: 
   foo
0    3
1    4
2    5
3    6

In [5]: Out # Still has all our temporary DataFrame objects!
Out[5]: 
{3:    foo
 0    2
 1    3
 2    4
 3    5, 4:    foo
 0    3
 1    4
 2    5
 3    6}
</code></pre>

<p>You can fix this by typing <code>%reset Out</code> to clear your history. Alternatively, you can adjust how much history ipython keeps with <code>ipython --cache-size=5</code> (default is 1000).</p>

<h2>Reducing Dataframe Size</h2>

<p>Wherever possible, avoid using object dtypes.</p>

<pre><code>&gt;&gt;&gt; df.dtypes
foo    float64 # 8 bytes per value
bar      int64 # 8 bytes per value
baz     object # at least 48 bytes per value, often more
</code></pre>

<p>Values with an object dtype are boxed, which means the numpy array just contains a pointer and you have a full Python object on the heap for every value in your dataframe. This includes strings.</p>

<p>Whilst numpy supports fixed-size strings in arrays, pandas does not (<a href=""https://github.com/pydata/pandas/issues/3209#issuecomment-15659304"" rel=""noreferrer"">it's caused user confusion</a>). This can make a significant difference:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; arr = np.array(['foo', 'bar', 'baz'])
&gt;&gt;&gt; arr.dtype
dtype('S3')
&gt;&gt;&gt; arr.nbytes
9

&gt;&gt;&gt; import sys; import pandas as pd
&gt;&gt;&gt; s = pd.Series(['foo', 'bar', 'baz'])
dtype('O')
&gt;&gt;&gt; sum(sys.getsizeof(x) for x in s)
120
</code></pre>

<p>You may want to avoid using string columns, or find a way of representing string data as numbers.</p>

<p>If you have a dataframe that contains many repeated values (NaN is very common), then you can use a <a href=""http://pandas.pydata.org/pandas-docs/stable/sparse.html"" rel=""noreferrer"">sparse data structure</a> to reduce memory usage:</p>

<pre><code>&gt;&gt;&gt; df1.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 39681584 entries, 0 to 39681583
Data columns (total 1 columns):
foo    float64
dtypes: float64(1)
memory usage: 605.5 MB

&gt;&gt;&gt; df1.shape
(39681584, 1)

&gt;&gt;&gt; df1.foo.isnull().sum() * 100. / len(df1)
20.628483479893344 # so 20% of values are NaN

&gt;&gt;&gt; df1.to_sparse().info()
&lt;class 'pandas.sparse.frame.SparseDataFrame'&gt;
Int64Index: 39681584 entries, 0 to 39681583
Data columns (total 1 columns):
foo    float64
dtypes: float64(1)
memory usage: 543.0 MB
</code></pre>

<h2>Viewing Memory Usage</h2>

<p>You can view the memory usage (<a href=""http://pandas.pydata.org/pandas-docs/stable/faq.html#dataframe-memory-usage"" rel=""noreferrer"">docs</a>):</p>

<pre><code>&gt;&gt;&gt; df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 39681584 entries, 0 to 39681583
Data columns (total 14 columns):
...
dtypes: datetime64[ns](1), float64(8), int64(1), object(4)
memory usage: 4.4+ GB
</code></pre>

<p>As of pandas 0.17.1, you can also do <code>df.info(memory_usage='deep')</code> to see memory usage including objects.</p>
";;0;;2016-09-07T19:25:24.353;;39377643;2016-09-07T19:57:26.767;2017-05-23T12:32:33.300;;-1.0;;509706.0;39100971.0;2;17;;;
67859;67859;;;"<p>To add to DSM's answer and building on <a href=""https://stackoverflow.com/questions/30926670/pandas-add-multiple-empty-columns-to-dataframe"">this associated question</a>, I'd split the approach into two cases:</p>

<ul>
<li><p>Adding a single column: Just assign empty values to the new columns, e.g. <code>df['C'] = np.nan</code></p></li>
<li><p>Adding multiple columns: I'd suggest using the <code>.reindex(columns=[...])</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer"">method of pandas</a> to add the new columns to the dataframe's column index. This also works for adding multiple new rows.</p></li>
</ul>

<p>Here is an example adding multiple columns:</p>

<pre><code>mydf = mydf.reindex( mydf.columns.tolist() + ['newcol1','newcol2'])  # version &gt;= 0.20.0
</code></pre>

<p>or </p>

<pre><code>mydf = mydf.reindex( mydf.columns.values + ['newcol1','newcol2'])  # version &lt; 0.20.0
</code></pre>

<p>You can also always concatenate a new (empty) dataframe to the existing dataframe, but that doesn't feel as pythonic to me :)</p>
";;0;;2016-09-09T06:56:47.103;;39405540;2017-08-25T04:39:35.623;2017-08-25T04:39:35.623;;3585557.0;;4619397.0;16327055.0;2;10;;;
68241;68241;;;"<p>The accepted answer works for columns that are of datatype <code>string</code>. For completeness: I come across this question when searching how to do this when the columns are of datatypes: date and time. </p>

<pre><code>df.apply(lambda r : pd.datetime.combine(r['date_column_name'],r['time_column_name']),1)
</code></pre>
";;0;;2016-09-13T16:24:48.117;;39474812;2016-09-13T16:24:48.117;;;;;3333687.0;17978092.0;2;9;;;
68275;68275;;;"<p>The below code worked for me.</p>

<pre><code>import pandas

df = pandas.read_csv('somefile.txt')

df = df.fillna(0)
</code></pre>
";;0;;2016-09-13T20:59:31.637;;39478896;2016-09-13T21:13:37.977;2016-09-13T21:13:37.977;;5292302.0;;3622344.0;13295735.0;2;6;;;
68301;68301;;;"<p>The suggested solutions are incompatible with Seaborn 0.7.1</p>

<p>giving the following errors because the Seaborn interface has changed:</p>

<pre><code>AttributeError: 'AxesSubplot' object has no attribute 'fig'
When trying to access the figure

AttributeError: 'AxesSubplot' object has no attribute 'savefig'
when trying to use the savefig directly as a function
</code></pre>

<p>The following calls allow you to access the figure (Seaborn 0.7.1 compatible):</p>

<pre><code>swarm_plot = sns.swarmplot(...)
fig = swarm_plot.get_figure()
fig.savefig(...) 
</code></pre>

<p>as seen previously in <a href=""https://stackoverflow.com/a/35670528/1079075"">this answer</a>.</p>

<p><strong>UPDATE:</strong>
I have recently used PairGrid object from seaborn to generate a plot similar to the one in <a href=""http://seaborn.pydata.org/examples/pairgrid_dotplot.html"" rel=""noreferrer"">this example</a>.
In this case, since GridPlot is not a plot object like, for example, sns.swarmplot, it has no get_figure() function.
It is possible to directly access the matplotlib figure by</p>

<pre><code>fig = myGridPlotObject.fig
</code></pre>

<p>Like previously suggested in other posts in this thread.</p>
";;2;;2016-09-14T04:16:31.797;;39482402;2017-02-16T21:02:33.310;2017-05-23T12:18:01.060;;-1.0;;4384381.0;32244753.0;2;27;;;
68810;68810;;;"<p>As of Pandas 0.19, <code>read_json</code> has native support for <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#io-jsonl"" rel=""noreferrer"">line-delimited JSON</a>:</p>

<pre><code>pd.read_json(jsonfile, lines=True)
</code></pre>
";;0;;2016-09-18T23:45:34.183;;39563662;2016-09-18T23:45:34.183;;;;;171236.0;20037430.0;2;11;;;
69134;69134;;;"<p>Solution wich handles also negative values with sample float formating.</p>

<p>Still needs tweaking offsets.</p>

<pre><code>df=pd.DataFrame({'A':np.random.rand(2)-1,'B':np.random.rand(2)},index=['val1','val2'] )
ax = df.plot(kind='bar', color=['r','b']) 
x_offset = -0.03
y_offset = 0.02
for p in ax.patches:
    b = p.get_bbox()
    val = ""{:+.2f}"".format(b.y1 + b.y0)        
    ax.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset))
</code></pre>

<p><a href=""https://i.stack.imgur.com/zt7KZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zt7KZ.png"" alt=""value labeled bar plot""></a></p>
";;0;;2016-09-21T16:26:52.963;;39621872;2017-03-27T13:30:05.363;2017-03-27T13:30:05.363;;3805131.0;;3805131.0;25447700.0;2;9;;;
69193;69193;;;"<p>Because if you don't make a copy then the indices can still be manipulated elsewhere even if you assign the dataFrame to a different name.</p>

<p>For example:</p>

<pre><code>df2 = df
func1(df2)
func2(df)
</code></pre>

<p>func1 can modify df by modifying df2, so to avoid that:</p>

<pre><code>df2 = df.copy()
func1(df2)
func2(df)
</code></pre>
";;0;;2016-09-22T01:27:07.940;;39628860;2016-09-22T01:27:07.940;;;;;5405782.0;27673231.0;2;11;;;
69219;69219;;;"<p>Thought to add this here incase someone finds it useful. @ostrokach already mentioned how you can merge the data frames across rows which is</p>

<pre><code>df_row_merged = pd.concat([df_a, df_b], ignore_index=True)
</code></pre>

<p>To merge across columns, you can use the following syntax:</p>

<pre><code>df_col_merged =pd.concat([df_a, df_b], axis=1)
</code></pre>
";;0;;2016-09-22T08:38:50.830;;39634150;2016-09-22T08:38:50.830;;;;;720655.0;12850345.0;2;6;;;
69356;69356;;;"<p>A much faster implementation would be to use <code>list-comprehension</code> if you need to rename a single column.</p>

<pre><code>df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]
</code></pre>

<p>If the need arises to rename multiple columns, either use conditional expressions like:</p>

<pre><code>df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]
</code></pre>

<p>Or, construct a mapping using a <code>dictionary</code> and perform the <code>list-comprehension</code> with it's <code>get</code> operation by setting default value as the old name:</p>

<pre><code>col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key?old name, value?new name

df.columns = [col_dict.get(x, x) for x in df.columns]
</code></pre>

<p><strong>Timings:</strong></p>

<pre><code>%%timeit
df.rename(columns={'gdp':'log(gdp)'}, inplace=True)
10000 loops, best of 3: 168 s per loop

%%timeit
df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]
10000 loops, best of 3: 58.5 s per loop
</code></pre>
";;0;;2016-09-23T09:17:35.220;;39657077;2016-09-23T09:17:35.220;;;;;6207849.0;19758364.0;2;10;;;
69453;69453;;;"<p>You can simply use sklearn for this</p>

<pre><code>from sklearn.utils import shuffle
df = shuffle(df)
</code></pre>
";;0;;2016-09-24T07:17:19.683;;39673666;2016-09-24T07:42:10.263;2016-09-24T07:42:10.263;;1905949.0;;6128923.0;29576430.0;2;27;;;
69958;69958;;;"<p>it seems that <code>pd.set_option('display.max_colwidth', -1)</code> is indeed the only option. To prevent irreversible global changes of how dataframes are presented in the console, you may save the previous setting in a variable and restore it immediately after the usage, as follows:</p>

<pre><code>    old_width = pd.get_option('display.max_colwidth')
    pd.set_option('display.max_colwidth', -1)
    open('some_file.html', 'w').write(some_data.to_html())
    pd.set_option('display.max_colwidth', old_width)
</code></pre>
";;1;;2016-09-29T06:24:04.177;;39762850;2016-09-29T06:24:04.177;;;;;17523.0;26277757.0;2;7;;;
70000;70000;;;"<h1>Column names vs Names of Series</h1>

<p>I would like to explain a bit what happens behind the scenes.</p>

<p>Dataframes are a set of Series.</p>

<p>Series in turn are an extension of a <code>numpy.array</code></p>

<p><code>numpy.array</code>s have a property <code>.name</code></p>

<p>This is the name of the series. It is seldom that pandas respects this attribute, but it lingers in places and can be used to hack some pandas behaviors.</p>

<h1>Naming the list of columns</h1>

<p>A lot of answers here talks about the <code>df.columns</code> attribute being a <code>list</code> when in fact it is a <code>Series</code>. This means it has a <code>.name</code> attribute.</p>

<p>This is what happens if you decide to fill in the name of the columns <code>Series</code>:</p>

<pre><code>df.columns = ['column_one', 'column_two']
df.columns.names = ['name of the list of columns']
df.index.names = ['name of the index']

name of the list of columns     column_one  column_two
name of the index       
0                                    4           1
1                                    5           2
2                                    6           3
</code></pre>

<p>Note that the name of the index always comes one column lower.</p>

<h2>Artifacts that linger</h2>

<p>The <code>.name</code> attribute lingers on sometimes. If you set <code>df.columns = ['one', 'two']</code> then the <code>df.one.name</code> will be <code>'one'</code>.</p>

<p>If you set <code>df.one.name = 'three'</code> then <code>df.columns</code> will still give you <code>['one', 'two']</code>, and <code>df.one.name</code> will give you <code>'three'</code></p>

<h3>BUT</h3>

<p><code>pd.DataFrame(df.one)</code> will return</p>

<pre><code>    three
0       1
1       2
2       3
</code></pre>

<p>Because pandas reuses the <code>.name</code> of the already defined <code>Series</code>.</p>

<h1>Multi level column names</h1>

<p>Pandas has ways of doing multi layered column names. There is not so much magic involved but I wanted to cover this in my answer too since I don't see anyone picking up on this here.</p>

<pre><code>    |one            |
    |one      |two  |
0   |  4      |  1  |
1   |  5      |  2  |
2   |  6      |  3  |
</code></pre>

<p>This is easily achievable by setting columns to lists, like this:</p>

<pre><code>df.columns = [['one', 'one'], ['one', 'two']]
</code></pre>
";;0;;2016-09-29T12:30:40.073;;39770407;2016-09-29T12:30:40.073;;;;;3730397.0;11346283.0;2;19;;;
70287;70287;;;"<p>For more general boolean functions that you would like to use as a filter and that depend on more than one column, you can use: </p>

<pre><code>df = df[df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)]
</code></pre>

<p>where f is a function that is applied to every pair of elements (x1, x2) from col_1 and col_2 and returns True or False depending on any condition you want on (x1, x2).</p>
";;0;;2016-10-02T18:37:23.883;;39820329;2016-10-02T18:37:23.883;;;;;6901940.0;22086116.0;2;8;;;
70330;70330;;;"<p>In version <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#read-csv-supports-parsing-categorical-directly""><code>0.19.0</code></a> you can use parameter <code>dtype='category'</code> in <code>read_csv</code>:</p>

<pre><code>data = 'col1,col2,col3\na,b,1\na,b,2\nc,d,3'
df = pd.read_csv(StringIO(data), dtype='category')
print (df)

  col1 col2  col3
0    a    b     1
1    a    b     2
2    c    d     3

print (df.dtypes)
col1    category
col2    category
col3    category
dtype: object
</code></pre>
";;1;;2016-10-03T11:24:56.323;;39830464;2016-10-03T11:24:56.323;;;;;2901002.0;30272300.0;2;11;;;
70371;70371;;;"<pre><code>mydict = dict(zip(df.id, df.value))
</code></pre>
";;0;;2016-10-03T17:41:02.203;;39837358;2016-10-03T17:41:02.203;;;;;6096272.0;18695605.0;2;19;;;
70687;70687;;;"<p>It is not guaranteed that the slicing returns a view or a copy. You can do</p>

<pre><code>df['column']=df['column'].fillna(value)
</code></pre>
";;1;;2016-10-06T09:10:08.180;;39891994;2016-10-06T09:10:08.180;;;;;5138923.0;13295735.0;2;19;;;
70882;70882;;;"<p>No need to hack settings. There is a simple way:</p>

<pre><code>print(df.to_string())
</code></pre>
";;6;;2016-10-07T18:45:36.123;;39923958;2016-10-07T18:45:36.123;;;;;3022273.0;19124601.0;2;88;;;
71008;71008;;;"<p>Here's a <a href=""https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py#L61-L95"" rel=""nofollow noreferrer"" title=""Source on GitHub: cognoma/genes repository"">function I wrote</a> for this common task. It's more efficient than the <code>Series</code>/<code>stack</code> methods. Column order and names are retained.</p>

<pre><code>def tidy_split(df, column, sep='|', keep=False):
    """"""
    Split the values of a column and expand so the new DataFrame has one split
    value per row. Filters rows where the column is missing.

    Params
    ------
    df : pandas.DataFrame
        dataframe with the column to split and expand
    column : str
        the column to split and expand
    sep : str
        the string used to split the column's values
    keep : bool
        whether to retain the presplit value as it's own row

    Returns
    -------
    pandas.DataFrame
        Returns a dataframe with the same columns as `df`.
    """"""
    indexes = list()
    new_values = list()
    df = df.dropna(subset=[column])
    for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) &gt; 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)
    new_df = df.iloc[indexes, :].copy()
    new_df[column] = new_values
    return new_df
</code></pre>

<p>With this function, the <a href=""https://stackoverflow.com/q/12680754/4651668"">original question</a> is as simple as:</p>

<pre><code>tidy_split(a, 'var1', sep=',')
</code></pre>
";;1;;2016-10-09T17:57:42.287;;39946744;2016-10-09T18:03:58.223;2017-05-23T12:02:48.590;;-1.0;;4651668.0;12680754.0;2;8;;;
71364;71364;;;"<p>Below is an example that sets up time series data to train an LSTM.  The model output is nonsense as I only set it up to demonstrate how to build the model.</p>

<pre><code>import pandas as pd
import numpy as np
# Get some time series data
df = pd.read_csv(""https://raw.githubusercontent.com/plotly/datasets/master/timeseries.csv"")
df.head()
</code></pre>

<p>Time series dataframe:</p>

<pre><code>Date      A       B       C      D      E      F      G
0   2008-03-18  24.68  164.93  114.73  26.27  19.21  28.87  63.44
1   2008-03-19  24.18  164.89  114.75  26.22  19.07  27.76  59.98
2   2008-03-20  23.99  164.63  115.04  25.78  19.01  27.04  59.61
3   2008-03-25  24.14  163.92  114.85  27.41  19.61  27.84  59.41
4   2008-03-26  24.44  163.45  114.84  26.86  19.53  28.02  60.09
</code></pre>

<p>You can build put inputs into a vector and then use pandas <code>.cumsum()</code> function to build the sequence for the time series:</p>

<pre><code># Put your inputs into a single list
df['single_input_vector'] = df[input_cols].apply(tuple, axis=1).apply(list)
# Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements
df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])
# Use .cumsum() to include previous row vectors in the current row list of vectors
df['cumulative_input_vectors'] = df.single_input_vector.cumsum()
</code></pre>

<p>The output can be set up in a similar way, but it will be a single vector instead of a sequence:</p>

<pre><code># If your output is multi-dimensional, you need to capture those dimensions in one object
# If your output is a single dimension, this step may be unnecessary
df['output_vector'] = df[output_cols].apply(tuple, axis=1).apply(list)
</code></pre>

<p>The input sequences have to be the same length to run them through the model, so you need to pad them to be the max length of your cumulative vectors:</p>

<pre><code># Pad your sequences so they are the same length
from keras.preprocessing.sequence import pad_sequences

max_sequence_length = df.cumulative_input_vectors.apply(len).max()
# Save it as a list   
padded_sequences = pad_sequences(df.cumulative_input_vectors.tolist(), max_sequence_length).tolist()
df['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)
</code></pre>

<p>Training data can be pulled from the dataframe and put into numpy arrays.  <strong>Note that the input data that comes out of the dataframe will not make a 3D array.  It makes an array of arrays, which is not the same thing.</strong></p>

<p>You can use hstack and reshape to build a 3D input array.</p>

<pre><code># Extract your training data
X_train_init = np.asarray(df.padded_input_vectors)
# Use hstack to and reshape to make the inputs a 3d vector
X_train = np.hstack(X_train_init).reshape(len(df),max_sequence_length,len(input_cols))
y_train = np.hstack(np.asarray(df.output_vector)).reshape(len(df),len(output_cols))
</code></pre>

<p>To prove it:</p>

<pre><code>&gt;&gt;&gt; print(X_train_init.shape)
(11,)
&gt;&gt;&gt; print(X_train.shape)
(11, 11, 6)
&gt;&gt;&gt; print(X_train == X_train_init)
False
</code></pre>

<p>Once you have training data you can define the dimensions of your input layer and output layers.</p>

<pre><code># Get your input dimensions
# Input length is the length for one input sequence (i.e. the number of rows for your sample)
# Input dim is the number of dimensions in one input vector (i.e. number of input columns)
input_length = X_train.shape[1]
input_dim = X_train.shape[2]
# Output dimensions is the shape of a single output vector
# In this case it's just 1, but it could be more
output_dim = len(y_train[0])
</code></pre>

<p>Build the model:</p>

<pre><code>from keras.models import Model, Sequential
from keras.layers import LSTM, Dense

# Build the model
model = Sequential()

# I arbitrarily picked the output dimensions as 4
model.add(LSTM(4, input_dim = input_dim, input_length = input_length))
# The max output value is &gt; 1 so relu is used as final activation.
model.add(Dense(output_dim, activation='relu'))

model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=['accuracy'])
</code></pre>

<p>Finally you can train the model and save the training log as history:</p>

<pre><code># Set batch_size to 7 to show that it doesn't have to be a factor or multiple of your sample size
history = model.fit(X_train, y_train,
              batch_size=7, nb_epoch=3,
              verbose = 1)
</code></pre>

<p>Output:</p>

<pre><code>Epoch 1/3
11/11 [==============================] - 0s - loss: 3498.5756 - acc: 0.0000e+00     
Epoch 2/3
11/11 [==============================] - 0s - loss: 3498.5755 - acc: 0.0000e+00     
Epoch 3/3
11/11 [==============================] - 0s - loss: 3498.5757 - acc: 0.0000e+00 
</code></pre>

<p>That's it.  Use <code>model.predict(X)</code> where <code>X</code> is the same format (other than the number of samples) as <code>X_train</code> in order to make predictions from the model.</p>
";;12;;2016-10-12T18:25:12.780;;40005797;2016-10-12T18:25:12.780;;;;;5066140.0;39674713.0;2;16;;;
71385;71385;;;"<p>If anyone wonders how to perform a related problem: <strong><em>""Select column by partial string""</em></strong> </p>

<p>Use:</p>

<pre><code>df.loc[:, df.columns.to_series().str.contains('a').tolist()]
</code></pre>
";;1;;2016-10-12T21:04:32.060;;40008322;2016-11-01T11:13:46.110;2016-11-01T11:13:46.110;;5853731.0;;5853731.0;11350770.0;2;7;;;
71999;71999;;;"<p>I was looking for a solution to this and I stumbled on an obvious approach which is to just filter the data frame and assign back to the original data frame so</p>

<pre><code>df= df[df[""score""] &gt; 50]
</code></pre>
";;0;;2016-10-18T14:01:04.270;;40110335;2016-10-18T14:01:04.270;;;;;5240970.0;13851535.0;2;20;;;
72235;72235;;;"<p>Use iloc and select all rows (<code>:</code>) against the last column (<code>-1</code>):</p>

<pre><code>df.iloc[:,-1]
</code></pre>
";;0;;2016-10-20T04:36:23.577;;40145561;2017-05-02T16:01:15.580;2017-05-02T16:01:15.580;;4325994.0;;624829.0;40144769.0;2;22;;;
72640;72640;;;"<p>You can transpose the single-row dataframe (which still results in a dataframe) and then <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html?highlight=squeeze#squeezing"" rel=""noreferrer"">squeeze</a> the results into a series (the inverse of <code>to_frame</code>).</p>

<pre><code>df = pd.DataFrame([list(range(5))], columns=[""a{}"".format(i) for i in range(5)])

&gt;&gt;&gt; df.T.squeeze()
a0    0
a1    1
a2    2
a3    3
a4    4
Name: 0, dtype: int64
</code></pre>
";;2;;2016-10-23T22:20:34.013;;40208359;2016-10-23T22:20:34.013;;;;;2411802.0;33246771.0;2;8;;;
72648;72648;;;"<p>the df you gave us is :</p>

<pre><code>             A
2012-12-31   0
2013-01-01   1
2013-01-02   2
2013-01-03   3
2013-01-04   4
2013-01-05   5
2013-01-06   6
2013-01-07   7
2013-01-08   8
2013-01-09   9
2013-01-10  10
</code></pre>

<p>you could create your rolling 5-day sum series and then resample it. I can't think of a more efficient way than this. overall this should be relatively time efficient.</p>

<pre><code>df.rolling(5,min_periods=5).sum().dropna().resample('3D').first()
Out[36]: 
                 A
2013-01-04 10.0000
2013-01-07 25.0000
2013-01-10 40.0000
</code></pre>
";;3;;2016-10-24T01:59:45.343;;40209737;2016-11-01T12:56:21.640;2016-11-01T12:56:21.640;;5626112.0;;5626112.0;40209520.0;2;8;;;
72670;72670;;;"<h1>Pandas dataframe copy warning</h1>

<p>When you go and do something like this:</p>

<pre><code>quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
</code></pre>

<p><code>pandas.ix</code> <em>in this case</em> returns a new, stand alone dataframe.</p>

<p>Any values you decide to change in this dataframe, will not change the original dataframe.</p>

<p>This is what pandas tries to warn you about.</p>

<h1>Why <code>.ix</code> is a bad idea</h1>

<p>The <code>.ix</code> object tries to do more than one thing, and for anyone who has read anything about clean code, this is a strong smell.</p>

<p>Given this dataframe:</p>

<pre><code>df = pd.DataFrame({""a"": [1,2,3,4], ""b"": [1,1,2,2]})
</code></pre>

<p>Two behaviors:</p>

<pre><code>dfcopy = df.ix[:,[""a""]]
dfcopy.a.ix[0] = 2
</code></pre>

<p>Behavior one: <code>dfcopy</code> is now a stand alone dataframe. Changing it will not change <code>df</code></p>

<pre><code>df.ix[0, ""a""] = 3
</code></pre>

<p>Behavior two: This changes the original dataframe.</p>

<h1>Use <code>.loc</code> instead</h1>

<p>The pandas developers recognized that the <code>.ix</code> object was quite smelly[speculatively] and thus created two new objects which helps in the accession and assignment of data. (The other being <code>.iloc</code>)</p>

<p><code>.loc</code> is faster, because it does not try to create a copy of the data.</p>

<p><code>.loc</code> is meant to modify your existing dataframe inplace, which is more memory efficient.</p>

<p><code>.loc</code> is predictable, it has one behavior.</p>

<h1>The solution</h1>

<p>What you are doing in your code example is loading a big file with lots of columns, then modifying it to be smaller.</p>

<p>The <code>pd.read_csv</code> function can help you out with a lot of this and also make the loading of the file a lot faster.</p>

<p>So instead of doing this</p>

<pre><code>quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}
quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)
quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
</code></pre>

<p>Do this</p>

<pre><code>columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']
df = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])
df.columns = columns
</code></pre>

<p>This will only read the columns you are interested in, and name them properly. No need for using the evil <code>.ix</code> object to do magical stuff.</p>
";;5;;2016-10-24T09:01:35.203;;40214434;2016-11-14T08:48:49.907;2016-11-14T08:48:49.907;;3730397.0;;3730397.0;20625582.0;2;19;;;
72755;72755;;;"<p>As suggested by @StevenG himself, a better answer:</p>

<pre><code>df.columns = pd.MultiIndex.from_product([df.columns, ['C']])

print(df)
#    A  B
#    C  C
# a  0  0
# b  1  1
# c  2  2
# d  3  3
# e  4  4
</code></pre>
";;2;;2016-10-24T19:12:44.040;;40225796;2016-10-24T19:37:03.110;2016-10-24T19:37:03.110;;4413446.0;;4413446.0;40225683.0;2;6;;;
72780;72780;;;"<p>Seaborn's heatmap version:</p>

<pre><code>import seaborn as sns
corr = dataframe.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)
</code></pre>
";;1;;2016-10-24T22:45:06.460;;40228738;2016-10-24T22:45:06.460;;;;;3038791.0;29432629.0;2;22;;;
74002;74002;;;"<p>You can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"" rel=""noreferrer"">pandas.Series.isin</a> although it's a little bit longer than <code>'a' in s.values</code>:</p>

<pre><code>In [2]: s = pd.Series(list('abc'))

In [3]: s
Out[3]: 
0    a
1    b
2    c
dtype: object

In [3]: s.isin(['a'])
Out[3]: 
0    True
1    False
2    False
dtype: bool

In [4]: s[s.isin(['a'])].empty
Out[4]: False

In [5]: s[s.isin(['z'])].empty
Out[5]: True
</code></pre>

<p>But this approach can be more flexible if you need to match multiple values at once for a DataFrame (see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html"" rel=""noreferrer"">DataFrame.isin</a>)</p>

<pre><code>&gt;&gt;&gt; df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})
&gt;&gt;&gt; df.isin({'A': [1, 3], 'B': [4, 7, 12]})
       A      B
0   True  False  # Note that B didn't match 1 here.
1  False   True
2   True   True
</code></pre>
";;0;;2016-11-04T18:41:03.640;;40429755;2016-12-30T13:19:41.727;2016-12-30T13:19:41.727;;4897391.0;;4897391.0;21319929.0;2;6;;;
74061;74061;;;"<p>All of the above seem unnecessarily heavy and tedious methods --there's a one line solution to the problem. This applies if some column names are duplicated and you wish to remove them:</p>

<pre><code>df = df.loc[:,~df.columns.duplicated()]
</code></pre>

<h3>[update] How it works:</h3>

<p>Suppose the columns of the data frame are <code>['alpha','beta','alpha']</code></p>

<p><code>df.columns.duplicated()</code> returns a boolean array: a <code>True</code> or <code>False</code> for each column. If it is <code>False</code> then the column name is unique up to that point, if it is <code>True</code> then the column name is duplicated earlier. For example, using the given example, the returned value would be <code>[False,False,True]</code>. </p>

<p><code>Pandas</code> allows one to index using boolean values whereby it selects only the <code>True</code> values. Since we want to keep the unduplicated columns, we need the above boolean array to be flipped (ie <code>[True, True, False] = ~[False,False,True]</code>)</p>

<p>Finally, <code>df.loc[:,[True,True,False]]</code> selects only the non-duplicated columns using the aforementioned indexing capability. </p>

<p><strong>Note</strong>: the above only checks columns names, <em>not</em> column values.</p>
";;3;;2016-11-05T06:15:37.200;;40435354;2017-08-24T17:34:44.620;2017-08-24T17:34:44.620;;1978817.0;;1978817.0;14984119.0;2;48;;;
74111;74111;;;"<p>Consider also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.between.html"" rel=""noreferrer"">series between</a>:</p>

<pre><code>df = df[df['closing_price'].between(99, 101, inclusive=True)]
</code></pre>
";;0;;2016-11-05T20:18:24.690;;40442778;2016-11-05T20:18:24.690;;;;;1422451.0;31617845.0;2;12;;;
74153;74153;;;"<p><strong>UPDATE2:</strong> more generic vectorized function, which will work for multiple <code>normal</code> and multiple <code>list</code> columns</p>

<pre><code>def explode(df, lst_cols, fill_value=''):
    # make sure `lst_cols` is a list
    if lst_cols and not isinstance(lst_cols, list):
        lst_cols = [lst_cols]
    # all columns except `lst_cols`
    idx_cols = df.columns.difference(lst_cols)

    # calculate lengths of lists
    lens = df[lst_cols[0]].str.len()

    if (lens &gt; 0).all():
        # ALL lists in cells aren't empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .loc[:, df.columns]
    else:
        # at least one list in cells is empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \
          .loc[:, df.columns]
</code></pre>

<p>Demo:</p>

<p>Multiple <code>list</code> columns - all <code>list</code> columns must have the same # of elements in each row:</p>

<pre><code>In [36]: df
Out[36]:
   aaa  myid        num          text
0   10     1  [1, 2, 3]  [aa, bb, cc]
1   11     2     [1, 2]      [cc, dd]
2   12     3         []            []
3   13     4         []            []

In [37]: explode(df, ['num','text'], fill_value='')
Out[37]:
   aaa  myid num text
0   10     1   1   aa
1   10     1   2   bb
2   10     1   3   cc
3   11     2   1   cc
4   11     2   2   dd
2   12     3
3   13     4
</code></pre>

<p>Setup:</p>

<pre><code>df = pd.DataFrame({
 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},
 'myid': {0: 1, 1: 2, 2: 3, 3: 4},
 'num': {0: [1, 2, 3], 1: [1, 2], 2: [], 3: []},
 'text': {0: ['aa', 'bb', 'cc'], 1: ['cc', 'dd'], 2: [], 3: []}
})
</code></pre>

<p>CSV column:</p>

<pre><code>In [46]: df
Out[46]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')
Out[47]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>

<p>using this little trick we can convert CSV-like column to <code>list</code> column:</p>

<pre><code>In [48]: df.assign(var1=df.var1.str.split(','))
Out[48]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ
</code></pre>

<hr>

<p><strong>UPDATE:</strong> <strong>generic vectorized approach (will work also for multiple columns):</strong></p>

<p>Original DF:</p>

<pre><code>In [177]: df
Out[177]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ
</code></pre>

<p><strong>Solution:</strong></p>

<p>first let's convert CSV strings to lists:</p>

<pre><code>In [178]: lst_col = 'var1' 

In [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})

In [180]: x
Out[180]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ
</code></pre>

<p>Now we can do this:</p>

<pre><code>In [181]: pd.DataFrame({
     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())
     ...:     for col in x.columns.difference([lst_col])
     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]
     ...:
Out[181]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>

<hr>

<p><strong>OLD answer:</strong></p>

<p>Inspired by <a href=""https://stackoverflow.com/a/28182629/5741205"">@AFinkelstein solution</a>, i wanted to make it bit more generalized which could be applied to DF with more than two columns and as fast, well almost, as fast as AFinkelstein's solution):</p>

<pre><code>In [2]: df = pd.DataFrame(
   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},
   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]
   ...: )

In [3]: df
Out[3]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [4]: (df.set_index(df.columns.drop('var1',1).tolist())
   ...:    .var1.str.split(',', expand=True)
   ...:    .stack()
   ...:    .reset_index()
   ...:    .rename(columns={0:'var1'})
   ...:    .loc[:, df.columns]
   ...: )
Out[4]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>
";;2;;2016-11-06T13:12:51.043;;40449726;2017-08-14T17:35:30.473;2017-08-14T17:35:30.473;;5741205.0;;5741205.0;12680754.0;2;22;;;
74530;74530;;;"<p>More simpler version</p>

<p><code>df = DataFrame(columns=('col1', 'col2', 'col3'))
for i in range(5):
   df.loc[i] = ['&lt;some value for first&gt;','&lt;some value for second&gt;','&lt;some value for third&gt;']</code></p>
";;1;;2016-11-09T07:25:57.013;;40501868;2016-11-09T07:25:57.013;;;;;2741434.0;17091769.0;2;6;;;
74745;74745;;;"<p>Inspired by user6178746's answer above, I have the following:</p>

<pre><code># Given a dict of dataframes, for example:
# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}

writer = pd.ExcelWriter(filename, engine='xlsxwriter')
for sheetname, df in dfs.items():  # loop through `dict` of dataframes
    df.to_excel(writer, sheet_name=sheetname)  # send df to writer
    worksheet = writer.sheets[sheetname]  # pull worksheet object
    for idx, col in enumerate(df):  # loop through all columns
        series = df[col]
        max_len = max((
            series.astype(str).map(len).max(),  # len of largest item
            len(str(series.name))  # len of column name/header
            )) + 1  # adding a little extra space
        worksheet.set_column(idx, idx, max_len)  # set column width
writer.save()
</code></pre>
";;1;;2016-11-10T19:23:47.110;;40535454;2016-11-10T19:23:47.110;;;;;5076471.0;17326973.0;2;6;;;
75131;75131;;;"<p>From Pandas 0.18.1 onwards, there's a <code>normalize</code> option:</p>

<pre><code>In [1]: pd.crosstab(df.A,df.B, normalize='index')
Out[1]:

B              A           B           C
A           
one     0.333333    0.333333    0.333333
three   0.333333    0.333333    0.333333
two     0.333333    0.333333    0.333333
</code></pre>

<p>Where you can normalise across either <code>all</code>, <code>index</code> (rows), or <code>columns</code>.</p>

<p>More details are available <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html"" rel=""noreferrer"">in the documentation</a>.</p>
";;0;;2016-11-14T16:16:55.460;;40593226;2016-11-14T16:35:18.487;2016-11-14T16:35:18.487;;1108828.0;;1108828.0;21247203.0;2;17;;;
75357;75357;;;"<p>You can sort the dataFrame by count and then remove duplicates. I think it's easier:</p>

<pre><code>df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])
</code></pre>
";;2;;2016-11-16T10:14:22.037;;40629420;2016-11-16T10:14:22.037;;;;;5737533.0;15705630.0;2;14;;;
75774;75774;;;"<p>I'm using python 3.4 and Anaconda3 4.2.</p>

<p>I had the same problem, but it worked (the <code>import pandas</code> works now anyway) for me to install pandas with pip by writing:</p>

<p><code>python -m pip install pandas</code></p>

<p>Good luck!</p>
";;1;;2016-11-18T14:50:59.857;;40679977;2016-11-18T15:25:41.023;2016-11-18T15:25:41.023;;268592.0;;7166707.0;32565302.0;2;6;;;
76229;76229;;;"<p>Consider a temp table which would be exact replica of your final table, cleaned out with each run:</p>

<pre><code>engine = create_engine('postgresql+psycopg2://user:pswd@mydb')
df.to_sql('temp_table', engine, if_exists='replace')

sql = ""UPDATE final_table AS f"" + \
      "" SET col1 = t.col1"" + \
      "" FROM temp_table AS t"" + \
      "" WHERE f.id = t.id""

with engine.begin() as conn:     # TRANSACTION
    conn.execute(sql)
</code></pre>
";;0;;2016-11-22T16:03:53.043;;40746465;2016-11-22T16:03:53.043;;;;;1422451.0;31988322.0;2;7;;;
76360;76360;;;"<p>Nowadays, you can simply convert the Series to a DataFrame with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.to_frame.html"">to_frame()</a>. So (if joining on index):</p>

<pre><code>df.merge(s.to_frame(), left_index=True, right_index=True)
</code></pre>
";;0;;2016-11-23T11:03:11.060;;40762674;2016-11-23T11:03:11.060;;;;;5428013.0;26265819.0;2;32;;;
76396;76396;;;"<p>Why not keep it simple?!</p>

<pre><code>GB=DF.groupby([(DF.index.year),(DF.index.month)]).sum()
</code></pre>

<p>giving you,</p>

<pre><code>print(GB)
        abc  xyz
2013 6   80  250
     8   40   -5
2014 1   25   15
     2   60   80
</code></pre>

<p>and then you can plot like asked using,</p>

<pre><code>GB.plot('abc','xyz',kind='scatter')
</code></pre>
";;0;;2016-11-23T17:09:48.500;;40770463;2016-11-23T17:09:48.500;;;;;2592560.0;26646191.0;2;13;;;
76683;76683;;;"<p><strong>For better readability</strong>, we can simply use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental"" rel=""noreferrer"">the <code>query()</code> Method</a>, to avoid the lengthy <code>df.index.get_level_values()</code> and <code>reset_index</code>/<code>set_index</code> to and fro.</p>

<p>Here is the target <code>DataFrame</code>:</p>

<pre><code>In [12]: df                                                                    
Out[12]:                                                                       
          C                                                                    
A   B                                                                          
1.1 111  68                                                                    
    222  40                                                                    
3.3 222  20                                                                    
    333  11                                                                    
5.5 333  80                                                                    
6.6 777  51 
</code></pre>

<hr>

<p>Answer for <strong>Q1</strong> (<code>A</code> in range <code>[3.3, 6.6]</code>):                                                                   </p>

<pre><code>In [13]: df.query('3.3 &lt;= A &lt;= 6.6') # for closed interval                       
Out[13]:                                                                       
          C                                                                    
A   B                                                                          
3.3 222  20                                                                    
    333  11                                                                    
5.5 333  80                                                                    
6.6 777  51                                                                    

In [14]: df.query('3.3 &lt; A &lt; 6.6') # for open interval                         
Out[14]:                                                                       
          C                                                                    
A   B                                                                          
5.5 333  80
</code></pre>

<p>and of course one can play around with <code>&lt;, &lt;=, &gt;, &gt;=</code> for any kind of inclusion.</p>

<hr>

<p>Similarly, answer for <strong>Q2</strong> (<code>A</code> in range <code>[2.0, 4.0]</code>):</p>

<pre><code>In [15]: df.query('2.0 &lt;= A &lt;= 4.0')                                        
Out[15]:                                                                    
          C                                                                 
A   B                                                                       
3.3 222  20                                                                 
    333  11 
</code></pre>

<hr>

<p>Answer for <strong>Q3</strong> (<code>B</code> in range <code>[111, 500]</code>):</p>

<pre><code>In [16]: df.query('111 &lt;= B &lt;= 500')                                        
Out[16]:                                                                    
          C                                                                 
A   B                                                                       
1.1 111  68                                                                 
    222  40                                                                 
3.3 222  20                                                                 
    333  11                                                                 
5.5 333  80
</code></pre>

<hr>

<p>And moreover, you can <strong>COMBINE</strong> the query for col <code>A</code> and <code>B</code> very naturally!</p>

<pre><code>In [17]: df.query('0 &lt; A &lt; 4 and 150 &lt; B &lt; 400')                            
Out[17]:                                                                    
          C                                                                 
A   B                                                                       
1.1 222  40                                                                 
3.3 222  20                                                                 
    333  11
</code></pre>
";;0;;2016-11-26T10:44:15.120;;40817489;2016-11-26T10:44:15.120;;;;;2303761.0;17921010.0;2;6;;;
76688;76688;;;"<h3>Replace</h3>

<p><code>DataFrame</code> object has powerful and flexible <code>replace</code> method:</p>

<pre><code>DataFrame.replace(
        to_replace=None,
        value=None,
        inplace=False,
        limit=None,
        regex=False, 
        method='pad',
        axis=None)
</code></pre>

<p>Note, if you need to make changes in place, use <code>inplace</code> boolean argument for <code>replace</code> method:</p>

<h3>Inplace</h3>

<blockquote>
  <p><strong>inplace</strong>: boolean, default <code>False</code>
  If <code>True</code>, in place. Note: this will modify any other views on this object (e.g. a column form a DataFrame). Returns the caller if this is <code>True</code>.</p>
</blockquote>

<h3>Snippet</h3>

<pre><code>df['BrandName'].replace(
    to_replace=['ABC', 'AB'],
    value='A',
    inplace=True
)
</code></pre>
";;1;;2016-11-26T13:08:11.983;;40818627;2017-07-19T19:16:57.707;2017-07-19T19:16:57.707;;1391671.0;;629685.0;27060098.0;2;6;;;
76797;76797;;;"<p>Late answer but you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">query()</a>, i.e:</p>

<pre><code>df_filtered = df.query('a == 4 &amp; b != 2')
</code></pre>
";;0;;2016-11-27T21:21:46.480;;40834052;2017-04-06T08:48:48.073;2017-04-06T08:48:48.073;;797495.0;;797495.0;22591174.0;2;15;;;
77048;77048;;;"<p>The <code>.size()</code> built-in method of DataFrameGroupBy objects actually returns a Series object with the group sizes and not a DataFrame. If you want a DataFrame whose column is the group sizes, indexed by the groups, with a custom name, you can use the <code>.to_frame()</code> method and use the desired column name as its argument. </p>

<pre><code>grpd = df.groupby(['A','B']).size().to_frame('size')
</code></pre>

<p>If you wanted the groups to be columns again you could add a <code>.reset_index()</code> at the end. </p>
";;0;;2016-11-29T17:56:47.803;;40872584;2016-11-29T17:56:47.803;;;;;3478917.0;17995024.0;2;8;;;
77516;77516;;;"<p><strong>to_frame()</strong>: </p>

<p>Starting with the following Series, df:</p>

<pre><code>email
email1@email.com    A
email2@email.com    B
email3@email.com    C
dtype: int64
</code></pre>

<p>I use <strong>to_frame</strong> to convert the series to DataFrame:</p>

<pre><code>df = df.to_frame().reset_index()

    email               0
0   email1@email.com    A
1   email2@email.com    B
2   email3@email.com    C
3   email4@email.com    D
</code></pre>

<p>Now all you need is to rename the column name and name the index column:</p>

<pre><code>df = df.rename(columns= {0: 'list'})
df.index.name = 'index'
</code></pre>

<p>Your DataFrame is ready for further analysis. </p>

<p>Update: I just came across <a href=""https://stackoverflow.com/questions/28503445/assigning-column-names-to-a-pandas-series"">this link</a> where the answers are surprisingly similar to mine here. </p>
";;0;;2016-12-03T00:58:36.657;;40943143;2017-01-05T21:32:43.490;2017-05-23T11:47:26.357;;-1.0;;5553237.0;26097916.0;2;9;;;
77624;77624;;;"<p>The current (as of version 0.20) method for changing column names after a groupby operation is to chain the <code>rename</code> method. See <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""nofollow noreferrer"">this deprecation note</a> in the documentation for more detail.</p>

<h1>Deprecated Answer as of pandas version 0.20</h1>

<p>This is the first result in google and although the top answer works it does not really answer the question. There is a <a href=""https://stackoverflow.com/questions/19078325/naming-returned-columns-in-pandas-aggregate-function"">better answer here</a> and a long <a href=""https://github.com/pandas-dev/pandas/issues/9052"" rel=""nofollow noreferrer"">discussion on github</a> about the full functionality of passing dictionaries to the <code>agg</code> method. </p>

<p>These answers unfortunately do not exist in the documentation but the general format for grouping, aggregating and then renaming columns uses a dictionary of dictionaries. The keys to the outer dictionary are column names that are to be aggregated. The inner dictionaries have keys that the new column names with values as the aggregating function. </p>

<p>Before we get there, let's create a four column DataFrame. </p>

<pre><code>df = pd.DataFrame({'A' : list('wwwwxxxx'), 
                   'B':list('yyzzyyzz'), 
                   'C':np.random.rand(8), 
                   'D':np.random.rand(8)})

   A  B         C         D
0  w  y  0.643784  0.828486
1  w  y  0.308682  0.994078
2  w  z  0.518000  0.725663
3  w  z  0.486656  0.259547
4  x  y  0.089913  0.238452
5  x  y  0.688177  0.753107
6  x  z  0.955035  0.462677
7  x  z  0.892066  0.368850
</code></pre>

<p>Let's say we want to group by columns <code>A, B</code> and aggregate column <code>C</code> with <code>mean</code> and <code>median</code> and aggregate column <code>D</code> with <code>max</code>. The following code would do this.</p>

<pre><code>df.groupby(['A', 'B']).agg({'C':['mean', 'median'], 'D':'max'})

            D         C          
          max      mean    median
A B                              
w y  0.994078  0.476233  0.476233
  z  0.725663  0.502328  0.502328
x y  0.753107  0.389045  0.389045
  z  0.462677  0.923551  0.923551
</code></pre>

<p>This returns a DataFrame with a hierarchical index. The original question asked about renaming the columns in the same step. This is possible using a dictionary of dictionaries:</p>

<pre><code>df.groupby(['A', 'B']).agg({'C':{'C_mean': 'mean', 'C_median': 'median'}, 
                            'D':{'D_max': 'max'}})

            D         C          
        D_max    C_mean  C_median
A B                              
w y  0.994078  0.476233  0.476233
  z  0.725663  0.502328  0.502328
x y  0.753107  0.389045  0.389045
  z  0.462677  0.923551  0.923551
</code></pre>

<p>This renames the columns all in one go but still leaves the hierarchical index which the top level can be dropped with <code>df.columns = df.columns.droplevel(0)</code>.</p>
";;0;;2016-12-04T18:35:21.367;;40962126;2017-05-08T23:20:45.707;2017-05-23T12:02:51.417;;-1.0;;3707607.0;19523277.0;2;10;;;
78050;78050;;;"<p>To iterate through DataFrame's row in pandas way one can use:</p>

<ul>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html"" rel=""noreferrer"">DataFrame.iterrows()</a></p>

<pre><code>for index, row in df.iterrows():
    print row[""c1""], row[""c2""]
</code></pre></li>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html"" rel=""noreferrer"">DataFrame.itertuples()</a></p>

<pre><code>for row in df.itertuples(index=True, name='Pandas'):
    print getattr(row, ""c1""), getattr(row, ""c2"")
</code></pre></li>
</ul>

<p><code>itertuples()</code> is supposed to be faster than <code>iterrows()</code></p>

<p>But be aware, according to the docs (pandas 0.19.1 at the moment):</p>

<ul>
<li>iterrows: data's <code>dtype</code> might not match from row to row

<blockquote>
  <p>Because iterrows returns a Series for each row, it <strong>does not preserve</strong> dtypes across the rows (dtypes are preserved across columns for DataFrames)
  *</p>
</blockquote></li>
<li><p>iterrows: Do not modify rows</p>

<blockquote>
  <p>You should <strong>never modify</strong> something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.</p>
</blockquote>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""noreferrer"">DataFrame.apply()</a> instead:</p>

<pre><code>new_df = df.apply(lambda x: x * 2)
</code></pre></li>
<li><p>itertuples: </p>

<blockquote>
  <p>The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.</p>
</blockquote></li>
</ul>
";;0;;2016-12-07T16:41:28.513;;41022840;2016-12-07T18:44:00.267;2016-12-07T18:44:00.267;;3844376.0;;3844376.0;16476924.0;2;38;;;
78965;78965;;;"<p>Install the new module:</p>

<pre><code>pip install xlrd
</code></pre>
";;0;;2016-12-14T19:15:55.433;;41150420;2016-12-14T19:17:07.793;2016-12-14T19:17:07.793;;4687348.0;;4487782.0;31329627.0;2;13;;;
79106;79106;;;"<p>I was having trouble with the not (~) symbol as well, so here's another way from another <a href=""https://stackoverflow.com/questions/11350770/pandas-dataframe-select-by-partial-string"">StackOverflow thread</a>:</p>

<pre><code>    df[df[""col""].str.contains('this'|'that')==False]
</code></pre>
";;2;;2016-12-15T21:10:34.647;;41173392;2016-12-15T21:10:34.647;2017-05-23T12:17:58.683;;-1.0;;5398328.0;17097643.0;2;9;;;
79245;79245;;;"<p>Here's one approach  -</p>

<pre><code>mask = np.isnan(arr)
idx = np.where(~mask,np.arange(mask.shape[1]),0)
np.maximum.accumulate(idx,axis=1, out=idx)
out = arr[np.arange(idx.shape[0])[:,None], idx]
</code></pre>

<p>If you don't want to create another array and just fill the NaNs in <code>arr</code> itself, replace the last step with this -</p>

<pre><code>arr[mask] = arr[np.nonzero(mask)[0], idx[mask]]
</code></pre>

<p>Sample input, output -</p>

<pre><code>In [179]: arr
Out[179]: 
array([[  5.,  nan,  nan,   7.,   2.,   6.,   5.],
       [  3.,  nan,   1.,   8.,  nan,   5.,  nan],
       [  4.,   9.,   6.,  nan,  nan,  nan,   7.]])

In [180]: out
Out[180]: 
array([[ 5.,  5.,  5.,  7.,  2.,  6.,  5.],
       [ 3.,  3.,  1.,  8.,  8.,  5.,  5.],
       [ 4.,  9.,  6.,  6.,  6.,  6.,  7.]])
</code></pre>
";;2;;2016-12-16T19:20:45.237;;41191127;2016-12-16T20:04:32.853;2016-12-16T20:04:32.853;;3293881.0;;3293881.0;41190852.0;2;8;;;
79247;79247;;;"<p>For the Googlers who come upon this old question:</p>

<p>Regarding @kekert's comment on @Garrett's answer to use the new</p>

<pre><code>df.groupby('id')['x'].rolling(2).mean()
</code></pre>

<p>rather than the now-deprecated </p>

<pre><code>df.groupby('id')['x'].apply(pd.rolling_mean, 2, min_periods=1)
</code></pre>

<p>curiously, it seems that the new .rolling().mean() approach returns a multi-indexed series, indexed by the group_by column first and then the index.  Whereas, the old approach would simply return a series indexed singularly by the original df index, which perhaps makes less sense, but made it very convenient for adding that series as a new column into the original dataframe.</p>

<p>So I think I've figured out a solution that uses the new rolling() method and still works the same:</p>

<pre><code>df.groupby('id')['x'].rolling(2).mean().reset_index(0,drop=True)
</code></pre>

<p>which should give you the series</p>

<pre><code>0    0.0
1    0.5
2    1.5
3    3.0
4    3.5
5    4.5
</code></pre>

<p>which you can add as a column:</p>

<pre><code>df['x'] = df.groupby('id')['x'].rolling(2).mean().reset_index(0,drop=True)
</code></pre>
";;0;;2016-12-16T19:31:54.603;;41191278;2016-12-16T19:31:54.603;;;;;7006570.0;13996302.0;2;10;;;
79379;79379;;;"<p>Inorder to get the frequency counts of the values in a given interval binned range, we could make use of <a href=""http://pandas.pydata.org/pandas-docs/version/0.19.0/generated/pandas.cut.html"" rel=""noreferrer""><code>pd.cut</code></a> which returns indices of half open bins for each element along with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts</code></a> for computing their respective counts. </p>

<p>To plot their counts, a bar plot can be then made.</p>

<pre><code>step = 50
bin_range = np.arange(-200, 1000+step, step)
out, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)
out.value_counts(sort=False).plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/sZBWd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sZBWd.png"" alt=""enter image description here""></a></p>

<p>Frequency for each interval sorted in descending order of their counts:</p>

<pre><code>out.value_counts().head()
[-100, -50)    18
[0, 50)        16
[800, 850)      2
[-50, 0)        2
[950, 1000)     1
dtype: int64
</code></pre>

<hr>

<p>To modify the plot to include just the lower closed interval of the range for aesthetic purpose, you could do:</p>

<pre><code>out.cat.categories = bins[:-1]
out.value_counts(sort=False).plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Wu0XU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Wu0XU.png"" alt=""enter image description here""></a></p>
";;0;;2016-12-18T16:35:04.333;;41210491;2016-12-18T16:35:04.333;;;;;6207849.0;13129618.0;2;7;;;
79431;79431;;;"<p>Using a list of column names, change the type for multiple columns with .applymap() or for a single column with .apply().</p>

<pre><code>    df = pd.DataFrame(10*np.random.rand(3, 4), columns=list(""ABCD""))

              A         B         C         D
    0  8.362940  0.354027  1.916283  6.226750
    1  1.988232  9.003545  9.277504  8.522808
    2  1.141432  4.935593  2.700118  7.739108

    cols = ['A', 'B']
    df[cols] = df[cols].applymap(np.int64)

       A  B         C         D
    0  8  0  1.916283  6.226750
    1  1  9  9.277504  8.522808
    2  1  4  2.700118  7.739108

    df['C'] = df['C'].apply(np.int64)
       A  B  C         D
    0  8  0  1  6.226750
    1  1  9  9  8.522808
    2  1  4  2  7.739108
</code></pre>
";;3;;2016-12-19T08:30:17.023;;41218519;2016-12-19T08:30:17.023;;;;;4322543.0;21291259.0;2;8;;;
79483;79483;;;"<p>From version 0.19.0 of Pandas you can use the <code>lines</code> parameter, like so:</p>

<pre><code>import pandas as pd

data = pd.read_json('/path/to/file.json', lines=True)
</code></pre>
";;2;;2016-12-19T16:04:22.933;;41226605;2016-12-19T16:04:22.933;;;;;918.0;30088006.0;2;28;;;
79503;79503;;;"<p>We can create a series with both index and values equal to the index keys using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.to_series.html"" rel=""noreferrer""><code>to_series</code></a> and then compute the differences between successive rows which would result in <code>timedelta64[ns]</code> dtype. After obtaining this, via the <code>.dt</code> property, we could access the seconds attribute of the time portion and finally divide each element by 60 to get it outputted in minutes(optionally filling the first value with 0).</p>

<pre><code>In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)
    ...: df                                 # use .astype(int) to obtain integer values
Out[13]: 
                     value  deltaT
time                              
2012-03-16 23:50:00      1     0.0
2012-03-16 23:56:00      2     6.0
2012-03-17 00:08:00      3    12.0
2012-03-17 00:10:00      4     2.0
2012-03-17 00:12:00      5     2.0
2012-03-17 00:20:00      6     8.0
2012-03-20 00:43:00      7    23.0
</code></pre>

<hr>

<p><strong><em>simplification:</em></strong></p>

<p>When we perform <code>diff</code>:</p>

<pre><code>In [8]: ser_diff = df.index.to_series().diff()

In [9]: ser_diff
Out[9]: 
time
2012-03-16 23:50:00               NaT
2012-03-16 23:56:00   0 days 00:06:00
2012-03-17 00:08:00   0 days 00:12:00
2012-03-17 00:10:00   0 days 00:02:00
2012-03-17 00:12:00   0 days 00:02:00
2012-03-17 00:20:00   0 days 00:08:00
2012-03-20 00:43:00   3 days 00:23:00
Name: time, dtype: timedelta64[ns]
</code></pre>

<p>Seconds to minutes conversion:</p>

<pre><code>In [10]: ser_diff.dt.seconds.div(60, fill_value=0)
Out[10]: 
time
2012-03-16 23:50:00     0.0
2012-03-16 23:56:00     6.0
2012-03-17 00:08:00    12.0
2012-03-17 00:10:00     2.0
2012-03-17 00:12:00     2.0
2012-03-17 00:20:00     8.0
2012-03-20 00:43:00    23.0
Name: time, dtype: float64
</code></pre>

<hr>

<p>If suppose you want to include even the <code>date</code> portion as it was excluded previously(only time portion was considered), <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.total_seconds.html"" rel=""noreferrer""><code>dt.total_seconds</code></a> would give you the elapsed duration in seconds with which minutes could then be calculated again by division.</p>

<pre><code>In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)
Out[12]: 
time
2012-03-16 23:50:00       0.0
2012-03-16 23:56:00       6.0
2012-03-17 00:08:00      12.0
2012-03-17 00:10:00       2.0
2012-03-17 00:12:00       2.0
2012-03-17 00:20:00       8.0
2012-03-20 00:43:00    4343.0    # &lt;-- number of minutes in 3 days 23 minutes
Name: time, dtype: float64
</code></pre>
";;2;;2016-12-19T17:49:19.020;;41228272;2017-01-12T13:50:21.103;2017-01-12T13:50:21.103;;6207849.0;;6207849.0;16777570.0;2;8;;;
79506;79506;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.days.html"" rel=""noreferrer""><code>dt.days</code></a> to obtain the days attribute as integers.</p>

<p>For eg:</p>

<pre><code>In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))

In [15]: s
Out[15]: 
0    1 days 00:00:00
1    3 days 02:00:00
2    5 days 04:00:00
3    7 days 06:00:00
4    9 days 08:00:00
5   11 days 10:00:00
dtype: timedelta64[ns]

In [16]: s.dt.days
Out[16]: 
0     1
1     3
2     5
3     7
4     9
5    11
dtype: int64
</code></pre>

<p>More generally - You can use the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.dt.components.html"" rel=""noreferrer""><code>.components</code></a> property to access a reduced form of <code>timedelta</code>.</p>

<pre><code>In [17]: s.dt.components
Out[17]: 
   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
0     1      0        0        0             0             0            0
1     3      2        0        0             0             0            0
2     5      4        0        0             0             0            0
3     7      6        0        0             0             0            0
4     9      8        0        0             0             0            0
5    11     10        0        0             0             0            0
</code></pre>

<p>Now, to get the <code>hours</code> attribute:</p>

<pre><code>In [23]: s.dt.components.hours
Out[23]: 
0     0
1     2
2     4
3     6
4     8
5    10
Name: hours, dtype: int64
</code></pre>
";;0;;2016-12-19T18:30:46.110;;41228807;2016-12-19T18:30:46.110;;;;;6207849.0;18215317.0;2;7;;;
81097;81097;;;"<p>Some of the above solutions did not work for me. The <code>.fig</code> attribute was not found when I tried that and I was unable to use <code>.savefig()</code> directly. However, what did work was:</p>

<pre><code>sns_plot.figure.savefig(""output.png"")
</code></pre>

<p>I am a newer Python user, so I do not know if this is due to an update. I wanted to mention it in case anybody else runs into the same issues as I did.</p>
";;0;;2017-01-03T21:09:58.860;;41452422;2017-01-03T21:09:58.860;;;;;4769229.0;32244753.0;2;7;;;
81195;81195;;;"<p>We will leverage <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html"" rel=""nofollow noreferrer""><code>np.searchsorted</code></a> and logic to find cluster edges.</p>

<p>First, let's take a closer look at what <code>np.searchsorted</code> does:</p>

<blockquote>
  <p>Find the indices into a sorted array a such that, if the corresponding elements in v were inserted before the indices, the order of a would be preserved.</p>
</blockquote>

<p>What I'll do is execute <code>np.searchsorted</code> with <code>a</code> using <code>a - delta_left</code>.  Let's look at that for <code>delta_left = 1</code></p>

<pre><code># a =
# [ 0  2  3  4  5 10 11 11 14 19 20 20]
# 
# a - delta_left
# [-1  1  2  3  4  9 10 10 13 18 19 19]
</code></pre>

<ul>
<li><code>-1</code> would get inserted at position <code>0</code> to maintain order</li>
<li><code>1</code> would get inserted at position <code>1</code> to maintain order</li>
<li><code>2</code> would get inserted at position <code>1</code> as well, indicating that <code>2</code> might be in the same cluster as <code>1</code></li>
<li><code>3</code> would get inserted at position <code>2</code> indicating that <code>3</code> might be in the same cluster as <code>2</code></li>
<li>so on and so forth</li>
</ul>

<p>What we notice is that only when an element less delta would get inserted at its current position would we consider a new cluster starting.</p>

<p>We do this again for the right side with a difference.  The difference is that by default if a bunch of elements are the same, <code>np.searchsorted</code> assumes to insert into the front of values.  To identify the ends of clusters, I'm going to want to insert after the identical elements.  Therefore I'll use the paramater <code>side='right'</code></p>

<blockquote>
  <p>If left, the index of the first suitable location found is given. If right, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of a).</p>
</blockquote>

<p>Now the logic.  A cluster can only begin if a prior cluster has ended, with the exception of the first cluster.  We'll then consider a shifted version of the results of our second <code>np.searchsorted</code></p>

<hr>

<p>Let's now define our function</p>

<pre><code>def delta_cluster(a, dleft, dright):
    # use to track whether searchsorted results are at correct positions
    rng = np.arange(len(a))

    edge_left = a.searchsorted(a - dleft)
    starts = edge_left == rng

    # we append 0 to shift
    edge_right = np.append(0, a.searchsorted(a + dright, side='right')[:-1])
    ends = edge_right == rng

    return (starts &amp; ends).cumsum()
</code></pre>

<hr>

<p><strong><em>demonstration</em></strong>  </p>

<p>with left, right deltas equal to 1 and 1</p>

<pre><code>print(delta_cluster(a, 1, 1))

[1 2 2 2 2 3 3 3 4 5 5 5]
</code></pre>

<p>with left, right deltas equal to 2 and 1</p>

<pre><code>print(delta_cluster(a, 2, 1))

[1 1 1 1 1 2 2 2 3 4 4 4]
</code></pre>

<p><strong><em>Extra Credit</em></strong><br>
What if <code>a</code> isn't sorted?<br>
I'll utilize information learned from <a href=""https://stackoverflow.com/q/41394595/2336654"">this post</a></p>

<pre><code>def delta_cluster(a, dleft, dright):

    s = a.argsort()

    size = s.size

    if size &gt; 1000:
        y = np.empty(s.size, dtype=np.int64)
        y[s] = np.arange(s.size)
    else:
        y = s.argsort()

    a = a[s]

    rng = np.arange(len(a))

    edge_left = a.searchsorted(a - dleft)
    starts = edge_left == rng

    edge_right = np.append(0, a.searchsorted(a + dright, side='right')[:-1])
    ends = edge_right == rng

    return (starts &amp; ends).cumsum()[y]
</code></pre>

<p><strong><em>demonstration</em></strong>  </p>

<pre><code>b = np.random.permutation(a)
print(b)

[14 10  3 11 20  0 19 20  4 11  5  2]
</code></pre>

<hr>

<pre><code>print(delta_cluster(a, 2, 1))

[1 1 1 1 1 2 2 2 3 4 4 4]
</code></pre>

<hr>

<pre><code>print(delta_cluster(b, 2, 1))

[3 2 1 2 4 1 4 4 1 2 1 1]
</code></pre>

<hr>

<pre><code>print(delta_cluster(b, 2, 1)[b.argsort()])

[1 1 1 1 1 2 2 2 3 4 4 4]
</code></pre>
";;0;;2017-01-04T12:45:43.440;;41464194;2017-01-05T01:37:07.187;2017-05-23T11:53:07.243;;-1.0;;2336654.0;41464177.0;2;9;;;
81675;81675;;;"<p>I don't know if this is new in <code>sklearn</code> or <code>pandas</code>, but I'm able to pass the data frame directly to <code>sklearn</code> without converting the data frame to a numpy array or any other data types.</p>

<pre><code>from sklearn import linear_model

reg = linear_model.LinearRegression()
reg.fit(df[['B', 'C']], df['A'])

&gt;&gt;&gt; reg.coef_
array([  4.01182386e-01,   3.51587361e-04])
</code></pre>
";;0;;2017-01-07T02:51:14.893;;41517319;2017-01-07T02:51:14.893;;;;;5839618.0;19991445.0;2;8;;;
81750;81750;;;"<p><strong><em>If you simply want to create an empty data frame and fill it with some incoming data frames later, try this:</em></strong> </p>

<p>In this example I am using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"" rel=""noreferrer"">this pandas doc</a> to create a new data frame and then using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html"" rel=""noreferrer"">append</a> to write to the newDF with data from oldDF. </p>

<hr>

<p><strong>Have a look at this</strong></p>

<pre><code>newDF = pd.DataFrame() #creates a new dataframe that's empty
newDF = newDF.append(oldDF, ignore_index = True) # ignoring index is optional
# try printing some data from newDF
print newDF.head() #again optional 
</code></pre>

<ul>
<li>if I have to keep appending new data into this newDF from more than
one oldDFs, I just use a for loop to iterate over
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html"" rel=""noreferrer"">pandas.DataFrame.append()</a></li>
</ul>
";;1;;2017-01-08T04:18:18.757;;41529411;2017-05-23T15:56:27.003;2017-05-23T15:56:27.003;;5893695.0;;5893695.0;13784192.0;2;28;;;
81763;81763;;;"<p>one easy way by using <strong>Pandas</strong>: (here I want to use mean normalization)</p>

<pre><code>normalized_df=(df-df.mean())/df.std()
</code></pre>

<p>to use min-max normalization:</p>

<pre><code>normalized_df=(df-df.min())/(df.max()-df.min())
</code></pre>
";;2;;2017-01-08T11:25:26.210;;41532180;2017-01-19T10:26:18.660;2017-01-19T10:26:18.660;;3659286.0;;3659286.0;26414913.0;2;14;;;
81924;81924;;;"<p>if its just counting nan values in a pandas column here is a quick way</p>

<pre><code>import pandas as pd
## df1 as an example data frame 
## col1 name of column for which you want to calculate the nan values
sum(pd.isnull(df1['col1']))
</code></pre>
";;0;;2017-01-09T18:49:19.973;;41554866;2017-01-09T18:49:19.973;;;;;3156200.0;26266362.0;2;6;;;
82270;82270;;;"<p>tf.initialize_all_variables() is deprecated. Instead initialize tensorflow variables with: </p>

<pre><code>tf.global_variables_initializer()
</code></pre>

<p>A common example usage is:</p>

<pre><code>with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())
</code></pre>
";;3;;2017-01-12T07:16:36.377;;41607207;2017-01-12T07:16:36.377;;;;;3144836.0;34001922.0;2;13;;;
82455;82455;;;"<p>df.loc will do the job :</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(3, 2), columns=['A','B'])
&gt;&gt;&gt; df
          A         B
0 -0.269036  0.534991
1  0.069915 -1.173594
2 -1.177792  0.018381
&gt;&gt;&gt; df.loc[13] = df.loc[1]
&gt;&gt;&gt; df
           A         B
0  -0.269036  0.534991
1   0.069915 -1.173594
2  -1.177792  0.018381
13  0.069915 -1.173594
</code></pre>
";;0;;2017-01-13T07:44:59.290;;41629653;2017-01-13T07:44:59.290;;;;;3592751.0;16824607.0;2;8;;;
82765;82765;;;"<p>This is an alternative answer that can be much faster when your dictionary has more than a couple of keys.  If your dictionary exhaustively maps all possible values, this takes a very simple form:</p>

<pre><code>df['col1'].map(di)
</code></pre>

<p>Although <code>map</code> most commonly takes a function as its argument, it can alternatively take a dictionary or series:  <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""noreferrer"">Documentation for Pandas.series.map</a></p>

<p>Without an exhaustive mapping, you need to add <code>update</code>:</p>

<pre><code>df['col1'].update( df['col1'].map(di) )   # note: series update is an inplace operation
</code></pre>

<p>Here are some timings for the exhaustive case on a dataframe with 100,000 rows and 8 dictionary keys (it's about 10x faster).</p>

<pre><code>di = {1: ""A"", 2: ""B"", 3: ""C"", 4: ""D"", 5: ""E"", 6: ""F"", 7: ""G"", 8: ""H"" }
df = pd.DataFrame({ 'col1': np.random.choice( range(1,9), 100000 ) })

%timeit df.replace({""col1"": di})
10 loops, best of 3: 55.6 ms per loop

%timeit df['col1'].map(di)
100 loops, best of 3: 4.16 ms per loop
</code></pre>
";;2;;2017-01-16T14:40:56.937;;41678874;2017-07-11T14:17:56.317;2017-07-11T14:17:56.317;;3877338.0;;3877338.0;20250771.0;2;16;;;
83463;83463;;;"<pre><code>df[df.duplicated(['ID'], keep=False)]
</code></pre>

<p>it'll return all duplicated rows back to you.</p>

<p>reference:
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html</a></p>

<p>keep : {first, last, False}, default first</p>

<ul>
<li>first : Mark duplicates as True except for the first occurrence.</li>
<li>last : Mark duplicates as True except for the last occurrence.</li>
<li>False : Mark all duplicates as True.</li>
</ul>
";;0;;2017-01-22T02:50:32.567;;41786821;2017-01-22T02:50:32.567;;;;;7046704.0;14657241.0;2;6;;;
83540;83540;;;"<p>You can use the <code>isin</code> method on the <code>date</code> column like so
<code>df[df[""date""].isin(pd.date_range(start_date, end_date))]</code></p>

<p><strong>Example:</strong>   </p>

<pre><code>import numpy as np   
import pandas as pd

# Make a DataFrame with dates and random numbers
df = pd.DataFrame(np.random.random((30, 3)))
df['date'] = pd.date_range('2017-1-1', periods=30, freq='D')

# Select the rows between two dates
in_range_df = df[df[""date""].isin(pd.date_range(""2017-01-15"", ""2017-01-20""))]

print(in_range_df)  # print result
</code></pre>

<p>which gives</p>

<pre><code>           0         1         2       date
14  0.960974  0.144271  0.839593 2017-01-15
15  0.814376  0.723757  0.047840 2017-01-16
16  0.911854  0.123130  0.120995 2017-01-17
17  0.505804  0.416935  0.928514 2017-01-18
18  0.204869  0.708258  0.170792 2017-01-19
19  0.014389  0.214510  0.045201 2017-01-20
</code></pre>
";;0;;2017-01-23T08:47:36.237;;41802199;2017-04-12T09:50:55.657;2017-04-12T09:50:55.657;;7404658.0;;7404658.0;29370057.0;2;7;;;
83649;83649;;;"<p>I know this is a bit of an old thread but with pandas 19.02, you can do:</p>

<pre><code>df.select_dtypes(include=['float64']).apply(your_function)
df.select_dtypes(exclude=['string','object']).apply(your_other_function)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.select_dtypes.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.select_dtypes.html</a></p>
";;1;;2017-01-23T21:45:21.177;;41816244;2017-01-23T21:45:21.177;;;;;3761689.0;22697773.0;2;15;;;
83851;83851;;;"<p>And if your dates are standardized by importing datetime package, you can simply use:</p>

<pre><code>df[(df['date']&gt;datetime.date(2016,1,1)) &amp; (df['date']&lt;datetime.date(2016,3,1))]  
</code></pre>

<p>For standarding your date string using datetime package, you can use this function:</p>

<pre><code>import datetime
datetime.datetime.strptime
</code></pre>
";;0;;2017-01-25T07:02:52.043;;41845355;2017-01-25T07:02:52.043;;;;;7467432.0;22898824.0;2;7;;;
84093;84093;;;"<p><a href=""https://stackoverflow.com/a/20903553"">This answer</a> uses the DataFrame.filter method to do this without list comprehension:</p>

<pre><code>import pandas as pd

data = {'spike-2': [1,2,3], 'hey spke': [4,5,6]}
df = pd.DataFrame(data)

print(df.filter(like='spike').columns)
</code></pre>

<p>Will output just 'spike-2'.  You can also use regex, as some people suggested in comments above:</p>

<pre><code>print(df.filter(regex='spike|spke').columns)
</code></pre>

<p>Will output both columns: ['spike-2', 'hey spke']</p>
";;0;;2017-01-26T15:10:21.010;;41876593;2017-01-26T15:10:21.010;2017-05-23T10:31:30.213;;-1.0;;4404015.0;21285380.0;2;9;;;
84132;84132;;;"<p>In the latest version of pandas (<code>0.19.2</code>) you can directly pass the url</p>

<pre><code>import pandas as pd

url=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""
c=pd.read_csv(url)
</code></pre>
";;2;;2017-01-26T18:34:37.083;;41880513;2017-01-26T18:34:37.083;;;;;1894184.0;32400867.0;2;35;;;
84206;84206;;;"<p>References</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Coskewness""><strong><em>Coskewness</em></strong></a></li>
<li><a href=""https://en.wikipedia.org/wiki/Cokurtosis""><strong><em>Cokurtosis</em></strong></a></li>
</ul>

<h1>Calculating <code>coskew</code></h1>

<p>My interpretation of coskew is the ""correlation"" between one series and the variance of another.  As such, you can actually have two types of coskew depending on which series we are calculating the variance of.  Wikipedia shows these two formula</p>

<p><strong><em><code>'left'</code></em></strong><br>
<a href=""https://i.stack.imgur.com/qnHME.png""><img src=""https://i.stack.imgur.com/qnHME.png"" alt=""enter image description here""></a><br>
<strong><em><code>'right'</code></em></strong><br>
<a href=""https://i.stack.imgur.com/APYb6.png""><img src=""https://i.stack.imgur.com/APYb6.png"" alt=""enter image description here""></a></p>

<p>Fortunately, when we calculate the coskew matrix, one is the transpose of the other.</p>

<pre><code>def coskew(df, bias=False):
    v = df.values
    s1 = sigma = v.std(0, keepdims=True)
    means = v.mean(0, keepdims=True)

    # means is 1 x n (n is number of columns
    # this difference broacasts appropriately
    v1 = v - means

    s2 = sigma ** 2

    v2 = v1 ** 2

    m = v.shape[0]

    skew = pd.DataFrame(v2.T.dot(v1) / s2.T.dot(s1) / m, df.columns, df.columns)

    if not bias:
        skew *= ((m - 1) * m) ** .5 / (m - 2)

    return skew
</code></pre>

<h2>demonstration</h2>

<pre><code>coskew(df)

          a         b
a -0.369380  0.096974
b  0.325311  0.067020
</code></pre>

<p>We can compare this to <code>df.skew()</code> and check that the diagonals are the same</p>

<pre><code>df.skew()

a   -0.36938
b    0.06702
dtype: float64
</code></pre>

<h1>Calculating <code>cokurtosis</code></h1>

<p>My interpretation of cokurtosis is one of two</p>

<ol>
<li>""correlation"" between a series and the skew of another</li>
<li>""correlation"" between the variances of two series</li>
</ol>

<p>For option 1. we again have both a left and right variant that in matrix form are transposes of one another.  So, we will only focus on the left variant.  That leaves us with calculating a total of two variations.</p>

<p><strong><em><code>'left'</code></em></strong><br>
<a href=""https://i.stack.imgur.com/XTDJ1.png""><img src=""https://i.stack.imgur.com/XTDJ1.png"" alt=""enter image description here""></a><br>
<strong><em><code>'middle'</code></em></strong><br>
<a href=""https://i.stack.imgur.com/cEFoB.png""><img src=""https://i.stack.imgur.com/cEFoB.png"" alt=""enter image description here""></a></p>

<pre><code>def cokurt(df, bias=False, fisher=True, variant='middle'):
    v = df.values
    s1 = sigma = v.std(0, keepdims=True)
    means = v.mean(0, keepdims=True)

    # means is 1 x n (n is number of columns
    # this difference broacasts appropriately
    v1 = v - means

    s2 = sigma ** 2
    s3 = sigma ** 3

    v2 = v1 ** 2
    v3 = v1 ** 3

    m = v.shape[0]

    if variant in ['left', 'right']:
        kurt = pd.DataFrame(v3.T.dot(v1) / s3.T.dot(s1) / m, df.columns, df.columns)
        if variant == 'right':
            kurt = kurt.T
    elif variant == 'middle':
        kurt = pd.DataFrame(v2.T.dot(v2) / s2.T.dot(s2) / m, df.columns, df.columns)

    if not bias:
        kurt = kurt * (m ** 2 - 1) / (m - 2) / (m - 3) - 3 * (m - 1) ** 2 / (m - 2) / (m - 3)
    if not fisher:
        kurt += 3

    return kurt
</code></pre>

<h2>demonstration</h2>

<pre><code>cokurt(df, variant='middle', bias=False, fisher=False)

          a        b
a  1.882817  0.86649
b  0.866490  1.63200

cokurt(df, variant='left', bias=False, fisher=False)

          a        b
a  1.882817  0.19175
b -0.020567  1.63200
</code></pre>

<p>The diagonal should be equal to <code>kurtosis</code></p>

<pre><code>df.kurtosis() + 3

a    1.882817
b    1.632000
dtype: float64
</code></pre>
";;0;;2017-01-27T09:41:18.470;;41890871;2017-01-27T09:41:18.470;;;;;2336654.0;41890870.0;2;9;;;
84511;84511;;;"<p>I think you need:</p>

<p><code>groupby</code> with <code>apply</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_values.html"" rel=""noreferrer""><code>sort_values</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.diff.html"" rel=""noreferrer""><code>diff</code></a>, convert <code>Timedelta</code> to minutes by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.seconds.html"" rel=""noreferrer""><code>seconds</code></a> and floor division <code>60</code></p>

<p><code>fillna</code> and <code>sort_index</code>, remove level <code>2</code> in index</p>

<pre><code>df = df.groupby(['from','to']).datetime
       .apply(lambda x: x.sort_values().diff().dt.seconds // 60)
       .fillna(0)
       .sort_index()
       .reset_index(level=2, drop=True)
       .reset_index(name='timediff in minutes')

print (df)

   from  to  timediff in minutes 
0    11   1                 120.0
1    11   1                 255.0
2    11   1                 225.0
3    11   1                   0.0
4    11  12                 300.0
5    11  12                   0.0
6    11  18                   0.0
7    12   3                   0.0
8    12   3                   0.0
</code></pre>

<hr>

<pre><code>df = df.join(df.groupby(['from','to'])
               .datetime
               .apply(lambda x: x.sort_values().diff().dt.seconds // 60)
               .fillna(0)
               .reset_index(level=[0,1], drop=True)
               .rename('timediff in minutes'))
print (df)
   from  to            datetime other  timediff in minutes
0    11   1 2016-11-06 22:00:00     -                120.0
1    11   1 2016-11-06 20:00:00     -                255.0
2    11   1 2016-11-06 15:45:00     -                225.0
3    11  12 2016-11-06 15:00:00     -                300.0
4    11   1 2016-11-06 12:00:00     -                  0.0
5    11  18 2016-11-05 10:00:00     -                  0.0
6    11  12 2016-11-05 10:00:00     -                  0.0
7    12   3 2016-10-05 10:00:59     -                  0.0
8    12   3 2016-09-06 10:00:34     -                  0.0
</code></pre>
";;2;;2017-01-30T06:10:09.017;;41929910;2017-01-30T06:40:32.720;2017-01-30T06:40:32.720;;2901002.0;;2901002.0;41929772.0;2;14;;;
84512;84512;;;"<p>Almost as above, but without <code>apply</code>:</p>

<pre><code>result = df.sort_values(['from','to','datetime'])\
           .groupby(['from','to'])['datetime']\
           .diff().dt.seconds.fillna(0)
</code></pre>
";;0;;2017-01-30T06:13:04.163;;41929939;2017-01-30T06:13:04.163;;;;;4492932.0;41929772.0;2;10;;;
84513;84513;;;"<pre><code>df.assign(
    timediff=df.sort_values(
        'datetime', ascending=False
    ).groupby(['from', 'to']).datetime.diff(-1).dt.seconds.div(60).fillna(0))
</code></pre>

<p><a href=""https://i.stack.imgur.com/NCgJR.png""><img src=""https://i.stack.imgur.com/NCgJR.png"" alt=""enter image description here""></a></p>
";;0;;2017-01-30T06:15:06.567;;41929958;2017-01-30T06:15:06.567;;;;;2336654.0;41929772.0;2;12;;;
85619;85619;;;"<p>A nice way to do this in one line using <code>pandas.concat()</code>:</p>

<pre><code>import pandas as pd

pd.concat([df], keys=['Foo'], names=['Firstlevel'])
</code></pre>

<p>This can be generalized to many data frames, see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""noreferrer"">docs</a>.</p>
";;1;;2017-02-07T16:11:06.527;;42094658;2017-02-07T16:11:06.527;;;;;2136626.0;14744068.0;2;10;;;
85906;85906;;;"<p>You have many options. Collating some of the answers above and the <a href=""https://stackoverflow.com/questions/19960077/how-to-implement-in-and-not-in-for-pandas-dataframe"">accepted answer from this post</a> you can do:<br>
1. <code>df[-df[""column""].isin([""value""])]</code><br>
2. <code>df[~df[""column""].isin([""value""])]</code><br>
3. <code>df[df[""column""].isin([""value""]) == False]</code><br>
4. <code>df[np.logical_not(df[""column""].isin([""value""]))]</code></p>

<p>Note: for option 4 for you'll need to <code>import numpy as np</code></p>
";;0;;2017-02-09T09:52:50.773;;42133330;2017-08-08T10:31:35.703;2017-08-08T10:31:35.703;;7404658.0;;7404658.0;14057007.0;2;11;;;
86281;86281;;;"<p>You could also specify the sheet name as a parameter:</p>

<pre><code>data_file = pd.read_excel('path_to_file.xls', sheetname=""sheet_name"")
</code></pre>

<p>will upload only the sheet ""sheet_name""</p>
";;0;;2017-02-11T19:37:17.173;;42180357;2017-02-11T19:37:17.173;;;;;7528153.0;26521266.0;2;6;;;
86780;86780;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.days.html"" rel=""noreferrer""><code>dt.days</code></a> attribute. Supposing <code>td</code> is the name of your timedelta Series, access this attribute via:</p>

<pre><code>td.dt.days
</code></pre>
";;1;;2017-02-15T10:50:14.973;;42247228;2017-02-15T10:50:14.973;;;;;1892435.0;25646200.0;2;8;;;
87108;87108;;;"<pre><code>pd.set_option('display.max_columns', None)  
</code></pre>

<p><code>id</code> can fully show the columns. </p>
";;0;;2017-02-17T09:09:53.783;;42293737;2017-02-17T09:32:05.943;2017-02-17T09:32:05.943;;4609855.0;;7579768.0;25351968.0;2;7;;;
87158;87158;;;"<p><strong>Obtained a incredible saving of time!</strong><br></p>

<p><strong>Output:</strong><br>
Size of a_list: 49998 Randomized unique values<br>
<strong>percentile_1 (Your given df - scipy)</strong><br>
computed percentile 104 times - 104 records in 0:00:07.777022  </p>

<p><strong>percentile_9 (class PercentileOfScore(rank_searchsorted_list) using given df)</strong><br>
computed percentile 104 times - 104 records in 0:00:00.000609<br>
<code>_    dt       src dest    a       b             pct           scipy   _
 0: 2016-01-01 YYZ SFO   54812  279.28  74.81299251970079  74.8129925197
 1: 2016-01-01 DFW PDX  111.35  -65.5   24.66698667946718  24.6669866795
 2: 2016-02-01 YYZ SFO   64.84  342.35  76.4810592423697   76.4810592424
 3: 2016-02-01 DFW PDX   63.81   61.64  63.84655386215449  63.8465538622
...
24: 2017-01-01 YYZ SFO   97.04  338.28  76.3570542821712   76.3570542822
25: 2017-01-01 DFW PDX  133.94 -129.69  21.4668586743469   21.4668586743
</code></p>

<p>Looking at the implementation of <code>scipy.percentileofscore</code> i found that the whole <code>list( a )</code>
are - copied, inserted, sorted, searched - on every call of <code>percentileofscore</code>.<br></p>

<p>I implemented my own <code>class PercentileOfScore</code></p>

<pre><code>import numpy as np
class PercentileOfScore(object):

    def __init__(self, aList):
        self.a = np.array( aList )
        self.a.sort()
        self.n = float(len(self.a))
        self.pct = self.__rank_searchsorted_list
    # end def __init__

    def __rank_searchsorted_list(self, score_list):
        adx = np.searchsorted(self.a, score_list, side='right')
        pct = []
        for idx in adx:
            # Python 2.x needs explicit type casting float(int)
            pct.append( (float(idx) / self.n) * 100.0 )

        return pct
    # end def _rank_searchsorted_list
# end class PercentileOfScore
</code></pre>

<hr>

<p>I don't think that <code>def percentile_7</code> will fit your needs. <code>dt</code> will not considered.</p>

<pre><code>PctOS = None
def percentile_7(df_flat):
    global PctOS
    result = {}
    for k in df_flat.pair_dict.keys():
        # df_flat.pair_dict = { 'src.dst': [b,b,...bn] }
        result[k] = PctOS.pct( df_flat.pair_dict[k] )

    return result
# end def percentile_7
</code></pre>

<hr>

<p>In your manual sample you use the whole <code>df.a</code>. In this sample its <code>dt_flat.a_list</code>, but i'm not sure if this is what you want?</p>

<pre><code>from PercentileData import DF_flat
def main():
    # DF_flat.data = {'dt.src.dest':[a,b]}
    df_flat = DF_flat()

    # Instantiate Global PctOS
    global PctOS
    # df_flat.a_list = [a,a,...an]
    PctOS = PercentileOfScore(df_flat.a_list)

    result = percentile_7(df_flat)
    # result = dict{'src.dst':[pct,pct...pctn]}
</code></pre>

<p><strong><em>Tested with Python:3.4.2 and 2.7.9  - numpy: 1.8.2</em></strong></p>
";;3;;2017-02-17T18:46:00.817;;42305299;2017-02-24T09:43:02.773;2017-02-24T09:43:02.773;;7414759.0;;7414759.0;42076126.0;2;6;;;
87364;87364;;;"<p>Before pulling out the multiprocessing hammer, your first step should be to do some profiling. Use cProfile to quickly look through to identify which functions are taking a long time. Unfortunately if your lines are all in a single function call, they'll show up as library calls. line_profiler is better but takes a little more setup time. </p>

<p>NOTE. If using ipython, you can use %timeit (magic command for the timeit module) and %prun (magic command for the profile module) both to time your statements as well as functions. A google search will show some guides.</p>

<p>Pandas is a wonderful library, but I've been an occasional victim of using it poorly with atrocious results. In particular, be wary of  append()/concat() operations. That might be your bottleneck but you should profile to be sure.  Usually, the numpy.vstack() and numpy.hstack() operations are faster if you don't need to perform index/column alignment. In your case it looks like you might be able to get by with Series or 1-D numpy ndarrays which can save time.</p>

<p>BTW, a <code>try</code> block in python is much slower often 10x or more than checking for an invalid condition, so be sure you absolutely need it when sticking it into a loop for every single line. This is probably the other hogger of time; I imagine you stuck the try block to check for AttributeError in case of a match.group(1) failure. I would check for a valid match first. </p>

<p>Even these small modifications should be enough for your program to run significantly faster before trying anything drastic like multiprocessing. Those Python libraries are awesome but bring a fresh set of challenges to deal with.</p>
";;1;;2017-02-20T02:25:59.783;;42335527;2017-02-20T02:25:59.783;;;;;3878013.0;42157944.0;2;11;;;
87744;87744;;;"<p>This answer is based on the 2nd tip from this blog post: <a href=""https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/"" rel=""nofollow noreferrer"">28 Jupyter Notebook tips, tricks and shortcuts</a></p>

<p>You can add the following code to the top of your notebook</p>

<pre><code>from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""
</code></pre>

<p>This tells Jupyter to print the results for any variable or statement on its own line. So you can then execute a cell solely containing   </p>

<pre><code>df1
df2
</code></pre>

<p>and it will ""print out the beautiful tables for both datasets"".</p>
";;1;;2017-02-22T13:26:48.073;;42392805;2017-04-11T14:30:53.593;2017-04-11T14:30:53.593;;7404658.0;;7404658.0;26873127.0;2;10;;;
87842;87842;;;"<p>I've used this many times as it's a particular easy implementation of multiprocessing. </p>

<pre><code>import pandas as pd
from multiprocessing import Pool

def reader(filename):
    return pd.read_excel(filename)

def main():
    pool = Pool(4) # number of cores you want to use
    file_list = [file1.xlsx, file2.xlsx, file3.xlsx, ...]
    df_list = pool.map(reader, file_list) #creates a list of the loaded df's
    df = pd.concat(df_list) # concatenates all the df's into a single df

if __name__ == '__main__':
    main()
</code></pre>

<p>Using this you should be able to substantially increase the speed of your program without too much work at all. If you don't know how many processors you have, you can check by pulling up your shell and typing</p>

<pre><code>echo %NUMBER_OF_PROCESSORS%
</code></pre>

<p>EDIT: To make this run even faster, consider changing your files to csvs and using pandas function <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer"">pandas.read_csv</a></p>
";;1;;2017-02-22T20:49:07.797;;42401977;2017-02-23T14:40:49.583;2017-02-23T14:40:49.583;;7605829.0;;7605829.0;42157944.0;2;7;;;
88010;88010;;;"<p>Turns out this is incredibly easy once you realize how the function actually works... </p>

<pre><code>print(
    df
    .head(10)
    .to_string(
        formatters={""total_bill"": ""${:,.2f}"".format, 
                    ""tip"": ""${:,.2f}"".format,
                    ""date"": lambda x: ""{:%m/%d/%Y}"".format(pd.to_datetime(x, unit=""D""))
        }
    )
)

  total_bill   tip     sex smoker  day    time  size       date
0     $16.99 $1.01  Female     No  Sun  Dinner     2 02/08/2017
1     $10.34 $1.66    Male     No  Sun  Dinner     3 02/09/2017
2     $21.01 $3.50    Male     No  Sun  Dinner     3 02/10/2017
3     $23.68 $3.31    Male     No  Sun  Dinner     2 02/11/2017
4     $24.59 $3.61  Female     No  Sun  Dinner     4 02/12/2017
5     $25.29 $4.71    Male     No  Sun  Dinner     4 02/13/2017
6      $8.77 $2.00    Male     No  Sun  Dinner     2 02/14/2017
7     $26.88 $3.12    Male     No  Sun  Dinner     4 02/15/2017
8     $15.04 $1.96    Male     No  Sun  Dinner     2 02/16/2017
9     $14.78 $3.23    Male     No  Sun  Dinner     2 02/17/2017
</code></pre>
";;4;;2017-02-23T21:44:10.100;;42426751;2017-02-24T13:55:42.610;2017-02-24T13:55:42.610;;2789863.0;;2789863.0;42347868.0;2;18;;;
88350;88350;;;"<p>An easier way:</p>

<pre><code>from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""
</code></pre>

<p>It saves you having to repeatedly type ""Display""</p>
";;3;;2017-02-27T01:00:07.317;;42476224;2017-02-27T01:00:07.317;;;;;5977551.0;34398054.0;2;12;;;
88649;88649;;;"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html"" rel=""noreferrer""><code>idxmax</code></a> for get index of first <code>True</code> value and then set by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""noreferrer""><code>loc</code></a>:</p>

<pre><code>np.random.seed(1)
df = pd.DataFrame(np.random.randint(4, size=(5,1)))
print (df)
   0
0  1
1  3
2  0
3  0
4  3

print ((df[0] == 0).idxmax())
2

df.loc[(df[0] == 0).idxmax(), 0] = 100
print (df)
     0
0    1
1    3
2  100
3    0
4    3
</code></pre>

<hr>

<pre><code>df.loc[(df[0] == 3).idxmax(), 0] = 200
print (df)
     0
0    1
1  200
2    0
3    0
4    3
</code></pre>

<p>EDIT:</p>

<p>Solution with not unique index:</p>

<pre><code>np.random.seed(1)
df = pd.DataFrame(np.random.randint(4, size=(5,1)), index=[1,2,2,3,4])
print (df)
   0
1  1
2  3
2  0
3  0
4  3

df = df.reset_index()
df.loc[(df[0] == 3).idxmax(), 0] = 200
df = df.set_index('index')
df.index.name = None
print (df)
     0
1    1
2  200
2    0
3    0
4    3
</code></pre>

<p>EDIT1:</p>

<p>Solution with <code>MultiIndex</code>:</p>

<pre><code>np.random.seed(1)
df = pd.DataFrame(np.random.randint(4, size=(5,1)), index=[1,2,2,3,4])
print (df)
   0
1  1
2  3
2  0
3  0
4  3

df.index = [np.arange(len(df.index)), df.index]
print (df)
     0
0 1  1
1 2  3
2 2  0
3 3  0
4 4  3

df.loc[(df[0] == 3).idxmax(), 0] = 200
df = df.reset_index(level=0, drop=True)

print (df)
     0
1    1
2  200
2    0
3    0
4    3
</code></pre>

<p>EDIT2:</p>

<p>Solution with double <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.cumsum.html"" rel=""noreferrer""><code>cumsum</code></a>:</p>

<pre><code>np.random.seed(1)
df = pd.DataFrame([4,0,4,7,4], index=[1,2,2,3,4])
print (df)
   0
1  4
2  0
2  4
3  7
4  4

mask = (df[0] == 0).cumsum().cumsum()
print (mask)
1    0
2    1
2    2
3    3
4    4
Name: 0, dtype: int32

df.loc[mask == 1, 0] = 200
print (df)
     0
1    4
2  200
2    4
3    7
4    4
</code></pre>
";;12;;2017-02-28T18:30:03.490;;42516230;2017-03-03T06:43:10.427;2017-03-03T06:43:10.427;;2901002.0;;2901002.0;42516070.0;2;14;;;
88883;88883;;;"<p><code>axis</code> refers to the dimension of the array, in the case of <code>pd.DataFrame</code>s <code>axis=0</code> is the dimension that points downwards and <code>axis=1</code> the one that points to the right. </p>

<p><strong>Example:</strong> Think of an <code>ndarray</code> with shape <code>(3,5,7)</code>.   </p>

<pre><code>a = np.ones((3,5,7))
</code></pre>

<p><code>a</code> is a 3 dimensional <code>ndarray</code>, i.e. it has <strong>3 axes</strong> (""axes"" is plural of ""axis""). The configuration of <code>a</code> will look like 3 slices of bread where each slice is of dimension 5-by-7. <code>a[0,:,:]</code> will refer to the 0-th slice, <code>a[1,:,:]</code> will refer to the 1-st slice etc.</p>

<p><code>a.sum(axis=0)</code> will apply <code>sum()</code> along the 0-th axis of <code>a</code>. You will add all the slices and end up with one slice of shape <code>(5,7)</code>.</p>

<p><code>a.sum(axis=0)</code> is equivalent to </p>

<pre><code>b = np.zeros((5,7))
for i in range(5):
    for j in range(7):
        b[i,j] += a[:,i,j].sum()
</code></pre>

<p><code>b</code> and <code>a.sum(axis=0)</code> will both look like this</p>

<pre><code>array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.]])
</code></pre>

<p>In a <code>pd.DataFrame</code>, axes work the same way as in <code>numpy.array</code>s: <code>axis=0</code> will apply <code>sum()</code> or any other reduction function for each column. </p>

<p><strong>N.B.</strong> In @zhangxaochen's answer, I find the phrases ""along the rows"" and ""along the columns"" slightly confusing. <code>axis=0</code> should refer to ""along each column"", and <code>axis=1</code> ""along each row"".</p>
";;0;;2017-03-02T02:08:17.457;;42545576;2017-03-02T02:08:17.457;;;;;1354975.0;22149584.0;2;6;;;
88912;88912;;;"<p>If performance is important go down to numpy level:</p>

<pre><code>import numpy as np

df = pd.DataFrame( {'a':np.random.randint(0,60,600), 'b':[1,2,5,5,4,6]*100})

def f(df):
         keys,values=df.sort_values('a').values.T
         ukeys,index=np.unique(keys,True)
         arrays=np.split(values,index[1:])
         df2=pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})
         return df2
</code></pre>

<p>Tests:</p>

<pre><code>In [301]: %timeit f(df)
1000 loops, best of 3: 1.64 ms per loop

In [302]: %timeit df.groupby('a')['b'].apply(list)
100 loops, best of 3: 5.26 ms per loop
</code></pre>
";;0;;2017-03-02T08:42:03.877;;42550516;2017-08-15T22:54:22.507;2017-08-15T22:54:22.507;;1205871.0;;4016285.0;22219004.0;2;6;;;
89657;89657;;;"<p>Passing the dataframes to concat in a dictionary, results in a multi-index dataframe from which you can easily delete the duplicates, which results in a multi-index dataframe with the differences between the dataframes:</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3:
    from StringIO import StringIO
else:
    from io import StringIO
import pandas as pd

DF1 = StringIO(""""""Date       Fruit  Num  Color 
2013-11-24 Banana 22.1 Yellow
2013-11-24 Orange  8.6 Orange
2013-11-24 Apple   7.6 Green
2013-11-24 Celery 10.2 Green
"""""")
DF2 = StringIO(""""""Date       Fruit  Num  Color 
2013-11-24 Banana 22.1 Yellow
2013-11-24 Orange  8.6 Orange
2013-11-24 Apple   7.6 Green
2013-11-24 Celery 10.2 Green
2013-11-25 Apple  22.1 Red
2013-11-25 Orange  8.6 Orange"""""")


df1 = pd.read_table(DF1, sep='\s+')
df2 = pd.read_table(DF2, sep='\s+')
#%%
dfs_dictionary = {'DF1':df1,'DF2':df2}
df=pd.concat(dfs_dictionary)
df.drop_duplicates(keep=False)
</code></pre>

<p>Result:</p>

<pre><code>           Date   Fruit   Num   Color
2 4  2013-11-25   Apple  22.1     Red
  5  2013-11-25  Orange   8.6  Orange
</code></pre>
";;2;;2017-03-07T15:26:37.277;;42652112;2017-08-16T15:46:41.133;2017-08-16T15:46:41.133;;5853731.0;;3215345.0;20225110.0;2;7;;;
90757;90757;;;"<p>As you say, this is pretty easy is SQL, so why not do it in SQL?</p>

<pre><code>import pandas as pd
import sqlite3

#We'll use firelynx's tables:
presidents = pd.DataFrame({""name"": [""Bush"", ""Obama"", ""Trump""],
                           ""president_id"":[43, 44, 45]})
terms = pd.DataFrame({'start_date': pd.date_range('2001-01-20', periods=5, freq='48M'),
                      'end_date': pd.date_range('2005-01-21', periods=5, freq='48M'),
                      'president_id': [43, 43, 44, 44, 45]})
war_declarations = pd.DataFrame({""date"": [datetime(2001, 9, 14), datetime(2003, 3, 3)],
                                 ""name"": [""War in Afghanistan"", ""Iraq War""]})
#Make the db in memory
conn = sqlite3.connect(':memory:')
#write the tables
terms.to_sql('terms', conn, index=False)
presidents.to_sql('presidents', conn, index=False)
war_declarations.to_sql('wars', conn, index=False)

qry = '''
    select  
        start_date PresTermStart,
        end_date PresTermEnd,
        wars.date WarStart,
        presidents.name Pres
    from
        terms join wars on
        date between start_date and end_date join presidents on
        terms.president_id = presidents.president_id
    '''
df = pd.read_sql_query(qry, conn)
</code></pre>

<p>df:</p>

<pre><code>         PresTermStart          PresTermEnd             WarStart  Pres
0  2001-01-31 00:00:00  2005-01-31 00:00:00  2001-09-14 00:00:00  Bush
1  2001-01-31 00:00:00  2005-01-31 00:00:00  2003-03-03 00:00:00  Bush
</code></pre>
";;2;;2017-03-14T20:49:05.440;;42796283;2017-03-14T20:49:05.440;;;;;6883295.0;30627968.0;2;6;;;
91094;91094;;;"<p>My approach was, but I can't guarantee that this is the fastest solution.</p>

<pre><code>df = pd.Dataframe(columns=[""firstname"", ""lastname""])
df = df.append({
     ""firstname"": ""John"",
     ""lastname"":  ""Johny""
      }, ignore_index=True)
</code></pre>
";;1;;2017-03-16T15:00:51.587;;42837693;2017-03-16T15:00:51.587;;;;;7120057.0;17091769.0;2;10;;;
91378;91378;;;"<p>You can do it with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html"" rel=""nofollow noreferrer""><code>numpy.eye</code></a> and a using the array element selection mechanism:</p>

<pre><code>import numpy as np
nb_classes = 6
data = [[2, 3, 4, 0]]

def indices_to_one_hot(data, nb_classes):
    """"""Convert an iterable of indices to one-hot encoded labels.""""""
    targets = np.array(data).reshape(-1)
    return np.eye(nb_classes)[targets]
</code></pre>

<p>The the return value of <code>indices_to_one_hot(nb_classes, data)</code> is now</p>

<pre><code>array([[[ 0.,  0.,  1.,  0.,  0.,  0.],
        [ 0.,  0.,  0.,  1.,  0.,  0.],
        [ 0.,  0.,  0.,  0.,  1.,  0.],
        [ 1.,  0.,  0.,  0.,  0.,  0.]]])
</code></pre>

<p>The <code>.reshape(-1)</code> is there to make sure you have the right labels format (you might also have <code>[[2], [3], [4], [0]]</code>).</p>
";;2;;2017-03-18T13:19:17.963;;42874900;2017-08-12T05:46:05.100;2017-08-12T05:46:05.100;;562769.0;;562769.0;37292872.0;2;9;;;
91406;91406;;;"<p>You can use numpy.eye function.</p>

<pre><code>import numpy as np

def one_hot_encode(x, n_classes):
    """"""
    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.
    : x: List of sample Labels
    : return: Numpy array of one-hot encoded labels
     """"""
    return np.eye(n_classes)[x]

def main():
    list = [0,1,2,3,4,3,2,1,0]
    n_classes = 5
    one_hot_list = one_hot_encode(list, n_classes)
    print(one_hot_list)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>Result</p>

<pre><code>D:\Desktop&gt;python test.py
[[ 1.  0.  0.  0.  0.]
 [ 0.  1.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  1.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  1.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  1.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]]
</code></pre>
";;0;;2017-03-18T21:00:54.583;;42879831;2017-03-18T21:16:05.157;2017-03-18T21:16:05.157;;1275087.0;;1275087.0;37292872.0;2;7;;;
91865;91865;;;"<p>However, one approach to dividing the dataset into <code>train</code>, <code>test</code>, <code>cv</code> with <code>0.6</code>, <code>0.2</code>, <code>0.2</code> would be to use the <code>train_test_split</code> method twice.</p>

<pre><code>x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)
x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)
</code></pre>
";;4;;2017-03-21T16:10:59.477;;42932524;2017-05-10T11:38:21.807;2017-05-10T11:38:21.807;;5747414.0;;5747414.0;38250710.0;2;8;;;
92149;92149;;;"<p>Here's a succinct and generic solution to use a seaborn color palette.</p>

<p>First <a href=""http://seaborn.pydata.org/tutorial/color_palettes.html"" rel=""noreferrer"">find a color palette</a> you like and optionally visualize it:</p>

<pre><code>sns.palplot(sns.color_palette(""Set2"", 8))
</code></pre>

<p>Then you can use it with <code>matplotlib</code> doing this:</p>

<pre><code># Unique category labels: 'D', 'F', 'G', ...
color_labels = df['color'].unique()

# List of RGB triplets
rgb_values = sns.color_palette(""Set2"", 8)

# Map label to RGB
color_map = dict(zip(color_labels, rgb_values))

# Finally use the mapped values
plt.scatter(df['carat'], df['price'], c=df['color'].map(color_map))
</code></pre>
";;2;;2017-03-23T01:59:55.247;;42965916;2017-03-23T01:59:55.247;;;;;542156.0;26139423.0;2;7;;;
92227;92227;;;"<p>You can observe the relation between features either by drawing a heat map from seaborn or scatter matrix from pandas. </p>

<p>Scatter Matrix:</p>

<pre><code>pd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
</code></pre>

<p>If you want to visualize each feature's skewness as well - use seaborn pairplots. </p>

<pre><code>sns.pairplot(dataframe)
</code></pre>

<p>Sns Heatmap:</p>

<pre><code>import seaborn as sns

f, ax = pl.subplots(figsize=(10, 8))
corr = dataframe.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
</code></pre>

<p>The output will be a correlation map of the features. i.e. see the below example. </p>

<p><a href=""https://i.stack.imgur.com/DSvaM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DSvaM.png"" alt=""enter image description here""></a></p>

<p>The correlation between grocery and detergents is high. Similarly:</p>

Pdoducts With High Correlation:

<ol>
<li>Grocery and Detergents. </li>
</ol>

Products With Medium Correlation:

<ol>
<li>Milk and Grocery</li>
<li>Milk and Detergents_Paper</li>
</ol>

Products With Low Correlation:

<ol>
<li>Milk and Deli</li>
<li>Frozen and Fresh.</li>
<li>Frozen and Deli. </li>
</ol>

<p>From Pairplots: You can observe same set of relations from pairplots or scatter matrix. But from these we can say that whether the data is normally distributed or not. </p>

<p><a href=""https://i.stack.imgur.com/W7toh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/W7toh.png"" alt=""enter image description here""></a></p>

<p>Note: The above is same graph taken from the data, which is used to draw heatmap.</p>
";;1;;2017-03-23T13:48:20.603;;42977946;2017-03-23T13:54:05.700;2017-03-23T13:54:05.700;;6755116.0;;6755116.0;29432629.0;2;10;;;
92230;92230;;;"<p>There are two ways to handle the situation where we do not want the index to be stored in csv file.</p>

<ol>
<li><p>As others have stated you can use <strong><em>index=False</em></strong> while saving your<br>
dataframe to csv file.</p>

<blockquote>
  <p><code>df.to_csv('file_name.csv',index=False)</code></p>
</blockquote></li>
<li>You can also save your dataframe into file as it is with an index and then while reading this file, you can drop the column <strong>unnamed 0</strong> containing your previous index.

<blockquote>
  <p><code>df.to_csv(' file_name.csv ')</code><br>
  <code>df_new = pd.read_csv('file_name.csv').drop(['unnamed 0'],axis=1)</code></p>
</blockquote></li>
</ol>
";;1;;2017-03-23T13:55:50.307;;42978156;2017-08-19T11:06:09.190;2017-08-19T11:06:09.190;;5747414.0;;5747414.0;20845213.0;2;7;;;
93153;93153;;;"<p>For each of your dataframe column, you could get quantile with:</p>

<pre><code>q = df[""col""].quantile(0.99)
</code></pre>

<p>and then filter with:</p>

<pre><code>df[df[""col""] &lt; q]
</code></pre>
";;0;;2017-03-29T12:22:09.333;;43093390;2017-03-29T12:22:09.333;;;;;6903745.0;23199796.0;2;14;;;
93922;93922;;;"<h1>Super simple column assignment</h1>

<p>A pandas dataframe is implemented as an ordered dict of columns.</p>

<p>This means that the <code>__getitem__</code> <code>[]</code> can not only be used to get a certain column, but <code>__setitem__</code> <code>[] =</code> can be used to assign a new column.</p>

<p>For example, this dataframe can have a column added to it by simply using the <code>[]</code> accessor</p>

<pre><code>    size      name color
0    big      rose   red
1  small    violet  blue
2  small     tulip   red
3  small  harebell  blue

df['protected'] = ['no', 'no', 'no', 'yes']

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<p>Note that this works even if the index of the dataframe is off.</p>

<pre><code>df.index = [3,2,1,0]
df['protected'] = ['no', 'no', 'no', 'yes']
    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h3>[]= is the way to go, but watch out!</h3>

<p>However, if you have a <code>pd.Series</code> and try to assign it to a dataframe where the indexes are off, you will run in to trouble. See example:</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>This is because a <code>pd.Series</code> by default has an index enumerated from 0 to n. And the pandas <code>[] =</code> method <strong>tries</strong> <em>to be ""smart""</em></p>

<h2>What actually is going on.</h2>

<p>When you use the <code>[] =</code> method pandas is quietly performing an outer join or outer merge using the index of the left hand dataframe and the index of the right hand series. <code>df['column'] = series</code></p>

<h3>Side note</h3>

<p>This quickly causes cognitive dissonance, since the <code>[]=</code> method is trying to do a lot of different things depending on the input, and the outcome cannot be predicted unless you <em>just know</em> how pandas works. I would therefore advice against the <code>[]=</code> in code bases, but when exploring data in a notebook, it is fine.</p>

<h2>Going around the problem</h2>

<p>If you have a <code>pd.Series</code> and want it assigned from top to bottom, or if you are coding productive code and you are not sure of the index order, it is worth it to safeguard for this kind of issue.</p>

<p>You could downcast the <code>pd.Series</code> to a <code>np.ndarray</code> or a <code>list</code>, this will do the trick.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values
</code></pre>

<p>or</p>

<pre><code>df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))
</code></pre>

<p><strong>But this is not very explicit.</strong></p>

<p>Some coder may come along and say ""Hey, this looks redundant, I'll just optimize this away"".</p>

<h3>Explicit way</h3>

<p>Setting the index of the <code>pd.Series</code> to be the index of the <code>df</code> is explicit.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)
</code></pre>

<p>Or more realistically, you probably have a <code>pd.Series</code> already available.</p>

<pre><code>protected_series = pd.Series(['no', 'no', 'no', 'yes'])
protected_series.index = df.index

3     no
2     no
1     no
0    yes
</code></pre>

<p>Can now be assigned</p>

<pre><code>df['protected'] = protected_series

    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h2>Alternative way with <code>df.reset_index()</code></h2>

<p>Since the index dissonance is the problem, if you feel that the index of the dataframe <em>should</em> not dictate things, you can simply drop the index, this should be faster, but it is not very clean, since your function now <em>probably</em> does two things.</p>

<pre><code>df.reset_index(drop=True)
protected_series.reset_index(drop=True)
df['protected'] = protected_series

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<h2>Note on <code>df.assign</code></h2>

<p>While <code>df.assign</code> make it more explicit what you are doing, it actually has all the same problems as the above <code>[]=</code></p>

<pre><code>df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>Just watch out with <code>df.assign</code> that your column is not called <code>self</code>. It will cause errors. This makes <code>df.assign</code> <strong>smelly</strong>, since there are these kind of artifacts in the function.</p>

<pre><code>df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])
TypeError: assign() got multiple values for keyword argument 'self'
</code></pre>

<p>You may say, ""Well, I'll just not use <code>self</code> then"". But who knows how this function changes in the future to support new arguments. Maybe your column name will be an argument in a new update of pandas, causing problems with upgrading.</p>
";;1;;2017-04-03T08:59:22.753;;43180437;2017-04-03T08:59:22.753;;;;;3730397.0;12555323.0;2;6;;;
93998;93998;;;"<p>Here is an implementation that I find very versatile: </p>

<pre><code>In [1]: import pandas as pd 

In [2]: df = pd.DataFrame([[0, 'the', 'quick', 'brown'],
   ...:                    [1, 'fox', 'jumps', 'over'], 
   ...:                    [2, 'the', 'lazy', 'dog']],
   ...:                   columns=['c0', 'c1', 'c2', 'c3'])

In [3]: def str_join(df, sep, *cols):
   ...:     from functools import reduce
   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), 
   ...:                   [df[col] for col in cols])
   ...: 

In [4]: df['cat'] = str_join(df, '-', 'c0', 'c1', 'c2', 'c3')

In [5]: df
Out[5]: 
   c0   c1     c2     c3                cat
0   0  the  quick  brown  0-the-quick-brown
1   1  fox  jumps   over   1-fox-jumps-over
2   2  the   lazy    dog     2-the-lazy-dog
</code></pre>
";;1;;2017-04-03T17:05:10.717;;43190411;2017-04-03T17:05:10.717;;;;;1476240.0;19377969.0;2;6;;;
94784;94784;;;"<p>you can set the types explicitly with pandas <code>DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs)</code> and pass in a dictionary with the dtypes you want to <code>dtype</code></p>

<p>here's an example:</p>

<pre><code>import pandas as pd
wheel_number = 5
car_name = 'jeep'
minutes_spent = 4.5

# set the columns
data_columns = ['wheel_number', 'car_name', 'minutes_spent']

# create an empty dataframe
data_df = pd.DataFrame(columns = data_columns)
df_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns)
data_df = data_df.append(df_temp, ignore_index=True) 

In [11]: data_df.dtypes
Out[11]:
wheel_number     float64
car_name          object
minutes_spent    float64
dtype: object

data_df = data_df.astype(dtype= {""wheel_number"":""int64"",
        ""car_name"":""object"",""minutes_spent"":""float64""})
</code></pre>

<p>now you can see that it's changed</p>

<pre><code>In [18]: data_df.dtypes
Out[18]:
wheel_number       int64
car_name          object
minutes_spent    float64
</code></pre>
";;0;;2017-04-08T01:26:14.400;;43289220;2017-04-08T01:26:14.400;;;;;6312126.0;21197774.0;2;8;;;
95978;95978;;;"<p>If the DataFrame is huge, and the number of rows to drop is large as well, then simple drop by index <code>df.drop(df.index[])</code> takes too much time. </p>

<p>In my case, I have a multi-indexed DataFrame of floats with <code>100M rows x 3 cols</code>, and I need to remove <code>10k</code> rows from it. The fastest method I found is, quite counterintuitively, to <code>take</code> the remaining rows.</p>

<p>Let <code>indexes_to_drop</code> be an array of positional indexes to drop (<code>[1, 2, 4]</code> in the question).</p>

<pre><code>indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)
df_sliced = df.take(list(indexes_to_keep))
</code></pre>

<p>In my case this took <code>20.5s</code>, while the simple <code>df.drop</code> took <code>5min 27s</code> and consumed a lot of memory. The resulting DataFrame is the same.</p>
";;0;;2017-04-15T01:57:42.187;;43421391;2017-04-15T01:57:42.187;;;;;304209.0;14661701.0;2;12;;;
97040;97040;;;"<p>Here a possible solution (I had quite a lot of fun!):</p>

<pre><code>df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'],
                 name='Genus')).round(2)

ax = df.plot(kind='bar',figsize=(10,4), rot = 0)

# ""Activate"" minor ticks
ax.minorticks_on()

# Get location of the center of each rectangle
rects_locs = map(lambda x: x.get_x() +x.get_width()/2., ax.patches)
# Set minor ticks there
ax.set_xticks(rects_locs, minor = True)


# Labels for the rectangles
new_ticks = reduce(lambda x, y: x + y, map(lambda x: [x] * df.shape[0], df.columns.tolist()))
# Set the labels
from matplotlib import ticker
ax.xaxis.set_minor_formatter(ticker.FixedFormatter(new_ticks))  #add the custom ticks

# Move the category label further from x-axis
ax.tick_params(axis='x', which='major', pad=15)

# Remove minor ticks where not necessary
ax.tick_params(axis='x',which='both', top='off')
ax.tick_params(axis='y',which='both', left='off', right = 'off')
</code></pre>

<p>Here's what I get:</p>

<p><a href=""https://i.stack.imgur.com/yCK4h.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yCK4h.png"" alt=""enter image description here""></a></p>
";;0;;2017-04-21T15:54:14.843;;43547088;2017-04-21T15:59:16.063;2017-04-21T15:59:16.063;;6108661.0;;6108661.0;43545879.0;2;7;;;
97044;97044;;;"<p>Here is a solution. You can get the positions of the bars and set some minor xticklabels accordingly. </p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'], 
                 name='Genus')).round(2)


df.plot(kind='bar',figsize=(10,4))

ax = plt.gca()
pos = []
for bar in ax.patches:
    pos.append(bar.get_x()+bar.get_width()/2.)


ax.set_xticks(pos,minor=True)
lab = []
for i in range(len(pos)):
    l = df.columns.values[i//len(df.index.values)]
    lab.append(l)

ax.set_xticklabels(lab,minor=True)
ax.tick_params(axis='x', which='major', pad=15, size=0)
plt.setp(ax.get_xticklabels(), rotation=0)

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Tl16c.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Tl16c.png"" alt=""enter image description here""></a></p>
";;0;;2017-04-21T16:05:07.243;;43547282;2017-04-21T16:05:07.243;;;;;4124317.0;43545879.0;2;6;;;
97118;97118;;;"<pre><code>from sklearn.datasets import load_iris
import pandas as pd

data = load_iris()
df = pd.DataFrame(data.data, columns=data.feature_names)
df.head()
</code></pre>

<p>This tutorial maybe of interest: <a href=""http://www.neural.cz/dataset-exploration-boston-house-pricing.html"" rel=""noreferrer"">http://www.neural.cz/dataset-exploration-boston-house-pricing.html</a></p>
";;1;;2017-04-21T22:40:42.207;;43553043;2017-04-21T22:40:42.207;;;;;3462174.0;38105539.0;2;8;;;
97168;97168;;;"<p>here is my 2 cent on this with a very simple example why the warning is important.</p>

<p>so assuming that I am creating a df such has</p>

<pre><code>x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])
print(x)
   a  b
0  0  0
1  1  1
2  2  2
3  3  3
</code></pre>

<p>now I want to create a new dataframe based on a subset of the original and modify it such has:</p>

<pre><code> q = x.loc[:, 'a']
</code></pre>

<p>now <strong>this is a slice of the original</strong> and whatever I do on it will affect x:</p>

<pre><code>q += 2
print(x)  # checking x again, wow! it changed!
   a  b
0  2  0
1  3  1
2  4  2
3  5  3
</code></pre>

<p><em>this is what the warning is telling you. you are working on a slice, so everything you do on it will be reflected on the original DataFrame</em></p>

<p>now <strong>using <code>.copy()</code>, it won't be a slice of the original</strong>, so doing an operation on q wont affect x :</p>

<pre><code>x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])
print(x)
   a  b
0  0  0
1  1  1
2  2  2
3  3  3

q = x.loc[:, 'a'].copy()
q += 2
print(x)  # oh, x did not change because q is a copy now
   a  b
0  0  0
1  1  1
2  2  2
3  3  3
</code></pre>

<p>and btw, a copy just mean that <code>q</code> will be a new object in memory. where a slice share the same original object in memory</p>

<p>imo, using <code>.copy()</code>is very safe. as an example <code>df.loc[:, 'a']</code> return a slice but <code>df.loc[df.index, 'a']</code> return a copy. Jeff told me that this was an unexpected behavior and <code>:</code> or <code>df.index</code> should have the same behavior as an indexer in .loc[], but using <code>.copy()</code> on both will return a copy, better be safe. so use <code>.copy()</code> if you don't want to affect the original dataframe.</p>

<p>now using <strong><code>.copy()</code> return a deepcopy of the DataFrame</strong>, which is a very safe approach not to get the phone call you are talking about.</p>

<p>but using <code>df.is_copy = None</code>, is just a trick that does not copy anything which is a very bad idea, <strong>you will still be working on a slice of the original DataFrame</strong></p>

<p><em>one more thing that people tend not to know:</em></p>

<p><code>df[columns]</code> <strong>may return</strong> a view.</p>

<p><code>df.loc[indexer, columns]</code> also <strong>may return</strong> a view, <strong>but almost always does not in practice.</strong>
emphasis on the <strong>may</strong> here</p>
";;1;;2017-04-22T12:57:05.233;;43559496;2017-04-22T14:29:19.877;2017-04-22T14:29:19.877;;5626112.0;;5626112.0;43423347.0;2;10;;;
97327;97327;;;"<p>You can calculate the cumulative max of <code>a</code> and then drop duplicates with <code>np.unique</code> with which you can also record the unique index so as to subset <code>b</code> correspondingly:</p>

<pre><code>a = np.array([2,1,2,3,4,5,4,6,5,7,8,9,8,10,11])
b = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])

a_cummax = np.maximum.accumulate(a)    
a_new, idx = np.unique(a_cummax, return_index=True)

a_new
# array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11])

b[idx]
# array([ 1,  4,  5,  6,  8, 10, 11, 12, 14, 15])
</code></pre>
";;1;;2017-04-23T23:53:24.433;;43577783;2017-04-23T23:53:24.433;;;;;4983450.0;43577744.0;2;14;;;
97329;97329;;;"<p>Here is a vanilla Python solution that does one pass:</p>

<pre><code>&gt;&gt;&gt; a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]
&gt;&gt;&gt; b = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
&gt;&gt;&gt; a_new, b_new = [], []
&gt;&gt;&gt; last = float('-inf')
&gt;&gt;&gt; for x, y in zip(a, b):
...     if x &gt; last:
...         last = x
...         a_new.append(x)
...         b_new.append(y)
...
&gt;&gt;&gt; a_new
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
&gt;&gt;&gt; b_new
[1, 4, 5, 6, 8, 10, 11, 12, 14, 15]
</code></pre>

<p>I'm curious to see how it compares to the <code>numpy</code> solution, which will have similar time complexity but does a couple of passes on the data.  </p>

<p>Here are some timings. First, setup:</p>

<pre><code>&gt;&gt;&gt; small = ([2,1,2,3,4,5,4,6,5,7,8,9,8,10,11], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])
&gt;&gt;&gt; medium = (np.random.randint(1, 10000, (10000,)), np.random.randint(1, 10000, (10000,)))
&gt;&gt;&gt; large = (np.random.randint(1, 10000000, (10000000,)), np.random.randint(1, 10000000, (10000000,)))
</code></pre>

<p>And now the two approaches:</p>

<pre><code>&gt;&gt;&gt; def monotonic(a, b):
...     a_new, b_new = [], []
...     last = float('-inf')
...     for x,y in zip(a,b):
...         if x &gt; last:
...             last = x
...             a_new.append(x)
...             b_new.append(y)
...     return a_new, b_new
...
&gt;&gt;&gt; def np_monotonic(a, b):
...     a_new, idx = np.unique(np.maximum.accumulate(a), return_index=True)
...     return a_new, b[idx]
...
</code></pre>

<p>Note, the approaches are not strictly equivalent, one stays in vanilla Python land, the other stays in <code>numpy</code> array land. We will compare performance assuming you are starting with the corresponding data structure (either <code>numpy.array</code> or <code>list</code>):</p>

<p>So first, a small list, the same from the OP's example, we see that <code>numpy</code> is not actually faster, which isn't surprising for small data structures:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit(""monotonic(a,b)"", ""from __main__ import monotonic, small; a, b = small"", number=10000)
0.039130652003223076
&gt;&gt;&gt; timeit.timeit(""np_monotonic(a,b)"", ""from __main__ import np_monotonic, small, np; a, b = np.array(small[0]), np.array(small[1])"", number=10000)
0.10779813499539159
</code></pre>

<p>Now a ""medium"" list/array of 10,000 elements, we start to see <code>numpy</code> advantages:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit(""monotonic(a,b)"", ""from __main__ import monotonic, medium; a, b = medium[0].tolist(), medium[1].tolist()"", number=10000)
4.642718859016895
&gt;&gt;&gt; timeit.timeit(""np_monotonic(a,b)"", ""from __main__ import np_monotonic, medium; a, b = medium"", number=10000)
1.3776302759943064
</code></pre>

<p>Now, interestingly, the advantage seems to narrow with ""large"" arrays, on the order of 1e7 elements:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit(""monotonic(a,b)"", ""from __main__ import monotonic, large; a, b = large[0].tolist(), large[1].tolist()"", number=10)
4.400254560023313
&gt;&gt;&gt; timeit.timeit(""np_monotonic(a,b)"", ""from __main__ import np_monotonic, large; a, b = large"", number=10)
3.593393853981979
</code></pre>

<p>Note, in the last pair of timings, I only did them 10 times each, but if someone has a better machine or more patience, please feel free to increase <code>number</code></p>
";;2;;2017-04-24T00:27:03.237;;43578010;2017-04-24T04:41:28.773;2017-04-24T04:41:28.773;;5014455.0;;5014455.0;43577744.0;2;10;;;
97331;97331;;;"<p><code>unique</code> with <code>return_index</code> uses <code>argsort</code>.  With <code>maximum.accumulate</code> that isn't needed.  So we can cannibalize <code>unique</code> and do:</p>

<pre><code>In [313]: a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]
In [314]: arr = np.array(a)
In [315]: aux = np.maximum.accumulate(arr)
In [316]: flag = np.concatenate(([True], aux[1:] != aux[:-1])) # key unique step
In [317]: idx = np.nonzero(flag)
In [318]: idx
Out[318]: (array([ 0,  3,  4,  5,  7,  9, 10, 11, 13, 14], dtype=int32),)
In [319]: arr[idx]
Out[319]: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
In [320]: np.array(b)[idx]
Out[320]: array([ 1,  4,  5,  6,  8, 10, 11, 12, 14, 15])

In [323]: np.unique(aux, return_index=True)[1]
Out[323]: array([ 0,  3,  4,  5,  7,  9, 10, 11, 13, 14], dtype=int32)
</code></pre>

<hr>

<pre><code>def foo(arr):
    aux=np.maximum.accumulate(arr)
    flag = np.concatenate(([True], aux[1:] != aux[:-1]))
    return np.nonzero(flag)[0]

In [330]: timeit foo(arr)
....
100000 loops, best of 3: 12.5 s per loop
In [331]: timeit np.unique(np.maximum.accumulate(arr), return_index=True)[1]
....
10000 loops, best of 3: 21.5 s per loop
</code></pre>

<p>With (10000,) shape <code>medium</code> this sort-less unique has a substantial speed advantage:</p>

<pre><code>In [334]: timeit np.unique(np.maximum.accumulate(medium[0]), return_index=True)[1]
1000 loops, best of 3: 351 s per loop
In [335]: timeit foo(medium[0])
The slowest run took 4.14 times longer ....
10000 loops, best of 3: 48.9 s per loop
</code></pre>

<p>[1]: Use <code>np.source(np.unique)</code> to see code, or ?? in IPython</p>
";;4;;2017-04-24T01:22:08.090;;43578378;2017-04-24T01:35:30.523;2017-04-24T01:35:30.523;;901925.0;;901925.0;43577744.0;2;10;;;
97332;97332;;;"<p>Running a version of @juanpa.arrivillaga's function with <code>numba</code></p>

<pre><code>import numba

def psi(A):
    a_cummax = np.maximum.accumulate(A)
    a_new, idx = np.unique(a_cummax, return_index=True)
    return idx

def foo(arr):
    aux=np.maximum.accumulate(arr)
    flag = np.concatenate(([True], aux[1:] != aux[:-1]))
    return np.nonzero(flag)[0]

@numba.jit
def f(A):
    m = A[0]
    a_new, idx = [m], [0]
    for i, a in enumerate(A[1:], 1):
        if a &gt; m:
            m = a
            a_new.append(a)
            idx.append(i)
    return idx
</code></pre>

<hr>

<p><strong><em>timing</em></strong>  </p>

<pre><code>%timeit f(a)
The slowest run took 5.37 times longer than the fastest. This could mean that an intermediate result is being cached.
1000000 loops, best of 3: 1.83 s per loop

%timeit foo(a)
The slowest run took 9.41 times longer than the fastest. This could mean that an intermediate result is being cached.
100000 loops, best of 3: 6.35 s per loop

%timeit psi(a)
The slowest run took 9.66 times longer than the fastest. This could mean that an intermediate result is being cached.
100000 loops, best of 3: 9.95 s per loop
</code></pre>
";;7;;2017-04-24T01:51:27.517;;43578600;2017-04-24T07:30:30.023;2017-04-24T07:30:30.023;;2336654.0;;2336654.0;43577744.0;2;11;;;
98055;98055;;;"<p>While the other answers provide good information about why one shouldn't simply ignore the warning, I think your original question has not been answered, yet.</p>

<p>@thn points out that using <code>copy()</code> completely depends on the scenario at hand. When you want that the original data is preserved, you use <code>.copy()</code>, otherwise you don't. If you are using <code>copy()</code> to circumvent the <code>SettingWithCopyWarning</code> you are ignoring the fact that you may introduce a logical bug into your software. As long as you are absolutely certain that this is what you want to do, you are fine.</p>

<p>However, when using <code>.copy()</code> blindly you may run into another issue, which is no longer really pandas specific, but occurs every time you are copying data.</p>

<p>I slightly modified your example code to make the problem more apparent:</p>

<pre><code>@profile
def foo():
    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))

    d1 = df[:]
    d1 = d1.copy()

if __name__ == '__main__':
    foo()
</code></pre>

<p>When using <a href=""https://pypi.python.org/pypi/memory_profiler"" rel=""nofollow noreferrer"">memory_profile</a> one can clearly see that <code>.copy()</code> doubles our memory consumption:</p>

<pre><code>&gt; python -m memory_profiler demo.py 
Filename: demo.py

Line #    Mem usage    Increment   Line Contents
================================================
     4   61.195 MiB    0.000 MiB   @profile
     5                             def foo():
     6  213.828 MiB  152.633 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))
     7                             
     8  213.863 MiB    0.035 MiB    d1 = df[:]
     9  366.457 MiB  152.594 MiB    d1 = d1.copy()
</code></pre>

<p>This relates to the fact, that there is still a reference (<code>df</code>) which points to the original data frame. Thus, <code>df</code> is not cleaned up by the garbage collector and is kept in memory. </p>

<p>When you are using this code in a production system, you may or may not get a <code>MemoryError</code> depending on the size of the data you are dealing with and your available memory.</p>

<p>To conclude, it is not a wise idea to use <code>.copy()</code> <em>blindly</em>. Not just because you may introduce a logical bug in your software, but also because it may expose runtime dangers such as a <code>MemoryError</code>. </p>

<hr>

<p><strong>Edit:</strong>
Even if you are doing <code>df = df.copy()</code>, and you can ensure that there are no other references to the original <code>df</code>, still <code>copy()</code> is evaluated before the assignment. Meaning that for a short time both data frames will be in memory. </p>

<p><em>Example (notice that you cannot see this behavior in the memory summary)</em>:</p>

<pre><code>&gt; mprof run -T 0.001 demo.py
Line #    Mem usage    Increment   Line Contents
================================================
     7     62.9 MiB      0.0 MiB   @profile
     8                             def foo():
     9    215.5 MiB    152.6 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))
    10    215.5 MiB      0.0 MiB    df = df.copy()
</code></pre>

<p><em>But if you visualise memory consumption over time, at 1.6s both data frames are in memory:</em></p>

<p><a href=""https://i.stack.imgur.com/qUl5a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qUl5a.png"" alt=""enter image description here""></a></p>
";;2;;2017-04-27T19:01:23.247;;43665978;2017-04-28T22:27:30.803;2017-04-28T22:27:30.803;;3423035.0;;3423035.0;43423347.0;2;6;;;
99429;99429;;;"<p>From version <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#groupby-enhancements"" rel=""noreferrer"">0.20.1</a> it is simplier:</p>

<blockquote>
  <p>Strings passed to <strong>DataFrame.groupby()</strong> as the <strong>by</strong> parameter may now reference either column names or index level names</p>
</blockquote>

<pre><code>arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]

index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])

df = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 3, 3],
                   'B': np.arange(8)}, index=index)

print (df)

              A  B
first second      
bar   one     1  0
      two     1  1
baz   one     1  2
      two     1  3
foo   one     2  4
      two     2  5
qux   one     3  6
      two     3  7

print (df.groupby(['second', 'A']).sum())
          B
second A   
one    1  2
       2  4
       3  6
two    1  4
       2  5
       3  7
</code></pre>
";;0;;2017-05-06T14:19:34.300;;43821531;2017-05-06T14:19:34.300;;;;;2901002.0;30925079.0;2;10;;;
100076;100076;;;"<p><strong>Best way found</strong>!!</p>

<p>the <code>date_column</code> has to be in date time format.</p>

<pre><code>import datetime as dt
df['month_year'] = df.date_column.dt.to_period('M')
</code></pre>
";;2;;2017-05-10T14:56:30.863;;43896119;2017-07-12T04:47:48.673;2017-07-12T04:47:48.673;;5103802.0;;3364687.0;25146121.0;2;11;;;
100082;100082;;;"<p>The currently accepted answer by unutbu describes are great way of doing this in pandas versions &lt;= 0.20. However, as of pandas 0.20, using this method raises a warning indicating that the syntax will not be available in future versions of pandas.</p>

<p>Series:</p>

<blockquote>
  <p>FutureWarning: using a dict on a Series for aggregation is deprecated and will be removed in a future version</p>
</blockquote>

<p>DataFrames:</p>

<blockquote>
  <p>FutureWarning: using a dict with renaming is deprecated and will be removed in a future version</p>
</blockquote>

<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""noreferrer"">pandas 0.20 changelog</a>, the recommended way of renaming columns while aggregating is as follows.</p>

<pre><code># Create a sample data frame
df = pd.DataFrame({'A': [1, 1, 1, 2, 2],
                   'B': range(5),
                   'C': range(5)})

# ==== SINGLE COLUMN (SERIES) ====
# Syntax soon to be deprecated
df.groupby('A').B.agg({'foo': 'count'})
# Recommended replacement syntax
df.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})

# ==== MULTI COLUMN ====
# Syntax soon to be deprecated
df.groupby('A').agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})
# Recommended replacement syntax
df.groupby('A').agg({'B': 'sum', 'C': 'min'}).rename(columns={'B': 'foo', 'C': 'bar'})
# As the recommended syntax is more verbose, parentheses can
# be used to introduce line breaks and increase readability
(df.groupby('A')
    .agg({'B': 'sum', 'C': 'min'})
    .rename(columns={'B': 'foo', 'C': 'bar'})
)
</code></pre>

<p>Please see the <a href=""http://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""noreferrer"">0.20 changelog</a> for additional details.</p>

<hr>

<h3>Update 2017-01-03 in response to @JunkMechanic's comment.</h3>

<p>With the old style dictionary syntax, it was possible to pass multiple <code>lambda</code> functions to <code>.agg</code>, since these would be renamed with the key in the passed dictionary:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': {'min': lambda x: x.min(), 'max': lambda x: x.max()}})

    B    
  max min
A        
1   2   0
2   4   3
</code></pre>

<p>Multiple functions can also be passed to a single column as a list:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': [np.min, np.max]})

     B     
  amin amax
A          
1    0    2
2    3    4
</code></pre>

<p>However, this does not work with lambda functions, since they are anonymous and all return <code>&lt;lambda&gt;</code>, which causes a name collision:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': [lambda x: x.min(), lambda x: x.max]})
SpecificationError: Function names must be unique, found multiple named &lt;lambda&gt;
</code></pre>

<p>To avoid the <code>SpecificationError</code>, named functions can be defined a priori instead of using <code>lambda</code>. Suitable function names also avoid calling <code>.rename</code> on the data frame afterwards. These functions can be passed with the same list syntax as above:</p>

<pre><code>&gt;&gt;&gt; def my_min(x):
&gt;&gt;&gt;     return x.min()

&gt;&gt;&gt; def my_max(x):
&gt;&gt;&gt;     return x.max()

&gt;&gt;&gt; df.groupby('A').agg({'B': [my_min, my_max]})

       B       
  my_min my_max
A              
1      0      2
2      3      4
</code></pre>
";;4;;2017-05-10T15:39:39.197;;43897124;2017-06-03T15:06:05.793;2017-06-03T15:06:05.793;;2166823.0;;2166823.0;19078325.0;2;17;;;
100691;100691;;;"<p>Updated for <code>pandas</code> <code>0.20</code> given that <code>ix</code> is deprecated.  This demonstrates not only how to use <code>loc</code>, <code>iloc</code>, <code>at</code>, <code>iat</code>, <code>set_value</code>, but how to accomplish, mixed positional/label based indexing.</p>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""noreferrer""><strong><code>loc</code></strong></a> - <strong>label based</strong><br>
Allows you to pass 1-D arrays as indexers.  Arrays can be either slices (subsets) of the index or column, or they can be boolean arrays which are equal in length to the index or columns.  </p>

<p><strong>Special Note:</strong> when a scalar indexer is passed, <code>loc</code> can assign a new index or column value that didn't exist before.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.loc[df.index[2], 'ColName'] = 3
</code></pre>

<hr>

<pre><code>df.loc[df.index[1:3], 'ColName'] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html"" rel=""noreferrer""><strong><code>iloc</code></strong></a> - <strong>position based</strong><br>
Similar to <code>loc</code> except with positions rather that index values.  However, you <strong>cannot</strong> assign new columns or indices.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
df.iloc[2, df.columns.get_loc('ColName')] = 3
</code></pre>

<hr>

<pre><code>df.iloc[2, 4] = 3
</code></pre>

<hr>

<pre><code>df.iloc[:3, 2:4] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.at.html"" rel=""noreferrer""><strong><code>at</code></strong></a> - <strong>label based</strong><br>
Works very similar to <code>loc</code> for scalar indexers.  <strong>Cannot</strong> operate on array indexers.  <strong>Can!</strong> assign new indices and columns.  </p>

<p><strong>Advantage</strong> over <code>loc</code> is that this is faster.<br>
<strong>Disadvantage</strong> is that you can't use arrays for indexers.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.at[df.index[2], 'ColName'] = 3
</code></pre>

<hr>

<pre><code>df.at['C', 'ColName'] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iat.html"" rel=""noreferrer""><strong><code>iat</code></strong></a> - <strong>position based</strong><br>
Works similarly to <code>iloc</code>.  <strong>Cannot</strong> work in array indexers.  <strong>Cannot!</strong> assign new indices and columns.</p>

<p><strong>Advantage</strong> over <code>iloc</code> is that this is faster.<br>
<strong>Disadvantage</strong> is that you can't use arrays for indexers.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
IBM.iat[2, IBM.columns.get_loc('PNL')] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""noreferrer""><strong><code>set_value</code></strong></a> - <strong>label based</strong><br>
Works very similar to <code>loc</code> for scalar indexers.  <strong>Cannot</strong> operate on array indexers.  <strong>Can!</strong> assign new indices and columns</p>

<p><strong>Advantage</strong> Super fast, because there is very little overhead!<br>
<strong>Disadvantage</strong> There is very little overhead because <code>pandas</code> is not doing a bunch of safety checks.  <strong><em>Use at your own risk</em></strong>.  Also, this is not intended for public use.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.set_value(df.index[2], 'ColName', 3)
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""noreferrer""><strong><code>set_value</code> with <code>takable=True</code></strong></a> - <strong>position based</strong><br>
Works similarly to <code>iloc</code>.  <strong>Cannot</strong> work in array indexers.  <strong>Cannot!</strong> assign new indices and columns.</p>

<p><strong>Advantage</strong> Super fast, because there is very little overhead!<br>
<strong>Disadvantage</strong> There is very little overhead because <code>pandas</code> is not doing a bunch of safety checks.  <strong><em>Use at your own risk</em></strong>.  Also, this is not intended for public use.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
df.set_value(2, df.columns.get_loc('ColName'), 3, takable=True)
</code></pre>
";;2;;2017-05-14T21:04:04.397;;43968774;2017-05-14T21:16:43.390;2017-05-14T21:16:43.390;;2336654.0;;2336654.0;28757389.0;2;14;;;
102082;102082;;;"<p><code>sort</code> was deprecated for DataFrames in favor of needing to either use <code>sort_values</code> or <code>sort_index</code>.  They're described <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""noreferrer"">here</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html"" rel=""noreferrer"">here</a>, respectively.</p>
";;1;;2017-05-23T00:17:35.160;;44123892;2017-05-23T00:17:35.160;;;;;7954504.0;44123874.0;2;26;;;
102808;102808;;;"<p>A simple way to do it is use a <code>dict</code> to hold previous visits:</p>

<pre><code>&gt;&gt;&gt; d = {}
&gt;&gt;&gt; [d.setdefault(tup, i) for i, tup in enumerate(tups)]
[0, 1, 2, 3, 4, 1, 2]
</code></pre>

<p>If you need to keep the numbers sequential then a slight change:</p>

<pre><code>&gt;&gt;&gt; from itertools import count
&gt;&gt;&gt; c = count()
&gt;&gt;&gt; [d[tup] if tup in d else d.setdefault(tup, next(c)) for tup in tups]
[0, 1, 2, 3, 4, 1, 2, 5]
</code></pre>

<p>Or alternatively written:</p>

<pre><code>&gt;&gt;&gt; [d.get(tup) or d.setdefault(tup, next(c)) for tup in tups]
[0, 1, 2, 3, 4, 1, 2, 5]
</code></pre>
";;2;;2017-05-26T18:29:43.517;;44208076;2017-05-27T14:45:07.937;2017-05-27T14:45:07.937;;2750492.0;;2750492.0;44207926.0;2;7;;;
102812;102812;;;"<p>Initialize your list of tuples as a Series, then call <code>factorize</code>:</p>

<pre><code>pd.Series(tups).factorize()[0]

[0 1 2 3 4 1 2]
</code></pre>
";;1;;2017-05-26T18:40:31.313;;44208229;2017-05-26T18:40:31.313;;;;;3339965.0;44207926.0;2;6;;;
103700;103700;;;"<p><code>.ix</code> indexer works okay for pandas version prior to 0.20.0, but since pandas 0.20.0, the <code>.ix</code> indexer is <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecated</a>, so you should avoid using it. Instead, you can use <code>.loc</code> or <code>iloc</code> indexers. You can solve this problem by:</p>

<pre><code>mask = df.my_channel &gt; 20000
column_name = 'my_channel'
df.loc[mask, column_name] = 0
</code></pre>

<p><code>mask</code> helps you to select the rows in which <code>df.my_channel &gt; 20000</code> is <code>True</code>, while <code>df.loc[mask, column_name] = 0</code> sets the value 0 to the selected rows where <code>mask</code>holds in the column which name is <code>column_name</code>.</p>

<p><strong>Update:</strong>
In this case, you should use <code>loc</code> because if you use <code>iloc</code>, you will get a <code>NotImplementedError</code> telling you that <em>iLocation based boolean indexing on an integer type is not available</em>.</p>
";;3;;2017-06-01T15:18:48.507;;44311454;2017-07-14T16:47:41.887;2017-07-14T16:47:41.887;;3705840.0;;3705840.0;21608228.0;2;19;;;
107342;107342;;;"<p>I advice you to put your dataframes into single csv file by concatenation. Then to read your csv file.</p>

<p>Execute that:</p>

<pre><code># write df1 content in file.csv
df1.to_csv('file.csv', index=False)
# append df2 content to file.csv
df2.to_csv('file.csv', mode='a', columns=False, index=False)
# append df3 content to file.csv
df3.to_csv('file.csv', mode='a', columns=False, index=False)

# free memory
del df1, df2, df3

# read all df1, df2, df3 contents
df = pd.read_csv('file.csv')
</code></pre>

<p>If this solution isn't enougth performante, to concat larger files than usually. Do:</p>

<pre><code>df1.to_csv('file.csv', index=False)
df2.to_csv('file1.csv', index=False)
df3.to_csv('file2.csv', index=False)

del df1, df2, df3
</code></pre>

<p>Then run bash command:</p>

<pre><code>cat file1.csv &gt;&gt; file.csv
cat file2.csv &gt;&gt; file.csv
cat file3.csv &gt;&gt; file.csv
</code></pre>

<p>Or concat csv files in python :</p>

<pre><code>def concat(file1, file2):
    with open(file2, 'r') as filename2:
        data = file2.read()
    with open(file1, 'a') as filename1:
        file.write(data)

concat('file.csv', 'file1.csv')
concat('file.csv', 'file2.csv')
concat('file.csv', 'file3.csv')
</code></pre>

<p>After read:</p>

<pre><code>df = pd.read_csv('file.csv')
</code></pre>
";;2;;2017-06-23T15:07:29.723;;44724677;2017-07-02T00:48:06.750;2017-07-02T00:48:06.750;;8071126.0;;8071126.0;44715393.0;2;9;;;
107495;107495;;;"<h1>2017 Answer - pandas 0.20: .ix is deprecated. Use .loc</h1>

<p>See the <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecation in the docs</a></p>

<p><code>.loc</code> uses label based indexing to select both rows and columns. The labels being the values of the index or the columns. Slicing with <code>.loc</code> includes the last element. </p>

<blockquote>
  <p>Let's assume we have a DataFrame with the following columns:<br>
  <code>foo</code>, <code>bar</code>, <code>quz</code>, <code>ant</code>, <code>cat</code>, <code>sat</code>, <code>dat</code>.</p>
</blockquote>

<pre><code># selects all rows and all columns beginning at 'foo' up to and including 'ant'
df.loc[:, 'foo':'sat']
# foo bar quz ant cat sat
</code></pre>

<p><code>.loc</code> accepts the same slice notation that Python lists do for both row and columns. Slice notation being <code>start:stop:step</code></p>

<pre><code># slice from 'foo' to 'cat' by every 2nd column
df.loc[:, 'foo':'cat':2]
# foo quz cat

# slice from the beginning to 'bar'
df.loc[:, :'bar']
# foo bar

# slice from 'quz' to the end by 3
df.loc[:, 'quz'::3]
# quz sat

# attempt from 'sat' to 'bar'
df.loc[:, 'sat':'bar']
# no columns returned

# slice from 'sat' to 'bar'
df.loc[:, 'sat':'bar':-1]
sat cat ant quz bar

# slice notation is syntatic sugar for the slice function
# slice from 'quz' to the end by 2 with slice function
df.loc[:, slice('quz',None, 2)]
# quz cat dat

# select specific columns with a list
# select columns foo, bar and dat
df.loc[:, ['foo','bar','dat']]
# foo bar dat
</code></pre>

<p>You can slice by rows and columns. For instance if you have 5 rows with labels <code>v</code>, <code>w</code>, <code>x</code>, <code>y</code>, <code>z</code></p>

<pre><code># slice from 'w' to 'y' and 'foo' to 'ant' by 3
df.loc['w':'y', 'foo':'ant':3]
#    foo ant
# w
# x
# y
</code></pre>
";;0;;2017-06-24T12:38:58.427;;44736467;2017-07-01T13:45:12.327;2017-07-01T13:45:12.327;;3707607.0;;3707607.0;10665889.0;2;18;;;
108143;108143;;;"<p>Similar to what @glegoux suggests, also <code>pd.DataFrame.to_csv</code> can write in append mode, so you can do something like:</p>

<pre><code>df1.to_csv(filename)
df2.to_csv(filename, mode='a', columns=False)
df3.to_csv(filename, mode='a', columns=False)

del df1, df2, df3
df_concat = pd.read_csv(filename)
</code></pre>
";;0;;2017-06-28T10:37:06.283;;44800142;2017-06-28T10:37:06.283;;;;;5712507.0;44715393.0;2;7;;;
109146;109146;;;"<p>Starting from pandas 0.20 <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">ix is deprecated</a>. The right way to perform this task is using <code>loc</code></p>

<p>here is a working example </p>

<pre><code>&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; df = pd.DataFrame({""A"":[0,1,0], ""B"":[2,0,5]}, columns=list('AB'))
&gt;&gt;&gt; df.loc[df.A == 0, 'B'] = np.nan
&gt;&gt;&gt; df
   A   B
0  0 NaN
1  1   0
2  0 NaN
&gt;&gt;&gt; 
</code></pre>

<p>For more information check the advanced indexing documentation <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"" rel=""noreferrer"">here</a>. </p>
";;0;;2017-07-04T20:27:54.927;;44913631;2017-07-04T20:27:54.927;;;;;1762211.0;12307099.0;2;11;;;
109494;109494;;;"<p>Use the <code>chunksize</code> parameter to read one chunk at the time and save the files to disk. This will split the original file in equal parts by 100000 rows each:</p>

<pre><code>file = ""./data.csv""
chunks = pd.read_csv(file, sep=""/"", header=0, dtype=str, chunksize = 100000)

for it, chunk in enumerate(chunks):
    chunk.to_csv('chunk_{}.csv'.format(it), sep=""/"") 
</code></pre>

<p>If you know the number of rows of the original file you can calculate the exact <code>chunksize</code> to split the file in 8 equal parts (<code>nrows/8</code>). </p>
";;2;;2017-07-06T12:04:41.633;;44948390;2017-07-06T12:10:14.043;2017-07-06T12:10:14.043;;5497294.0;;5497294.0;44946141.0;2;6;;;
109971;109971;;;"<p>Here's a single-liner</p>

<pre><code>In [22]: from itertools import combinations

In [23]: [x for x in combinations(df.columns, 2) if (df[x[0]] == df[x[-1]]).all()]
Out[23]: [('A', 'C'), ('B', 'D')]
</code></pre>

<p>Alternatively, using NumPy broadcasting. Better, look at Divakar's <a href=""https://stackoverflow.com/a/44999009/2137255"">solution</a></p>

<pre><code>In [124]: cols = df.columns

In [125]: dftv = df.T.values

In [126]: cross = pd.DataFrame((dftv == dftv[:, None]).all(-1), cols, cols)

In [127]: cross
Out[127]:
       A      B      C      D      E      F
A   True  False   True  False  False  False
B  False   True  False   True  False  False
C   True  False   True  False  False  False
D  False   True  False   True  False  False
E  False  False  False  False   True  False
F  False  False  False  False  False   True

# Only take values from lower triangle
In [128]: s = cross.where(np.tri(*cross.shape, k=-1)).unstack()

In [129]: s[s == 1].index.tolist()
Out[129]: [('A', 'C'), ('B', 'D')]
</code></pre>
";;8;;2017-07-09T16:07:43.577;;44998387;2017-07-09T18:25:31.353;2017-07-09T18:25:31.353;;2137255.0;;2137255.0;44998223.0;2;6;;;
109985;109985;;;"<p>Here's one NumPy approach -</p>

<pre><code>def group_duplicate_cols(df):
    a = df.values
    sidx = np.lexsort(a)
    b = a[:,sidx]

    m = np.concatenate(([False], (b[:,1:] == b[:,:-1]).all(0), [False] ))
    idx = np.flatnonzero(m[1:] != m[:-1])
    C = df.columns[sidx].tolist()
    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]
</code></pre>

<p>Sample runs -</p>

<pre><code>In [100]: df
Out[100]: 
    A  B  C  D  E  F
a1  1  2  1  2  3  1
a2  2  4  2  4  4  1
a3  3  2  3  2  2  1
a4  4  1  4  1  1  1
a5  5  9  5  9  2  1

In [101]: group_duplicate_cols(df)
Out[101]: [['A', 'C'], ['B', 'D']]

# Let's add one more duplicate into group containing 'A'
In [102]: df.F = df.A

In [103]: group_duplicate_cols(df)
Out[103]: [['A', 'C', 'F'], ['B', 'D']]
</code></pre>

<p>Converting to do the same, but for rows(index), we just need to switch the operations along the other axis, like so -</p>

<pre><code>def group_duplicate_rows(df):
    a = df.values
    sidx = np.lexsort(a.T)
    b = a[sidx]

    m = np.concatenate(([False], (b[1:] == b[:-1]).all(1), [False] ))
    idx = np.flatnonzero(m[1:] != m[:-1])
    C = df.index[sidx].tolist()
    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]
</code></pre>

<p>Sample run -</p>

<pre><code>In [260]: df2
Out[260]: 
   a1  a2  a3  a4  a5
A   3   5   3   4   5
B   1   1   1   1   1
C   3   5   3   4   5
D   2   9   2   1   9
E   2   2   2   1   2
F   1   1   1   1   1

In [261]: group_duplicate_rows(df2)
Out[261]: [['B', 'F'], ['A', 'C']]
</code></pre>

<hr>

<h2>Benchmarking</h2>

<p>Approaches -</p>

<pre><code># @John Galt's soln-1
from itertools import combinations
def combinations_app(df):
    return[x for x in combinations(df.columns, 2) if (df[x[0]] == df[x[-1]]).all()]

# @Abdou's soln
def pandas_groupby_app(df):
    return [tuple(d.index) for _,d in df.T.groupby(list(df.T.columns)) if len(d) &gt; 1]                        

# @COLDSPEED's soln
def triu_app(df):
    c = df.columns.tolist()
    i, j = np.triu_indices(len(c), 1)
    x = [(c[_i], c[_j]) for _i, _j in zip(i, j) if (df[c[_i]] == df[c[_j]]).all()]
    return x

# @cmaher's soln
def lambda_set_app(df):
    return list(filter(lambda x: len(x) &gt; 1, list(set([tuple([x for x in df.columns if all(df[x] == df[y])]) for y in df.columns]))))
</code></pre>

<p>Note : <code>@John Galt's soln-2</code> wasn't included because the inputs being of size <code>(8000,500)</code> would blow up with the proposed <code>broadcasting</code> for that one.</p>

<p>Timings -</p>

<pre><code>In [179]: # Setup inputs with sizes as mentioned in the question
     ...: df = pd.DataFrame(np.random.randint(0,10,(8000,500)))
     ...: df.columns = ['C'+str(i) for i in range(df.shape[1])]
     ...: idx0 = np.random.choice(df.shape[1], df.shape[1]//2,replace=0)
     ...: idx1 = np.random.choice(df.shape[1], df.shape[1]//2,replace=0)
     ...: df.iloc[:,idx0] = df.iloc[:,idx1].values
     ...: 

# @John Galt's soln-1
In [180]: %timeit combinations_app(df)
1 loops, best of 3: 24.6 s per loop

# @Abdou's soln
In [181]: %timeit pandas_groupby_app(df)
1 loops, best of 3: 3.81 s per loop

# @COLDSPEED's soln
In [182]: %timeit triu_app(df)
1 loops, best of 3: 25.5 s per loop

# @cmaher's soln
In [183]: %timeit lambda_set_app(df)
1 loops, best of 3: 27.1 s per loop

# Proposed in this post
In [184]: %timeit group_duplicate_cols(df)
10 loops, best of 3: 188 ms per loop
</code></pre>

<hr>

<p><strong>Super boost with NumPy's view functionality</strong></p>

<p>Leveraging NumPy's view functionality that lets us view each group of elements as one dtype, we could gain further noticeable performance boost, like so -</p>

<pre><code>def view1D(a): # a is array
    a = np.ascontiguousarray(a)
    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))
    return a.view(void_dt).ravel()

def group_duplicate_cols_v2(df):
    a = df.values
    sidx = view1D(a.T).argsort()
    b = a[:,sidx]

    m = np.concatenate(([False], (b[:,1:] == b[:,:-1]).all(0), [False] ))
    idx = np.flatnonzero(m[1:] != m[:-1])
    C = df.columns[sidx].tolist()
    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]
</code></pre>

<p>Timings -</p>

<pre><code>In [322]: %timeit group_duplicate_cols(df)
10 loops, best of 3: 185 ms per loop

In [323]: %timeit group_duplicate_cols_v2(df)
10 loops, best of 3: 69.3 ms per loop
</code></pre>

<p>Just crazy speedups!</p>
";;0;;2017-07-09T17:15:30.483;;44999009;2017-07-09T18:44:46.033;2017-07-09T18:44:46.033;;3293881.0;;3293881.0;44998223.0;2;9;;;
110080;110080;;;"<pre><code>import numpy as np
from multiprocessing import Pool

def processor(df):

    # Some work

    df.sort_values('id', inplace=True)
    return df

size = 8
df_split = np.array_split(df, size)

cores = 8
pool = Pool(cores)
for n, frame in enumerate(pool.imap(processor, df_split), start=1):
    frame.to_csv('{}'.format(n))
pool.close()
pool.join()
</code></pre>
";;8;;2017-07-10T13:46:57.610;;45013739;2017-07-10T13:46:57.610;;;;;6453238.0;44946141.0;2;8;;;
111787;111787;;;"<p>You can use <code>apply</code> with <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.nsmallest.html"" rel=""noreferrer""><code>nsmallest</code></a>:</p>

<pre><code>n = 3
df.apply(lambda x: pd.Series(x.nsmallest(n).index))

#   X   Y   Z
#0  A   J   J
#1  B   I   I
#2  C   H   H
</code></pre>
";;0;;2017-07-19T14:18:47.377;;45193340;2017-07-19T14:18:47.377;;;;;4983450.0;45193131.0;2;9;;;
111791;111791;;;"<p>Faster numpy solution with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html"" rel=""noreferrer""><code>numpy.argsort</code></a>:</p>

<pre><code>a = np.argsort(-df.values, axis=0)[-3:]
print (a)
[[2 7 7]
 [1 8 8]
 [0 9 9]]

b = pd.DataFrame(df.index[a][::-1], columns=df.columns)
print (b)
   X  Y  Z
0  A  J  J
1  B  I  I
2  C  H  H
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>In [300]: %timeit (df.apply(lambda x: pd.Series(x.nsmallest(3).index)))
100 loops, best of 3: 2.1 ms per loop

In [301]: %timeit (pd.DataFrame(df.index[np.argsort(-df.values, axis=0)[-3:]][::-1], columns=df.columns))
10000 loops, best of 3: 144 s per loop
</code></pre>
";;0;;2017-07-19T14:28:53.337;;45193636;2017-07-19T14:28:53.337;;;;;2901002.0;45193131.0;2;8;;;
113174;113174;;;"<p>Using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""noreferrer""><code>pd.concat</code></a> to combine the DataFrames, and <a href=""http://toolz.readthedocs.io/en/latest/api.html#toolz.itertoolz.interleave"" rel=""noreferrer""><code>toolz.interleave</code></a> reorder the columns:</p>

<pre><code>from toolz import interleave

pd.concat([d1, d2], axis=1)[list(interleave([d1, d2]))]
</code></pre>

<p>The resulting output is as expected:</p>

<pre><code>   0  3  1  4  2
a  1  0  1  0  1
b  1  0  1  0  1
c  1  0  1  0  1
</code></pre>
";;3;;2017-07-26T16:55:40.133;;45333133;2017-07-26T16:55:40.133;;;;;3339965.0;45332960.0;2;12;;;
113176;113176;;;"<p>Here's one NumPy approach -</p>

<pre><code>def numpy_interweave(d1, d2):
    c1 = list(d1.columns)
    c2 = list(d2.columns)
    N = (len(c1)+len(c2))
    cols = [None]*N
    cols[::2] = c1
    cols[1::2] = c2

    out_dtype = np.result_type(d1.values.dtype, d2.values.dtype)
    out = np.empty((d1.shape[0],N),dtype=out_dtype)
    out[:,::2] = d1.values
    out[:,1::2] = d2.values

    df_out = pd.DataFrame(out, columns=cols, index=d1.index)
    return df_out
</code></pre>

<p>Sample run -</p>

<pre><code>In [346]: d1
Out[346]: 
   x  y  z
a  6  7  4
b  3  5  6
c  4  6  2

In [347]: d2
Out[347]: 
   p  q
a  4  2
b  7  7
c  7  2

In [348]: numpy_interweave(d1, d2)
Out[348]: 
   x  p  y  q  z
a  6  4  7  2  4
b  3  7  5  7  6
c  4  7  6  2  2
</code></pre>
";;0;;2017-07-26T17:03:31.647;;45333267;2017-07-26T17:08:43.987;2017-07-26T17:08:43.987;;3293881.0;;3293881.0;45332960.0;2;6;;;
113463;113463;;;"<p>You should use <em>drop()</em>. Suppose your dataframe name is <em>df</em>. </p>

<pre><code>#for dropping single column 
df = df.drop('your_column', axis=1)

#for dropping multiple columns
df = df.drop(['col_1','col_2','col_3'], axis=1)
</code></pre>
";;0;;2017-07-27T17:48:15.380;;45357725;2017-07-27T17:48:15.380;;;;;4294015.0;13411544.0;2;10;;;
114806;114806;;;"<p>Convert the list to a boolean array and then use boolean indexing:</p>

<pre><code>df = pd.DataFrame(np.random.randint(10, size=(10, 3)))

df[np.array(lst).astype(bool)]
Out: 
   0  1  2
1  8  6  3
4  2  7  3
5  7  2  3
9  1  3  4
</code></pre>
";;0;;2017-08-03T21:21:10.173;;45494714;2017-08-03T21:21:10.173;;;;;2285236.0;45494649.0;2;13;;;
114807;114807;;;"<p>You can use masking here:</p>

<pre><code>df[np.array([0,1,0,0,1,1,0,0,0,1],dtype=bool)]
</code></pre>

<p>So we construct a boolean array with true and false. Every place where the array is True is a row we select.</p>

<p>Mind that we do <em>not</em> filter inplace. In order to retrieve the result, you have to assign the result to an (optionally different) variable:</p>

<pre><code>df2 = df[np.array([0,1,0,0,1,1,0,0,0,1],dtype=bool)]
</code></pre>
";;0;;2017-08-03T21:21:48.803;;45494718;2017-08-03T21:21:48.803;;;;;67579.0;45494649.0;2;15;;;
114808;114808;;;"<p><strong>Setup</strong><br>
Borrowed @ayhan's setup  </p>

<pre><code>df = pd.DataFrame(np.random.randint(10, size=(10, 3)))
</code></pre>

<p><strong>Without <code>numpy</code></strong><br>
not the fastest, but it holds its own and is definitely the shortest.</p>

<pre><code>df[list(map(bool, lst))]

   0  1  2
1  3  5  6
4  6  3  2
5  5  7  6
9  0  0  1
</code></pre>

<hr>

<p><strong>Timing</strong>  </p>

<pre><code>results.div(results.min(1), 0).round(2).pipe(lambda d: d.assign(Best=d.idxmin(1)))

         ayh   wvo   pir   mxu   wen Best
N                                        
1       1.53  1.00  1.02  4.95  2.61  wvo
3       1.06  1.00  1.04  5.46  2.84  wvo
10      1.00  1.00  1.00  4.30  2.73  ayh
30      1.00  1.05  1.24  4.06  3.76  ayh
100     1.16  1.00  1.19  3.90  3.53  wvo
300     1.29  1.00  1.32  2.50  2.38  wvo
1000    1.54  1.00  2.19  2.24  3.85  wvo
3000    1.39  1.00  2.17  1.81  4.55  wvo
10000   1.22  1.00  2.21  1.35  4.36  wvo
30000   1.19  1.00  2.26  1.39  5.36  wvo
100000  1.19  1.00  2.19  1.31  4.82  wvo
</code></pre>

<hr>

<pre><code>fig, (a1, a2) = plt.subplots(2, 1, figsize=(6, 6))
results.plot(loglog=True, lw=3, ax=a1)
results.div(results.min(1), 0).round(2).plot.bar(logy=True, ax=a2)
fig.tight_layout()
</code></pre>

<p><a href=""https://i.stack.imgur.com/jScmh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jScmh.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong>Testing Code</strong>  </p>

<pre><code>ayh = lambda d, l: d[np.array(l).astype(bool)]
wvo = lambda d, l: d[np.array(l, dtype=bool)]
pir = lambda d, l: d[list(map(bool, l))]
wen = lambda d, l: d.loc[[i for i, x in enumerate(l) if x == 1], :]

def mxu(d, l):
    a = np.array(l)
    return d.query('@a != 0')

results = pd.DataFrame(
    index=pd.Index([1, 3, 10, 30, 100, 300,
                    1000, 3000, 10000, 30000, 100000], name='N'),
    columns='ayh wvo pir mxu wen'.split(),
    dtype=float
)

for i in results.index:
    d = pd.concat([df] * i, ignore_index=True)
    l = lst * i
    for j in results.columns:
        stmt = '{}(d, l)'.format(j)
        setp = 'from __main__ import d, l, {}'.format(j)
        results.set_value(i, j, timeit(stmt, setp, number=10))
</code></pre>
";;3;;2017-08-03T21:30:13.390;;45494808;2017-08-03T21:58:03.823;2017-08-03T21:58:03.823;;2336654.0;;2336654.0;45494649.0;2;11;;;
114809;114809;;;"<p>yet another ""creative"" approach:</p>

<pre><code>In [181]: a = np.array(lst)

In [182]: df.query(""index * @a &gt; 0"")
Out[182]:
   0  1  2
1  1  5  5
4  0  2  0
5  4  9  9
9  2  2  5
</code></pre>

<p>or <a href=""https://stackoverflow.com/questions/45494649/pandas-dataframe-return-subset-based-on-a-list-of-boolean-values/45494910?noredirect=1#comment77951484_45494910"">much better variant from @ayhan</a>:</p>

<pre><code>In [183]: df.query(""@a != 0"")
Out[183]:
   0  1  2
1  1  5  5
4  0  2  0
5  4  9  9
9  2  2  5
</code></pre>

<p>PS i've also borrowed @Ayhan's setup</p>
";;2;;2017-08-03T21:38:01.287;;45494910;2017-08-03T21:41:57.227;2017-08-03T21:41:57.227;;5741205.0;;5741205.0;45494649.0;2;7;;;
115560;115560;;;"<p>In <code>pandas 0.20.2</code> you can do:</p>

<pre><code>from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype

is_string_dtype(df['A'])
&gt;&gt;&gt;&gt; True

is_numeric_dtype(df['B'])
&gt;&gt;&gt;&gt; True
</code></pre>

<p>So your code becomes:</p>

<pre><code>for y in agg.columns:
    if (is_string_dtype(agg[y])):
        treat_str(agg[y])
    elif (is_numeric_dtype(agg[y])):
        treat_numeric(agg[y])
</code></pre>
";;0;;2017-08-08T12:20:18.850;;45568211;2017-08-08T12:20:18.850;;;;;1797204.0;22697773.0;2;6;;;
117400;117400;;;"<p><strong>UPDATE:</strong> now it parses the clipboard - i.e. no need to save it beforehand</p>

<pre><code>def read_clipboard_mi(index_names_row=None, **kwargs):
    encoding = kwargs.pop('encoding', 'utf-8')

    # only utf-8 is valid for passed value because that's what clipboard
    # supports
    if encoding is not None and encoding.lower().replace('-', '') != 'utf8':
        raise NotImplementedError(
            'reading from clipboard only supports utf-8 encoding')

    from pandas import compat, read_fwf
    from pandas.io.clipboard import clipboard_get
    from pandas.io.common import StringIO
    data = clipboard_get()

    # try to decode (if needed on PY3)
    # Strange. linux py33 doesn't complain, win py33 does
    if compat.PY3:
        try:
            text = compat.bytes_to_str(
                text, encoding=(kwargs.get('encoding') or
                                get_option('display.encoding'))
            )
        except:
            pass

    index_names = None
    if index_names_row:
        if isinstance(index_names_row, int):
            index_names = data.splitlines()[index_names_row].split()
            skiprows = [index_names_row]
            kwargs.update({'skiprows': skiprows})
        else:
            raise Exception('[index_names_row] must be of [int] data type')

    df = read_fwf(StringIO(data), **kwargs)
    unnamed_cols = df.columns[df.columns.str.contains(r'Unnamed:')].tolist()

    if index_names:
        idx_cols = df.columns[range(len(index_names))].tolist()
    elif unnamed_cols:
        idx_cols = df.columns[range(len(unnamed_cols))].tolist()
        index_names = [None] * len(idx_cols)

    df[idx_cols] = df[idx_cols].ffill()
    df = df.set_index(idx_cols).rename_axis(index_names)

    return df
</code></pre>

<p>testing multi-index DF without index names:</p>

<pre><code>In [231]: read_clipboard_mi()
Out[231]:
          C
1.1 111  20
    222  31
3.3 222  24
    333  65
5.5 333  22
6.6 777  74
</code></pre>

<p>testing multi-index DF with index names:</p>

<pre><code>In [232]: read_clipboard_mi(index_names_row=1)
Out[232]:
          C
A   B
1.1 111  20
    222  31
3.3 222  24
    333  65
5.5 333  22
6.6 777  74
</code></pre>

<p><strong>NOTE:</strong> </p>

<ol>
<li>it's not well tested</li>
<li>it does NOT support multi-level columns</li>
<li>see point 1 ;-)</li>
</ol>

<p><strong>NOTE2:</strong> please feel free to use this code or to create <a href=""https://github.com/pandas-dev/pandas/pulls"" rel=""noreferrer"">a pull request on Pandas github</a></p>
";;11;;2017-08-17T17:54:06.040;;45741989;2017-08-20T19:53:03.803;2017-08-20T19:53:03.803;;5741205.0;;5741205.0;45740537.0;2;13;;;
118738;118738;;;"<p>You could use the following code:</p>

<pre><code>cum_r = (1 + r).cumprod()
result = cum_r * v0
for date in r.index[r.index.is_quarter_end]:
     result[date:] -= cum_r[date:] * (dist / cum_r.loc[date])
</code></pre>

<p>You would make:</p>

<ul>
<li>1 cumulative product for all monthly returns. </li>
<li>1 vector multiplication with scalar<code>v0</code></li>
<li><code>n</code> vector multiplication with scalar <code>dist / cum_r.loc[date]</code></li>
<li><code>n</code> vector subtractions</li>
</ul>

<p>where <code>n</code> is the number of quarter ends.</p>

<p>Based on this code we can optimize further:</p>

<pre><code>cum_r = (1 + r).cumprod()
t = (r.index.is_quarter_end / cum_r).cumsum()
result = cum_r * (v0 - dist * t)
</code></pre>

<p>which is</p>

<ul>
<li>1 cumulative product <code>(1 + r).cumprod()</code></li>
<li>1 division between two series <code>r.index.is_quarter_end / cum_r</code></li>
<li>1 cumulative sum of the above division</li>
<li>1 multiplication of the above sum with scalar <code>dist</code></li>
<li>1 subtraction of scalar <code>v0</code> with <code>dist * t</code></li>
<li>1 dotwise multiplication of <code>cum_r</code> with <code>v0 - dist * t</code></li>
</ul>
";;4;;2017-08-24T22:27:09.873;;45871837;2017-08-24T22:36:26.757;2017-08-24T22:36:26.757;;4526187.0;;4526187.0;45820242.0;2;7;;;
